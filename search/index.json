[{"categories":["notes"],"contents":" pytorch local development environments without tears, root access or spack dev-build\n Background Build systems are a bit of a nightmare. I spend most of my time SSH\u0026rsquo;ed into more powerful CPU machines, and sometimes on machines with more exotic compute devices. micromamba is typically my poison of choice where nix is not an option.\nHowever, micromamba doesn\u0026rsquo;t allow a whole lot in the way of setting up environments which do not use packages already on conda-forge. Spack fills a nice niche for such situations1. Additionally, it can be coerced into use for local development and the same cannot easily be said of conda based workflows.\nHumble Origins We\u0026rsquo;ll start from scratch, grabbing spack and bootstrapping clingo for its dependency resolution as per the documentation.\n1git clone -c feature.manyFiles=true https://github.com/spack/spack.git 2. spack/share/spack/setup-env.sh # Assumes zsh || bash 3spack spec zlib # Or anything really 4spack --bootstrap find # Should have clingo Environments and Purity Unlike nix, spack does not offer strong guarantees of purity. There are primarily two approaches to using environments in spack, somewhat analogously to the conda environment logic.\n Named Environments These are essentially equivalent to conda environments, and can be activated and deactivated at will. However, the folder structure in this case, along with the dependencies are localized within $SPACK_HOME/var/spack/environments/$ENV_NAME Anonymous Environments These can be setup inside any directory, and can be activated but not deactivated (despacktivate will not work). These are useful for development environments.  We will use both kinds of environments. Additionally, spack supports multiple variants which can be queried by spack info $PKG_NAME. These are used to support various build configurations while providing a unified interface build through spack install.\nBasic Anonymous Environments For starters we will need to setup a development environment.\n1mkdir spackPyTorch 2cd spackPyTorch 3spack env create -d . To ensure that the dependencies are resolved consistently, the concretization option to needs to be set to together. We will start by adding some packages.\n1spack -e . add py-ipython 2spack -e . config edit # Make changes For ease of manual customization, it is best to lightly edit the spack.yaml file to have each dependency on its own line. At this point a bare-minimum near-empty environment is ready.\n1spack:2specs:3- py-ipython4view:true5concretization:togetherTo ensure dependencies are resolved simultaneously, concretize is set to together.\n1spack -e . concretize # doesn\u0026#39;t install 2spack -e . install 3spack env activate . # or spacktivate . 4# Can also use the fully qualified path instead of . Recall that changes are propagated via:\n1spack -e . concretize --force 2spack -e . install This establishes baseline environments, but precludes setting up development workflows. Most packages can be included as described above, with the exception of compiler toolchains.\nCompiler Setups Setting up compiler toolchains like a fortran compiler (perhaps for openblas) can take a bit more effort. Although this discussion will focus on obtaining a fortran compiler it is equally applicable to updating a version. For spack, unlike many system package managers, gcc will install the C, C++ and Fortran toolchains. Thus:\n1# outside the spack environment 2spack install gcc@11.2.0 # will take a while Now we need to let spack register this compiler.\n1spack compiler find $(spack location -i gcc@11.2.0) 2spack compilers # should now list GCC 11.2.0 Finally we can edit our spack.yml environment to reflect the new compiler toolchain.\n1spack:2definitions:3- compilers:[gcc@11.2.0]4specs:5- py-ipython6view:true7concretization:togetherOne of the nicer parts of spack being an HPC environment management tool is that there is first-class support for proprietary compiler toolchains like Intel and families of compilers can also be specified with the %intel syntax as well. Much more fine-tuning is also possible including registering system compilers if required.\nPyTorch Local Development One of the caveats of local development with spack is that the base URL needs to be updated from within the local copy of spack. This means editing:\n1vim $SPACK_ROOT/var/spack/repos/builtin/packages/py-torch/package.py # edit The patch is minimally complicated, for a fork and a working branch like npeye it would look like this diff:\n1diff --git a/var/spack/repos/builtin/packages/py-torch/package.py b/var/spack/repos/builtin/packages/py-torch/package.py 2index 8190e01102..c276009d10 100644 3--- a/var/spack/repos/builtin/packages/py-torch/package.py 4+++ b/var/spack/repos/builtin/packages/py-torch/package.py 5@@ -14,7 +14,7 @@ class PyTorch(PythonPackage, CudaPackage): 6 with strong GPU acceleration.\u0026#34;\u0026#34;\u0026#34; 78homepage = \u0026#34;https://pytorch.org/\u0026#34; 9- git = \u0026#34;https://github.com/pytorch/pytorch.git\u0026#34; 10+ git = \u0026#34;https://github.com/HaoZeke/pytorch.git\u0026#34; 11 12maintainers = [\u0026#39;adamjstewart\u0026#39;] 1314@@ -22,6 +22,7 @@ class PyTorch(PythonPackage, CudaPackage): 15 # core libraries to ensure that the package was successfully installed. 16import_modules = [\u0026#39;torch\u0026#39;, \u0026#39;torch.autograd\u0026#39;, \u0026#39;torch.nn\u0026#39;, \u0026#39;torch.utils\u0026#39;] 1718+ version(\u0026#39;npeye\u0026#39;, branch=\u0026#39;npeye\u0026#39;, submodules=True) 19 version(\u0026#39;master\u0026#39;, branch=\u0026#39;master\u0026#39;, submodules=True) 20version(\u0026#39;1.11.0\u0026#39;, tag=\u0026#39;v1.11.0\u0026#39;, submodules=True) 21version(\u0026#39;1.10.2\u0026#39;, tag=\u0026#39;v1.10.2\u0026#39;, submodules=True) 22@@ -348,7 +349,8 @@ def enable_or_disable(variant, keyword=\u0026#39;USE\u0026#39;, var=None, newer=False): 23 elif \u0026#39;~onnx_ml\u0026#39; in self.spec: 24env.set(\u0026#39;ONNX_ML\u0026#39;, \u0026#39;OFF\u0026#39;) 2526- if not self.spec.satisfies(\u0026#39;@master\u0026#39;): 27+ if not (self.spec.satisfies(\u0026#39;@master\u0026#39;) or 28+ self.spec.satisfies(\u0026#39;@npeye\u0026#39;)): 29 env.set(\u0026#39;PYTORCH_BUILD_VERSION\u0026#39;, self.version) 30env.set(\u0026#39;PYTORCH_BUILD_NUMBER\u0026#39;, 0) Applying such a patch is straightforward.\n1cd $SPACK_ROOT 2# Assuming it is named pytorchspack.diff 3git apply pytorchspack.diff This modified environment can now be enabled for use with the appropriate variants (details found with spack info py-torch). However, there is one rather important variant missing for local development, DEBUG. The application of this patch will rectify this until an upstream PR is merged.\n1diff --git a/var/spack/repos/builtin/packages/py-torch/package.py b/var/spack/repos/builtin/packages/py-torch/package.py 2index 8190e01102..d7b68ae4bd 100644 3--- a/var/spack/repos/builtin/packages/py-torch/package.py 4+++ b/var/spack/repos/builtin/packages/py-torch/package.py 5@@ -49,6 +49,7 @@ class PyTorch(PythonPackage, CudaPackage): 6 7# All options are defined in CMakeLists.txt. 8# Some are listed in setup.py, but not all. 9+ variant(\u0026#39;debug\u0026#39;, default=False, description=\u0026#34;Build with debugging support\u0026#34;) 10 variant(\u0026#39;caffe2\u0026#39;, default=True, description=\u0026#39;Build Caffe2\u0026#39;, when=\u0026#39;@1.7:\u0026#39;) 11variant(\u0026#39;test\u0026#39;, default=False, description=\u0026#39;Build C++ test binaries\u0026#39;) 12variant(\u0026#39;cuda\u0026#39;, default=not is_darwin, description=\u0026#39;Use CUDA\u0026#39;) 13@@ -343,6 +344,12 @@ def enable_or_disable(variant, keyword=\u0026#39;USE\u0026#39;, var=None, newer=False): 14 enable_or_disable(\u0026#39;gloo\u0026#39;, newer=True) 15enable_or_disable(\u0026#39;tensorpipe\u0026#39;) 1617+ if \u0026#39;+debug\u0026#39; in self.spec: 18+ env.set(\u0026#39;DEBUG\u0026#39;, 1) 19+ elif \u0026#39;-debug\u0026#39; or \u0026#39;~debug\u0026#39; in self.spec: 20+ env.set(\u0026#39;DEBUG\u0026#39;, \u0026#39;0\u0026#39;) 21+ 22+ 23 if \u0026#39;+onnx_ml\u0026#39; in self.spec: 24env.set(\u0026#39;ONNX_ML\u0026#39;, \u0026#39;ON\u0026#39;) 25elif \u0026#39;~onnx_ml\u0026#39; in self.spec: All together now the variants can be used in conjunction with the development branch. To focus on the pytorch workflow, we will unpin python@3.10 and add ipython instead.\nCPU Normally, for a CPU build we would setup something like the following:\n1DEBUG=1 USE_DISTRIBUTED=0 USE_MKLDNN=0 USE_CUDA=0 BUILD_TEST=0 USE_FBGEMM=0 USE_NNPACK=0 USE_QNNPACK=0 USE_XNNPACK=0 BUILD_CAFFE2=0 USE_KINETO=0 python setup.py develop The rationale behind the build flags can be found in the upstream contributing guide. In an appropriately defined environment. For us, this translates to (with the patch added for debug):\n1spacktivate . 2# CPU only, disable cuda 3# Also disable a bunch of optionals 4spack add py-torch@npeye -cuda -fbgemm -nnpack -mkldnn -test -qnnpack +debug 5spack concretize -f However, we need to also register this package for development (with the branch setup previously), which is accomplished by:\n1spack develop py-torch@npeye -cuda -fbgemm -nnpack -mkldnn -test -qnnpack +debug This generates a sub-directory py-torch with the right branch checked out, along with the dependencies needed for the build. Additionally, the python version is also localized to the spack installation spec.\n1spack install # concretizes and installs If additional dependencies are required for testing or other purposes, they are easily obtained. After making changes spack install will rebuild with the appropriate flags.\nCUDA Setup The good news is that updating the build system to use CUDA is very straightforward. While changing variants, it is occasionally necessary to forcibly clear out the cache.\n1cd py-torch 2rm -rf build 3python setup.py clean 4# More extreme cases 5git submodule deinit -f . 6git clean -xdf 7python setup.py clean 8git submodule update --init --recursive --jobs 0 9python setup.py develop We will require the CUDA version to install the appropriate tool-chain.\n1export NVCC_CUDA_VERSION=$(nvidia-smi -q | awk -F\u0026#39;: \u0026#39; \u0026#39;/CUDA Version/ {print $2}\u0026#39;) 2spack add cuda@$NVCC_CUDA_VERSION 3spack concretize --force 4spack install One caveat of CUDA installations (which cannot be dealt with here) is that spack needs to have read/write access to /tmp/cuda-installer.log because of a ridiculous upstream bug.\nFinally we update the spack.yaml to reflect our new changes:\n1spack:2definitions:3- compilers:[gcc@11.2.0]4# add package specs to the `specs` list5specs:6- py-ipython7- cuda@11.38- py-torch@npeye+cuda+debug~fbgemm~mkldnn~nnpack~qnnpack~test9view:true10concretization:together11develop:12py-torch:13spec:py-torch@npeye+cuda+debug~fbgemm~mkldnn~nnpack~qnnpack~testWhich now works seamlessly with spack install.\nBaseline Environment Finally, to test changes against the main branch upstream, it is useful to define an environment for the same. This can be named since it allows for better usage semantics with deactivate.\n1spack env create pytorchBaseline 2spack -e pytorchBaseline add py-torch@master -cuda -fbgemm -nnpack -mkldnn -test -qnnpack 3spack -e pytorchBaseline add py-ipython 4spack -e pytorchBaseline config edit 5# Add compilers, concretization 6spacktivate pytorchBaseline 7spack concretize --force 8spack install 9# Do tests, compare 10# ... 11# Wrap up and deactivate 12despacktivate pytorchBaseline Conclusions Dependency management is always painful. CUDA management doubly so. Better workflows with spack dev-build are available for some packages, like KOKKOS, but spack dev-build doesn\u0026rsquo;t work yet for pytorch, and also appears to be removed from the present set of tutorials. Personally, I\u0026rsquo;d still prefer nix, but where micromamba falls short in terms of source builds, spack is a good alternative, if one has the resources to rebuild everything needed.\n  It also integrates nicely with typical HPC modular workflows like LMod and has reasonable support for Windows\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/spack-pytorch-dev-workflow/","tags":["ramblings","pytorch"],"title":"Spack and PyTorch Development Workflows"},{"categories":["code"],"contents":"I often need to set up quick virtual environments. Unfortunately, the standard approach to work with this in nix deals with the local nixpkgs mechanism for python dependencies:\n1nix-shell -p \u0026#34;python38.withPackages(ps: [ ps.numpy ps.sh ])\u0026#34; However there is a catch for packages which are not officially present upstream.\n1# Fails! 2nix-shell -p \u0026#34;python38.withPackages(ps: [ ps.numpy ps.sh ps.lieer ])\u0026#34; However, the mach-nix project can indeed be used to work around this, at the cost of a somewhat longer command.\n1# Works 2nix-shell -p \u0026#39;(callPackage (fetchTarball https://github.com/DavHau/mach-nix/tarball/3.0.2) {python=\u0026#34;python38\u0026#34;;}).mkPython{requirements=\u0026#34;numpy\\n lieer\\n ipython\u0026#34;;}\u0026#39; --command ipython mkPythonShell does not generate an environment which can be used here, since that is not a derivation which can be built. This is most useful in the context of systems which use asdf or other PATH shim approaches.\n","permalink":"https://rgoswami.me/snippets/mach-nix-shell-env/","tags":["nix","python","cmdline"],"title":"Mach-Nix and Shell Environments"},{"categories":["notes"],"contents":" Rootless SSH servers with TinySSH for local GitPod containers\n Background act works well enough for most Github Actions workflows, until something fails in the pipeline, or interactive debugging is required. Naturally this is beyond the purview of act itself, but it is easy enough to set up shop with a project\u0026rsquo;s GitPod docker image being used. For the purposes of the post we\u0026rsquo;ll consider numpy.\n1docker run -v $(pwd):/home/gitpod/f2py_skel --name f2py_npdev -it numpy/numpy-dev:latest docker attach can be used in tandem with docker stop and docker start, but these have the disadvantage of only providing one synchronous view of the machine.\nAn SSH server seems like the most natural solution. Rather than cover OpenSSH and its superuser based configuration, we will instead consider the TinySSH project, which works well without superuser access.\nTinySSH We also need to be able to listen for incoming TCP connections and run tinysshd in response to a connection. This is a picture perfect scenario for tcpserver which is part of the uscpi-tcp set of tools.\n1sudo apt install tinysshd uscpi-tcp # for tcpserver 2# On focal it is: 3# sudo apt install tinysshd ucspi-tcp-ipv6 Although this assumes sudo access, compiling both uscpi-tcp and tinysshd from source is a remarkably simple process.\nAs tinyssh does not implement RSA and older cryptography algorithms, it is easiest to make a new key pair on the host machine.\n1# On the host 2ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519_tiny 3ssh-add ~/.ssh/id_ed25519_tiny 4# Copy the public key 5cat ~/.ssh/id_ed25519_tiny.pub | wl-copy # wayland 6# or 7cat ~/.ssh/id_ed25519_tiny.pub | xclip # X11 tinyssh supports only key based authentication, we need to set up the authorized_keys correctly.\n1# On the client 2mkdir $HOME/.ssh 3touch $HOME/.ssh/authorized_keys 4chmod 700 .ssh/ 5echo $COPIED_KEY \u0026gt;\u0026gt; $HOME/.ssh/authorized_keys 6chmod 600 .ssh/authorized_keys Finally we can setup server keys and run the server itself, on a port which does not require superuser privileges.\n1tinysshd-makekey $HOME/tinysshkeydir 2tcpserver -HRDl0 0.0.0.0 2200 /usr/sbin/tinysshd -v $HOME/tinysshkeydir From the host, we need to know the IP to connect to.\n1docker inspect -f \u0026#34;{{ .NetworkSettings.IPAddress }}\u0026#34; $IMG_NAME 2ssh $USER@$IP -p 2200 File Permissions A common issue with docker images is that the user and group permissions on the shared folders do not match the permissions within the container. The most obvious fix is to force the id by passing --user $(id -u):$(id -g) when creating the container. However, a better approach is to change the group and user id from within the container itself.\n1id # On the host, note the uid and gid 2docker start f2py_npdev 3docker attach f2py_npdev 4# In docker container 5sudo groupmod -g $HOSTGID $USER 6sudo usermod -u $HOSTUID $USER 7exit # re-enter When re-entering the container, the correct permissions will be set for the shared folder, but any existing files will need to be fixed up. We can also get the uid and gid of the shared folder directly with stat.\n1# in the container 2export HOSTGID=$(stat -c \u0026#34;%g\u0026#34; $SHARED_VOLUME) 3export HOSTUID=$(stat -c \u0026#34;%u\u0026#34; $SHARED_VOLUME) 4export DOCKERUID=$(stat -c \u0026#34;%u\u0026#34; $HOME) 5export DOCKERGID=$(stat -c \u0026#34;%g\u0026#34; $HOME) For the fix-up, we can simply use the populated variables.\n1sudo find / -xdev -user \u0026#34;$DOCKERUID\u0026#34; -exec chown -h \u0026#34;$HOSTUID\u0026#34; {} \\; 2sudo find / -xdev -group \u0026#34;$DOCKERGID\u0026#34; -exec chgrp -h \u0026#34;$HOSTGID\u0026#34; {} \\; Conclusions Like many modern developers, I spend far too long messing around with continuous integration systems and jumping around different development environments. numpy and other projects have opted into Gitpod as a lower barrier of entry method to aid contributors. Often the docker images backing these do not have systemd or service management set up, and so local environments based off of such images can be a little off putting.\nNothing in this post constitutes any kind of best practice. In an ideal world one would have either physical machines to access or well configured virtual machines. Nevertheless, for edge cases, this works well enough in practice.\n","permalink":"https://rgoswami.me/posts/tinyssh-dockerdev-env/","tags":["tools","workflow"],"title":"TinySSH for Docker Development Environments"},{"categories":["notes"],"contents":" Ruminating on wayland and sway for daily use with ArchLinux\n Background Many years ago, I found time to make an attempt to switch away from the X11 Window system to Wayland. At the time, I ended up switching back to Xorg, but I did want to revisit it. Since I returned from home recently and was gifted a Gen 6 ThinkPad X1 Carbon, I had a perfect opportunity to do so 1.\nPart of this post will be interspersed with generic non-Wayland observations, but it has been a few years since my last setup and things have changed. Rather than regurgitate the fantastic ArchWiki installation guide, a FAQ style short-doc seems more appropriate.\nAs before, none of these configurations should be considered complete/recommended/normative; use the installation guide and follow the Arch Way. These are install notes, not tutorials.\nArchLinux baseline  Networking The days of wifi-menu are gone. The new iwd interface (ArchWiki) is the newest, swankiest kid on the block 2 and works great with NetworkManager Input devices loadkeys for the console, libinput configurations can be handled via the window manager under Wayland Audio pulseaudio works well as always, though pipewire is pretty neat as well Nix Via the multi-user install Time Being as this is a laptop configuration, it is best to configure chrony (described here) as the ntp client and server of choice AUR Helper trizen, though I retain a soft spot for yay Portability Details are presented concisely on the ArchWiki Device details The ArchWiki has an entry, notably including modem setup woes and also throttled details for better power management (I ended up using throttled over thermald) Window and login sway, and greetd  Configurations In general the idea is to run as few services as possible so systemctl --type=service is very helpful.\n1systemctl --type=service 2UNIT LOAD ACTIVE SUB DESCRIPTION 3bolt.service loaded active running Thunderbolt system service 4chronyd.service loaded active running NTP client/server 5dbus.service loaded active running D-Bus System Message Bus 6greetd.service loaded active running Greeter daemon 7iwd.service loaded active running Wireless service 8kmod-static-nodes.service loaded active exited Create List of Static Device Nodes 9lenovo_fix.service loaded active running Stop Intel throttling 10lvm2-monitor.service loaded active exited Monitoring of LVM2 mirrors, snapshots etc. using dmeventd or progress polling 11NetworkManager.service loaded active running Network Manager 12open-fprintd.service loaded active running Open FPrint Daemon 13... rEFInd Stanzas Some things I forgot which I should not have, and which should be taken in more of a RTFM vein.\n boot/refind_linux.conf for kernel parameters used with auto-detected stanzas, e.g. enabling app-armor everywhere via lsm=landlock,lockdown,yama,apparmor,bpf  This is not where boot stanzas are to be defined   boot/EFI/refind/refind.conf for manual stanzas, settings, themes and other things  Also random observations.\n /etc/fstab is completely useless for booting, but should mount boot correctly otherwise updates get messed up cat /proc/cmdline will dump the current kernel parameters root=UUID=BLAH or root=PARTUUID=BLAH is meant to be the arch root, not the boot partition  volume is the boot partition and should be set to root=    Network and Time Networking with iwd is very well described on the ArchWiki, with the exception of its interaction with NetworkManager which essentially involves configuring it to use the IWD backend via /etc/NetworkManager/NetworkManager.conf:\n1[device] 2wifi.backend=iwd Now all that remains is to disable wpa and friends.\n1systemctl stop wpa_supplicant 2systemctl disable wpa_supplicant 3systemctl start iwd 4systemctl start NetworkManager Personally I combine this the NetworkManager dispatcher script for chrony.\nEduroam For the most part, auto configuration works. However, for eduroam I needed to setup /var/lib/iwd/eduroam.8021x with the following:\n1[Security] 2EAP-Method=TTLS 3EAP-Identity=$BLAH@DOMAIN 4EAP-TTLS-Phase2-Method=MSCHAPV2 5EAP-TTLS-Phase2-Identity=$BLAH@DOMAIN 67[Settings] 8Autoconnect=true With this iwctl works well enough:\n1station wlan0 connect eduroam Mobile broadband The Fibocom L850-GL hardware can be setup via the experimental native xmm7360-pci driver. Unfortunately, upstream has not processed a couple of important pull requests. So using my fork:\n1git clone git@github.com:HaoZeke/xmm7360-pci.git 2cd xmm7360-pci 3sudo /usr/bin/pip install --user pyroute2 ConfigArgParse 4make \u0026amp;\u0026amp; make load 5make \u0026amp;\u0026amp; make load # first one doesn\u0026#39;t take 6sudo /usr/bin/python rpc/open_xdatachannel.py --apn net.nova.is # or whatever your apn is To test that everything is working, a new device should have shown up with ip addr.\n16: wwan0: \u0026lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UNKNOWN group default qlen 1000 2link/none It should have some inet addresses as well. Unfortunately, the steps above need to be run every time, so whenever needed:\n1make \u0026amp;\u0026amp; make load 2make \u0026amp;\u0026amp; make load # first one doesn\u0026#39;t take 3sudo /usr/bin/python rpc/open_xdatachannel.py --apn net.nova.is Audio As discussed earlier, no special requirements need to be considered beyond setting up /etc/libao.conf which in itself is only needed for pianobar.\n1default_drive=pulse 2# dev=default 3quiet Actually this also holds true for pipewire and that\u0026rsquo;s pretty much what I decided to go with this time around.\nSnap and AppArmor In general, it seems that snap packages have better support for wayland, like for Telegram. Setting this up involves working out apparmor as well via the kernel parameters. However, for me and some other people apparently, enabling apparmor.service corresponds to taking a few seconds of added boot time. Unacceptable for me at any rate. So no snapd either.\nFlatpak uses namespaces, but honestly on an ArchLinux machine it is always better to just build things.\nIntel graphics Following along the standard wiki page, I ended up setting fastboot and enable_fbc, verified with:\n1sudo systool -m i915 -av The settings can be passed through a kernel parameter or more simply via:\n1# sudoedit /etc/modprobe.d/i915.conf 2options i915 enable_fbc=1 fastboot=1 intel_iommu=on,igfx_off Window / Session Management Window and session managers are not exactly a dime a dozen. The X window system has served me well for many many years yet, for this post, of the X window system, the less said the better. Wayland on the other hand, is fast, shiny, and doesn\u0026rsquo;t as yet have 233 configuration flags.\nSway basics Weston remains the reference implementation, and GNOME has continued championing the cause of Wayland which is great. However, since my foray in 2018; tiling window managers have come a long way. Migrating away from the MacOS\u0026rsquo; \u0026ldquo;tiling managers\u0026rdquo; was seamless and very reminsicent of my older i3-gaps setup.\nHigh DPI An interesting quirk of sway is that Hi-DPI support is still at the draft stage. For the most part this is becoming a non-issue as more applications support Wayland out of the box, or at-least optionally.\nHowever, certain applications are unusable without the HiDPI support patches, including most java packages like cryptomator. Thus it is best to grab the modified versions and configure them.\n1trizen -S sway-hidpi-git wlroots-hidpi-git xorg-xwayland-hidpi-git Note that by default sway supports fractional scaling too via, swaymsg output $MON scale 1.5 and also when in the configuration. However, the patches are required for xwayland force scale 2 and sundry commands.\n This turned out to be a dead end, even though it worked reasonably well, the scaling of icons was messy and it still necessitated a set of environment variables for Desktop entries\n Display management A Reddit user WhyNotHugo, has an excellent line of reasoning for why a display manager is needed, rephrased to be:\n Enough system users / applications expect the existence of a display-manager including systemd-logind and plymouth No special handling needs to be done to ensure only tty1 spawns a graphical session The DM is meant to ensure that window manager crashes don\u0026rsquo;t yield an unlocked desktop  Anyway, the basic agreety should suffice for the most part; though a more pleasant setup is with greetd-tuigreet.\nMore importantly, the session itself should include the appropriate environment variables as described here and reproduced below. Essentially this involves using a runner script, /usr/local/bin/sway-run.sh which ensures the correct variables are setup.\n1#!/bin/sh 2 3# Session 4export XDG_SESSION_TYPE=wayland 5export XDG_SESSION_DESKTOP=sway 6export XDG_CURRENT_DESKTOP=sway 78source /usr/local/bin/wayland_enablement.sh 910systemd-cat --identifier=sway sway $@ Where the sourced file is:\n1#!/bin/sh 2export GDK_BACKEND=wayland,x11 3export MOZ_ENABLE_WAYLAND=1 4export CLUTTER_BACKEND=wayland 5export QT_QPA_PLATFORM=wayland-egl 6export ECORE_EVAS_ENGINE=wayland-egl 7export ELM_ENGINE=wayland_egl 8export SDL_VIDEODRIVER=wayland 9export _JAVA_AWT_WM_NONREPARENTING=1 10export NO_AT_BRIDGE=1 11export BEMENU_BACKEND=wayland Now instead of using tuigreet --cmd sway in /etc/greetd/config.toml we can use sway-run as the cmd.\nIt is important to optionally provide GDK_BACKEND with a fallback option otherwise zotero and others might fail with Error: cannot open display: :0.\nThis still breaks for a bunch of special cases for which I went with a specialized desktop file in $HOME/.local/share/applications/BLAH. For example, consider the enpass file (since it only supports xcb):\n1[Desktop Entry] 2Type=Application 3Name=Enpass XCB 4Exec=env QT_QPA_PLATFORM=xcb QT_SCALE_FACTOR=2 enpass 5Icon=enpass However, this is still not ideal.\nXWayland proxies Rather than work with the patched sway and wlroots packages, a recent approach by Thomas Leonard based on isolating the XWayland setup works a lot better for high DPI screens and systems. There are some caveats and still requires modified desktop files, but it works a lot better and has fewer bugs than the standard XWayland setup with sway. However, xlsclients does not track the X11 applications started in the manner described here.\n1git clone https://github.com/talex5/wayland-proxy-virtwl.git 2cd wayland-proxy-virtwl 3opam install . 4# Installs to $HOME/.opam/default/bin/wayland-proxy-virtwl For my system, the corresponding service required both a change in DPI, and the \u0026ldquo;unscaling\u0026rdquo;.\n1[Unit] 2Description=Wayland-proxy-virtwl 34[Service] 5ExecStart=/home/$USER/.opam/default/bin/wayland-proxy-virtwl --tag=\u0026#34;[my-vm] \u0026#34; --wayland-display wayland-0 --x-display=0 --xrdb Xft.dpi:150 --x-unscale=2 67[Install] 8WantedBy=default.target Note that the service needs the full path, not the censored one in the snippet. Now any subsequent X11 applications can be used via:\n1systemctl restart --user wayland-proxy-virtwl.service 2DISPLAY=\u0026#34;:0\u0026#34; WAYLAND_DISPLAY=\u0026#34;wayland-1\u0026#34; QT_QPA_PLATFORM=xcb QT_SCALE_FACTOR=1.5 enpass 3DISPLAY=\u0026#34;:0\u0026#34; GDK_BACKEND=\u0026#34;x11\u0026#34; WAYLAND_DISPLAY=\u0026#34;wayland-1\u0026#34; gvim When things work out, then the applications will have [my-vm] in the window title. This still requires that xorg-xwayland be installed, and in some situations it needs to be restarted, but by and large this is a seamless setup.\nMiscellaneous Terminal Emulators Though kitty and a few other stalwarts do support wayland, foot seems to be the fastest, especially since it has a server client mode much like emacs with foot -s and footclient.\nScreenshots slurm and grim still function well together, making this mostly painless.\n1slurp | grim -g - - | wl-copy Typically this is bound to WIN+O for me.\nScreen Recorders Unfortunately, the zoom desktop client does not support sway which is problematic. However, with pipewire setup, Firefox passes the gUM test for screen-sharing which is good enough. For recording stuff, OBS has a plugin, but simplescreenrecorder has a fork which works out of the box, with XWayland required for the UI though.\n1trizen -S simplescreenrecorder-wlroots-git 2DISPLAY=\u0026#34;:0\u0026#34; WAYLAND_DISPLAY=\u0026#34;wayland-1\u0026#34; simplescreenrecorder Reference Management I\u0026rsquo;ve been a zotero user for many years now, however it suffers from the same UI scaling issues and cannot be fixed either by forcing xwayland execution nor by trying other scaling methods. The last resort is to set up a user.js file in $HOME/.zotero/zotero/$PROFILE/user.js with:\n1user_pref(\u0026#39;layout.css.devPixelsPerPx\u0026#39;, \u0026#39;2\u0026#39;); 2user_pref(\u0026#34;extensions.zotero.fontSize\u0026#34;, \u0026#34;0.3\u0026#34;); With this, the toolbar text is still too large, as are all the preference text elements, however it is still moderately usable.\nSomehow this does conflict and error out with the proxied XWayland, but the regular settings work alright.\nMail Clients The mail client story is increasingly complicated on Linux machines in general. Since Thunderbird tanked with XUL ecosystem eons ago there haven\u0026rsquo;t been many contenders. Given the situation with window management, a localserver approach to web-clients like Mailpile 3 would have been perfect\u0026hellip; Except it is still based around python2 and shows no sign of moving forward. I really like Astroid, but it doesn\u0026rsquo;t seem to have been updated in a while either. Thunderbird works fine too, and does not require XWayland which is a plus point.\nElectron Anything with a recent electron binary (around 13) will just need a few flags.\n1code --enable-features=UseOzonePlatform --ozone-platform=wayland 2obsidian --enable-features=UseOzonePlatform --ozone-platform=wayland Browsers Personally I prefer firefox in-spite of its shitty tendencies on the regular editions (e.g. not allowing unsigned extensions).\nFirefox Firefox\u0026rsquo;s wayland build on the AUR remains a nightmare to compile and work with. The developer version can leverage the MOZ_ENABLE_WAYLAND=1 to get reasonably good results.\nChrome As of this post the stable branch of google-chrome does indeed support Wayland with a switch:\n1google-chrome-stable --enable-features=UseOzonePlatform --ozone-platform=wayland The stable branch is rather boring anyway, on Arch Linux atleast, the launchers support $XDG_CONFIG_HOME/chrome-dev-flags.conf or its equivalents.\n1--enable-features=UseOzonePlatform 2--ozone-platform=wayland RStudio No wayland support. Needs to be forced into xwayland and have the fonts scaled via:\n1# For the hidpi setup 2Exec=env WAYLAND_DISPLAY= QT_SCALE_FACTOR=2 /usr/bin/rstudio-bin %F 3# Or with the proxied xwayland 4DISPLAY=\u0026#34;:0\u0026#34; WAYLAND_DISPLAY=\u0026#34;wayland-1\u0026#34; QT_SCALE_FACTOR=2 QT_QPA_PLATFORM=xcb /usr/bin/rstudio-bin Conclusions Things have definitely improved in three years, however the overall aggravation and microaggressions which come with working around window manager issues seem to be too high a price for the potential battery / quality of life savings. In particular, as before, forcing programs to run in xwayland makes them effectively worse than their x11 counterparts and requires far more maintenance load. It seems likely then, that in-spite of great progress it is still probably best to stick to X11; especially for new users who might not immediately suspect the display server as a source of buggy UX elements (or screen-sharing woes).\nThough the conclusions above are still valid, with the XWayland fixes, this is definitely viable and will remain my daily driver.\n  This precludes any actual tasks I have, naturally, the color of the bike shed and the wool of a Llama take precedence\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Naturally I haven\u0026rsquo;t really taken time to dive into why, as evidenced by the many many many adjectives\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n A good initial set of thoughts on this are provided by this post\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/revisiting-wayland-2021-archlinux/","tags":["rambling","workflow"],"title":"Revisiting Wayland for ArchLinux"},{"categories":["notes"],"contents":" A closer look at the standard Fortran C compatibility layer by exploring objects and linkers\n Background Derived types and their interoperability have been covered previously in the context of usage from Python. However, much of the focus of the previous approach revolved around the iso_c_binding intrinsic module. A closer inspection of the functionality provided therein is the first step towards extending beyond the standards to support calling type bound procedures. This is an often overlooked aspect of the derived type usage pattern, in terms of interoperability.\nSeries Thoughts relating to interoperability of Fortran with things can be vaguely collated into the following series.\n NumPy Meson Fortran Simple Fortran Derived Types and Python Exploring ISO_C_BINDING and type-bound procedures \u0026lt;\u0026ndash; You are here!  Setup We would like to understand the effect of the intrinsic iso_c_binding module. So, we will re-use at first, the basic cartesian type of the previous post, along with a non-type-bound procedure which is to be called from C.\n1! vec.f90 2module vec 3implicit none 4integer, parameter :: dd = selected_real_kind(15,9) 56type :: cartesian 7real(kind=dd) :: x,y,z 8end type cartesian 910contains 1112subroutine unit_move(vec) 13type(cartesian), intent(inout) :: vec 14print*, \u0026#34;Modifying from Fortran\u0026#34; 15vec%x = vec%x + 1 16vec%y = vec%y + 1 17vec%z = vec%z + 1 18end subroutine unit_move 1920end module vec There isn\u0026rsquo;t a whole lot going on, except that the iso_c_binding labels have been dropped, and correspondingly, instead of c_double we now have an approximate kind mapping.\nThe associated C driver is exactly the same, though in this particular situation the function signature is optional.\n1/* vecfc.c */ 2#include\u0026lt;stdlib.h\u0026gt;3#include\u0026lt;stdio.h\u0026gt;4#include\u0026lt;vecfc.h\u0026gt;5 6void* unit_move(cartesian *word); 78int main(int argc, char* argv[argc+1]) { 9puts(\u0026#34;Initializing the struct\u0026#34;); 10cartesian a={3.0, 2.5, 6.2}; 11printf(\u0026#34;%f %f %f\u0026#34;,a.x,a.y,a.z); 12puts(\u0026#34;\\nFortran function with derived type from C:\u0026#34;); 13unit_move(\u0026amp;a); 14puts(\u0026#34;\\nReturned from Fortran\u0026#34;); 15printf(\u0026#34;%f %f %f\u0026#34;,a.x,a.y,a.z); 16return EXIT_SUCCESS; 17} To complete the translation unit, our header contains the struct definition.\n1#ifndef VECFC_H 2#define VECFC_H 3 4typedef struct { 5double x,y,z; 6} cartesian; 78#endif /* VECFC_H */So as to not detract from the main focus of the post, instead of using meson or another build system, we will compile everything by hand.\nCompilation We will attempt to follow along the same logical process as in the bind(c) situation:\n Compile and assemble the Fortran module Compile, assemble and link the C driver with the module  A very literal attempt at satisfying the two step process above is perhaps as simple as:\n1gfortran -c vec.f90 # generates vec.o 2gcc vec.o vecfc.c -I./ -lgfortran -o gf_vec 3/usr/bin/ld: /tmp/ccOOdInK.o: in function `main\u0026#39;: 4vecfc.c:(.text+0x9a): undefined reference to `unit_move\u0026#39; 5collect2: error: ld returned 1 exit status Naturally, the linker will fail at this point. Recall that we can check which symbols are actually part of vec.o.\n1nm vec.o 2U _gfortran_st_write 3U _gfortran_st_write_done 4U _gfortran_transfer_character_write 5U _GLOBAL_OFFSET_TABLE_ 6U __stack_chk_fail 70000000000000000 T __vec_MOD___copy_vec_Cartesian 80000000000000000 B __vec_MOD___def_init_vec_Cartesian 9000000000000002c T __vec_MOD_unit_move 100000000000000000 D __vec_MOD___vtab_vec_Cartesian Where the first few undefined functions are to be resolved by -lgfortran directive to the linker 1. Note that the function we care to call is actually called __vec_MOD_unit_move. We can also check the symbols required by our program.\n1gcc -c vecfc.c -I./ -o gf_vec.o 2nm -u gf_vec.o 3U _GLOBAL_OFFSET_TABLE_ 4U printf 5U puts 6U __stack_chk_fail 7U unit_move So our problem is essentially one of renaming to the right symbol.\nSymbol renaming A first approximation towards a solution is then evident, we will forcibly rename the symbol in our Fortran code, thus allowing us to link and call the function. objcopy is fantastic for this.\n1gfortran -c vec.f90 # get vec.o 2# Rename symbol 3objcopy --redefine-sym=__vec_MOD_unit_move=unit_move vec.o 4# Compile and link in one shot 5gcc vec.o vecfc.c -I./ -lgfortran -o gf_vec 6# Profit 7./gf_vec 8Initializing the struct 93.000000 2.500000 6.200000 10Fortran function with derived type from C: 11Modifying the derived type now! 1213Returned from Fortran 144.000000 3.500000 7.200000 Indeed, apart from this very satisfying result, we can verify that our symbols are not undefined as well.\n1nm -u gf_vec 2# nothing but library functions Switching compilers There are a number of caveats with the approach described so far, but perhaps one of the more striking ones has to do with changing the compiler. Consider the symbols generated by the Intel compiler.\n1ifort -c vec.f90 2nm vec.f90 3U for_write_seq_lis 40000000000000000 r __STRLITPACK_0 50000000000000000 r __STRLITPACK_1.0.2 60000000000000000 T vec._ 70000000000000010 T vec_mp_unit_move_ As Fortran remains one of the few languages to have a rich and varied set of compilers with varying levels of support and standardisation, it would be rather a tall order to keep track of the symbol mangling approaches used by every compiler. Indeed, problematically, it is not just that the symbols mangled differently, the code generated is quantitatively different as well.\n1ifort vec.o vecfc.o -lc 2ld: vecfc.o: in function `main\u0026#39;: 3vecfc.c:(.text+0x0): multiple definition of `main\u0026#39;; /opt/intel/oneapi/compiler/2021.2.0/linux/bin/intel64/../../compiler/lib/intel64_lin/for_main.o:for_main.c:(.text+0x0): first defined here 4ld: cannot find -lirng 5ifort -c vec.f90 -fPIE 6gcc vec.o vecfc.c -I./ -o gf_vec 7/usr/bin/ld: vec.o: in function `unit_move\u0026#39;: 8vec.f90:(.text+0x55): undefined reference to `for_write_seq_lis\u0026#39; 9collect2: error: ld returned 1 exit status In any case, we recall from the info pages of gfortran that:\n Note that just because the names match does not mean that the interface implemented by GNU Fortran for an external name matches the interface implemented by some other language for that same name. That is, getting code produced by GNU Fortran to link to code produced by some other compiler using this or any other method can be only a small part of the overall solution\u0026ndash;getting the code generated by both compilers to agree on issues other than naming can require significant effort, and, unlike naming disagreements, linkers normally cannot detect disagreements in these other areas.\n Simplifying labels The first functionality of the iso_c_binding is actually rather easy to implement from a vendor perspective, but vastly simplifying for the end-user, the ability to provide a single binding label. Recall the bind(c) variant of the previous post.\n1! vec_bind.f90 2module vec 3use, intrinsic :: iso_c_binding 4implicit none 56type, bind(c) :: cartesian 7real(c_double) :: x,y,z 8end type cartesian 910contains 1112subroutine unit_move(array) bind(c) 13type(cartesian), intent(inout) :: array 14print*, \u0026#34;Modifying the derived type now!\u0026#34; 15array%x=array%x+1 16array%y=array%y+1 17array%z=array%z+1 18end subroutine unit_move 1920end module vec Which generates the following symbols.\n1gfortran -o vec.o -c vec_bind.f90 2nm vec.o 3U _gfortran_st_write 4U _gfortran_st_write_done 5U _gfortran_transfer_character_write 6U _GLOBAL_OFFSET_TABLE_ 7U __stack_chk_fail 8000000000000002c T unit_move 90000000000000000 T __vec_MOD___copy_vec_Cartesian 100000000000000000 B __vec_MOD___def_init_vec_Cartesian 110000000000000000 D __vec_MOD___vtab_vec_Cartesian This true across compilers as well, all without any invocations of objcopy and other approaches.\n1ifort -c vec.f90 -o vec.o 2nm vec.o 3U for_write_seq_lis 40000000000000000 r __STRLITPACK_1 50000000000000000 r __STRLITPACK_2.0.2 60000000000000010 T unit_move 70000000000000000 T vec._ Type-bound procedures We would like to extend the discussion to beyond where the light of the standard reaches, that is to consider calling type-bound procedures, which are not supported by the standard. We will begin by a suitable modification of our code. In an ideal world, we would simply annotate the type-bound procedure.\n1! vec_typeb.f90 2module vec 3use, intrinsic :: iso_c_binding 4implicit none 56type, bind(c) :: cartesian 7real(c_double) :: x,y,z 8contains 9procedure, pass(self) :: unitmv 10end type cartesian 1112contains 1314subroutine unit_move(self) bind(c) 15class(cartesian), intent(in) :: self 16print*, \u0026#34;Modifying the derived type now!\u0026#34; 17self%x=self%x+1 18self%y=self%y+1 19self%z=self%z+1 20end subroutine unit_move 2122end module vec However, this understandably does not go very well.\n1ifort -c vec_typeb.f90 2vec_typeb.f90(7): error #8575: A derived type with the BIND attribute shall not have a type bound procedure part. 3contains 4-----^ 5vec_typeb.f90(14): error #8224: A derived type used with the CLASS keyword shall not have the BIND attribute or SEQUENCE property. [CARTESIAN] 6class(cartesian), intent(in) :: self 7gfortran -c vec.f90 8vec.f90:7:13: 9107 | contains 11| 1 12Error: Derived-type ‘cartesian’ with BIND(C) must not have a CONTAINS section at (1) Removing the attribute, from both the type and the subroutine and rearranging slightly, we get the following.\n1module vec 2use, intrinsic :: iso_c_binding 3implicit none 45type :: cartesian 6real(kind=8) :: x,y,z 7contains 8procedure, pass(self) :: unitmv 9end type cartesian 1011contains 1213subroutine unitmv(self) 14class(cartesian), intent(inout) :: self 15print*, \u0026#34;Modifying the derived type now!\u0026#34; 16self%x=self%x+1.0 17self%y=self%y+1.0 18self%z=self%z+1.0 19end subroutine unitmv 2021end module vec The class attribute is required here instead of type since,\n1# ifort 2error #8264: The passed-object dummy argument must be a polymorphic dummy data object if the type being defined is extensible. In any case, we are now in a position to inspect the generated object.\n1gfortran -c vec_typeb.f90 2nm vec_typeb.o 3U _gfortran_st_write 4U _gfortran_st_write_done 5U _gfortran_transfer_character_write 6U _GLOBAL_OFFSET_TABLE_ 7U __stack_chk_fail 80000000000000000 T __vec_MOD___copy_vec_Cartesian 90000000000000000 B __vec_MOD___def_init_vec_Cartesian 10000000000000002c T __vec_MOD_unitmv 110000000000000000 D __vec_MOD___vtab_vec_Cartesian Which does not seem to be very different at all. We will make an attempt to directly modify the symbol-table as before.\n1objcopy --redefine-sym=__vec_MOD_unitmv=unit_move vec_typeb.o An attempt to call this is bound for failure, a segmentation fault to be exact. In order to be able to call the type bound procedure then, we can begin by writing a wrapper subroutine.\n1subroutine unit_move(cartobj) 2type(cartesian), intent(inout) :: cartobj 3call cartobj%unitmv() 4end subroutine This does allow for the existing C interface to work.\n1gfortran -c vec_typeb.f90 2objcopy --redefine-sym=__vec_MOD_unit_move=unit_move vec_typeb.o 3gcc vec_typeb.o vecfc.c -I./ -o gf_vec -lgfortran 4./gf_vec 5Initializing the struct 63.000000 2.500000 6.200000 7Fortran function with derived type from C: 8Modifying the derived type now! 910Returned from Fortran 114.000000 3.500000 7.200000% It is useful to at this point, that Fortran passes arguments by reference and not by value, so there are no additional copy related overheads incurred by this \u0026ldquo;wrapper\u0026rdquo; approach.\n It was correctly pointed out on the Fortran Discourse that the pass-by-reference calling convention is not mandated by the standards, though in practice many compilers do pass by reference when the VALUE attribute is missing.\n On the other hand, it does leave the programmer in a world bereft of the iso_c_binding, which means also that the mapping of types and precisions become rather fluid.\nConclusions This brief interlude on derived types and functions is more relevant in the context of automated binding generators like f2py Peterson (2009).\nPerhaps a more formal approach to wrapper generation is the one elaborated upon in great detail in the literature Gray, Roberts, and Evans (1999) which details the concept of a logical interface and a physical interface.\nA similar but more pragmatic approach is the opaque pointer method used for derived types with pointers in Pletzer et al. (2008) and forms the basis for the implementation in f90wrap Kermode (2020).\nThe (ab)use of type-bound procedures in the context of these binding generation methodologies is to follow at some point since none of them make any explicit mention of the same. Given that type-bound procedures were not introduced before 2003 Reid (2003), it is not surprising they have been overlooked, however their usage is foundational for long-lasting, sustainable Fortran code.\nReferences Gray, M. G., R. M. Roberts, and T. M. Evans. 1999. \u0026ldquo;Shadow-Object Interface Between Fortran 95 and C++.\u0026rdquo; Computing in Science Engineering 1 (2): 63\u0026ndash;70. https://doi.org/10.1109/5992.753048.\n Kermode, James R. 2020. \u0026ldquo;F90wrap: An Automated Tool for Constructing Deep Python Interfaces to Modern Fortran Codes.\u0026rdquo; Journal of Physics: Condensed Matter 32 (30): 305901. https://doi.org/10.1088/1361-648X/ab82d2.\n Peterson, Pearu. 2009. \u0026ldquo;F2py: A Tool for Connecting Fortran and Python Programs.\u0026rdquo; International Journal of Computational Science and Engineering 4 (4): 296. https://doi.org/10.1504/IJCSE.2009.029165.\n Pletzer, Alexander, Douglas McCune, Stefan Muszala, Srinath Vadlamani, and Scott Kruger. 2008. \u0026ldquo;Exposing Fortran Derived Types to C and Other Languages.\u0026rdquo; Computing in Science Engineering 10 (4): 86\u0026ndash;92. https://doi.org/10.1109/MCSE.2008.94.\n Reid, John. 2003. \u0026ldquo;The New Features of Fortran 2003,\u0026rdquo; 38. https://wg5-fortran.org/N1551-N1600/N1579.pdf.\n    As in all things, info nm provides the details on interpreting the remaining symbols\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/iso-c-type-bound-fortran/","tags":["fortran","f2py","rambling"],"title":"Exploring ISO_C_BINDING and type-bound procedures"},{"categories":["code"],"contents":"Very quick set of ugly commands to grab build environments. A much better approach is to make a custom Dockerfile or even better, use nix.\nHowever it does work in a pinch.\n1docker pull IMG:TAG 2sudo docker run -v LOCALDIR:DIRINDOCKER -it debian:experimental-20211115 bash 3# Don\u0026#39;t be root for long 4apt update 5apt install sudo vim zsh 6# Use the same username --\u0026gt; easier to manage permissions 7useradd -m -s /bin/zsh $USER -G sudo 8passwd $USER # Some crap 9# Or just add to the sudoers file 10echo \u0026#34;$USERALL=(ALL:ALL) ALL\u0026#34; \u0026gt;\u0026gt; /etc/sudoers 11su $USER 12# numpy stuff 13sudo apt install gcc gfortran libopenblas-dev python3.10-dev python3.10-dbg git python3-distutils python3-setuptools libx11-dev build-essential pkg-config python3-pip python3-pytest python3-hypothesis 14python3.10-dbg runtests.py -s f2py -vvv ","permalink":"https://rgoswami.me/snippets/docker-dev-envs/","tags":["devenv","cmdline"],"title":"Docker Development Environments"},{"categories":["code"],"contents":"Whenever I need to access a server running on an HPC which does not support ngrok or localtunnel or even gsocket; the fallback approach is always to rely on SSH port forwarding.\nThe sample problem here is running an HTTP server for viewing graphics in R via httpgd.\n1# Local 2export PORT=9899 \u0026amp;\u0026amp; ssh -L \u0026#34;${PORT}:localhost:${PORT}\u0026#34; \u0026#34;rog32@krafla.rhi.hi.is\u0026#34; -N -L \u0026#34;${PORT}:localhost:${PORT}\u0026#34; elja 3# New tab 4ssh krafla 5ssh elja 6radian # or R Now in R.\n1library(\u0026#34;httpgd\u0026#34;) 2# else install.packages(c(\u0026#34;httpgd\u0026#34;), repos=\u0026#39;https://cran.hafro.is/\u0026#39;) 3hgd(port=9899) Conda and R Annoyingly if you are depending on conda or micromamba or some variant thereof on the remote sever then install.packages will fail and instead the environment needs to have the corresponding R packages installed after searching anaconda.org.\n1# minimal 2micromamba create -p $(pwd)/.tmp -c conda-forge r-tidyverse git R 3micromamba activate $(pwd)/.tmp 4micromamba install -c nclibz r-httpgd These issues are not present when using a nix-shell.\n","permalink":"https://rgoswami.me/snippets/ssh-port-forwarding/","tags":["ssh"],"title":"SSH Port Forwarding"},{"categories":["notes"],"contents":" Moving simple Fortran derived types to Python and back via C\n Background Object oriented programming has been part of Fortran for longer than I have been alive 1. Fortran has derived types now. They\u0026rsquo;ve been around for around for over three decades. The standards at the same time, have been supporting more and more interoperable operations. Details of these pleasant historical improvements are pretty much the most the Fortran standards committee have managed to date in the 21st century. The point of this exploration is to design interfaces to Python, mostly for the long term advancement of f2py, and just for the heck of it. Simple in the context of the title means that we consider only derived types whose members have direct C equivalents as defined in the Fortran 2003 Reid (2003) standard.\nSeries Without intending it to be so, it appears that a series has been established, though not in strict order, the progression of the standards and their modern equivalents for Fortran-Python can be traced as seen in:\n NumPy Meson Fortran Simple Fortran Derived Types and Python \u0026lt;\u0026ndash; You are here!  C Structures and Derived Types - I Consider, first an example of what shall be a rather simple collection of records, the prototypical darling of most simulations, the point in Euclidean space, \\(\\mathbb{R}^{3}\\).\n1type, bind(c) :: cartesian 2real(c_double) :: x,y,z 3end type cartesian Now by definition, ever since the ISO_C_BINDING of the Fortran 2003 Reid (2003) standard, we know this is equivalent to:\n1typedef struct { 2double x,y,z; 3} cartesian; To try this out, we need to have a procedure which operates on the type, due to a chronic lack of imagination, let us simply take a point and add one to each axis.\n1subroutine unit_move(cartpoint) bind(c) 2type(cartesian), intent(inout) :: cartpoint 3print*, \u0026#34;Modifying the derived type now!\u0026#34; 4cartpoint%x=cartpoint%x+1 5cartpoint%y=cartpoint%y+1 6cartpoint%z=cartpoint%z+1 7end subroutine unit_move Now we can wrap these into a nice little module:\n1module vec 2use, intrinsic :: iso_c_binding 3implicit none 45type, bind(c) :: cartesian 6real(c_double) :: x,y,z 7end type cartesian 89contains 1011subroutine unit_move(array) bind(c) 12type(cartesian), intent(inout) :: array 13print*, \u0026#34;Modifying the derived type now!\u0026#34; 14array%x=array%x+1 15array%y=array%y+1 16array%z=array%z+1 17end subroutine unit_move 1819end module vec Call it with a simple main.\n1! main.f90 2program main 3use, intrinsic :: iso_c_binding 4use vec 5implicit none 6type(cartesian) :: cartvec 7cartvec = cartesian(0.2,0.5,0.3) 8print*, cartvec 9call unit_move(cartvec) 10print*, cartvec 11end program main Finally we need a build system, lets say meson.\n1# meson.build2project(\u0026#39;fcbase\u0026#39;,\u0026#39;fortran\u0026#39;,3version:\u0026#39;0.1\u0026#39;,4default_options:[\u0026#39;warning_level=3\u0026#39;])56executable(\u0026#39;fcbase\u0026#39;,7\u0026#39;vec.f90\u0026#39;,8\u0026#39;main.f90\u0026#39;,9install:true)Now we can finally try this out.\n1meson setup testfc 2meson compile -C testfc 3./testfc/fcbase 40.20000000298023224 0.50000000000000000 0.30000001192092896 5Modifying the derived type now! 61.2000000029802322 1.5000000000000000 1.3000000119209290 This probably surprised no one. So let\u0026rsquo;s get to something more interesting.\nC and Fortran We can start with a simple C header.\n1#ifndef VECFC_H 2#define VECFC_H 3 4typedef struct { 5double x,y,z; 6} cartesian; 78#endif /* VECFC_H */Along with its associated translation unit.\n1/* vecfc.c */ 2#include\u0026lt;stdlib.h\u0026gt;3#include\u0026lt;stdio.h\u0026gt;4#include\u0026lt;vecfc.h\u0026gt;5 6void* unit_move(cartesian *word); 78int main(int argc, char* argv[argc+1]) { 9puts(\u0026#34;Initializing the struct\u0026#34;); 10cartesian a={3.0, 2.5, 6.2}; 11printf(\u0026#34;%f %f %f\u0026#34;,a.x,a.y,a.z); 12puts(\u0026#34;\\nFortran function with derived type from C:\u0026#34;); 13unit_move(\u0026amp;a); 14puts(\u0026#34;\\nReturned from Fortran\u0026#34;); 15printf(\u0026#34;%f %f %f\u0026#34;,a.x,a.y,a.z); 16return EXIT_SUCCESS; 17} The function declaration is straightforward, but for the fact that since we are passing something both in and out of the function, it must be a pointer, and as we have no real error propagation or feasible return type, void * makes sense too. The calling semantics are equally transparent, creating an instance of the struct and then passing it by reference.\nThe meson extension is fairly trivial, we only need to add the right file and declare the project language correctly.\n1project(\u0026#39;cderived\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;fortran\u0026#39;,2version:\u0026#39;0.1\u0026#39;,3default_options:[\u0026#39;warning_level=3\u0026#39;])45executable(\u0026#39;cderived\u0026#39;,6\u0026#39;vec.f90\u0026#39;,7# \u0026#39;vecfc.c\u0026#39;,8# \u0026#39;vecfc.h\u0026#39;,9\u0026#39;main.f90\u0026#39;,10install:true)This seems to work.\n1meson setup testfc 2meson compile -C testfc 3./testfc/fcbase 4Initializing the struct 53.000000 2.500000 6.200000 6Fortran function with derived type from C: 7Modifying the derived type now! 89Returned from Fortran 104.000000 3.500000 7.200000% Cython and Fortran So far so good. Let\u0026rsquo;s get to something slightly more interesting. Although in the long run cython isn\u0026rsquo;t really the approach numpy is going towards at the moment, it still makes sense to give it a shot as a first approximation.\n1cdef struct cartesian: 2double x,y,z 34cdef extern: 5void* unit_move(cartesian *word) 67def unitmv(dpt={\u0026#39;x\u0026#39;:0,\u0026#39;y\u0026#39;:0,\u0026#39;z\u0026#39;:0}): 8cdef cartesian A=dpt 9unit_move(\u0026amp;A) 10return(A) The most interesting aspect is that, having defined our C structure and the external function, we need something to offer to Python. Since cython structures can be mapped from simple python dictionaries, we take an input of one, construct the structure on the fly in the function and then use our Fortran subroutine. The return type can also be coerced into a list, and that\u0026rsquo;s what can be printed or manipulated further. This isn\u0026rsquo;t the most efficient way to work with this, but it will do for now.\nThe meson update follows from the explanation in the previous post, namely, that we need to include our Python sources.\n1project(\u0026#39;pointeuc\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;fortran\u0026#39;,2version:\u0026#39;0.1\u0026#39;,3default_options:[\u0026#39;warning_level=3\u0026#39;])45add_languages(\u0026#39;cython\u0026#39;)67py_mod=import(\u0026#39;python\u0026#39;)8py3=py_mod.find_installation()9py3_dep=py3.dependency()1011incdir_numpy=run_command(py3,12[\u0026#39;-c\u0026#39;,\u0026#39;import os; os.chdir(\u0026#34;..\u0026#34;); import numpy; print(numpy.get_include())\u0026#39;],13check:true14).stdout().strip()1516inc_np=include_directories(incdir_numpy)1718py3.extension_module(\u0026#39;veccyf\u0026#39;,19\u0026#39;veccyf.pyx\u0026#39;,20\u0026#39;vec.f90\u0026#39;,21include_directories:inc_np,22dependencies:py3_dep)The NumPy dependency in this situation is really optional, but makes for a more interoperable structure. Now we\u0026rsquo;re ready to try this out in an interactive manner.\n1meson setup testcpyth 2meson compile -C testcpyth 3cd testcpyth 4python -c \u0026#34;import veccyf; A=veccyf.unitmv({\u0026#39;x\u0026#39;:3.3,\u0026#39;y\u0026#39;:5.2,\u0026#39;z\u0026#39;:2.6}); print(A)\u0026#34; 5Modifying the derived type now! 6{\u0026#39;x\u0026#39;: 4.3, \u0026#39;y\u0026#39;: 6.2, \u0026#39;z\u0026#39;: 3.6} Taking this a bit slower, it is still as neat as ever.\n1import veccyf 2point = {\u0026#39;x\u0026#39;: 4.3, 3\u0026#39;y\u0026#39;: 2.6, 4\u0026#39;z\u0026#39;: 6.4} 5opoint = veccyf.unitmv(point) 6print(opoint) Aside from the output string, this works as expected, and is pretty neat, all things considered. This might feed into an f2py workflow, but then, it adds a layer of indirection which seems overly circuitous. The design could have also been far improved by using a bind(c, name=uniquname) as well.\nNote that a much more rational approach would have been to generate a class in Python.\nPrognosis for F2PY The essential information used from the fortran code for the cython types (and C structures) was essentially:\n The types of each variable for the derived type  double for every one of them in this case   The names of each variable  x, y, and z here    Apart from this, for functions which operate on aforementioned types,we require:\n The external declaration (due to the iso_c_binding) A mapping from structure to a python object, typically a dictionary  It so happens that much of the information needed for generation of the relevant C code is already present in f2py, and can be inspected via the crackfortran module.\n1# python -m numpy.f2py.crackfortran vec.f90 -show 2Reading fortran codes... 3Reading file \u0026#39;vec.f90\u0026#39; (format:free) 4{\u0026#39;before\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;this\u0026#39;: \u0026#39;use\u0026#39;, \u0026#39;after\u0026#39;: \u0026#39;, intrinsic :: iso_c_binding \u0026#39;} 5Line #2 in vec.f90:\u0026#34; use, intrinsic :: iso_c_binding \u0026#34; 6analyzeline: Could not crack the use statement. 7Line #5 in vec.f90:\u0026#34; type, bind(c) :: cartesian \u0026#34; 8analyzeline: No name/args pattern found for line. 9Post-processing... 10Block: vec 11Block: unknown_type 12Block: unit_move 13Post-processing (stage 2)... 14[{\u0026#39;args\u0026#39;: [], 15\u0026#39;block\u0026#39;: \u0026#39;module\u0026#39;, 16\u0026#39;body\u0026#39;: [{\u0026#39;args\u0026#39;: [], 17\u0026#39;block\u0026#39;: \u0026#39;type\u0026#39;, 18\u0026#39;body\u0026#39;: [], 19\u0026#39;entry\u0026#39;: {}, 20\u0026#39;externals\u0026#39;: [], 21\u0026#39;from\u0026#39;: \u0026#39;vec.f90:vec\u0026#39;, 22\u0026#39;interfaced\u0026#39;: [], 23\u0026#39;name\u0026#39;: \u0026#39;unknown_type\u0026#39;, 24\u0026#39;parent_block\u0026#39;: \u0026lt;Recursion on dict with id=4500228096\u0026gt;, 25\u0026#39;sortvars\u0026#39;: [\u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;, \u0026#39;z\u0026#39;], 26\u0026#39;varnames\u0026#39;: [\u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;, \u0026#39;z\u0026#39;], 27\u0026#39;vars\u0026#39;: {\u0026#39;x\u0026#39;: {\u0026#39;kindselector\u0026#39;: {\u0026#39;kind\u0026#39;: \u0026#39;c_double\u0026#39;}, 28\u0026#39;typespec\u0026#39;: \u0026#39;real\u0026#39;}, 29\u0026#39;y\u0026#39;: {\u0026#39;kindselector\u0026#39;: {\u0026#39;kind\u0026#39;: \u0026#39;c_double\u0026#39;}, 30\u0026#39;typespec\u0026#39;: \u0026#39;real\u0026#39;}, 31\u0026#39;z\u0026#39;: {\u0026#39;kindselector\u0026#39;: {\u0026#39;kind\u0026#39;: \u0026#39;c_double\u0026#39;}, 32\u0026#39;typespec\u0026#39;: \u0026#39;real\u0026#39;}}}, 33{\u0026#39;args\u0026#39;: [\u0026#39;array\u0026#39;], 34\u0026#39;block\u0026#39;: \u0026#39;subroutine\u0026#39;, 35\u0026#39;body\u0026#39;: [], 36\u0026#39;entry\u0026#39;: {}, 37\u0026#39;externals\u0026#39;: [], 38\u0026#39;from\u0026#39;: \u0026#39;vec.f90:vec\u0026#39;, 39\u0026#39;interfaced\u0026#39;: [], 40\u0026#39;name\u0026#39;: \u0026#39;unit_move\u0026#39;, 41\u0026#39;parent_block\u0026#39;: \u0026lt;Recursion on dict with id=4500228096\u0026gt;, 42\u0026#39;saved_interface\u0026#39;: \u0026#39;\\n\u0026#39; 43\u0026#39; subroutine unit_move(array) \\n\u0026#39; 44\u0026#39; type(cartesian), intent(inout) :: \u0026#39; 45\u0026#39;array\\n\u0026#39; 46\u0026#39; end subroutine unit_move\u0026#39;, 47\u0026#39;sortvars\u0026#39;: [\u0026#39;array\u0026#39;], 48\u0026#39;vars\u0026#39;: {\u0026#39;array\u0026#39;: {\u0026#39;attrspec\u0026#39;: [], 49\u0026#39;intent\u0026#39;: [\u0026#39;inout\u0026#39;], 50\u0026#39;typename\u0026#39;: \u0026#39;cartesian\u0026#39;, 51\u0026#39;typespec\u0026#39;: \u0026#39;type\u0026#39;}}}], 52\u0026#39;entry\u0026#39;: {}, 53\u0026#39;externals\u0026#39;: [], 54\u0026#39;from\u0026#39;: \u0026#39;vec.f90\u0026#39;, 55\u0026#39;implicit\u0026#39;: None, 56\u0026#39;interfaced\u0026#39;: [], 57\u0026#39;name\u0026#39;: \u0026#39;vec\u0026#39;, 58\u0026#39;sortvars\u0026#39;: [], 59\u0026#39;vars\u0026#39;: {}}] The information is clearly present, which brings us to the next phase.\nPython-C and Fortran The most natural way to feed into f2py would be to directly write out an equivalent Python-C API which effectively mimics the extension module of the last section.\nThe crucial definition essentially takes a dictionary, unrolls into a list, creates the C structure, then passes it through Fortran and then back.\n1static PyObject* eucli_unitinc(PyObject* self, PyObject* args) { 2cartesian a; 3PyObject* dict; 4PyArg_ParseTuple(args, \u0026#34;O\u0026#34;, \u0026amp;dict); 5PyObject* vals = PyDict_Values(dict); 6a.x = PyFloat_AsDouble(PyList_GetItem(vals,0)); 7a.y = PyFloat_AsDouble(PyList_GetItem(vals,1)); 8a.z = PyFloat_AsDouble(PyList_GetItem(vals,2)); 9// Call Fortran on it 10 unit_move(\u0026amp;a); 11// 12 PyObject* ret = Py_BuildValue(\u0026#34;{s:f,s:f,s:f}\u0026#34;, 13\u0026#34;x\u0026#34;, a.x, 14\u0026#34;y\u0026#34;, a.y, 15\u0026#34;z\u0026#34;, a.z); 16return ret; 17} This also works well enough with minor modifications to meson.\n1python -c \u0026#34;import eucli; A=eucli.unitinc({\u0026#39;x\u0026#39;:3.3,\u0026#39;y\u0026#39;:5.2,\u0026#39;z\u0026#39;:2.6}); print(A)\u0026#34; 2Modifying the derived type now! 3{\u0026#39;x\u0026#39;: 4.3, \u0026#39;y\u0026#39;: 6.2, \u0026#39;z\u0026#39;: 3.6} Boilerplate The rest of the hand written wrapper is boilerplate, but the full file is reproduced here:\n1#ifndef PY_SSIZE_T_CLEAN 2#define PY_SSIZE_T_CLEAN 3#include \u0026lt;stdio.h\u0026gt;4#endif /* PY_SSIZE_T_CLEAN */5 6#include \u0026#34;Python.h\u0026#34;7 8// Fortran-C stuff 9 10typedef struct { 11double x,y,z; 12} cartesian; 1314void* unit_move(cartesian *word); 1516// Python-C stuff 17 18static PyObject* eucli_unitinc(PyObject* self, PyObject* args) { 19cartesian a; 20PyObject* dict; 21PyArg_ParseTuple(args, \u0026#34;O\u0026#34;, \u0026amp;dict); 22PyObject* vals = PyDict_Values(dict); 23a.x = PyFloat_AsDouble(PyList_GetItem(vals,0)); 24a.y = PyFloat_AsDouble(PyList_GetItem(vals,1)); 25a.z = PyFloat_AsDouble(PyList_GetItem(vals,2)); 26// Call Fortran on it 27 unit_move(\u0026amp;a); 28// 29 PyObject* ret = Py_BuildValue(\u0026#34;{s:f,s:f,s:f}\u0026#34;, 30\u0026#34;x\u0026#34;, a.x, 31\u0026#34;y\u0026#34;, a.y, 32\u0026#34;z\u0026#34;, a.z); 33return ret; 34} 3536static PyMethodDef EucliMethods[] = { 37{\u0026#34;unitinc\u0026#34;, (PyCFunction)eucli_unitinc, METH_VARARGS, 38\u0026#34;Add 1 to the axes of a point\u0026#34;}, 39{NULL, NULL, 0, NULL} /* Sentinel */ 40}; 4142static struct PyModuleDef eucli_module = { 43PyModuleDef_HEAD_INIT, 44\u0026#34;eucli\u0026#34;, 45\u0026#34;blah\u0026#34;, 46-1, 47EucliMethods 48}; 4950PyMODINIT_FUNC PyInit_eucli(void) 51{ 52PyObject *m; 53m = PyModule_Create(\u0026amp;eucli_module); 54if (!m) { 55return NULL; 56} 5758return m; 59} Conclusions Slowly but surely, compilers have caught up with to the standards, and soon, with some support, numpy shall too. A subsequent post shall cover more exotic derived types, typically (until 2018) treated by non-portable opaque pointer tricks. There are still some rather awkward design decisions in this case; including things like considering the mapping and their interoperability with NumPy arrays, but by and large this would depend on the functions which use these. In terms of derived types of simple structures, without allocatable arrays, the extensions appear to be straightforward.\nReferences Reid, John. 2003. \u0026ldquo;The New Features of Fortran 2003,\u0026rdquo; 38. https://wg5-fortran.org/N1551-N1600/N1579.pdf.\n    This is more surprising with each passing year\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/cython-derivedtype-f2py/","tags":["python","fortran","numpy","f2py","rambling"],"title":"Simple Fortran Derived Types and Python"},{"categories":["conferences"],"contents":" A meta-post on twin presentations on F2PY and LFortran at FortranCon'21\n Background I had the honor and pleasure to have two presentations at the second annual FortranCon in 2021. The fun part was both talks featured the language standard prominently, but were meant to form two sides of a coin.\nf2py: Two Decades Later  Co-Authors Dr. Ralf Gommers (Quansight Labs), Dr. Melissa Mendonca (Quansight Labs), Dr. Pearu Peterson (Quansight Labs) Duration 20 minutes (15 + 5)  Abstract  f2py is the gold standard for interfacing Fortran and Python. The most famous downstream consumer of the generated wrappers is undoubtedly the scipy ecosystem. Crucially, f2py is not a compiler, and generates a best effort set of wrappers, aided by special comment lines. One of the key inter-operable approaches to compilation of the subsequent wrappers relies on numpy.distutils. Given the planned obsolescence of this module; we discuss the methodology by which the build phase can be split into a two phase process and document better the pyf enhancements made to facilitate callbacks. A brief outline of test suites and documentation within the numpy ecosystem and proposed enhancement proposals will be discussed as well. We lay out the roadmap towards f2py remaining relevant beyond f77 by implementing derived types and newer features like parallelism with co-arrays. Playing to its strengths as a code-enhancer and leveraging the flexibility of not being constrained by the actual compilation process allows for the reimagining of f2py as a tool and library.\n Other Content  NumPy, Meson and f2py  Slides  Best viewed here using a browser (in a new tab) A pdf copy of the slides are embedded below The orgmode source is here on the site\u0026rsquo;s GH repo   Video {{\u0026lt; youtube 9nMSKcRimuw \u0026gt;}}\nImplementing Fortran Standardese within LFortran  Co-Author(s) Dr. Ondřej Čertík (Los Alamos National Laboratory) Duration 5 minutes  Abstract  The Fortran standard is a rather imposing document; though with F2018 at around 600 pages it is significantly lighter than its C++ counterpart. The standard itself is rarely cited as a beginner resource, with a cottage industry of books designed to shed light on the dry prose and foreboding technical rigor. However, with LFortran, particularly viewed through the lens of the Abstract Semantic Representation (ASR) of any Fortran code, the standard suddenly gains clarity and rigor. Being as it is a Fortran-first representation, unlike GCC\u0026rsquo;s internal representations (parse trees and lowered GIMPLE) or opaque (proprietary) middle-layers of NAG/Intel; the ASR is meant to concretely represent the syntax in a way to make the Standard seem eminently rational. Implementing the intrinsic functions and standard computing models become pleasant C++ puzzles instead of pidgin nightmares of progressively lower representations. In this presentation, I will discuss the process of implementing Standard intrinsic functions, the lifeline of any applied physical science researcher. LFortran allows for further flexibility at runtime, which will come up briefly, but most of the focus for this short talk will be on the compile time evaluation of intrinsic functions and future directions.\n Other Content The whole GSoC series.\n  Fortran, GSoC21 and Me\n  GSoC21 W1: LFortran Kickoff\n  GSoC21 W2: LFortran Unraveling\n  GSoC21 W3: Kind, Characters, and Standards\n  GSoC21 W4: LFortran, Backends and Bugs\n  GSoC21 W5: LFortran Design Details and minidftatom\n  GSoC21 W6: LFortran ASR and Values\n  GSoC21 W7: LFortran Workflow Basics\n  GSoC21 W8: LFortran Refactors, and Compile Time Evaluations\n  GSoC21 W9: LFortran Bug Hunting Bonanza\n  GSoC21 W10: LFortran Runtime Library Design\n  GSoC21 LFortran and Computational Chemistry\n  Slides  Best viewed here using a browser (in a new tab) A pdf copy of the slides are embedded below The orgmode source is here on the site\u0026rsquo;s GH repo   Video From around 10:43 in.\n{{\u0026lt; youtube 4236hgpMBLY \u0026gt;}}\nThoughts I could go on about these topics all day, thankfully for the most part, I do.\n","permalink":"https://rgoswami.me/posts/fortrancon-2021-meta/","tags":["presentations","ramblings","fortran"],"title":"Presentation Supplements for FortranCon'21"},{"categories":["notes"],"contents":" Exploring meson for interfacing fortran and python via f2py and standard techniques, with benchmarks.\n Background A recent post gauging community interest in f2py brought to light (among other aspects) the fact that the build systems of f2py are rather opaque to the end user. There are good reasons for this, since many of the tools discussed in this post were not around / in any shape to be used during the active development of f2py1. Nevertheless, build systems have come a long way from the early 2000s and the next two decades of f2py, will certainly involve less dependence on the soon-to-be-gone numpy.distutils approach to building extension modules.\nThe Fibonacci example Though rather contrived and overused2, for the rest of this post I will assume the following Fortran program, in a variety of guises.\n1C FILE: FIB1.F 2SUBROUTINE FIB(A,N) 3C 4C CALCULATE FIRST N FIBONACCI NUMBERS 5C 6INTEGER N 7REAL*8 A(N) 8DO I=1,N 9IF (I.EQ.1) THEN 10A(I) = 0.0D0 11ELSEIF (I.EQ.2) THEN 12A(I) = 1.0D0 13ELSE 14A(I) = A(I-1) + A(I-2) 15ENDIF 16ENDDO 17END 18C END FILE FIB1.F Keeping in mind the fact that the fixed form syntax is deprecated, we will also consider the more standard compliant and modern form:\n1module fib1 2implicit none 3contains 4subroutine fib(a,n) 5integer, intent(in) :: n 6integer :: i 7real(8) :: a(n) 8do i=1, n 9if (i==1) then 10a(i) = 0.0d0 11else if (i==2) then 12a(i) = 1.0d0 13else 14a(i) = a(i-1) + a(i-2) 15end if 16end do 17end subroutine 18end module fib1 Note that this program in its current form is not really an executable, as it is essentially a subprogram; and needs to be coupled to a main program to work from fortran. This does not concern us really since we will be primarily using python to drive user interaction. However, for completeness (gfortran main.f fib1.f):\n1PROGRAM MAIN 2INTEGER, PARAMETER :: N=7 3REAL(8) :: A(N) 4CALL FIB(A,N) 5PRINT*, A 6END PROGRAM Along with the non-scream-case version (gfortran main.f90 fib1.f90):\n1program main 2use fib1, only: fib 3implicit none 4integer, parameter :: n = 7 5real(8) :: a(n) 6call fib(a,n) 7print*, a 8end program main NumPy Distutils It would be remiss to not cover the most basic example of working with this. We can compile and try this out with the following:\n1f2py -m fib -c fib1.f 2python -c \u0026#34;import fib; import numpy as np; a=np.zeros(7); fib.fib(a); print(a); exit();\u0026#34; This is good enough, and there are standard extensions to expose more information to the binding, detailed in the documentation. That, however isn\u0026rsquo;t the point. The -c flag, convenient in many ways though it is, is a proxy for a rather more involved set of files.\n1mkdir blah 2f2py -m fib -c fib1.f --build-dir blah 3tree blah 4blah 5├── blah 6│ └── src.macosx-10.9-x86_64-3.9 7│ ├── blah 8│ │ └── src.macosx-10.9-x86_64-3.9 9│ │ ├── fortranobject.o 10│ │ └── fortranobject.o.d 11│ ├── fibmodule.o 12│ └── fibmodule.o.d 13├── fib1.o 14└── src.macosx-10.9-x86_64-3.9 15├── blah 16│ └── src.macosx-10.9-x86_64-3.9 17│ ├── fortranobject.c 18│ └── fortranobject.h 19└── fibmodule.c 20217 directories, 8 files Indeed, the standard usage is to use a setup.py:\n1from numpy.distutils.core import Extension, setup 2fibby = Extension(name = \u0026#39;fib\u0026#39;, 3sources = [\u0026#39;fib1.f\u0026#39;]) 4if __name__ == \u0026#34;__main__\u0026#34;: 5setup(name = \u0026#39;fib\u0026#39;, ext_modules = [ fibby ]) Which can then be built simply with:\n1python setup.py build 2ag -g .so 3# build/lib.macosx-10.9-x86_64-3.9/fib.cpython-39-darwin.so However this does not scale very well with more general build systems, though a custom command approach for CMake and scikit-build is a nice workaround by Nick Wogan.\nMeson and F2PY As scipy moves towards meson mainly driven by Ralf Gommers, it makes sense to look into its use to drive the compilation of an extension module as well. We start by generating the files for the interface.\n1f2py fib1.f -m fib 2Reading fortran codes... 3Reading file \u0026#39;fib1.f\u0026#39; (format:fix,strict) 4Post-processing... 5Block: fib 6Block: FIB 7Post-processing (stage 2)... 8Building modules... 9Building module \u0026#34;fib\u0026#34;... 10Constructing wrapper function \u0026#34;FIB\u0026#34;... 11FIB(A,[N]) 12Wrote C/API module \u0026#34;fib\u0026#34; to file \u0026#34;./fibmodule.c\u0026#34; Before we can compile this with meson we need to make a few changes:\n Replace FIB with fib in fibmodule.c (since C is case-sensitive) Copy f2py\u0026rsquo;s Fortran-C-Python translation unit and header files fortranobject.{c,h}  With these in place, we can now setup our meson.build.\n1project(\u0026#39;test_builds\u0026#39;,\u0026#39;c\u0026#39;,2version:\u0026#39;0.1\u0026#39;,3default_options:[\u0026#39;warning_level=3\u0026#39;])45add_languages(\u0026#39;fortran\u0026#39;)67# https://mesonbuild.com/Python-module.html8# requires atleast 0.46.09py_mod=import(\u0026#39;python\u0026#39;)10py3=py_mod.find_installation()11py3_dep=py3.dependency()1213incdir_numpy=run_command(py3,14[\u0026#39;-c\u0026#39;,\u0026#39;import os; os.chdir(\u0026#34;..\u0026#34;); import numpy; print(numpy.get_include())\u0026#39;],15check:true16).stdout().strip()1718inc_np=include_directories(incdir_numpy)1920py3.extension_module(\u0026#39;fib1\u0026#39;,21\u0026#39;fib1.f\u0026#39;,22\u0026#39;fib1module.c\u0026#39;,23\u0026#39;fortranobject.c\u0026#39;,24include_directories:inc_np,25dependencies:py3_dep,26install:true)Now we\u0026rsquo;re all set.\n1meson setup builddir 2meson compile -C builddir 3cd builddir 4python -c \u0026#34;import fib1; import numpy as np; a=np.empty(7); fib1.fib(a); print(a); exit();\u0026#34; This makes for a far more pleasant experience, since meson works well with larger projects as well. However, note the sizes (and complexities) of the C portions.\n1wc -l fortranobject.c fortranobject.h fibmodule.c 21107 fortranobject.c 3132 fortranobject.h 4372 fibmodule.c 51611 total Not something we\u0026rsquo;d probably like to mess around with. Standardization reduces the burden of user-code, as we will see in the next section.\nStandard Bindings The standard compliant approach was aptly covered almost a decade ago by Ondřej Čertík and codified in his fortran90 best practices guide. We start by updating our free-form code to be compliant with newer ISO_C_BINDING guidelines.\n1module fib1 2use iso_c_binding 3implicit none 4contains 5subroutine fib(a,n) bind(c,name=\u0026#39;c_fib\u0026#39;) 6integer(c_int), intent(in), value :: n 7integer(c_int) :: i 8real(c_double) :: a(n) 9do i=1, n 10if (i==1) then 11a(i) = 0.0d0 12else if (i==2) then 13a(i) = 1.0d0 14else 15a(i) = a(i-1) + a(i-2) 16end if 17end do 18end subroutine 19end module fib1 The key differences are the c_type declarations and the bind(c,name='blah') attribute. Also, to reduce the number of pointers for easy to copy types, we used value; which causes behavior similar to the pass-by-value paradigm. With this, the a C driver can be as simple as:\n1/* fibby.c */ 2#include\u0026lt;stdlib.h\u0026gt;3#include\u0026lt;stdio.h\u0026gt;4 5void c_fib(double *a, int n); 67int main(int argc, char* argv[argc+1]) { 8puts(\u0026#34;Fortran fib from C:\u0026#34;); 9int n=7; 10double a[n]; 11c_fib(\u0026amp;a, n); 12for (int i=0; i\u0026lt;n; i++) { 13printf(\u0026#34;%f \u0026#34;,a[i]); 14}; 15return EXIT_SUCCESS; 16} This can now be built via:\n1gfortran -c fib1.f90 2gcc fibby.c fib1.o ctypes Wrapper To wrap this over to python we have a few options. We can omit main altogether and rely on ctypes. So with:\n1/* cfib.c */ 2void c_fib(double *a, int n); Followed by:\n1gfortran -c fib1.f90 2gcc -fPIC -shared -o libfib.so cfib.c fib1.o We can work with this in python as:\n1from ctypes import CDLL, POINTER, c_int, c_double 2import numpy as np 3fibby = CDLL(\u0026#34;libfib.so\u0026#34;) 4a = np.zeros(7) 5fibby.c_fib(a.ctypes.data_as(POINTER(c_double)), c_int(7)) 6print(a) Which works in the same exact way as the previous approaches.\n We have gone from over a \\(1000\\) lines of C to one line3\n Cython wrapper However, we can skip even the one line of C, by using cython.\n1# fibby.pyx 2from numpy cimport ndarray 3import numpy as np 45cdef extern: 6void c_fib(double *a, int n) 78def fib(double a, int n): 9cdef ndarray[double, mode=\u0026#34;c\u0026#34;] ret = np.zeros(n) 10c_fib(\u0026amp;ret,n) 11return ret This is closer to f2py in the sense that the C code is simply hidden. It requires a compilation step typically done via setuptools.py, but, this is much nicer with meson since from 0.59.0, Cython is supported.\n1project(\u0026#39;test_builds\u0026#39;, \u0026#39;c\u0026#39;, 2version : \u0026#39;0.1\u0026#39;, 3default_options : [\u0026#39;warning_level=3\u0026#39;]) 45add_languages(\u0026#39;fortran\u0026#39;, \u0026#39;cython\u0026#39;) 67py_mod = import(\u0026#39;python\u0026#39;) 8py3 = py_mod.find_installation() 9py3_dep = py3.dependency() 1011py3.extension_module(\u0026#39;fibby\u0026#39;, 12\u0026#39;fibby.pyx\u0026#39;, 13dependencies : py3_dep 14) As before.\n1meson setup bdircython 2meson compile -C bdircython 3cd bdircython 4python -c \u0026#34;import fibby; import numpy as np; a=np.empty(7); b=fibby.fib(a); print(b); exit();\u0026#34; Note the difference in the calling semantics here. Also, recall that .pyx files are actually used to generate C code.\n1cython fibby.pyx 2wc -l fibby.c 36376 fibby.c This is vastly more code compared to the f2py generated wrapper code; which seems pretty unattractive.\nPython-C API Neither ctypes nor cpython is as friendly/clean as loading an extension module, so it makes more sense to actually write a proper Python-C extension module for a more fair comparison. This however, involves quite some reference counting and familiarity with the NumPy-C API. Actually it is not too bad for the function we are considering. Even a well commented and naive hand crafted version clocks in below 90 lines of code.\n1#ifndef PY_SSIZE_T_CLEAN 2#define PY_SSIZE_T_CLEAN 3#endif /* PY_SSIZE_T_CLEAN */4 5#include \u0026#34;Python.h\u0026#34;6#include \u0026#34;numpy/ndarrayobject.h\u0026#34;7#include \u0026#34;numpy/ufuncobject.h\u0026#34;8 9static PyMethodDef FibbyMethods[] = { 10{NULL, NULL, 0, NULL} 11}; 1213void c_fib(double *a, int n); 1415/* The loop definition must precede the PyMODINIT_FUNC. */ 1617static void double_fib(char **args, npy_intp *dimensions, 18npy_intp* steps, void* data) 19{ 20int i; // Standard integer is fine here 21 npy_intp n = dimensions[0]; 22char *in = args[0], *out = args[1]; 23npy_intp in_step = steps[0], out_step = steps[1]; 2425double apointer[n]; 2627for (i = 0; i \u0026lt; n; i++) { 28apointer[i]=(double)in[i]; 29} 3031// Call the Fortran function 32 c_fib(apointer, n); 3334for (i = 0; i \u0026lt; n; i++) { 35/*BEGIN main ufunc computation*/ 36*((double *)out) = apointer[i]; 37/*END main ufunc computation*/ 3839in += in_step; 40out += out_step; 41} 42} 4344/*This a pointer to the above function*/ 45PyUFuncGenericFunction funcs[1] = {\u0026amp;double_fib}; 4647/* These are the input and return dtypes of fib.*/ 48static char types[2] = {NPY_DOUBLE, NPY_DOUBLE}; 4950static void *data[1] = {NULL}; 5152static struct PyModuleDef moduledef = { 53PyModuleDef_HEAD_INIT, 54\u0026#34;fibby\u0026#34;, /* module name */ 55NULL, /* module docstring */ 56-1, /* -1 means global state, no sub-interpreters */ 57FibbyMethods, /* Module level functions */ 58NULL, /* single phase initialization, NULL slots*/ 59NULL, /* traversal function for GC */ 60NULL, /* clear function for the GC */ 61NULL /* deallocation function for the GC */ 62}; 6364PyMODINIT_FUNC PyInit_fibby(void) 65{ 66PyObject *m, *fib, *d; 67m = PyModule_Create(\u0026amp;moduledef); 68if (!m) { 69return NULL; 70} 7172import_array(); 73import_umath(); 7475fib = PyUFunc_FromFuncAndData(funcs, data, types, 1, 1, 1, 76PyUFunc_None, \u0026#34;fib\u0026#34;, 77\u0026#34;Calls the fib.f90 Fibonacci subroutine\u0026#34;, 0); 7879d = PyModule_GetDict(m); 8081PyDict_SetItemString(d, \u0026#34;fib\u0026#34;, fib); 82Py_DECREF(fib); 8384return m; 85} Compiling this with meson is a snap.\n1project(\u0026#39;test_builds\u0026#39;, \u0026#39;c\u0026#39;, 2version : \u0026#39;0.1\u0026#39;, 3default_options : [\u0026#39;warning_level=3\u0026#39;]) 45add_languages(\u0026#39;fortran\u0026#39;, \u0026#39;cython\u0026#39;) 67py_mod = import(\u0026#39;python\u0026#39;) 8py3 = py_mod.find_installation() 9py3_dep = py3.dependency() 1011incdir_numpy = run_command(py3, 12[\u0026#39;-c\u0026#39;, \u0026#39;import os; os.chdir(\u0026#34;..\u0026#34;); import numpy; print(numpy.get_include())\u0026#39;], 13check : true 14).stdout().strip() 1516inc_np = include_directories(incdir_numpy) 1718py3.extension_module(\u0026#39;fibby\u0026#39;, 19\u0026#39;fib1.f90\u0026#39;, 20\u0026#39;fibbyhand.c\u0026#39;, 21include_directories: inc_np, 22dependencies : py3_dep 23) We can compile this much the same as the others.\n1meson setup bdircythonhand 2meson compile -C bdircythonhand 3cd bdircythonhand 4python -c \u0026#34;import fibby; import numpy as np; a=np.empty(7); b=fibby.fib(a); print(b); exit();\u0026#34; This has the lowest lines of code too.\n1wc -l fibbyhand.c 285 fibbyhand.c It is rather remarkable since the C code here is not optimized in the least. The twin for loops can almost certainly be handled more efficiently, and also from the point of view of the Fortran-C interface too, this is lacking. Nevertheless, it will suffice for our purpose here, which has more to do with wrapping automatically (and therefore rather naively) than generating optimal code (but that would be great).\nBenchmarks We can use hyperfine for basic benchmarks.\n1hyperfine \u0026#39;python -c \u0026#34;import fibby; import numpy as np; a=np.empty(7); b=fibby.fib(a); print(b); exit();\u0026#34;\u0026#39; --warmup 5 -s full -r 25 2Benchmark #1: python -c \u0026#34;import fibby; import numpy as np; a=np.empty(7); b=fibby.fib(a); print(b); exit();\u0026#34; 3Time (mean ± σ): 155.4 ms ± 9.7 ms [User: 133.1 ms, System: 40.2 ms] 4Range (min … max): 145.6 ms … 190.4 ms 25 runs Results for the various approaches are collected below in Table tbl:bench.\n\nTable 1: The results are for 25 runs with 5 warmup for np.empty(7) as the input     Command Mean [ms] Min [ms] Max [ms]     Handwritten NumPy-C-Fortran 126.0 ± 3.9 119.8 136.8   F2PY (F77) 129.1 ± 4.0 125.1 140.4   Cython 129.5 ± 6.8 121.4 149.1   F2PY (F90) 129.9 ± 5.1 123.9 145.8   ctypes 128.3 ± 7.8 122.7 159.8    As expected, ctypes is the slowest, since it requires the most massaging of inputs on the Python side. Surprisingly, loading the free form file with f2py was quite a bit slower than the f77 file. Given the complexity of the files involved, it is actually surprising that the handwritten wrapper shows a rather clear gain in performance; given its implementation; but nonetheless it supports the hypothesis that advances in the NumPy-C API and the Fortran-C interoperability has improved significantly over time.\nConclusions f2py, like Fortran, is here to stay. As the standards strive towards better compliance with C however, it also gets larger and rather more complicated 4. One thing which was not covered here was the actual ISO_Fortran_binding.h5 and other modifications to the C code while interpolating with Fortran itself.\nTo hold its place in the larger ecosystem, f2py will be essential in protecting the average user from the standard itself, automatically providing optimal bindings for users as initially planned by Pearu Peterson. Modernization efforts with Melissa Weber Mendonça, Ralf, Pearu, and others at Quansight (including the rest of the community and me) are sure to gravitate towards generating better, more reliable interfaces to bridge the gaps between Fortran and Python as supported by the NumPy core developers6. Clearly, the way forward is to work in lockstep with the NumPy-C API and recent Fortran standards.\n  Make, autotools and the like are definitely worse than np.distutils\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n It is the first example from the canonical reference\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This is a bit unfair, the complexity has been offloaded to the compiler, and they\u0026rsquo;re notorious for not supporting newer Fortran standards\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Though it is and will remain far more svelte compared to C++\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Some examples from the community are here\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Including a special shout out to Charles Harris for reviewing unreasonably large DOC PRs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/numpy-meson-f2py/","tags":["python","fortran","numpy","f2py","rambling"],"title":"NumPy, Meson and f2py"},{"categories":["code"],"contents":"nix aside1, I recently shifted to using asdf to manage different language versions.\n1git clone https://github.com/asdf-vm/asdf.git ~/.asdf --branch v0.8.1 The main reason to prefer asdf over competing language specific options like rvm or pyenv or nvm and company is simply uniformity of the interface. This can be coupled with zinit snippet OMZ::plugins/asdf for loading the completions. Note that the installation steps can take a while especially if openssl is being installed.\nConfiguration Actually one weird aspect of asdf is that it pollutes $HOME by default instead of respectfully storing its configuration in $HOME/.config/asdf like every other program. To change this; exporting ASDF_CONFIG_FILE works. Essentially a zsh configuration with zinit would look like this2:\n1if [[ ! -d ~/.asdf ]]; then 2mkdir ~/.asdf 3git clone https://github.com/asdf-vm/asdf.git ~/.asdf 4fi 56export ASDF_CONFIG_FILE=\u0026#34;~/.config/asdf/asdfrc\u0026#34; 7zinit snippet OMZ::plugins/asdf asdf has an option; legacy_version_file = yes which is meant to ensure that existing solutions (e.g. .nvmrc or .node-version) are compatible. There is also $HOME/.tool-versions for trickle-down language versions; that is language-version pairs defined in such files are activated for the directory and lower sub-directories. The rest of a reasonable configuration (described here) might look like:\n1legacy_version_file = yes 2always_keep_download = no Ruby This plugin leverages ruby-build and so can take a pick up an extended set of environment variables; it supports .ruby-version files as well.\n1asdf plugin add ruby 2asdf list all ruby 3asdf install ruby 3.0.2 4asdf global ruby 3.0.2 # Convenience NodeJS This plugin respects .nvmrc and .node-version files.\n1asdf plugin add nodejs 2asdf global nodejs latest 3asdf install nodejs latest Python Builds with python-build.\n1asdf plugin add python 2asdf install python 3.9.7 3asdf global python 3.9.7 Direnv I\u0026rsquo;m a huge fan of direnv; and a fantastic little plugin (with neat hyperfine benchmarks) for asdf removes a layer of shim indirection while playing nicely with direnv.\n1asdf plugin-add direnv 2asdf install direnv latest 3asdf global direnv latest This comes with an associated set of shell profile instructions:\n1# File: ~/.zshrc 2# Hook direnv into your shell. 3eval \u0026#34;$(asdf exec direnv hook zsh)\u0026#34; 4# A shortcut for asdf managed direnv. 5direnv() { asdf exec direnv \u0026#34;$@\u0026#34;; } Along with a commiserate $HOME/.config/direnv/direnvrc requirement:\n1source \u0026#34;$(asdf direnv hook asdf)\u0026#34; Now every new .envrc can start with use asdf which will now speed up evaluations.\nConclusions This method appears to be more robust than remembering the various idiosyncrasies and logic of a host of other tools.\n  This was during the build plan dynamism RFCs which themselves were symptomatic of the {yarn,composer,node,*}2nix issues\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This structure can be seen in my own Dotfiles as well\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/snippets/prog-lang-man/","tags":["shell","lang"],"title":"Programming Language Management"},{"categories":["code"],"contents":"My personal favorite for watching files and running context sensitive commands is to use the lovely filewatcher CLI utility written in Ruby.\n1gem install filewatcher 2gem install filewatcher-cli This can then be used with:\n1filewatcher \u0026#39;**/*.js\u0026#39; \u0026#39;node $FILENAME\u0026#39; However this hasn\u0026rsquo;t been updated in a while now and fails on newer versions of Ruby. So now I use watchexec.\n1cargo install watchexec-cli 2watchexec -w source/f2py \u0026#39;make html\u0026#39; ","permalink":"https://rgoswami.me/snippets/watch-files/","tags":["productivity","cmdline","ruby","rust"],"title":"Watching Files"},{"categories":["conferences"],"contents":" A meta-post on an oral presentation around Fortran and languages\n Background I was a participant at the 2021 Les Houches school of physics on \u0026ldquo;Green\u0026rsquo;s function approach to multiple scattering theory in electronic structure and spectroscopies\u0026rdquo;. I opted to give a student talk on programming languages and elementary functions as a cautionary tale for IEEE 754.\nDetails  Title Programming Language Interstices  Abstract  In this short talk I will discuss the changing landscape of programming languages from the viewpoint of “mixed language” bindings. That is, the representations of various programming languages and their standardisations to aid making library calls and generating wrappers between languages. In particular I will demonstrate the LFortran compilers’ ASR (Abstract Semantic Representation) and outline its usage for generating strictly correct Python wrappers; and discuss recent changes and future plans for the venerable f2py wrapper generator. f2py is currently part of Python’s numpy and is best known for being used extensively to generate scipy’s calls to BLAS and LAPACK; thus it forms a cornerstone of the fast growing Scientific Python ecosystem. Error analysis and the pitfalls of precision take a central role. Time permitting, some discussion of the Python-C and Fortran-C bindings will also take place. The context of these is to be concrete use cases directly applicable to the applied computational sciences.\n Other Content Anything tagged Fortran on this site.\nSlides  Best viewed here using a browser (in a new tab) A pdf copy of the slides are embedded below The orgmode source is here on the site\u0026rsquo;s GH repo   Video TBD\nThoughts This was the first time in a long while since I gave an in-person short presentation. I should have used a timer.\n","permalink":"https://rgoswami.me/posts/leshouches-2021-meta/","tags":["presentations","ramblings","fortran"],"title":"Presentation Supplements for Greens Function School"},{"categories":["personal"],"contents":" Towards sustainable FOSS with Quansight Labs\n Background Rumors of my affiliation with Quansight Labs are indeed true. I have had the pleasure of joining the Labs division as a software engineer 1, whose philosophy is fantastically and succintly described by their director Dr. Ralf Gommers. This means, in nutshell, that in my spare time I will be able to contribute to the continued maintenance of key FOSS projects.\nWhy? A reasonable question. I have been contributing to and managing FOSS projects for over ten years now under the assumption of one day aspiring to be described by Fig. 1. Over time however, there have been consequences which have required a restructuring of how I view my commitments. The projects I have had the pleasure to work have grown in scope to the point where it became increasingly difficult keep contributing to at a sustained pace.\n\n Figure 1: A vision of my understanding of FOSS maintenance courtesy xkcd\n  Interviews The interview process was a key factor in my decision making process. In particular, I was able to gainfully grow as a programmer and person through each one; which was incredibly refreshing.\n Ralf Gommers Ralf and I discussed several projects which might benefit from my commitment along with wider contexts surrounding FOSS ecosystems; I was also reintroduced to the meson build system Chris Ostrouchov We discussed the nix ecosystem and how it could and should be used to gain more ground in the High Performance Computing community 2 Ivan Yashchuk I met with Ivan, and we discussed parallelism in C++, the standards committee, and he introduced me to SYCL  These interviews convinced me that I would continue to grow during the proposed projects.\nProjects For starters I shall be working on the f2py section of Dr. Melissa Weber Mendonça\u0026rsquo;s CZI grant with Dr. Pearu Peterson.\nMisc Interestingly, I realized soon after I joined, that some of my old GSoD mentors Aaron Mueuer and Amit are also at Quansight, which was very heartening to know as well.\nConclusions This is a huge honor and will help focus and enhance my efforts towards helping the wider scientific computing community.\n  For 20-25 hours a week\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Which tied into a larger discussion taking place also at Tweag (and the Icelandic HPC cluster) revolving around Nix-HPC\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/joining-quansight-labs21/","tags":["history","ramblings","fortran"],"title":"Regarding Quansight Labs"},{"categories":["notes"],"contents":" Directed recollections of the GSoC21 timeline\n Background If the last ten weeks of weekly posts have not made it clear; as a student developer in the Google Summer of Code in the year 2021; I was privileged to work on the LFortran compiler alongside Gagandeep Singh and Thirumalai Shaktivel under the fantastic mentorship of Ondřej Čertík under the Fortran-Lang organization.\nSeries This post is going to be generally expositionary in tone; but the series is also reproduced.\n  GSoC21 W1: LFortran Kickoff\n  GSoC21 W2: LFortran Unraveling\n  GSoC21 W3: Kind, Characters, and Standards\n  GSoC21 W4: LFortran, Backends and Bugs\n  GSoC21 W5: LFortran Design Details and minidftatom\n  GSoC21 W6: LFortran ASR and Values\n  GSoC21 W7: LFortran Workflow Basics\n  GSoC21 W8: LFortran Refactors, and Compile Time Evaluations\n  GSoC21 W9: LFortran Bug Hunting Bonanza\n  GSoC21 W10: LFortran Runtime Library Design\n  Pencil Pushing This section describes the overall work done in a manner designed to delight minds enamoured by brevity and numbers. It is by no means a replacement for going over the series.\nProject Overview Given that the LFortran compiler is implemented in a manner reminiscent of both CPython and clang, the main goal of the project was to leverage the compiler to parse, compile, link and run a production code base. For this, the code base chosen was Čertík, Pask, and Vackář (2013), which was further trimmed down to a miniature version here. Practically speaking, this involved tackling the Fortran standard itself, segregating the functionality at various stages in the \u0026ldquo;automatic coding\u0026rdquo; pipeline; and implementing standardese. Also, a lot of ****debugging**** naturally.\nStatistics For a single overview of the work done; the following table of contributions sums things up nicely.\nTable 1: Code contributions to LFortran repository as of 2021-08-19T23:16:22+00:00\n  Contribution Open Closed/Merged Total\n Merge 10 22 36 Requests\nIssues 20 11 31 This is an ever evolving set of metrics, and the finer details of these contributions can be found by following the weekly series above. There were some extra contributions across the other repositories, involving minor fixes and some work done towards integrating with Meson.\nMR Overview These are better described in the weeks they were made; and only the highlights of the 22 will be briefly alluded to here.\n Implement more kind() [997] This was the first compile time enhancement / bugfix Restructures and Grammar changes [1024, 1037] Code quality enhancement for more readable ASR values Binary Operations [1045, 1072, 1080] Together these MRs implemented the population of value members for integer and real binary operations Split ast_to_asr [1096] A major code refactor for improved readablity and rapid development tiny implementations [1107, 1112] Together these implement the tiny intrinsic at compile time LLVM values [1131] This enhancement patchset furthered the population of value members at the backend Compile time functions (5) real [1114], selected_kinds [1154], floor [1176], int [1169], Integer2Real cast [1162] Runtime sin [1167, 1173] Together these showcase the hopes and dreams of the current runtime library implementation Testing framework for runtime benchmarks [1194] Jinja based rapid templating setup for benchmarking the runtime functions Bugfixes [1015, 1087] These span various growing pains, and were debugged mostly via the stacktrace  Reflections So, after weeks of thousands of words of (semi-)popular writing, countless lines of code and hours of fantastic meetings; where are we and where is LFortran and where do we go from here?\n This is a rather vague point, given that we have a fantastic set of issues Also, the Zulip instance is great for getting in touch with the community  One of the nicer parts of being associated with the project in such a visible manner was that I was able to funnel new talent as well    Target Practice In terms of finally compiling our minimum viable computational chemistry project, minidftatom; described in the Week 5 post, we are tantalizingly close; so much so that is a mere bugfix or two away; as can be seen from the state of this MR. Naturally my choice of issues are biased towards my own interests:\n Better benchmarking of the pure Fortran intrinsics Finish improving the CI (ninja) Meson integration More work on the runtime library  We plan to have multiple backends for the runtime (the relevant wiki) We need also to implement the compile time intrinsics in a more streamlined manner (a RFC issue for the same)   Document all things!  API documentation and other bugaboos are pet interest of mine    Major Accomplishments Perhaps more than anything else; the design strategies worked out over the twelve week span will have the most lasting impact. In particular:\n Compile Time Intrinsics These are the C++ equivalent of the Fortran standard, and are evaluated when possible at compile time (the backend then just uses the value) Runtime Library Design Inspite of strong community opposition to implementing elementary functions in pure Fortran; we ended up with a very well designed elegant runtime design I am immensely satisfied with. Broadly speaking (detailed in the [Week 10 post](/posts/gsoc21-w10/)), the design encompasses impure parts of the library, which cannot be implemented in pure Fortran, and pure sections which can be implemented without any external calls  Both of these intrinsic designs were accompanied by practical implementations and a template evolved over time to aid future contributions. One of the most interesting parts of the runtime design was using certifiably correct polynomial approximations generated with Sollya.\nImplementing features at compile time required an intimate understanding of the ASR, along with the way it is generated and consumed. This took weeks of effort at first; especially given the generated nature of much of the underlying code.\nLearning Compiler design touches almost every aspect of the compuation; from hardware to linkers, and I managed to expand my understanding of several key concepts early on with Cooper and Torczon (2011), Muller (2016), Levine (2000) and a host of other texts referenced in the early weeks.\nA special mention must be made of \u0026ldquo;J3/18-007r1 (F2018 Interpretation Document)\u0026rdquo; (n.d.), which happens to be the first full language standard I have read almost cover to cover.\nMisc Logs Like every week; there are some notes of the implementations done this week as well, focused around the ****runtime intrinsic**** functions.\nExp The method to calculate the exponent is largely taken from Detrey and de Dinechin (2005). Basically this needs a reduction to \\(-\\frac{log 2}{2}, \\frac{log 2}{2}\\) but just to get the ball rolling an initial terrible approximation is essentially:\n floor(x), and compute e to the integral power in a loop For the remaining part, use a 16th order Remez between [0,1], something like p=remez(exp(x),16,[0,1],default,1e-16,1e-30);  This is clearly a terrible idea, and there are far better approaches, however, it will suffice for now.\nBetter Trignometric Functions Visual inspection and basic trignometry (along with series expansions) brings about the understanding that our polynomial approximation should only contain odd terms. This is easily accomplished in Sollya with fpminimax.\n1P = fpminimax(sin(x),[|1,3,5,7,9,11,13,15,17,19,21|],[|1,D...|],[-pi/2;pi/2], fixed, relative); We can also compute the supnorm of the approximation over the interval:\n1supnorm(P,sin(x),[-pi/2;pi/2],relative,2^(-40)); Which for the seventh order polynomial above turns out to be around 2E-17 throughout. The double word (DD) variant is not worth the extra effort, since the real issue with the approximation here is still the range reduction.\nConclusions I like writing; but somehow this was one of the most difficult things to write about; mostly because I\u0026rsquo;m still itching to get back to the codebase. I believe this is probably a good sign. Surprisingly, GSoC21 turned out to be one of my most enriching experiences as a developer till date. Unsurprising given the fantastic people involved, actually. There is very much left to be said about LFortran, some of which will be said by me at the upcoming FortranCon 2021, and in other outlets.\nReferences Čertík, Ondřej, John E. Pask, and Jiří Vackář. 2013. \u0026ldquo;Dftatom: A Robust and General Schrödinger and Dirac Solver for Atomic Structure Calculations.\u0026rdquo; Computer Physics Communications 184 (7): 1777\u0026ndash;91. https://doi.org/10.1016/j.cpc.2013.02.014.\n Cooper, Keith, and Linda Torczon. 2011. Engineering a Compiler. Elsevier. https://books.google.com?id=_tgh4bgQ6PAC.\n Detrey, J., and F. de Dinechin. 2005. \u0026ldquo;A Parameterized Floating-Point Exponential Function for FPGAs.\u0026rdquo; In Proceedings. 2005 IEEE International Conference on Field-Programmable Technology, 2005., 27\u0026ndash;34. Singapore, China: IEEE. https://doi.org/10.1109/FPT.2005.1568520.\n \u0026ldquo;J3/18-007r1 (F2018 Interpretation Document).\u0026rdquo; n.d. Accessed June 28, 2021. https://j3-fortran.org/doc/year/18/18-007r1.pdf.\n Levine, John R. 2000. Linkers and Loaders. San Francisco: Morgan Kaufmann.\n Muller, Jean-Michel. 2016. Elementary Functions. Boston, MA: Birkhäuser Boston. https://doi.org/10.1007/978-1-4899-7983-4.\n  ","permalink":"https://rgoswami.me/posts/gsoc21-fin-reprot/","tags":["gsoc21","fortran","rambling"],"title":"GSoC21 LFortran and Computational Chemistry"},{"categories":["notes"],"contents":" The road to accurate elementary intrinsic functions is paved with IEEE/ISO standards\n Background Serialized update for the 2021 Google Summer of Code under the fortran-lang organization, mentored by Ondrej Certik.\nSeries This post is part of a series based around my weekly GSoC21 project check-ins.\n GSoC21 W1: LFortran Kickoff GSoC21 W2: LFortran Unraveling GSoC21 W3: Kind, Characters, and Standards GSoC21 W4: LFortran, Backends and Bugs GSoC21 W5: LFortran Design Details and minidftatom GSoC21 W6: LFortran ASR and Values GSoC21 W7: LFortran Workflow Basics GSoC21 W8: LFortran Refactors, and Compile Time Evaluations GSoC21 W9: LFortran Bug Hunting Bonanza GSoC21 W10: LFortran Runtime Library Design \u0026lt;\u0026ndash; You are here!  Logistics  Met with Ondrej daily from Sunday till Friday  On Sunday we finished the compile time evaluation design (partially)  Ondrej finished the rest overnight   On Monday, we started co working on runtime implemenations  Also added some more compile time stuff and pointers to clean things up Ondrej fixed a longstanding bug in the deserialise of serialized mod   On Wednesday there was more scaffolding work done on the generics  Added another implicit cast   On Thursday we implemented an all fortran sin function  Just as accurate as gfortran Compiles with LFortran Also fixed another serialization bug (floats were being truncated)   On Friday we redesigned the runtime library  Also discussed future plans and longterm goals Opened discussions around the runtime library and elementary function accuracy on the Zulip instance Considered adding fortran versions of LAMMPS (F77 and F90) to the compilation plans      Overview This was most definitely my most productive week by far.\nNew Merge Requests  llvm: More values when possible (1113) Backend values from compile time evaluations kind: Compile time (1132) A rather thorny refactor; but one with great benefits asr: Clean up extract_kind (1136) Fruitful usage of the new functions Compile Time Intrinsic: Real (1114) A cast based approach selected_kinds: Body visitors (1167) Possibly my favorite MR, a nice pure fortran implementation backed by a rough sollya approximation Initial sin implementation (1167) Possibly my favorite MR, a nice pure fortran implementation backed by a rough sollya approximation sin: Rework runtime to get better approximations (1173) Possibly my favorite MR, a nice pure fortran implementation backed by a rough sollya approximation Compile time floor (1176) Fairly standard implementation requiring casts Integer2real (1162) An implicit cast which had not previously been implement Compile time int (1169) Along the lines of real() Sincos tests (1194) Benchmarking and setting up of the new test harness  Freshly Assigned Issues  Semantics: Implement correct handling of PARAMETER (510) ASR Pass: Order functions (507) Semantics: Implement FLOOR (A [, KIND]) (506) Semantics: Implement MODULO (A, [, KIND]) (504) [RFC] Compile Time Intrinsic Design (501) Strange Runtime issue with Implicit Casts (500) Implicit cast for parameter (499) Better handling of intrinsic module names (494)  Additional Tasks Will write a closing report for next week.\nMisc Logs This was a really packed week. Much of the discussion has already taken place in the relevant issues and MRs; but some notes are in order for posterity.\nItems to fix  Some compile time function implementations are not standard compliatn  real   Implicit casting rules fail for certain cases  Notably for all Arrays    Strange bug when intrinsic functions are called on implicit cast\nGeneric Proceedures This was fun to work on.\nFortran Runtime Intrinsics A long term goal is to have a dog-fooded runtime library. We zeroed in on this design over on the wiki a few weeks ago. Now with generic proceedures in; we\u0026rsquo;ve begun implementing some of the intrinsic functions.\nSimple Functions The simplest ones are of course trivial:\n1abs Trignometric functions For more \u0026ldquo;mathematical\u0026rdquo; methods, we needed a template. One we found which was compatible with our license is the fdlibm versions. Consider for sin:\n We have s_sin and also k_sin  Written in C and assembly    Older compilers are much more readable compared to the newer ones.\nTrignometric Functions The quote is motivated by my (limited) reading of two texts in the field of implementing elementary functions, namely Muller (2016) and Muller et al. (2018).\nBrainstorming Sin  This and then the simpler version here Multiple interval based approximation used for AMD\u0026rsquo;s optimized sin  We were looking for a pedagogical, simple approach to calculating sin. Unsurprisingly, the entire ecosystem of compilers seems to mostly follow one implementation from the early 90s. Which is great, but Fortran doesn\u0026rsquo;t really implement IEEE 754\u0026rsquo;s argument reduction routine (a.k.a. quadmath). It turns out with a little massaging; the pedagogical approach presented in this blog post by Christer Ericson can be adapted for use.\nWe needed to do slightly better. For starters, the errors of around \\(10^{-16}\\) could be stomached, but not the ones around \\(10^{-10}\\). Think of this as the same sort of problem as the standard fitting of a potential energy surface.\nHow do we test these? Import from the intrinsic, and rename to a different symbol, then run a loop.\nSollya and Function Approximations Rather than go rooting around the annals of outdated compilers; and instead of boldly copying from existing libraries; an interesting approach is to calculate precise approximations including the Remez (or minmax), Taylor approximations, and Chebyshev approximations.\nThankfully testing this is trivial with Sollya. Since it does nothave a terminal inbuild we need rlwrap too.\n1nix-shell -p sollya rlwrap Consider a trial implementation:\n1! lfortran_intrinsic_sin.f90 2module lfortran_intrinsic_sin 3use, intrinsic :: iso_fortran_env, only: sp =\u0026gt; real32, dp =\u0026gt; real64 4implicit none 56interface sin 7module procedure dsin 8end interface 910contains 1112elemental real(dp) function dsin(x) result(r) 13real(dp), parameter :: pi = 3.1415926535897932384626433832795_dp 14real(dp), intent(in) :: x 15real(dp) :: y 16integer :: n 17if (abs(x) \u0026lt; pi/2) then 18r = kernel_dsin(x) 19else ! fold to pi/2 20! https://realtimecollisiondetection.net/blog/?p=9 21 y = modulo(x, 2*pi) 22y = min(y, pi - y) 23y = max(y, -pi - y) 24y = min(y, pi - y) 25r = kernel_dsin(y) 26end if 27end function 28! Implement kernel_dsin with Sollya (read post) The general approach to testing this is then:\n1! test_sina.f90 2program main 3use :: lfortran_intrinsic_sin, only: sin2 =\u0026gt; dsin 4real(kind=8) :: x,y 5integer(kind=8) :: i 6do i=1,1000000 7y = real(i)/100.0 8print*, y, abs(sin(y)-sin2(y)) 9end do 10end program Which can be compiled with:\n1gfortran -ffree-line-length-none lfortran_intrinsic_sin.f90 test_sina.f90 \u0026amp;\u0026amp; ./a.out Where the line length is important since we want to be able to copy and paste expressions from Sollya.\nRemez The test expressions can be generated as follows:\n1nix-shell -p sollya rlwrap 2rlwrap -A sollya 3prec=120; 4f=sin(x); 5d=[-pi/2;pi/2]; 6p=remez(f,16,d,default,1e-16,1e-30); 7p; 81.430896392112983438690712915149618875e-17 + x * (0.9999999999999989714281994951487325976 + x * (-3.349530544567829511297283045847078806e-16 + x * (-0.1666666666666470131512533841331726124 + x * (2.06038248351960358085265034956173799e-15 + x * (8.33333333322297677610158866083573121e-3 + x * (-5.200037492550081347598442689963074914e-15 + x * (-1.984126981335937217532222096272076263e-4 + x * (6.779661505197324337448766161086156236e-15 + x * (2.755731547177283303366066506722157076e-6 + x * (-4.994033469644991382509162854527493863e-15 + x * (-2.505182167856013032537240066679691216e-8 + x * (2.101039405474559290547827191709051192e-15 + x * (1.604654097883256924434495617086734883e-10 + x * (-4.71033700061518989435986419744537652e-16 + x * (-7.356804698944816248097669067062646244e-13 + x * 4.36551332303930597866240036869066221e-17))))))))))))))) Where the signature of remez is func, degree, interval, default=1, quality=1e-5, bounds=[0;infty]; Now we can implement kernel_dsin as follows:\n1elemental real(dp) function kernel_dsin(x) result(res) 2real(dp), intent(in) :: x 3res = 1.430896392112983438690712915149618875e-17_dp + x * (0.9999999999999989714281994951487325976_dp + x * (-3.349530544567829511297283045847078806e-16_dp + x * (-0.1666666666666470131512533841331726124_dp + x * (2.06038248351960358085265034956173799e-15_dp + x * (8.33333333322297677610158866083573121e-3_dp + x * (-5.200037492550081347598442689963074914e-15_dp + x * (-1.984126981335937217532222096272076263e-4_dp + x * (6.779661505197324337448766161086156236e-15_dp + x * (2.755731547177283303366066506722157076e-6_dp + x * (-4.994033469644991382509162854527493863e-15_dp + x * (-2.505182167856013032537240066679691216e-8_dp + x * (2.101039405474559290547827191709051192e-15_dp + x * (1.604654097883256924434495617086734883e-10_dp + x * (-4.71033700061518989435986419744537652e-16_dp + x * (-7.356804698944816248097669067062646244e-13_dp + x * 4.36551332303930597866240036869066221e-17_dp))))))))))))))) 45end function Note that we need to use double precision for the numbers, so we can replace SPACE+ with _dpSPACE+. We would like to also get some statistics of the output, in bash just becuse we can 1.\n1gfortran -ffree-line-length-none lfortran_intrinsic_sin.f90 test_sina.f90 \u0026amp;\u0026amp; ./a.out \u0026gt; remez16 2awk \u0026#39;NR == 1 || $2 \u0026lt; min {line = $0; min = $2}END{print line}\u0026#39; remez16 3awk \u0026#39;NR == 1 || $2 \u0026gt; max {line = $0; max = $2}END{print line}\u0026#39; remez16 Taylor and Chebyshev To extract information from the \u0026ldquo;certified\u0026rdquo; functions in Sollaya we need to remember that the return value is a list.\n1nix-shell -p sollya rlwrap 2rlwrap -A sollya 3prec=120; 4f=sin(x); 5d=[-pi/2;pi/2]; 6p=chebyshevform(f,16,d); 7p[2]; 8p[0]; 9quit; Similarly taylorform can be used to get Taylor series expansions. Given that we know exactly what we want to generate; we can also simply use sollya with an input script:\n1sollya \u0026lt; taylorform16 Where taylorform16 contains:\n1f=sin(x); 2d=[-pi/2;pi/2]; 3p=taylorform(f,16,d); 4p[0]; This workflow can be easily extended into a script based runner.\nBetter Polynomials Some basic understanding of series expansions and / or visual inspection would make it clear that for sin at-least; the odd terms are essentially \\(0\\). Sollya can handle these; as described in Muller (2016). We opted to go the rounding error route.\nRounding Errors With Ondrej, we were able to isolate the inaccuracy in our pure fortran implementation and we realized it stemmed from the manner in which our argument reduction is carried out. This is a well known problem with attempted solutions from 1980s Payne and Hanek (1983) (including a related implementation in BSD)2. In particular, as per this WG document of the SunPro group Ng (2006) describing the cannonical implementation; the error is actually from the division and difference. This is also touched upon elsewhere in the literature, Brisebarre et al. (2005) and Defour et al. (n.d.), along with Muller et al. (2018) . Naturally one of first places to check is the relevant ISO draft standard on elementary functions (N462Pdf?) .\n Figure 1: The ISO/IEC FCD 10967-2.4:1999(E) suggestion for sin\n  This isn\u0026rsquo;t immediately actionable; but might be something for later. Naturally the discussions here in context of IEEE 754 (IEEEStd754?) (which the Fortran standard does not fully support) and presumes basic knowledge of floating point numbers Goldberg (1991) . The clearest explanation for requiring higher precision for some constants in these reduction algorithms is given nicely by the draft standard:\n For reduction of an argument given in radians, implementations use one or several approximate value(s) of π (or of a multiple of π), valid to, say, n digits. The division implied in the argument reduction cannot be valid to more than n digits, which implies a maximum absolute angle value for which the reduction yields an accurate reduced angle value.\n Benchmarking and Testing To make the benchmarking of new lfortran runtime intrinsic functions as painless as the existing test runner framework; I ended up implementing a new framework for these. Currently this takes different compiler backends and a TOML file along with some Jinja2 templates and makes a CMake based project which is built and gives out a grid of points along with the gfortran values and abs differences. Some things to do there:\n Plot things automatically Use non-uniform grids Other QoL changes  This is really easy to work with:\n1cd src/runtime/pure/benchmarks 2nix-shell -p \u0026#39;python38.withPackages(ps: with ps; [seaborn matplotlib click jinja2 tomli pandas])\u0026#39; 3# Or just micromamba install seaborn matplotlib pandas click jinja2 tomli 4python gen_test.py --execute --build --compiler \u0026#34;gfortran\u0026#34; The TOML file has simple naming rules:\n1[[trig]] # [name of lfortran_MODULE.f90] 2fname = \u0026#34;sin\u0026#34; # Function to generate test for; must be the same as for gfortran 3only = \u0026#34;dsin\u0026#34; # if not testing same function 4range = 20 # [-range, +range] 5stepsize = 0.01 As does the template:\n1program {{ fname }}_test 2use, intrinsic :: iso_fortran_env, only: sp =\u0026gt; real32, dp =\u0026gt; real64 3use :: lfortran_intrinsic_trig, only: {{ fname }}2 =\u0026gt; {{ fname }} 4real(dp), parameter :: pi = 3.1415926535897932384626433832795_dp 5real(dp) :: ir, gf, lf 6print*, \u0026#34;Compiled with {{ compiler }}\u0026#34; 7print*, \u0026#34;x \u0026#34;, \u0026#34;gfortran_{{fname}}(x) \u0026#34;, \u0026amp; 8\u0026#34;lfortran_{{fname}}(x) \u0026#34;, \u0026#34;abs_error \u0026#34; 9ir = -{{ range }} 10do while (ir\u0026lt;={{ range }}) 11gf = {{ fname }}(ir) 12lf = {{ fname }}2(ir) 13print*, ir, gf, lf, abs(gf-lf) 14ir = ir + {{ stepsize }} 15end do 16end program Conclusions Rather than move forward with the closing comments; and going into more detail about the manner in which further sollya based polynomial approximations are being looked into; I will end this post here; as a trailer and invitation to the closing report. This has been the last of the weekly updates. This doesn\u0026rsquo;t mean anything will change other than the format. I quite like keeping track of the work I do in this manner; keeps me honest and not complacent 3. For those of you following along; this has been a fun ride; for those who come to this in the future; feel free to follow along from the beginning and drop comments below.\nReferences Brisebarre, Nicolas, David Defour, Peter Kornerup, and Jean-Michel Muller. 2005. \u0026ldquo;A New Range-Reduction Algorithm.\u0026rdquo; IEEE TRANSACTIONS ON COMPUTERS 54 (3): 9.\n Defour, P David, Peter Kornerup, Jean-Michel Muller, and Nathalie Revol. n.d. \u0026ldquo;A New Range Reduction Algorithm,\u0026rdquo; 13.\n Goldberg, David. 1991. \u0026ldquo;What Every Computer Scientist Should Know about Floating-Point Arithmetic.\u0026rdquo; ACM Computing Surveys 23 (1): 5\u0026ndash;48. https://doi.org/10.1145/103162.103163.\n Muller, Jean-Michel. 2016. Elementary Functions. Boston, MA: Birkhäuser Boston. https://doi.org/10.1007/978-1-4899-7983-4.\n Muller, Jean-Michel, Nicolas Brunie, Florent de Dinechin, Claude-Pierre Jeannerod, Mioara Joldes, Vincent Lefèvre, Guillaume Melquiond, Nathalie Revol, and Serge Torres. 2018. Handbook of Floating-Point Arithmetic. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-76526-6.\n Ng, K C. 2006. \u0026ldquo;Argument Reduction for Huge Arguments: Good to the Last Bit.\u0026rdquo;* Introduction*, 8.\n Payne, Mary H., and Robert N. Hanek. 1983. \u0026ldquo;Radian Reduction for Trigonometric Functions.\u0026rdquo; ACM SIGNUM Newsletter 18 (1): 19\u0026ndash;24. https://doi.org/10.1145/1057600.1057602.\n    From this stackoverflow post of course\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n From this blog post by Bert Hubert\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n I maintain similar notes for my research, but those sadly never see the light of day\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/gsoc21-w10/","tags":["gsoc21","fortran","rambling"],"title":"GSoC21 W10: LFortran Runtime Library Design"},{"categories":["conferences"],"contents":" A meta-post on social events and my presentation at TUG21\n Background Much the same as the rationale behind my other presentation meta-posts, but with some added shilling; since TUG is fantastic. Plus like the last year, I was gven the opportunity to coordiate social events with Jennifer Claudio.\nSocial Events  Since the 41st TUG in 2020; there is a Zulip instance which is active all year round 1  Can be joined here   This year there was a Topia instance for more \u0026ldquo;pseudo-real\u0026rdquo; interactions  Details sent to people who registered (free)    Slides  Title Continuous Integration and TeX with Org-Mode   In this talk I would like to introduce the usage of TeX and templates along with generating ad-hoc class and style files for working with orgmode. In particular, I will highlight also the process by which literate programming practices can be implemented with babel. This allows for a more native and flexible alternative to vendor locked in systems like Jupyterlab which rely on JS based inelegant approaches towards TeX typesetting. Finally, I would like to go into how to leverage CI methods for TeX workflows augmented with git.\n  A pdf copy of the slides are embedded below The orgmode source is here on the site\u0026rsquo;s GH repo   Video  As seen in the schedule:   Thoughts  We had a Zulip channel and separate streams for each talk  This was a bit confusing for some participants   Some people preferred Topia because of the point and click interface  However browser support and low connectivity were a problem    Conclusions The TeX community is one of the best online communities around. There aren\u0026rsquo;t any friendlier or better versed in the topics presented, so this is definitely a group I hope to continue to be a part of. There were some great suggestions for developing the concepts further to make things more user friendly as well; which might be great fodder for another presenation next year perhaps. Plus there\u0026rsquo;s TUGBoat.\n  Zulip sponsors TUG as an open source community and provides Zulip Standard for our users\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/tug-2021-meta/","tags":["presentations","ramblings","docs"],"title":"42nd Annual Confereence of the Tex Users Group"},{"categories":["notes"],"contents":" Continuing compile time intrinsic functions, ASR passes, and testing LAPACK\n Background Serialized update for the 2021 Google Summer of Code under the fortran-lang organization, mentored by Ondrej Certik.\nSeries This post is part of a series based around my weekly GSoC21 project check-ins.\n GSoC21 W1: LFortran Kickoff GSoC21 W2: LFortran Unraveling GSoC21 W3: Kind, Characters, and Standards GSoC21 W4: LFortran, Backends and Bugs GSoC21 W5: LFortran Design Details and minidftatom GSoC21 W6: LFortran ASR and Values GSoC21 W7: LFortran Workflow Basics GSoC21 W8: LFortran Refactors, and Compile Time Evaluations GSoC21 W9: LFortran Bug Hunting Bonanza \u0026lt;\u0026ndash; You are here!  Logistics  Daily checkins over Zulip  Code reviews on Gitlab   Met with Ondrej and Thirumalai on Thursday  Discussed comments and how to attach them  Decided that it would make sense to possibly store the comments in the ASR to make sure that the comments are attached to the right nodes     We discussed a set of outputs for the next week and wrap uppolicies  It was fun to see some of Thirumalai\u0026rsquo;s work in person!    Overview This has been a bit of a slow slog. With the fixed form parser completed, LAPACK is no longer a distant dream, so this week involved tracing through LAPACK and narrowing down issues for it. There are a few open issues which have arcane sources; but these are meant to be handled with prejudice and not merely complained about.\nNew Merge Requests I have decided to elide draft MRs for now.\n tiny: Body visitor implementation (1112) The population of value for tiny calls beyond the symbol table i.e. in the body  Freshly Assigned Issues  \u0026ndash;show-asr Scientific notation (480) A visual glitch in the prettied output; not really under my ambit but it might be fun Compiling LAPACK (481) This is the first fixed form program to be tested, there are some bugs to be ironed out but it should be very much within what we can do by the end of the program. This issue is mostly handled by Ondrej, but since it is application based I assigned myself as well Namespace proposal for compile time intrinsics (482) This is part of a refactoring design plan Semantics: Implement REAL(A [, KIND]) (483) Conversions! The bedrock of many a numerical program, this is one of the most relevant casts Semantics: Implement INT(A [, KIND]) (484) All conversions are similar, and sort of pointless in the SymbolTable, but still necessary and fun Formatting: Array pointout (485) Another minor visual glitch  Misc Logs LAPACK Setup We start with a setup similar to that of minidftatom and dftatom.\n1git clone git@github.com:Reference-LAPACK/lapack.git 2cd lapack 3git checkout v3.10.0 Now we can build it with gfortran as a test.\n1mkdir build \u0026amp;\u0026amp; cd build 2ccmake .. # Make sure to turn on testing 3FC=$(which gfortran) cmake .. -DCMAKE_INSTALL_LIBDIR=$(pwd)/tmp -G \u0026#34;Ninja\u0026#34; 4cmake --build . 5ctest This is a rather long process, with 4097 files to compile, 6814 if BLAS is required; the dependency tree is bound to be rather esoteric looking; so we won\u0026rsquo;t bother.\nRather than using cmake for the lfortran test we will use the makefile setup.\n1cp make.inc.example make.inc # Modify to set paths Well the very first issue is that we should remove the doxygen style comments which are marked by *\u0026gt;. Actually we should go ahead and remove all lines starting with *. The regexp for this is \\*.*$ which simply captures any line starting with *. I use sd (a grep alternative) but any standard tool will work, including grep -vE.\n1# Stadard 2cat blah.f | grep -vE \u0026#39;^\\*\u0026#39; 3# Easier 4# cargo install sd 5for file in ./*.f; do 6sd \u0026#39;\\*.*$\u0026#39; \u0026#39;\u0026#39; $file 7done Real Body Visitors I spent far too long staring at code which actually errored out for a different reason. The MR logic is solid, but Var doesn\u0026rsquo;t have a value assigned to it yet, even when it is decorated with parameter which was totally a silly mistake on my end. parameter, as Ondrej pointed out, effectively acts as though the variable is declared to be a constant. So there is no conceptual problem assigning an expr_t* value to such a statement.\nNot sure why the standard allows this, but it felt incredibly pointless to implement real for the symbol-table:\n1program main 2implicit none 3integer, parameter :: x=3 4real, parameter :: y=real(x) 5print*, y 6end program main Which gfortran accepts without a complaint. Not really much of a pain since there is a proposed refactor which is one of the primary goals of the next week after the bug fixes are in but still odd.\nConclusions Next week is crucial. The LLVM backend must gracefully accept value when present, and there are some additional tasks which simply must be completed. All the QoL MRs will be in next week as well. That said, I expect at this point to dedicate more time to the project, which is not a bad thing, but a bit of a sobering realization. Last week I wasn\u0026rsquo;t working over the weekend, but I have several catchup tasks to finish this time around; for draft MRs and issues.\n","permalink":"https://rgoswami.me/posts/gsoc21-w9/","tags":["gsoc21","fortran","rambling"],"title":"GSoC21 W9: LFortran Bug Hunting Bonanza"},{"categories":["notes"],"contents":" Language standardese and implementations\n Background Serialized update for the 2021 Google Summer of Code under the fortran-lang organization, mentored by Ondrej Certik.\nSeries This post is part of a series based around my weekly GSoC21 project check-ins.\n GSoC21 W1: LFortran Kickoff GSoC21 W2: LFortran Unraveling GSoC21 W3: Kind, Characters, and Standards GSoC21 W4: LFortran, Backends and Bugs GSoC21 W5: LFortran Design Details and minidftatom GSoC21 W6: LFortran ASR and Values GSoC21 W7: LFortran Workflow Basics GSoC21 W8: LFortran Refactors, and Compile Time Evaluations \u0026lt;\u0026ndash; You are here!  Logistics  Discussed more refactors over MRs and Zulip  Overview Intrinsic functions and more bug hunting. A lot of starts in different directions, but I will need to trim these down a bit. A major goal was working through the compile time evaluation of some intrinsic functions.\nNew Merge Requests  Split ast_to_asr An MR started last week, completed and approved this week tiny: Runtime implementation skeleton What will eventually be compiled, hooks into C for now tiny: Compile time implementation The population of value for tiny function calls Draft: Shift runtime intrinsic design Harmonizing the code-base, much of this is cleaning up my own earlier math implementations Draft: expr_value for Kind A WIP MR which will clean up the slightly strange extract_kind function Draft: Implement where construct An MR along the lines of if, related but distinct from Gagandeep\u0026rsquo;s masked optimization WIP  Freshly Assigned Issues  \u0026ndash;show-asr For larger values A visual glitch in the prettied output  Additional Tasks Some of my earlier clean up MRs are beginning to stagnate (CI stuff), will have to catch up on them.\nMisc Logs The splitting of files for the refactor was harsh work. Took a few hours. Mindless, but really needs precision. Thankfully vim folds help a whole lot. This was further enhanced by Ondrej to make things even cleaner.\nIntrinsic Design I have discussed the design and implementation of these a few times before, but perhaps another write up will give direction to my thoughts. We have two major points of contact with the intrinsic functions:\n Compile time These are intrinsic functions like tiny or kind or even sin which can be evaluated immediately to populate the expr_t* value object Runtime These are the actual implementations, currently the goal is to have these hook into C libraries  Tiny Concerns I am not a Fortran language lawyer, but I found myself puzzling over the legalese of the F-2018 draft standard with respect to the C++ nearest neighbor, std::numerical_limits.\nEssentially, the usage of tiny is meant to facilitate doing mathematics without worrying about the exact representability of the value; that is:\n1program main 2implicit none 3integer, parameter :: dp=kind(0.d0) 4real :: a=1.0000009 5if (abs(a-1)\u0026lt;tiny(1._dp)) then 6error stop \u0026#34;a-1 is effectively 0\u0026#34; 7end if 8end program Now by definition, tiny has the following properties:\n Description. Smallest positive model number Class. Inquiry function Argument. X shall be a real scalar or array Result Characteristics. Scalar with the same type and kind type parameter as X. Result Value. The result has the value \\(b^{e_{min}-1}\\) where \\(b\\) and \\(e_{min}\\) are defined in 16.4 for the model representing numbers of the same type and kind type parameter as X. Example. TINY(X) has the value \\(2^{-127}\\) for real X whose model is as in 16.4  This is fairly straightforward, once the model set for real \\(x\\) is understood as (from the section mentioned):\n\\[ x = 0 || x=s×bᵉ×∑_{k=1}ᵖfₖ×b^{-k} \\]\nWhere \\(b\\) and \\(p\\) are integers exceeding one; each \\(fₖ\\) is a nonnegative integer less than \\(b\\), with \\(f₁\\) nonzero; \\(s\\) is \\(+1\\) or \\(-1\\); and \\(e\\) is an integer that lies between the integer minimum and maxima. An extended model for real kinds relaxes the range of the exponent.\nSo far so good. The C equivalence is fairly straightforward, that is the FLT_MIN and DBL_MIN macros defined in \u0026lt;float.h\u0026gt;. This is infact what gfortran generates as well.\nFor a while though I was thrown by the fact that C++, within \u0026lt;limits\u0026gt; also has std::numeric_limits\u0026lt;T\u0026gt;lowest() (described here), which is smaller than the corresponding min() calls and has no direct C equivalent. It is infact, specifically mentioned to not be min for floating-point types.\nHowever, I recognized soon enough from the implementation that there is no real conflict, as it is simply -max(), which says nothing about the representability.\nTherefore, tiny must be FLT_MIN or DBL_MIN. I also took a small detour into std::variant before going with good old if-else early returns instead.\n1int tiny_kind = LFortran::ASRUtils::extract_kind_from_ttype_t(tiny_type); 2if (tiny_kind ==4){ 3float low_val = std::numeric_limits\u0026lt;float\u0026gt;::min(); 4value = ASR::down_cast\u0026lt;ASR::expr_t\u0026gt;(ASR::make_ConstantReal_t(al, x.base.base.loc, 5low_val, // value 6 tiny_type)); 7} else { 8double low_val = std::numeric_limits\u0026lt;double\u0026gt;::min(); 9value = ASR::down_cast\u0026lt;ASR::expr_t\u0026gt;(ASR::make_ConstantReal_t(al, x.base.base.loc, 10low_val, // value 11 tiny_type)); 12} Conclusions My thoughts have been turning towards scalability for a long time now. Good design before it is necessary is a premature optimization, but I think I would like to formulate a cleaner way of dealing with the intrinsic functions which can be reduced to values. The ISO_C_BINDING has been in my thoughts for a while now. Without it, runtime compilation of the ASR remains intractable. I expect the next week to continue along the same lines, populating value for all the intrinsic functions called by minidftatom. It would be best to also generate corresponding runtime implementations.\n","permalink":"https://rgoswami.me/posts/gsoc21-w8/","tags":["gsoc21","fortran","rambling"],"title":"GSoC21 W8: LFortran Refactors, and Compile Time Evaluations"},{"categories":null,"contents":" Leveraging better machines for Jupyter-Julia workshops\n Background I haven\u0026rsquo;t ever actually worked with julia before, however, some of the talks and workshops at this year\u0026rsquo;s Juliacon'21 were of great (applied) interest to me so I decided to follow along. In doing so, since my laptop is woefully inadequate for working through some of the materials (DFT), I decided to augment the installation instructions with remote machine usage notes.\nAssumptions The following gentle assumptions should not actually be much of a practical hurdle for this post, namely:\n You have a remote machine which runs some variant of Linux  No nix is assumed unfortunately Should be capable of running micromamba   Some code snippets assume a user-modifiable directory in the $PATH  Typically I assume something like export PATH=$HOME/.local/bin:$PATH    The code snippets arise in the context of the DFT workshop1, but are applicable across all the talks. Additionally, the snippets will assume no forks but in practice it is best to clone modifiable forks of these repositories so your mutations last forever.\nPaths and Directories For me, I use the following layout:\n1export jc21=$HOME/Git/Github/Julia/juliacon21 2mkdir -p $jc21 3cd $jc21 4git clone git@github.com:mfherbst/juliacon_dft_workshop.git Environment We will start by setting up a micromamba environment. This is probably the most copied snippet on my site.\nNow we can start with the (non-julia) scaffolding:\n1cd $jc21 2micromamba create -p ./.jc21tmp pymatgen python==3.8 jupyterlab pip ipython 3micromamba activate $(pwd)/.jc21tmp We need to grab a stable julia version which we will use jill.py for:\n1pip install jill 2jill install For all workshops, we need a kernel to work with jupyterhub which we get in a straightforward way with an interactive julia session.\n1# julia 2using Pkg 3Pkg.add(\u0026#34;IJulia\u0026#34;) Exposing Access To make sure we can access our remote server we can use ngrok 2 or ssh forwarding 3.\n1wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip 2unzip ngrok-stable-linux-amd64.zip 3mv ngrok $HOME/.local/bin 4ngrok authtoken \u0026lt;token\u0026gt; 5ngrok http 8888 We have assumed again, that $HOME/.local/bin is in our path. tmux is optional, but it is fantastic for all headless setups.It is probably better to not do this on your login / head node, and with ngrok we don\u0026rsquo;t need to care about how deeply nested we are in terms of exposing the tunnel, so just grab an interactive job. Assuming tmux then, the workflow is simply:\n1ssh mysuperhpc 2# 6.5 hours on 1 node 3srun -N 1 -t 6:30:00 --pty /bin/bash 4# Or whatever configuration works for you 5tmux new -s jc21 # enter an environment 6cd $jc21 7micromamba activate $(pwd)/.jc21tmp 8jupyter lab --ServerApp.allow_remote_access=1 \\ 9 --ServerApp.open_browser=False --port=8889 10# Ctrl+B --\u0026gt; : --\u0026gt; split-pane 11ngrok http 8889 Where the jupyter lab options are self evident and circumvent having to generate a configuration file. The second pane does not need to be run from the same location naturally. With this, pointing a local browser anywhere to the ngrok URL will bring you control over your remote machine and we\u0026rsquo;re off to the races.\n Now we get a standard server with the appropriate kernel and can continue working through the workshops.\n Conclusions I\u0026rsquo;m not jumping on the julia bandwagon. I still reach for C++ or Fortran or even Python; but JuliaCon is why I need to be conversant with julia. It is used for fantastic applied use-cases, and being practically able to follow along and hack around existing code is important, even if I\u0026rsquo;m not interested in it as a programming language per-say. In general these instructions could be generalized to other workflows/workshops/kernels etc. etc., so it helps to have it written down too. Keep in mind that some system admins will take umbrage at running servers like this.\n  Most of my existing experience with other codes like Quantum Espresso were at CECAM events with VMs\u0026hellip; so it wouldn\u0026rsquo;t be fair to plunge into dftk on my laptop\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Alan Shreve is a fantastic person and ngrok blows all competition out of the water, great student pricing too\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This is a very good resource about setting up the SSH chain required, but ngrok just works across nodes too\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/juliacon-remote-machines/","tags":null,"title":"JuliaCon and Remote Machines"},{"categories":["notes"],"contents":" Revisiting minidftatom and a birds eye view of lfortran\n Background Serialized update for the 2021 Google Summer of Code under the fortran-lang organization, mentored by Ondrej Certik.\nSeries This post is part of a series based around my weekly GSoC21 project check-ins.\n GSoC21 W1: LFortran Kickoff GSoC21 W2: LFortran Unraveling GSoC21 W3: Kind, Characters, and Standards GSoC21 W4: LFortran, Backends and Bugs GSoC21 W5: LFortran Design Details and minidftatom GSoC21 W6: LFortran ASR and Values GSoC21 W7: LFortran Workflow Basics \u0026lt;\u0026ndash; You are here!  Logistics  Met with Ondrej on Tuesday  Went over the debugging workflow Discussed the design of the SymbolTableVisitor and BodyVisitor Fixed a symbol scoping bug and discussed implementing value for kind   Kept in touch asynchrously over Zulip and MRs  Overview This week focused on implementing features and cleaning up parts of the project.\nNew Merge Requests  Cleanup Integer Binary Operations (1045) There was an extraneous check for something the LFORTRAN_ASSERT would catch Values for Real Binary Operations (1080) An implementation, which is similar conceptually and in implementation to the Integer Value MR Symbol Table Scope Fix for Function Calls (1087) A bug which haunted me for a few weeks turned out to have a similar fix in the BodyVisitor Intrinsic Issue Template (1088) I am rather fond of a particular style of opening missing intrinsic issues, and this codifies it for future use Draft: ci: Use Ninja everywhere (1085) A draft MR to return to after updating the CI sometime  Freshly Assigned Issues  Suggestion to refactor ast_to_asr (467) In particular this has to do with redundancies (of sort) in the SymbolTable and Body visitors Implement the sqrt intrinsic (468) Or at-least, for this issue, recognize it as a valid intrinsic Implement the where conditional (475) Should be implemented along the same lines as the other conditionals Semantic error: Variable not declared (476) Another fascinating episode of \u0026ldquo;bug detective\u0026rdquo;, this one might be more of a red herring Semantics: Minidftatom Math (477) Collection of intrinsic math functions to be implemented at the ASR level  Additional Tasks Perhaps adding some more templates would make things easier (for me and other contributors). I intend to update the CI images to leverage ninja.\nMisc Logs Build Times ninja is fantastic. The make and cmake based workflow I was using often had me stalling for upto half a minute.\nTable 1: Not a rigorous benchmark at all, estimates in seconds     Generator Cold Build Rebuild     \u0026ldquo;Unix Makefiles\u0026rdquo; 17 16   \u0026ldquo;Ninja\u0026rdquo; 11 10    Stacktrace Debugging Ondrej mentioned in the past that he does not use a debugger, and in our meeting we went over extracting information from the stacktrace produced by lfortran. I have personally found it to be rather verbose, but then he pointed out that I could ignore the generated lines from ast.h. That made things a lot easier.\nScoping Rules  Intrinsics are exposed to the global scope, which has a nullptr parent Inside a sub-module, we need to shift the scope, which was done for the BodyVisitor but not for the SymbolTableVisitor  Conclusions This week was possibly more productive in terms of code than any before. For that, I mostly have to thank that sudden flash of understanding which comes from working on a code-base for weeks (with a fantastic guide). Next week should see the rest of expr_value be completed, and I intend to try my hand at the LLVM backend to make it use expr_value when possible. Naturally, this will be done in tandem with the minidftatom issues, after-all it is back to basics in the sense that progress resumed on working through the main minidftatom roadmap as well.\n","permalink":"https://rgoswami.me/posts/gsoc21-w7/","tags":["gsoc21","fortran","rambling"],"title":"GSoC21 W7: LFortran Workflow Basics"},{"categories":["personal"],"contents":" Struggling to emulate klfc for VIM Colemak bindings on Darwin (macOS) systems with Hammerspoon and Karabiner\n Background I have mentioned in the past my customized Colemak dotfiles which I used with a customized keyboard layout. Unfortunately, the .keylayout system of MacOS is far more primitive than the elegant klfc setup 1. For an understanding of what we are trying to get at, the following poorly made video will suffice.\n{{\u0026lt; youtube BgqlUsenmKU \u0026gt;}}\nSeries This post is part of a series on Colemak and keyboard management in general.\n Switching to Colemak Refactoring Dotfiles For Colemak Remapping Keys with XKB and KLFC Keybindings and MacOS \u0026lt;\u0026ndash; You are here!  VIM Colemak Extensions Just as a reminder, my setup (or hzColemak) consists of an augmented VIM workflow, as shown below, and described in my previous post.\n\nTools There are essentially a few options:\n Manually write .keylayout These then go in $HOME/Library/Keyboard\\ Layouts Use Ukelele The incredibly poorly named (for search purposes) versatile tool is able to ease the pain slightly for writing .keylayout files Use Karabiner Elements This seems to be closer to AutoHotKey and the like, runs in the background and actively intercepts keys based on json configurations; though there seems to be a more rational method (a hidutil remapping file generator) for newer kernels 2 Script things with Hammerspoon Uses a lua engine to interact with the system, can be configured for the most part with Fennel using Spacehammer  Now of the four, I had a predilection to move towards manually writing, with the help of Ukelele. However, evidently, there is no real way to remove the stickiness from the Caps Lock key. It can either be remapped using system settings 3 to one of the other modifier keys, but not to Extend. The closest possible solution would be to do a very awkward Esc based layout. Also, rapid prototyping was out of the question, since Ukelele requires a log-out log-in cycle to set things up. Of Ukelele and manually writing things then, nothing more need be said.\nKarabiner This left me considering json re-writer. I have been using the basic Colemak layout with a simplistic Karabiner caps to delete for a while now, which allows for a standards compliant Colemak experience, but extending this like I needed was a little bit of a struggle.\n\nApparently it is possible to overload the keyboard system with a \u0026ldquo;Hyper\u0026rdquo; key 4, which is the closest to Extend.\nThis is setup through a karabiner.json file, since it appears that the \u0026ldquo;Complex modifications\u0026rdquo; referred to in the GUI (Fig. 2) don\u0026rsquo;t really allow for more than downloading rules off of the internet 5, like the one below.\n1\u0026#34;complex_modifications\u0026#34;: { 2\u0026#34;rules\u0026#34;: [ 3{ 4\u0026#34;manipulators\u0026#34;: [ 5{ 6\u0026#34;description\u0026#34;: \u0026#34;Change caps_lock to command+control+option+shift. Escape if no other key used.\u0026#34;, 7\u0026#34;from\u0026#34;: { 8\u0026#34;key_code\u0026#34;: \u0026#34;caps_lock\u0026#34;, 9\u0026#34;modifiers\u0026#34;: { 10\u0026#34;optional\u0026#34;: [ 11\u0026#34;any\u0026#34; 12] 13} 14}, 15\u0026#34;to\u0026#34;: [ 16{ 17\u0026#34;key_code\u0026#34;: \u0026#34;left_shift\u0026#34;, 18\u0026#34;modifiers\u0026#34;: [ 19\u0026#34;left_command\u0026#34;, 20\u0026#34;left_control\u0026#34;, 21\u0026#34;left_option\u0026#34; 22] 23} 24], 25\u0026#34;to_if_alone\u0026#34;: [ 26{ 27\u0026#34;key_code\u0026#34;: \u0026#34;escape\u0026#34;, 28\u0026#34;modifiers\u0026#34;: { 29\u0026#34;optional\u0026#34;: [ 30\u0026#34;any\u0026#34; 31] 32} 33} 34], 35\u0026#34;type\u0026#34;: \u0026#34;basic\u0026#34; 36} 37] 38} 39] 40}, Extending this is not pleasant, so we won\u0026rsquo;t go forward with this approach at all.\nWe will still need a bit of this unfortunately, so this will stick around. In particular we need to have a simple mapping, from caps_lock to one of the un-mapped function keys, f18 or f19 or some such.\nEffectively, this can be done with the GUI, but we prefer the configuration snippet makes it far more reproducible:\n1{ 2\u0026#34;type\u0026#34;: \u0026#34;basic\u0026#34;, 3\u0026#34;from\u0026#34;: { 4\u0026#34;key_code\u0026#34;: \u0026#34;caps_lock\u0026#34; 5}, 6\u0026#34;to\u0026#34;: 7{ 8\u0026#34;key_code\u0026#34;: \u0026#34;f19\u0026#34; 9} 10} Note that we do not need to define the hyper key to be a series of keystrokes or anything like that here with the complex_modifications in this configuration. The point of using complex_modifications is to not overwrite the Karabiner defaults. The following modifications also allow us to keep CAPS functionality bound to simultaneously pressing left and right shift together6.\n1{ 2\u0026#34;title\u0026#34;: \u0026#34;CAPS to fake hyper (f19) and Shift CAPS\u0026#34;, 3\u0026#34;rules\u0026#34;: [ 4{ 5\u0026#34;description\u0026#34;: \u0026#34;CAPS_LOCK : (HYPER)\u0026#34;, 6\u0026#34;manipulators\u0026#34;: [ 7{ 8\u0026#34;from\u0026#34;: { 9\u0026#34;key_code\u0026#34;: \u0026#34;caps_lock\u0026#34;, 10\u0026#34;modifiers\u0026#34;: { 11\u0026#34;optional\u0026#34;: [\u0026#34;any\u0026#34;] 12} 13}, 14\u0026#34;to\u0026#34;: 15{ 16\u0026#34;key_code\u0026#34;: \u0026#34;f19\u0026#34; 17}, 18\u0026#34;type\u0026#34;: \u0026#34;basic\u0026#34; 19} 20] 21}, 22{ 23\u0026#34;description\u0026#34;: \u0026#34;Toggle CAPS_LOCK with LEFT_SHIFT + RIGHT_SHIFT\u0026#34;, 24\u0026#34;manipulators\u0026#34;: [ 25{ 26\u0026#34;from\u0026#34;: { 27\u0026#34;key_code\u0026#34;: \u0026#34;left_shift\u0026#34;, 28\u0026#34;modifiers\u0026#34;: { 29\u0026#34;mandatory\u0026#34;: [\u0026#34;right_shift\u0026#34;], 30\u0026#34;optional\u0026#34;: [\u0026#34;caps_lock\u0026#34;] 31} 32}, 33\u0026#34;to\u0026#34;: [ 34{ 35\u0026#34;key_code\u0026#34;: \u0026#34;caps_lock\u0026#34; 36} 37], 38\u0026#34;to_if_alone\u0026#34;: [ 39{ 40\u0026#34;key_code\u0026#34;: \u0026#34;left_shift\u0026#34; 41} 42], 43\u0026#34;type\u0026#34;: \u0026#34;basic\u0026#34; 44}, 45{ 46\u0026#34;from\u0026#34;: { 47\u0026#34;key_code\u0026#34;: \u0026#34;right_shift\u0026#34;, 48\u0026#34;modifiers\u0026#34;: { 49\u0026#34;mandatory\u0026#34;: [\u0026#34;left_shift\u0026#34;], 50\u0026#34;optional\u0026#34;: [\u0026#34;caps_lock\u0026#34;] 51} 52}, 53\u0026#34;to\u0026#34;: [ 54{ 55\u0026#34;key_code\u0026#34;: \u0026#34;caps_lock\u0026#34; 56} 57], 58\u0026#34;to_if_alone\u0026#34;: [ 59{ 60\u0026#34;key_code\u0026#34;: \u0026#34;right_shift\u0026#34; 61} 62], 63\u0026#34;type\u0026#34;: \u0026#34;basic\u0026#34; 64} 65] 66} 67] 68} Hammerspoon Not that an operating system should need a scripting engine, but apparently Hammerspoon is worth looking into fox better control over the entire OS 7. This made it more attractive than writing a hidden configuration file for Karabiner.\nThe first thing we will need in our init.lua after installing the Hammerspoon application is one to reload the configuration automatically:\n1-- Reload config when any lua file in config directory changes 2-- From https://gist.github.com/prenagha/1c28f71cb4d52b3133a4bff1b3849c3e 3function reloadConfig(files) 4doReload = false 5for _,file in pairs(files) do 6if file:sub(-4) == \u0026#39;.lua\u0026#39; then 7doReload = true 8end 9end 10if doReload then 11hs.reload() 12end 13end 14local myWatcher = hs.pathwatcher.new(os.getenv(\u0026#39;HOME\u0026#39;) .. \u0026#39;/.hammerspoon/\u0026#39;, reloadConfig):start() 15hs.alert.show(\u0026#39;Config loaded\u0026#39;) An alternative to using Karabiner to map f19 is to use the foundation remapping plugin which needs to be placed in $HOME/.hammerspoon with the following added to our init.lua:\n1-- cd ~/.hammerspoon/ \u0026amp;\u0026amp; wget https://raw.githubusercontent.com/hetima/hammerspoon-foundation_remapping/master/foundation_remapping.lua 2-- init.lua 3local FRemap = require(\u0026#39;foundation_remapping\u0026#39;) 4local remapper = FRemap.new() 56remapper:remap(\u0026#39;capslock\u0026#39;, \u0026#39;f19\u0026#39;) 7remapper:register() The only problem with this is that it must be unregistered carefully. The Karabiner method is easier overall.\nAnyway, once f19 has been bound, one way or another, we need to setup the hyper key itself8:\n1-- init.lua 2-- A global variable for the Hyper Mode 3hyper = hs.hotkey.modal.new({}, \u0026#39;F18\u0026#39;) 45-- Enter Hyper Mode when F19 (Hyper/Capslock) is pressed 6function enterHyperMode() 7hyper.triggered = false 8hyper:enter() 9hs.alert.show(\u0026#39;Hyper on\u0026#39;) 10end 1112-- Leave Hyper Mode when F19 (Hyper/Capslock) is pressed, 13function exitHyperMode() 14hyper:exit() 15hs.alert.show(\u0026#39;Hyper off\u0026#39;) 16end 1718-- Bind the Hyper key 19f19 = hs.hotkey.bind({}, \u0026#39;F19\u0026#39;, enterHyperMode, exitHyperMode) 20f19cmd = hs.hotkey.bind({\u0026#39;cmd\u0026#39;}, \u0026#39;F19\u0026#39;, enterHyperMode, exitHyperMode) The alerts can help while setting up muscle memory, but they are optional. We also map cmd+f19 to prevent the cmd+h hide application annoyance.\nRemapping Keys The basic idea is to set a function which executes only when hyper:enter() is true. One thing to remember is that the signature for hs.eventtap.keyStroke has an optional delay which defaults to 200ms and is ridiculous. This means we need to explicitly set 0 for the delay.\nBasic Movements These correspond to the to the top level extend layer. We will disect one example here and refer to a later section for the details.\n1-- h - move left {{{3 2function left() hs.eventtap.keyStroke({}, \u0026#34;Left\u0026#34;, 0) end 3hyper:bind({}, \u0026#39;h\u0026#39;, left, nil, left) 4-- }}}3 Essentially, when hyper is active, then tapping h activates the keyStroke. The remaining hnei movements are similar. For the carriage return case we can go for a slightly more interesting option.\n1-- o - open new line below cursor {{{3 2hyper:bind({}, \u0026#39;o\u0026#39;, nil, function() 3local app = hs.application.frontmostApplication() 4if app:name() == \u0026#34;Finder\u0026#34; then 5hs.eventtap.keyStroke({\u0026#34;cmd\u0026#34;}, \u0026#34;o\u0026#34;, 0) 6else 7hs.eventtap.keyStroke({}, \u0026#34;Return\u0026#34;, 0) 8end 9end) 10-- }}}3 We can effectively setup application specific keyStrokes which will help in the long run.\nExtend Layers These are implemented in much the same way, since modifiers can be called in the bind function.\n1-- cmd+h - delete character before the cursor {{{3 2local function delete() 3hs.eventtap.keyStroke({}, \u0026#34;delete\u0026#34;, 0) 4end 5hyper:bind({\u0026#34;cmd\u0026#34;}, \u0026#39;h\u0026#39;, delete, nil, delete) 6-- }}}3 VIM Extras Since we have a whole scripting engine anyway, we might as well use it to get some additional mileage not possible from our older keyboard layout.\n1-- w - move to next word {{{3 2function word() hs.eventtap.keyStroke({\u0026#34;alt\u0026#34;}, \u0026#34;Right\u0026#34;, 0) end 3hyper:bind({}, \u0026#39;w\u0026#39;, word, nil, word) 4-- }}}3 These can be extended further into whole pseudo-VIM approach.\nAll together For this post, we opted for a minimal Hammerspoon setup which ended up with a monolithic setup defined in init.lua files as follows:\n1-- Reload config when any lua file in config directory changes 2-- From https://gist.github.com/prenagha/1c28f71cb4d52b3133a4bff1b3849c3e 3function reloadConfig(files) 4doReload = false 5for _,file in pairs(files) do 6if file:sub(-4) == \u0026#39;.lua\u0026#39; then 7doReload = true 8end 9end 10if doReload then 11hs.reload() 12end 13end 14local myWatcher = hs.pathwatcher.new(os.getenv(\u0026#39;HOME\u0026#39;) .. \u0026#39;/.hammerspoon/\u0026#39;, reloadConfig):start() 15hs.alert.show(\u0026#39;Config loaded\u0026#39;) 1617-- A global variable for the Hyper Mode 18hyper = hs.hotkey.modal.new({}, \u0026#39;F18\u0026#39;) 1920-- Enter Hyper Mode when F19 (Hyper/Capslock) is pressed 21function enterHyperMode() 22hyper.triggered = false 23hyper:enter() 24hs.alert.show(\u0026#39;Hyper on\u0026#39;) 25end 2627-- Leave Hyper Mode when F19 (Hyper/Capslock) is pressed, 28-- send ESCAPE if no other keys are pressed. 29function exitHyperMode() 30hyper:exit() 31-- if not hyper.triggered then 32-- hs.eventtap.keyStroke({}, \u0026#39;ESCAPE\u0026#39;) 33-- end 34hs.alert.show(\u0026#39;Hyper off\u0026#39;) 35end 3637-- Bind the Hyper key 38f19 = hs.hotkey.bind({}, \u0026#39;F19\u0026#39;, enterHyperMode, exitHyperMode) 3940-- Vim Colemak bindings (hzColemak) 41-- Basic Movements {{{2 4243-- h - move left {{{3 44function left() hs.eventtap.keyStroke({}, \u0026#34;Left\u0026#34;, 0) end 45hyper:bind({}, \u0026#39;h\u0026#39;, left, nil, left) 46-- }}}3 4748-- n - move down {{{3 49function down() hs.eventtap.keyStroke({}, \u0026#34;Down\u0026#34;, 0) end 50hyper:bind({}, \u0026#39;n\u0026#39;, down, nil, down) 51-- }}}3 5253-- e - move up {{{3 54function up() hs.eventtap.keyStroke({}, \u0026#34;Up\u0026#34;, 0) end 55hyper:bind({}, \u0026#39;e\u0026#39;, up, nil, up) 56-- }}}3 5758-- i - move right {{{3 59function right() hs.eventtap.keyStroke({}, \u0026#34;Right\u0026#34;, 0) end 60hyper:bind({}, \u0026#39;i\u0026#39;, right, nil, right) 61-- }}}3 6263-- ) - right programming brace {{{3 64function rbroundL() hs.eventtap.keyStrokes(\u0026#34;(\u0026#34;) end 65hyper:bind({}, \u0026#39;k\u0026#39;, rbroundL, nil, rbroundL) 66-- }}}3 6768-- ) - left programming brace {{{3 69function rbroundR() hs.eventtap.keyStrokes(\u0026#34;)\u0026#34;) end 70hyper:bind({}, \u0026#39;v\u0026#39;, rbroundR, nil, rbroundR) 71-- }}}3 7273-- o - open new line below cursor {{{3 74hyper:bind({}, \u0026#39;o\u0026#39;, nil, function() 75local app = hs.application.frontmostApplication() 76if app:name() == \u0026#34;Finder\u0026#34; then 77hs.eventtap.keyStroke({\u0026#34;cmd\u0026#34;}, \u0026#34;o\u0026#34;, 0) 78else 79hs.eventtap.keyStroke({}, \u0026#34;Return\u0026#34;, 0) 80end 81end) 82-- }}}3 8384-- Extend+AltGr layer 85-- Delete {{{3 8687-- cmd+h - delete character before the cursor {{{3 88local function delete() 89hs.eventtap.keyStroke({}, \u0026#34;delete\u0026#34;, 0) 90end 91hyper:bind({\u0026#34;cmd\u0026#34;}, \u0026#39;h\u0026#39;, delete, nil, delete) 92-- }}}3 9394-- cmd+i - delete character after the cursor {{{3 95local function fndelete() 96hs.eventtap.keyStroke({}, \u0026#34;Right\u0026#34;, 0) 97hs.eventtap.keyStroke({}, \u0026#34;delete\u0026#34;, 0) 98end 99hyper:bind({\u0026#34;cmd\u0026#34;}, \u0026#39;i\u0026#39;, fndelete, nil, fndelete) 100-- }}}3 101102-- ) - right programming brace {{{3 103function rbcurlyL() hs.eventtap.keyStrokes(\u0026#34;{\u0026#34;) end 104hyper:bind({\u0026#34;cmd\u0026#34;}, \u0026#39;k\u0026#39;, rbcurlyL, nil, rbcurlyL) 105-- }}}3 106107-- ) - left programming brace {{{3 108function rbcurlyR() hs.eventtap.keyStrokes(\u0026#34;}\u0026#34;) end 109hyper:bind({\u0026#34;cmd\u0026#34;}, \u0026#39;v\u0026#39;, rbcurlyR, nil, rbcurlyR) 110-- }}}3 111112-- Extend+Shift 113114-- ) - right programming brace {{{3 115function rbsqrL() hs.eventtap.keyStrokes(\u0026#34;[\u0026#34;) end 116hyper:bind({\u0026#34;shift\u0026#34;}, \u0026#39;k\u0026#39;, rbsqrL, nil, rbsqrL) 117-- }}}3 118119-- ) - left programming brace {{{3 120function rbsqrR() hs.eventtap.keyStrokes(\u0026#34;]\u0026#34;) end 121hyper:bind({\u0026#34;shift\u0026#34;}, \u0026#39;v\u0026#39;, rbsqrR, nil, rbsqrR) 122-- }}}3 123124-- Special Movements 125-- w - move to next word {{{3 126function word() hs.eventtap.keyStroke({\u0026#34;alt\u0026#34;}, \u0026#34;Right\u0026#34;, 0) end 127hyper:bind({}, \u0026#39;w\u0026#39;, word, nil, word) 128-- }}}3 129130-- b - move to previous word {{{3 131function back() hs.eventtap.keyStroke({\u0026#34;alt\u0026#34;}, \u0026#34;Left\u0026#34;, 0) end 132hyper:bind({}, \u0026#39;b\u0026#39;, back, nil, back) 133-- }}}3 Conclusions It has been very restrictive to not be able to use the keyboard layout I spent years crafting. In that regard, this post is a success story, even with the awkwardness of the implementation. On the other hand, it is baffling to see the lack of good FOSS tools on this ecosystem 9, but that is to be expected perhaps. For my purposes right now, this monolithic init.lua is enough, and the entire configuration corresponds to this commit in my Dotfiles. However, Hammerspoon is one of the more promising configuration systems especially considering the more complete VIM bindings which exist in the community and would bear a second look. It would make sense to look into using Spacehammer and more VIM bindings sometime soon.\n  There are some text snippet expansion methods, but nothing to simply modify keys.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n As discussed here on the Random Bits blog\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n A nauseating prospect, far too inelegant and difficult to keep track of a million manually changed GUI settings\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Coined in its current form by Brett Terpstra here or Jonathan Hollin here, first described by Steve Losh many years ago\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Yes, the original post was 4 years ago and it still isn\u0026rsquo;t part of the GUI, but it is now documented\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This is adapted from these regular VIM Karabiner rules\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n One of the FOSS tools, plus its in my favorite language (used in d-SEAMS) for embedding things, lua\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This is close to the method Rosco Kalis has\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n It might also just be that there is a lot more media spam, like Alfred is basically SpaceLauncher but the latter isn\u0026rsquo;t well known\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/keybindings-colevimhz-macos/","tags":["workflow","macos","tools"],"title":"Remapping Keys for ColemakVIM on MacOS"},{"categories":["programming"],"contents":" A short exploration of multi-user nix and interacting with the Nix User Repository without root\n Background For multi-user nix installations 1, the NIX_PATH variable is empty. Here I briefly go over two approaches to mitigate this, one with nix-channel and the other by manual pinning. Note that this post will eventually be superseded for most cases by a better flake workflow.\nChannels The idea behind using a channel is essentially that the tar at a particular commit / tag will be downloaded and stored, typically at $HOME/.nix-defexpr/channels. This is in-fact a symlink typically as seen below.\n1tree -L 2 ~/.nix-defexpr/ 2/users/home/rg/.nix-defexpr/ 3├── channels -\u0026gt; /nix/var/nix/profiles/per-user/rg/channels 4└── channels_root -\u0026gt; /nix/var/nix/profiles/per-user/root/channels This is in keeping with the profile data, since by default in a multi-user setup we have the equivalent of running nix-env --switch-profile /nix/var/nix/profiles/per-user/rg/profile.\nWith that aside out of the way, we can start by setting up baseline channels:\n1# Bad idea, see notes 2# nix-channel --add https://github.com/NixOS/nixpkgs/archive/nixos-21.05.tar.gz nixpkgs 3nix-channel --add https://github.com/NixOS/nixpkgs/archive/nixos-unstable.tar.gz unstable 4nix-channel --add https://github.com/nix-community/NUR/archive/master.tar.gz nur 5nix-channel --update This leads to:\n1tree /nix/var/nix/profiles/per-user/rg/channels 2/nix/var/nix/profiles/per-user/rg/channels 3├── manifest.nix -\u0026gt; /nix/store/smh16xjbbhcrlzhrlbl71nm16dhz7pb5-env-manifest.nix 4├── nixpkgs -\u0026gt; /nix/store/c1hfsx2v1cy50j7ax3y34gwwpzzvrvm5-nixpkgs-21.05/nixpkgs 5├── nur -\u0026gt; /nix/store/za2fqbj1z9q6hiip39642hjfa0a59mls-nur/nur 6└── unstable -\u0026gt; /nix/store/a4bb70ihv8x8zxj0zw4wdafl62n9a92q-unstable/unstable Now we can use these names ('\u0026lt;nur\u0026gt;' , '\u0026lt;unstable\u0026gt;' and '\u0026lt;nixpkgs\u0026gt;' ) to write a relatively straightforward config.nix as follows:\n1# Uses the channel form of the installation 2# Goes in ~/.config/nixpkgs/config.nix 3{ 4# NUR setup 5pkgs = import \u0026lt;unstable\u0026gt;; # nixos-unstable channel 6packageOverrides = pkgs: { 7nur = import \u0026lt;nur\u0026gt; { # Also defined as a channel 8inherit pkgs; 9}; 10}; 11} The above snippet can be modified to swap out unstable for nixpkgs as well. It is generally a good idea to define nixpkgs anyway to make sure we can do things like nix-env -iA cowsay -f '\u0026lt;nixpkgs\u0026gt;' . This allows us to now interact with nur packages without any trouble.\n1nix-shell -p nur.repos.mic92.hello-nur 2hello 3\u0026gt; Hello, NUR! Notes  We need to define pkgs since by default NIX_PATH is empty here For more on packageOverrides, there\u0026rsquo;s a pill for that For a refresher on profiles, there\u0026rsquo;s the nix manual, specifically chapter 10 Defining nixpkgs as a channel causes warning: name collision in input Nix expressions and it gets skipped The problem with channels is of course that it is hard to keep control of update which occur when users run nix-channel --update  Pinning Packages To mitigate the impurity introduced by nix-channel --update we can opt to instead pin both the pkgs and nur to a particular hash. The benefit is that we can dispense with any channel management and still be guaranteed to have the same behavior across different machines.\nThe star snippet here is focused around builtins.fetchTarball, which takes a url and a sha256 hash. Using this, our config.nix now becomes:\n1# A config.nix 2{ 3pkgs = import (builtins.fetchTarball { 4url = \u0026#34;https://github.com/NixOS/nixpkgs/archive/2588a6c9f1a3279d523e7bd0068da4e12db76ca8.tar.gz\u0026#34;; 5sha256 = \u0026#34;0x1q5sf04qk6d05gn4rr2hmlfz17mfkd9fsxwc0kdbhx5yl65zwy\u0026#34;; 6}); 7packageOverrides = pkgs: { 8nur = import 9(builtins.fetchTarball { 10url = \u0026#34;https://github.com/nix-community/NUR/archive/4a81d63c672ffdbbb999eb2549c0eb0934b3384b.tar.gz\u0026#34;; 11sha256 = \u0026#34;09bg0xifdk6rw35w472rk0dj69w08c1ksb3a3qrzay0ww69495zk\u0026#34;; 12} 13) 14{ 15inherit pkgs; 16}; 17}; 18} To update things in a controlled manner, use a commit from the repository as the URL, and then:\n1url=\u0026#34;https://github.com/nix-community/NUR/archive/3a6a6f4da737da41e27922ce2cfacf68a109ebce.tar.gz\u0026#34; 2nix-prefetch-url --unpack $url Which will emit the required hash. With this setup the usage is as before:\n1nix-env -f \u0026#39;\u0026lt;unstable\u0026gt;\u0026#39; -iA nur.repos.mic92.hello-nur 2hello 3\u0026gt; Hello, NUR! Note that the uninstall is still best done after checking the name with nix-env -qa:\n1nix-env -e hello # not hello-nur Bonus Locale Errors For some, dropping into nix-shell causes bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8). In this case add the following to the .bashrc or its equivalent rc file.\n1export LOCALE_ARCHIVE=\u0026#34;$(nix-build --no-out-link \u0026#34;\u0026lt;nixpkgs\u0026gt;\u0026#34; -A glibcLocales)/lib/locale/locale-archive\u0026#34; Conclusions Much of the channel workflow is discussed directly in this issue. The main usage of this is actually to setup and use user defined packages in a more natural way, and to move away from channels which typically only update NIX_PATH when run as root, which in turn causes much confusion. NUR is also flake compatible and this will be explored further in a another post.\n  Like the setup on the elja super-computing cluster of the Icelandic Research High Performance Compute cluster\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/nix-channel-nur/","tags":["tools","nix","workflow"],"title":"Nix, Channels and the NUR"},{"categories":["programming"],"contents":" doom-emacs as an ssh IDE with TRAMP using eglot and language servers.\n Background For most of my emacs configuration1, there normally isn\u0026rsquo;t very much to write about which isn\u0026rsquo;t immediately evident from my configuration site. However, since my shift to a MacBook, I have needed to fine tune my existing lsp-mode default setup for TRAMP and this post will cover a little bit of that. Though most of the post is about doom-emacs, it is also applicable to vanilla emacs after porting the snippets over to use-package instead.\nThere are two primary methods of interfacing emacs to language servers, eglot and lsp-mode. Not for me to do a full comparison, but some thoughts of the top of my head were:\n eglot Feels more minimal, maintained by the author of yassnippet lsp-mode Much more configuration, better documentation in some aspects, strangely difficult TRAMP setup  Note that the comparisons stated are neither fair nor particularly useful. I just need something to work quickly with my main programming languages at the moment. I went with eglot.\nDesiderata The things I really need, which both provide are:\n Easily look up doc-strings Auto-completion Renaming symbols Figuring out common mistakes Indexing and lookup for symbol definitions and usage  Eglot Getting started with eglot was surprisingly pleasant, for doom ships a pretty out-of the box configuration anyway.\n1;; init.el 2(lsp +eglot) ;; Activate eglot 3(python +lsp) ;; Python with pyls by default 4(cc +lsp) ;; C++ with clangd by default Usage was pretty sweet (after getting clang-tools for clangd), it can be activated by running eglot in any supported buffer, and it came with all the standard bells and whistles.\n For working with tramp too, after Emacs 27.1, in most cases it just works, one simply needs to supply the location of the language server executable and we\u0026rsquo;re off to the races. In-fact, even adding the full path to the binary to PATH is good enough for eglot.\nCaveats  No projectile support planned (though there are workarounds)  Uses project.el to pick root directories (i.e. .git)  In particular this means it works poorly with git sub-modules      Overall the main issue with eglot seems to be the insistence to be accepted into emacs core someday. This is great, and a good direction for the project to grow in, but it constrains my workflow unnecessarily. I\u0026rsquo;m more interested in working with doom-emacs than vanilla emacs and so am pretty invested in non-core libraries like projectile 2.\nLanguage Servers For getting the language server providers themselves, we will mostly leverage direct binaries where possible, but also, depending on the implementation, virtual environments3. Essentially we have the following Rosetta stone.\nTable 1: Language servers and installation methods     Language Server Installation Management     C++ clangd Manual (binary) None   Python pylsp pip Micromamba   Bash bash-language-server npm NVM   R languageserver install.packages N/A   TeX digestiff Manual (wrapper) None   Nix rnix-lsp Nix N/A   Fortran fortran-language-server pip Micromamba    So we need to setup the following, assuming an existing conda helper (micromamba as described here) setup.\n1# Get Micromamba 2mkdir -p ~/.local/bin 3export PATH=$HOME/.local/bin:$PATH 4wget -qO- https://micromamba.snakepit.net/api/micromamba/linux-64/latest | tar -xvj bin/micromamba 5mv bin/micromamba ~/.local/bin 6rm -rf bin 7micromamba shell init -s bash -p $HOME/.micromamba 8. ~/.bashrc Now we can use it.\n1micromamba create -n lsp 2micromamba activate lsp 3micromamba install python==3.9 pip -c conda-forge Note that for this to work out well, we need to activate this environment by default, with something like micromamba activate lsp added to your .bashrc. In fact, it is best to also add the full path, since eglot doesn\u0026rsquo;t seem to pick it up after micromamba activate.\n1export PATH=$HOME/.micromamba/envs/lsp/bin/:$PATH Also, we need to setup nvm.\n1wget -qO- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | bash 2. $HOME/.bashrc 3nvm install node 4nvm use node C++ For clangd (details here) and tramp this amounted to:\n1mkdir -p ~/.local/lsp 2cd ~/.local/lsp 3wget https://github.com/clangd/clangd/releases/download/12.0.0/clangd-linux-12.0.0.zip 4unzip clangd-linux-12.0.0.zip 5mv clangd-linux-12.0.0/* . In combination with the standard doom-emacs cc module, this is a very good workflow, with the only issue being the ability to set the project root and files to be considered. There is doom-emacs version of appending and setting the language server used, which will be used here:\n1(after! eglot 2:config 3(set-eglot-client! \u0026#39;cc-mode \u0026#39;(\u0026#34;clangd\u0026#34; \u0026#34;-j=3\u0026#34; \u0026#34;--clang-tidy\u0026#34;)) 4) Note that the best way to make a language server behave is to have a compilation database, which can be generated (for a CMake project) as follows:\n1# From src 2mkdir build 3cd build 4# cmake -DCMAKE_EXPORT_COMPILE_COMMANDS=1 .. -G \u0026#39;Unix Makefiles\u0026#39; # default on *nix 5cmake -DCMAKE_EXPORT_COMPILE_COMMANDS=1 .. -G \u0026#39;Ninja\u0026#39; # way faster 6ninja # or make 7cp compile_commands.json .. This will ensure the best experience, since the compile_commands.json (described here) file contains paths to all the files used, including system and write-only files 4. The setup here works well with enough with the ccls language server as well, which in turn is a more maintained fork of cquery.\nPython Palantir\u0026rsquo;s python-language-server is EOL, and was weird to begin with, but the Spyder team has taken up the gauntlet and maintains the python-lsp-server (repo).\n1micromamba activate lsp # should be done in the ~/.bashrc 2micromamba install python-lsp-server[all] -c conda-forge For working with this in an optimal manner the eglot readme suggests using .dir-local.el files. For example:\n1((python-mode 2. ((eglot-workspace-configuration 3. ((:pylsp . (:plugins (:jedi_completion (:include_params t))))))))) It so happens that this method is flexible enough to allow multi-configuration of servers as well. Additionally, we have options for working with the setup:\n1(after! eglot 2:config 3(set-eglot-client! \u0026#39;python-mode \u0026#39;(\u0026#34;pylsp\u0026#34;)) 4(set-eglot-client! \u0026#39;cc-mode \u0026#39;(\u0026#34;clangd\u0026#34; \u0026#34;-j=3\u0026#34; \u0026#34;--clang-tidy\u0026#34;)) 5) Or in vanilla emacs:\n1(add-to-list \u0026#39;eglot-server-programs 2\u0026#39;(python-mode \u0026#34;pylsp\u0026#34;)) Bash This is fairly straightforward, and implemented in js here.\n1nvm use node 2npm i -g bash-language-server Along with:\n1(sh +lsp) ;; in the init.el file Fortran The installation of fortls (repo) is simple enough.\n1micromamba activate lsp 2pip install fortran-language-server Unfortunately, doom-emacs does not actually support the fortran language server out of the box, so some additional emacs-lisp needs to go into the configuration.\n1(add-hook \u0026#39;f90-mode-hook \u0026#39;eglot-ensure) This works quite well for larger projects.\n Nix rnix-lsp (repo) has no non nix installation, and a slightly uncertain future, but it is still fantastic and shouldn\u0026rsquo;t be left out5.\n1nix-env -i -f https://github.com/nix-community/rnix-lsp/archive/master.tar.gz 2nix-env -i nixkpgs-fmt As this is another unsupported server in doom-emacs, the setup needs some tweaking.\n1(add-hook \u0026#39;nix-mode-hook \u0026#39;eglot-ensure) TeX The last of the language servers, digestif (repo) is implemented in lua, and probably shouldn\u0026rsquo;t be on a remote machine6, but nevertheless.\n1mkdir -p ~/.local/lsp/bin 2cd ~/.local/lsp/bin 3wget https://raw.githubusercontent.com/astoff/digestif/master/scripts/digestif 4chmod +x digestif 5./digestif This sets up $HOME/.digestif/bin which is yet another path to be managed. Note that this assumes a standard TexLive installation with luatex.\nLocal Usage For local use, assuming similar installation instructions, it is easier to manipulate exec-path.\n1(after! eglot 2:config 3(add-hook \u0026#39;nix-mode-hook \u0026#39;eglot-ensure) 4(add-hook \u0026#39;f90-mode-hook \u0026#39;eglot-ensure) 5(set-eglot-client! \u0026#39;cc-mode \u0026#39;(\u0026#34;clangd\u0026#34; \u0026#34;-j=3\u0026#34; \u0026#34;--clang-tidy\u0026#34;)) 6(set-eglot-client! \u0026#39;python-mode \u0026#39;(\u0026#34;pylsp\u0026#34;)) 7(when (string= (system-name) \u0026#34;blah\u0026#34;) 8(setq exec-path (append exec-path \u0026#39;( 9(concat (getenv \u0026#34;HOME\u0026#34;) \u0026#34;/.micromamba/envs/lsp/bin/\u0026#34;) ;; python, fortran 10(concat (getenv \u0026#34;HOME\u0026#34;) \u0026#34;/.local/lsp/bin/\u0026#34;) ;; clangd 11(concat (getenv \u0026#34;HOME\u0026#34;) \u0026#34;/.digestif/bin/\u0026#34;) ;; tex 12(concat (getenv \u0026#34;HOME\u0026#34;) \u0026#34;/.nvm/versions/node/v16.1.0/bin/bash-language-server\u0026#34;) 13))) 14) 15) Where you will need to replace \u0026quot;blah\u0026quot; with the output of (system-name).\nConclusions Embarrassingly, neither setup provided as pleasant an SSH based workflow as VS Code. However, that probably has a lot more to do with TRAMP than any of the language servers, and in any case Magit still makes emacs far superior. In any case, the setup described here is still far superior to not using anything other than projectile and ripgrep so I am satisfied for now. The path setup in this post is deliberately polluting for simplification however, and in practice my Dotfiles are managed a lot better with bombadil which is something for a later post. Each of these will take some getting used to, and new keybindings will probably be needed, especially for my idiomatic Colemak setup.\n  More than adequately managed by hlissner\u0026rsquo;s fantastic doom-emacs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n If you need more information, Doug Davis has a more in-depth vanilla emacs write-up on eglot and C++\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Though my university cluster has nix, most remote machines do not, so we don\u0026rsquo;t depend on it here\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This means, libraries like Eigen or even the standard library can be looked up via eglot\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The original author unfortunately passed away earlier this year\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Visualizing the resultant pdf would be a chore\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/emacs-lang-servers/","tags":["workflow","projects","tools","emacs"],"title":"Doom Emacs and Language Servers"},{"categories":["conferences"],"contents":" A meta-post on a talk I was to give at SERI'21\n Background Much the same as the rationale behind my other presentation meta-posts, that is:\n I would like to preserve questions I would like to collect the video, slides and other miscellaneous stuff in one location 1 It would be nice to have my own thoughts here afterwards  Details SERI or the Software Engineering Research in India meeting is the only informal event dedicated to software engineering in the subcontinent, so it was rather gratifying to be accepted, and, to present on Modern documentation across languages.\nSlides  Best viewed here using a browser (in a new tab) A pdf copy of the slides are embedded below The orgmode source is here on the site\u0026rsquo;s GH repo   Video {{\u0026lt; youtube weEqpRUvNqk \u0026gt;}}\n  One location I am going to be able to keep track of\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/seri-2021-meta/","tags":["presentations","ramblings","docs"],"title":"Talk Supplements for SERI'21"},{"categories":["notes"],"contents":" Enriching ASR nodes at compile time\n Background Serialized update for the 2021 Google Summer of Code under the fortran-lang organization, mentored by Ondrej Certik.\nSeries This post is part of a series based around my weekly GSoC21 project check-ins.\n  GSoC21 W1: LFortran Kickoff\n  GSoC21 W2: LFortran Unraveling\n  GSoC21 W3: Kind, Characters, and Standards\n  GSoC21 W4: LFortran, Backends and Bugs\n  GSoC21 W5: LFortran Design Details and minidftatom\n  GSoC21 W6: LFortran ASR and Values \u0026lt;\u0026ndash; You are here!\n  GSoC21 W7: LFortran Workflow Basics\n  Logistics  Met with Ondrej on Monday and Wednesday  Discussed the design choices w.r.t. class hierarchies (and the lack thereof)    Overview Note that the title is rather misleading, this post has nothing to do with the values of the LFortran project (which are by the way, fantastic), but instead is about adding more detail to the ASR nodes.\nNew Merge Requests  Implement expr values for Integer Binops (1045) First example of using value-enriched nodes for operating during the ast2asr pass  Misc Logs False Starts Coming from a \u0026ldquo;throw it in a debugger\u0026rdquo; mindset, the ASR codebase and design of LFortran can be a bit disconcerting. This is because it is by design implemented mostly in terms of C structs which are cast explicitly throughout the code. Ondrej mentioned that the overhead of having a virtual function table for each instance of LFortran::ASR was prohibitive.\n Why bother with such speed at the cost of the C++ guarantees? Well for one thing, we are building a compiler, and one with an interactive kernel at that, the overhead must be kept low. No one wants to grab a coffee everytime they execute a Python jupyter cell and the goal should be the same for LFortran too. Why not just add value to the expr_t structure? This was a bit of a head scratcher to me, especially my debugger (lldb) offered no information on the value stored even in Constant* structures. The real solution, which required more of an understanding of the code (offered freely and kindly by my mentor) involves casting with down_cast.  Essentially, as far as the code base goes, we have a class based hierarchy, which we manipulate with casts.\nCode duplication is a non-squinter though, since only integer and real have similar implementations, and Ondrej pointed out that over-engineering a design simply because two if-esle branches are similar is effectively yak shaving.\nConclusions real was initially implemented as a string, which has now been replaced by Ondrej in 1066, making an extension to it trivial. I expect to focus on getting started on functions which will have the added benefit of easily adding functionality to the backend (since the LLVM backend can choose to simply use value). I intend to refocus my efforts towards getting minidftatom to work as well.\n","permalink":"https://rgoswami.me/posts/gsoc21-w6/","tags":["gsoc21","fortran","rambling"],"title":"GSoC21 W6: LFortran ASR and Values"},{"categories":["notes"],"contents":" Project scaffolding and compiler design\n Background Serialized update for the 2021 Google Summer of Code under the fortran-lang organization, mentored by Ondrej Certik.\nSeries This post is part of a series based around my weekly GSoC21 project check-ins.\n  GSoC21 W1: LFortran Kickoff\n  GSoC21 W2: LFortran Unraveling\n  GSoC21 W3: Kind, Characters, and Standards\n  GSoC21 W4: LFortran, Backends and Bugs\n  GSoC21 W5: LFortran Design Details and minidftatom \u0026lt;\u0026ndash; You are here!\n  GSoC21 W6: LFortran ASR and Values\n  GSoC21 W7: LFortran Workflow Basics\n  Logistics  Met with Ondrej on Tuesday, Wednesday and Thursday  Discussed the AST, ASR and backends in more detail   Zeroed in on several alternate designs implementations of intrinsic functions  We have prototype calls for the two kinds of compile evaluation My sin implementation (pre-GSoC21) is pretty unwieldy and a cleaner method which we elected to pursue is the minval strategy   Discussed concrete methods of working Talked about the runtime library  This is a late stage issue, after the ASR generation    Overview This week focused around a subset of dftatom which Ondrej prepared. In fact the main highlight was working through possible code design directions and iterating on these with Ondrej (and Gagandeep asynchrously) to come up with the intrinsic function design methods and the compile time evaluation of expressions plan issue (discussed below in issue 420).\nNew Merge Requests  Restructuring of the ASR (1024) A code cleanup task, to reduce code smell from using statements and to refactor into more logical file structures Grammar modifications (1037) Name changes to keep a consistent clear implementation structure  Freshly Assigned Issues  minidftatom Roadmap (401) A clone of the SNAP and dftatom issues size() not handled correctly for arrays (416) This is an ASR bug, with a fix which should not be too difficult to track down Compile time evaluation of expressions (420) This is an internal implementation design specification document, which essentially introduces a value option for the ASR  Additional Tasks Continue down the road for 401 and also consider perhaps, more runtime library logic.\nMisc Logs Since running on a new project is roughly similar, I will only briefly go over the steps.\n1git clone git@gitlab.com:lfortran/examples/minidftatom.git 2cd minidftatom 3FC=gfortran cmake . 4make -j$(nproc) 5./F_atom_U Which generates:\n1Z= 92 N= 5500 2E_tot= -25658.417889 3state E occupancy 41s -3689.355140 2.000 52s -639.778728 2.000 62p -619.108550 6.000 73s -161.118073 2.000 83p -150.978980 6.000 93d -131.977358 10.000 104s -40.528084 2.000 114p -35.853321 6.000 124d -27.123212 10.000 134f -15.027460 14.000 145s -8.824089 2.000 155p -7.018092 6.000 165d -3.866175 10.000 175f -0.366543 3.000 186s -1.325976 2.000 196p -0.822538 6.000 206d -0.143190 1.000 217s -0.130948 2.000 22Print the first 10 values of the 1st and 2nd orbitals: 231753.2892641883584 1753.2812175211841 1753.2731512531607 1753.2650645481160 1753.2569495840532 1753.2488203554758 1753.2406710770906 1753.2325014795395 1753.2243110250588 1753.2160993495806 24587.50702296395195 587.50432661503442 587.50162369317059 587.49891391928361 587.49619467268258 587.49347064291044 587.49073989144335 587.48800232804547 587.48525777252200 587.48250610287175 Parsing and AST Generation The test for the parser is essentially to check if the output before and after formatting with lfortran still compiles to the same result. Note that because of the nature of this, we need to ensure -ffree-line-length-none is passed when working with the gfortran compiler.\n1cd minidftatom 2# -i --\u0026gt; Inplace modification 3for file in *.f90; do lfortran fmt -i $file; done 4make clean 5FC=gfortran cmake . 6make -j$(nproc) 7./F_atom_U Dependency Tree Before going on to the ASR, a dependency tree is required. As before, it is reproduced in excruciating detail in the issue, and represented by a pretty picture shown in Fig. mdftatomgit for this post.\n1pip install fortdepend 2cd minidftatom/src 3fortdepend -g -c \n Figure 1: minidftatom compilation dependency graph\n  ASR Generation I found a new favorite command, which I should have remembered while it was being implemented (by Thirumalai Shaktivel in 765); namely the --indent option which makes for a much more readable ASR output. Apart from that git diff --color-words works well enough as well.\nConclusions The main work plan outcomes for the week were the new design specifications for the compile time evaluation of expressions. It is always an enriching experience to work with Ondrej directly (even over the ether) and this week felt more productive than most. I hope to keep the same momentum going. This will allow faster progress on the main issue of getting ASR generated for minidftatom.\n","permalink":"https://rgoswami.me/posts/gsoc21-w5/","tags":["gsoc21","fortran","rambling"],"title":"GSoC21 W5: LFortran Design Details and minidftatom"},{"categories":["notes"],"contents":" Towards the mid-summer evaluation and redirecting efforts\n Background Serialized update for the 2021 Google Summer of Code under the fortran-lang organization, mentored by Ondrej Certik.\nSeries This post is part of a series based around my weekly GSoC21 project check-ins.\n  GSoC21 W1: LFortran Kickoff\n  GSoC21 W2: LFortran Unraveling\n  GSoC21 W3: Kind, Characters, and Standards\n  GSoC21 W4: LFortran, Backends and Bugs \u0026lt;\u0026ndash; You are here!\n  GSoC21 W5: LFortran Design Details and minidftatom\n  GSoC21 W6: LFortran ASR and Values\n  GSoC21 W7: LFortran Workflow Basics\n  Overview This week was a bit of a sticky wicket. My unfamiliarity with the LLVM backend and its internals caught up with me just around the same time I ran into several fixture and rent related idiosyncrasies which led to yet another shift in my weekly meeting. Thanks to a timely reminder from my mentor Ondrej, I managed, nevertheless, to make some concrete progress working on the compilation of dftatom modules.\nNew Merge Requests  ASR Bug with Arrays (997) Fixes a bug in the existing handling of arrays and allocate calls  Freshly Assigned Issues  Semantics: Implement TINY (388) A classic intrinsic for error handling Semantics: Array Initializer Expressions (389) A bugfix related to the merge request ODE1 Error (390) Placeholder issue to be narrowed and chiseled into something actionable ASR mismatch between `gfortran` modules and source (395) Issue relating to the completeness and equivalence of mod formats  Additional Tasks I\u0026rsquo;d like to make some time to continue exploring the LLVM, and continue familiarizing myself with formal language and automata theory.\nLogistics  I met with Ondrej on Friday this week  He was kind enough to make a standing meeting twice a week, Tuesdays and Fridays This was done to ensure concrete progress is made beyond intangibles   Discussed the possibility of extending the lfortran mod interface  Currently this has a copy of the ASR In principle then, later modules are larger since they have relevant code While gfortran does not bother with having a concrete representation in the mod file   Discussed forming a functional subset of the dftatom code for now  This is related to the slight change in direction which is being considered   Rather than stick to one feature / function from front end to back end, it was decided that a more breadth first approach would be better  Though I am still encouraged to delve into the backend, it is good to set up a fully working rational ASR pass first   Eventually, backends aside, work will need to be undertaken for the runtime library as well  Misc Log Some other things which came my way this week.\nLinking Troubles Sometime down the line of modules and the roadmap to dftatom described in the master issue here, I ran into an odd linker error. These seem to show up with both miniconda and nix, however, it is unclear to me at the moment if it warrants an issue until I check on a native ArchLinux machine. Another natural fix might be to actually bring in and compile llvm; then use the lld linker instead of the bfd.\n1lfortran mod types.mod --show-asr 2Traceback (most recent call last): 3File \u0026#34;/build/glibc-2.32/csu/../sysdeps/x86_64/start.S\u0026#34;, line 120, in _start() 4Binary file \u0026#34;/nix/store/hp8wcylqr14hrrpqap4wdrwzq092wfln-glibc-2.32-37/lib/libc.so.6\u0026#34;, in __libc_start_main() 5File \u0026#34;/users/home/rog32/Git/Github/Fortran/mylf/src/bin/lfortran.cpp\u0026#34;, line 1086, in ?? 6asr = LFortran::mod_to_asr(al, arg_mod_file); 7LFortranException: Unknown module file format In any case, for now, switching to micromamba and using direnv just works.\nMissing Symbols This actually seems related to the more pertinent issue namely, that my lapack module didn\u0026rsquo;t seem to contain relevant symbols at all:\n1lfortran -c types.f90 -o types.o 2lfortran -c lapack.f90 -o lapack.o 3lfortran -c constants.f90 -o constants.o 4lfortran -c interpolation.f90 -o interpolation.o 5Semantic error: The symbol \u0026#39;dgesv\u0026#39; not found in the module \u0026#39;lapack\u0026#39; This is true actually:\n1cat lapack.mod | grep dgesv Truncated Stacktraces Also related, probably, was an issue I noticed while co-working with Ondrej, namely, that my stacktraces in nix were and are a good deal less informative than his, probably due to the default nix lack of debug symbols. This can be quickly reproduced with:\n1lfortran character_array.f90 2BFD: DWARF error: could not find variable specification at offset 2722f 3BFD: DWARF error: could not find variable specification at offset 2723e 4BFD: DWARF error: could not find variable specification at offset 2736f 5BFD: DWARF error: could not find variable specification at offset 2722f 6BFD: DWARF error: could not find variable specification at offset 2723e 7BFD: DWARF error: could not find variable specification at offset 2736f 8BFD: DWARF error: could not find variable specification at offset 2722f 9BFD: DWARF error: could not find variable specification at offset 2723e 10BFD: DWARF error: could not find variable specification at offset 2736f 11BFD: DWARF error: could not find variable specification at offset 2722f 12BFD: DWARF error: could not find variable specification at offset 2723e 13BFD: DWARF error: could not find variable specification at offset 2736f 14BFD: DWARF error: could not find variable specification at offset 2722f 15BFD: DWARF error: could not find variable specification at offset 2723e 16BFD: DWARF error: could not find variable specification at offset 2736f 17BFD: DWARF error: could not find variable specification at offset 2722f 18BFD: DWARF error: could not find variable specification at offset 2723e 19BFD: DWARF error: could not find variable specification at offset 2736f 20BFD: DWARF error: could not find variable specification at offset 615b9 21Traceback (most recent call last): 22File \u0026#34;/build/glibc-2.32/csu/../sysdeps/x86_64/start.S\u0026#34;, line 120, in _start() 23Binary file \u0026#34;/nix/store/hp8wcylqr14hrrpqap4wdrwzq092wfln-glibc-2.32-37/lib/libc.so.6\u0026#34;, in __libc_start_main() 24File \u0026#34;/users/home/rog32/Git/Github/Fortran/mylf/src/bin/lfortran.cpp\u0026#34;, line 1250, in ?? 25err = compile_to_object_file(arg_file, tmp_o, false, 26File \u0026#34;/users/home/rog32/Git/Github/Fortran/mylf/src/bin/lfortran.cpp\u0026#34;, line 601, in (anonymous namespace)::compile_to_object_file(std::__cxx11::basic_string\u0026lt;char, std::char_traits\u0026lt;char\u0026gt;, std::allocator\u0026lt;char\u0026gt; \u0026gt; const\u0026amp;, std::__cxx11::basic_string\u0026lt;char, std::char_traits\u0026lt;char\u0026gt;, std::allocator\u0026lt;char\u0026gt; \u0026gt; const\u0026amp;, bool, bool) [clone .isra.0] 27result = fe.get_asr2(input); 28File \u0026#34;/users/home/rog32/Git/Github/Fortran/mylf/src/lfortran/codegen/evaluator.cpp\u0026#34;, line 469, in LFortran::FortranEvaluator::get_asr2(std::__cxx11::basic_string\u0026lt;char, std::char_traits\u0026lt;char\u0026gt;, std::allocator\u0026lt;char\u0026gt; \u0026gt; const\u0026amp;) 29asr = ast_to_asr(al, *ast, symbol_table); 30File \u0026#34;/users/home/rog32/Git/Github/Fortran/mylf/src/lfortran/semantics/ast_to_asr.cpp\u0026#34;, line 3091, in LFortran::ast_to_asr(Allocator\u0026amp;, LFortran::AST::TranslationUnit_t\u0026amp;, LFortran::SymbolTable*) 31v.visit_TranslationUnit(ast); 32File \u0026#34;/users/home/rog32/Git/Github/Fortran/mylf/src/lfortran/ast.h\u0026#34;, line 3235, in LFortran::SymbolTableVisitor::visit_TranslationUnit(LFortran::AST::TranslationUnit_t const\u0026amp;) 33case modType::Program: { v.visit_Program((const Program_t \u0026amp;)x); return; } 34File \u0026#34;/users/home/rog32/Git/Github/Fortran/mylf/src/lfortran/ast.h\u0026#34;, line 3555, in LFortran::AST::BaseVisitor\u0026lt;LFortran::SymbolTableVisitor\u0026gt;::visit_program_unit(LFortran::AST::program_unit_t const\u0026amp;) 35void visit_program_unit(const program_unit_t \u0026amp;b) { visit_program_unit_t(b, self()); } 36File \u0026#34;/users/home/rog32/Git/Github/Fortran/mylf/src/lfortran/ast.h\u0026#34;, line 3561, in LFortran::AST::BaseVisitor\u0026lt;LFortran::SymbolTableVisitor\u0026gt;::visit_unit_decl2(LFortran::AST::unit_decl2_t const\u0026amp;) 37void visit_unit_decl2(const unit_decl2_t \u0026amp;b) { visit_unit_decl2_t(b, self()); } 38File \u0026#34;/users/home/rog32/Git/Github/Fortran/mylf/src/lfortran/ast.h\u0026#34;, line 3624, in LFortran::AST::BaseVisitor\u0026lt;LFortran::SymbolTableVisitor\u0026gt;::visit_expr(LFortran::AST::expr_t const\u0026amp;) 39void visit_expr(const expr_t \u0026amp;b) { visit_expr_t(b, self()); } 40File \u0026#34;/users/home/rog32/Git/Github/Fortran/mylf/src/lfortran/ast.h\u0026#34;, line 3369, in void LFortran::AST::visit_expr_t\u0026lt;LFortran::SymbolTableVisitor\u0026gt;(LFortran::AST::expr_t const\u0026amp;, LFortran::SymbolTableVisitor\u0026amp;) 41static void visit_expr_t(const expr_t \u0026amp;x, Visitor \u0026amp;v) { 42Binary file \u0026#34;/nix/store/hp8wcylqr14hrrpqap4wdrwzq092wfln-glibc-2.32-37/lib/libc.so.6\u0026#34;, in killpg() 43Segfault: Signal SIGSEGV (segmentation fault) received With a rather simple test problem I was using for my allocatable length character feature.\n1! character_array.f90 2program character_array 3implicit none 45character(15) :: something 6something = \u0026#34;sUpErWeiRdCaSe\u0026#34; 78call upcase(something) 910something = upcase_func(something) 1112contains 1314subroutine upcase(s) 15! Returns string \u0026#39;s\u0026#39; in uppercase 16character(*), intent(in) :: s 17character(len(s)) :: t 18integer :: i, diff 19t = s; diff = ichar(\u0026#39;A\u0026#39;)-ichar(\u0026#39;a\u0026#39;) 20do i = 1, len(t) 21if (ichar(t(i:i)) \u0026gt;= ichar(\u0026#39;a\u0026#39;) .and. ichar(t(i:i)) \u0026lt;= ichar(\u0026#39;z\u0026#39;)) then 22! if lowercase, make uppercase 23 t(i:i) = char(ichar(t(i:i)) + diff) 24end if 25end do 26print*, \u0026#34;Subroutine\u0026#34; 27print*, \u0026#34;Length of arg is \u0026#34;, len(s) 28print*, \u0026#34;Converted \u0026#34; // s // \u0026#34; to \u0026#34; // t 2930end subroutine 3132function upcase_func(s) result(t) 33! Returns string \u0026#39;s\u0026#39; in uppercase 34character(*), intent(in) :: s 35character(len(s)) :: t 36integer :: i, diff 37t = s; diff = ichar(\u0026#39;A\u0026#39;)-ichar(\u0026#39;a\u0026#39;) 38do i = 1, len(t) 39if (ichar(t(i:i)) \u0026gt;= ichar(\u0026#39;a\u0026#39;) .and. ichar(t(i:i)) \u0026lt;= ichar(\u0026#39;z\u0026#39;)) then 40! if lowercase, make uppercase 41 t(i:i) = char(ichar(t(i:i)) + diff) 42end if 43end do 44! print*, new_line(\u0026#39;c\u0026#39;) 45print*, \u0026#34;Function\u0026#34; 46print*, \u0026#34;Length of arg is \u0026#34;, len(s) 47print*, \u0026#34;Converted \u0026#34; // s // \u0026#34; to \u0026#34; // t 48end function 4950end program character_array Gfortran Modules One of the early goals was inter-operability with the gfortran module format. This is a lisp like syntax which changes occasionally.\n1! simple module 2module b 3implicit none 4private 5public g 6contains 7integer function g() 8g = 5 9end function 10end module The corresponding asr is given by:\n1(TranslationUnit (SymbolTable 1 {b: (Module (SymbolTable 2 {g: (Function (SymbolTable 3 {g: (Variable 3 g ReturnVar () Default (Integer 4 []) Source Public Required)}) g [] [(= (Var 3 g) (ConstantInteger 5 (Integer 4 [])))] (Var 3 g) Source Public Implementation)}) b [] .false.)}) []) For the currently supported gfortran module (v14) we get:\n1lfortran mod b14.mod --show-asr 2(TranslationUnit (SymbolTable 1 {g: (Subroutine (SymbolTable 2 {}) g [] [] GFortranModule Public Interface)}) []) Which is contains a lower amount of information. The differences between the module versions are not major:\n More importantly however, the module versions correspond to being compressed and this needs to be eventually handled as well.\nConclusions For the next week, I shall be finalizing work on the allocatable character lengths and moving on to some more semantics and intrinsics. Naturally as these are completed there will be more progress down the dftatom module listing. The main takeaway from this week for me was the retargeting of efforts. I do wish to continue struggling with the backend simply because I feel it is exciting and fun, but I shall endeavor to be more productive by applying a breadth first approach by contributing more concrete code every week. I have high hopes for being able to power through the intended project goals by the end of the program, and can always clean up / document afterwards.\n","permalink":"https://rgoswami.me/posts/gsoc21-w4/","tags":["gsoc21","fortran","rambling"],"title":"GSoC21 W4: LFortran, Backends and Bugs"},{"categories":["conferences"],"contents":" A meta-post on the workshop I held for the IOP CAPS'21 student conference on Web Development for Physicists\n Background Much the same as the rationale behind my other presentation meta-posts, that is:\n I would like to preserve questions I would like to collect the video, slides and other miscellaneous stuff in one location 1 It would be nice to have my own thoughts here afterwards  Details  Workshop listing on conference site  Blurb:\n Social media might be here to stay, however, it is a poor medium to share expert content. In particular, short sound bites aside, two main cornerstones of scientific work in the 21st century, programming and mathematics, are ill suited to most commercial platforms. We will journey into the depths of web development, skimming for the most part for the components needed to develop a well designed platform from which non-peer reviewed content can be disseminated, and expertise can be proven. We will also discuss the types of content and how to decide between multiple possible content outlets.\n Slides  Best viewed here using a browser (in a new tab) A pdf copy of the slides are embedded below The orgmode source is here on the site\u0026rsquo;s GH repo   Video {{\u0026lt; youtube wsn8c5vSBD8 \u0026gt;}}\n  One location I am going to be able to keep track of\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/iop-caps-webdev-2021-meta/","tags":["presentations","ramblings","webdev"],"title":"Talk Supplements for IOP CAPS'21 Webdev Workshop"},{"categories":["notes"],"contents":" Standard practice pragmatic approaches to kind for dftatom\n Background Serialized update for the 2021 Google Summer of Code under the fortran-lang organization, mentored by Ondrej Certik.\nSeries This post is part of a series based around my weekly GSoC21 project check-ins.\n  GSoC21 W1: LFortran Kickoff\n  GSoC21 W2: LFortran Unraveling\n  GSoC21 W3: Kind, Characters, and Standards \u0026lt;\u0026ndash; You are here!\n  GSoC21 W4: LFortran, Backends and Bugs\n  GSoC21 W5: LFortran Design Details and minidftatom\n  GSoC21 W6: LFortran ASR and Values\n  GSoC21 W7: LFortran Workflow Basics\n  Logistics  Met with Ondrej on Tuesday  Went over my kind implementation Merged older approved MRs Worked on generating tests   Talked about the test methodology in general  Most of the tests are better off in their integration form (discussed below) Some aspects of the passes may be tested using the doctest setup   Set an additional time to discuss the implementation of assumed length character declarations  These are not actually used in any dftatom routines but they are very common for utility functions   Met with Ondrej on Thursday  Discussed repercussions of backends  Better, more explicit ASR rules can stem from not relying on the CPP backend     Talked about the number of passes (SRC-\u0026gt;AST-\u0026gt;ASR-\u0026gt;LLVM) Started working on getting the right thing happen when faced with character(len=*)  Overview This week also saw an increase in community activities on the Fortran discourse, since the J3 meeting is now underway and user polling 1 is in full swing.\nNew Merge Requests  Implement more kind() (997) Added tests and code to the AST-\u0026gt;ASR pass for kind calls relevant to dftatom Draft: Implement assumed length (1000) More of a trailer for the next week; also happens to be the 1000th MR (which is neat)  Freshly Assigned Issues  Some more Kind considerations (373) More of a speculative issue about standards compliance, led to the more concrete and general 375  Additional Tasks Still unofficially planning to take a stab at issue 350; regarding the main lfortran.org site.\nKinds The standard allows for any constant to be used in a kind function call. Some commonly seen variants in the wild are:\n1integer, parameter :: dp=kind(0.d0), \u0026amp; ! double precision 2 hp=selected_real_kind(15), \u0026amp; ! high precision 3 qp=selected_real_kind(32), \u0026amp; ! quadruple precision 4 sp = kind(0.) ! single precision Most commonly; kind(0.d0) is seen in the wild. Currently the lfortran ASR defines: ConstantInteger, ConstantReal, ConstantComplex, ConstantLogical of which only ConstantLogical is implemented.\nThe idea is that 0. is single precision; while 1._dp, 1.d0 or 0.d0 are double precision. Essentially then the solution presented itself naturally; to check if d is present; assign double precision if true, otherwise stick to single precision. At the moment, lfortran considers either 4 for single or 8 for double.\nMore generally, it might be useful to make the single and double precision values managed by a pre-processor. For the other kinds of precision, some more thought is required.\nAt this stage, it compiles with the LLVM backend.\n1lfortran -v -c --show-asr --show-stacktrace lapack.f90 2lfortran -v -c --backend=llvm --show-stacktrace lapack.f90 Tests While implementing I used the simplest of debugging concepts, that of making manual changes and writing out results. However, no compiler can survive without rigorous unit tests, and lfortran is no different.\nFortran Integration Tests So called because these are run by a python driver and store verbatim the stdout in files. Note that this form of testing, though convenient, does require the user to be certain of the test before writing it; due to the update process it is possible to store the wrong result and consistently get it wrong.\n1./run_tests.py -u # Update Unit Tests for Passes These are not favored, as any change in any of the passes causes a large amount of test metadata to be invalidated. There are some tests of this nature, and the framework used is doctest.\nTowards Assumed Lengths The first step towards implementing assumed lengths was to isolate the problem, which was done through the age-old comment recompile and test methodology. The offending function is a rather innocuous looking helper function:\n1function upcase(s) result(t) 2! Returns string \u0026#39;s\u0026#39; in uppercase 3character(*), intent(in) :: s 4character(len(s)) :: t 5integer :: i, diff 6t = s; diff = ichar(\u0026#39;A\u0026#39;)-ichar(\u0026#39;a\u0026#39;) 7do i = 1, len(t) 8if (ichar(t(i:i)) \u0026gt;= ichar(\u0026#39;a\u0026#39;) .and. ichar(t(i:i)) \u0026lt;= ichar(\u0026#39;z\u0026#39;)) then 9! if lowercase, make uppercase 10 t(i:i) = char(ichar(t(i:i)) + diff) 11end if 12end do 13end function Having zeroed in on a prototypical failure point within dftatom, in the utils.f90 file, the next steps are documented in the relevant issue and PR.\nThe AST representation of the code is :\n1(TranslationUnit [(Module utils [] [(ImplicitNone [])] [(Declaration () [(SimpleAttribute AttrPrivate)] []) (Declaration () [(SimpleAttribute AttrPublic)] [(upcase [] [] () None ())])] [(Function upcase [(s)] [] t () [] [] [] [(Declaration (AttrType TypeCharacter [(len () Star)] ()) [(AttrIntent In)] [(s [] [] () None ())]) (Declaration (AttrType TypeCharacter [(() 20 Value)] ()) [] [(t [] [] () None ())])] [] [])])]) No simplifications can be expected to occur from the AST representation to the ASR for this situation so the first order of business is to let the ASR accept the AST representation and pass it through to the backend.\nEarly on, I considered managing this particular feature in the cpp backend, but Ondrej pointed out that the ease-of-use features and guarantees of the cpp compiler would lead to a more sloppy ASR implementation.\nConclusions This third week led me down the rabbit hole with regards to the standard, and a statement comes to mind from Clerman and Spector (2012):\n The standard is the contract between the compiler writer and the application developer.\n For most of my life I\u0026rsquo;ve been the latter, but now, the intricate legalese of the standard haunts much of my day.\nThis attempt at standards level rigor in particular, for kind (#373) lead to the discussion on the best way to work with constant expressions at compile time (#375).\nMy vaccination with the single shot Janssen vaccine and subsequent side effects, have had a deliberating effect. However, the additional meeting with Ondrej more than made up for it in terms of project productivity.\nI also satiated mostly my interest in the historical development of \u0026ldquo;automatic coding\u0026rdquo; or compiling as we would now call it in the context of Fortran largely due to Cipra (n.d.), Backus (1998); and its use at the University of Iceland 2. It is very liberating to be able to keep a public record of my reading habits and fancies, however 20 hours of text and code consumed along with ruminative thoughts is rather too much for a single weekly post, might need to eventually step it up. Other interesting things which just haven\u0026rsquo;t made the cut for this post include finally going through the gfortran implementation in some more detail and looking into the linking and loading process via Levine (2000).\nThe primary goal for next week shall remain implementing assumed length character arrays and then continuing with the laundry list of dftatom tests.\nReferences Backus, J. 1998. \u0026ldquo;The History of Fortran I, II, and III.\u0026rdquo; IEEE Annals of the History of Computing 20 (4): 68\u0026ndash;78. https://doi.org/10.1109/85.728232.\n Cipra, Barry A. n.d. \u0026ldquo;The Best of the 20th Century: Editors Name Top 10 Algorithms,\u0026rdquo; 2.\n Clerman, Norman S., and Walter Spector. 2012. Modern Fortran: Style and Usage. New York: Cambridge University Press.\n Levine, John R. 2000. Linkers and Loaders. San Francisco: Morgan Kaufmann.\n    For example, this poll on conditional expressions\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The community discourse has historical records too\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/gsoc21-w3/","tags":["gsoc21","fortran","rambling"],"title":"GSoC21 W3: Kind, Characters, and Standards"},{"categories":["notes"],"contents":" Delving into language standards and back-ends for lfortran\n Background As discussed in a previous post in this series, I have been spending roughly half of each working day with LFortran as part of the 2021 Google Summer of Code under the fortran-lang organization, mentored by Ondrej Certik.\nSeries This post is part of a series based around my weekly GSoC21 project check-ins.\n  GSoC21 W1: LFortran Kickoff\n  GSoC21 W2: LFortran Unraveling \u0026lt;\u0026ndash; You are here!\n  GSoC21 W3: Kind, Characters, and Standards\n  GSoC21 W4: LFortran, Backends and Bugs\n  GSoC21 W5: LFortran Design Details and minidftatom\n  GSoC21 W6: LFortran ASR and Values\n  GSoC21 W7: LFortran Workflow Basics\n  Logistics Some of the meeting points are to be expanded on below.\n Met with Ondrej on Tuesday, as discussed previously  Talked about language server implementations  Looked into rtags and generating a compilation-database Discussed how the C++ concept of having file based units makes this simpler than the Fortran form, which recognizes no file based program units     Talked about the status of the different back-ends Discussed LLVM and MLIR, in the context of Flang (the f18 compiler)  Also briefly touched upon legacy-flang and historical issues   Discussed standardization of the mod-files  Decided this is not a good idea, because a lot of build systems expect Fortran compilers to generate .mod files, even if they are completely incompatible The standard does not specify what should be contained in a mod file Every mod file of every compiler is unique and needs to be handled separately The goal (of this project too) is to handle at the very least conversion of gfortran module files to lfortran module files    Overview For the second week of my project on getting lfortran to compile dftatom and in general form a usable compiler ecosystem to facilitate greater adoption in the community, I opted to take a bit of a step back and delve into the historical evolution of both the Fortran standard itself Lyon (1980) and also the compiler ecosystem. This was also in no small part due to the fact that I ended up moving during this week, which forced me to spend a lot of time cleaning and playing with adult LEGO 1. This naturally left me with plenty of time to both participate 2 in the monthly Fortran-lang call (which was rather explosive 3) and also contemplate the overall ecosystem of the standards committee and compilers. Many more issues were set up and will be completed over the weekend.\nMerge Requests  Minor installation bug (985) Fixed a small issue with the cmake files  Assigned Issues  Handling kind in the ASR (357) A straightforward correction to make the ASR handle more common, but non-standard use cases GFortran Module v15 support (355) Currently only supports v14 Runtime Math and the CPP back-end (354) Might take a little longer, does not compile at the moment  Additional Tasks I intend to write more about the Fortran standard and go through more of the compiler code I can get my hands on, since that should give me a better handle on the different way the standards have been implemented (and augmented!). As part of this, I\u0026rsquo;ll probably handle some of issue 350; regarding the main lfortran.org site.\nLFortran and Back-ends LLVM This is the default, and the most performance oriented. It is however, slightly more complicated to extend due to the intricacies of the LLVM syntax.\nC++ This is a newer back-end, perfect for rapid prototyping; and can be selected by passing the --backend=cpp flag to lfortran (e.g. with FFLAGS). This is easier to work with in that many features boil down to stdlib calls. However, this back-end requires KOKKOS. It also has uglier error handling as shown in Fig. 1. Since this is more attractive as a candidate for the math intrinsics as a first approximation, it is of considerable interest to me.\nIt is however, still fairly straightforward to work with.\n1cd $LFROOT # LFortran root 2export LFORTRAN_KOKKOS_DIR=$LFROOT/ext/kokkos 3mkdir extsrc \u0026amp;\u0026amp; cd extsrc 4gh repo clone kokkos/kokkos 5cd kokkos \u0026amp;\u0026amp; mkdir build \u0026amp;\u0026amp; cd build 6cmake -DCMAKE_INSTALL_PREFIX=$LFORTRAN_KOKKOS_DIR -DKokkos_ARCH_HSW=On .. 7make -j$(nproc) 8make install 9cd $LFORTRAN_KOKKOS_DIR # Need to make a symlink 10ln -sf lib64/ lib 11cd ../../examples/project1 12FC=lfortran FFLAGS=\u0026#34;--backend=cpp\u0026#34; cmake . 13make 14./project1 # Profit \n Figure 1: Contrasting backends\n  Inspecting intermediates The module files cannot currently be read or queried from lfortran, however a representation of what gets compressed into these files can be obtained with the --show-asr flag. Additionally, a very python inspired --show-stacktrace is implemented to help narrow down areas which need to be augmented.\ngfortran 1cd $DFTATM/src 2gfortran -c types.f90 3# Read gfortran module 4zcat types.mod 5# Output 6GFORTRAN module version \u0026#39;15\u0026#39; created from types.f90 7(() () () () () () () () () () () () () () () () () () () () () () () () 8() () ()) 910() 1112() 1314() 1516() 1718() 1920(2 \u0026#39;dp\u0026#39; \u0026#39;types\u0026#39; \u0026#39;\u0026#39; 1 ((PARAMETER UNKNOWN-INTENT UNKNOWN-PROC UNKNOWN 21IMPLICIT-SAVE 0 0) () (INTEGER 4 0 0 0 INTEGER ()) 0 0 () (CONSTANT ( 22INTEGER 4 0 0 0 INTEGER ()) 0 \u0026#39;8\u0026#39; ()) () 0 () () () 0 0) 23) 2425(\u0026#39;dp\u0026#39; 0 2) lfortran 1cd $DFTATM/src 2lfortran --show-asr -c types.f90 3# Output 4(TranslationUnit (SymbolTable 1 {lfortran_intrinsic_kind: (Module (SymbolTable 4 {dkind: (Function (SymbolTable 5 {r: (Variable 5 r ReturnVar () Default (Integer 4 []) Source Public), x: (Variable 5 x In () Default (Real 8 []) Source Public)}) dkind [(Var 5 x)] [(= (Var 5 r) (ConstantInteger 8 (Integer 4 [])))] (Var 5 r) Intrinsic Public Implementation), kind: (Function (SymbolTable 6 {r: (Variable 6 r ReturnVar () Default (Integer 4 []) Source Public), x: (Variable 6 x In () Default (Logical 4 []) Source Public)}) kind [(Var 6 x)] [(= (Var 6 r) (ConstantInteger 4 (Integer 4 [])))] (Var 6 r) Intrinsic Public Implementation), lkind: (Function (SymbolTable 7 {r: (Variable 7 r ReturnVar () Default (Integer 4 []) Source Public), x: (Variable 7 x In () Default (Logical 4 []) Source Public)}) lkind [(Var 7 x)] [(= (Var 7 r) (ConstantInteger 4 (Integer 4 [])))] (Var 7 r) Intrinsic Public Implementation), selected_int_kind: (Function (SymbolTable 8 {R: (Variable 8 R In () Default (Integer 4 []) Source Public), res: (Variable 8 res ReturnVar () Default (Integer 4 []) Source Public)}) selected_int_kind [(Var 8 R)] [(If (Compare (Var 8 R) Lt (ConstantInteger 10 (Integer 4 [])) (Logical 4 [])) [(= (Var 8 res) (ConstantInteger 4 (Integer 4 [])))] [(= (Var 8 res) (ConstantInteger 8 (Integer 4 [])))])] (Var 8 res) Intrinsic Public Implementation), selected_real_kind: (Function (SymbolTable 9 {R: (Variable 9 R In () Default (Integer 4 []) Source Public), res: (Variable 9 res ReturnVar () Default (Integer 4 []) Source Public)}) selected_real_kind [(Var 9 R)] [(If (Compare (Var 9 R) Lt (ConstantInteger 7 (Integer 4 [])) (Logical 4 [])) [(= (Var 9 res) (ConstantInteger 4 (Integer 4 [])))] [(= (Var 9 res) (ConstantInteger 8 (Integer 4 [])))])] (Var 9 res) Intrinsic Public Implementation), skind: (Function (SymbolTable 10 {r: (Variable 10 r ReturnVar () Default (Integer 4 []) Source Public), x: (Variable 10 x In () Default (Real 4 []) Source Public)}) skind [(Var 10 x)] [(= (Var 10 r) (ConstantInteger 4 (Integer 4 [])))] (Var 10 r) Intrinsic Public Implementation)}) lfortran_intrinsic_kind [] .true.), types: (Module (SymbolTable 2 {dp: (Variable 2 dp Local (FunctionCall 2 kind () [(ConstantReal \u0026#34;0.d0\u0026#34; (Real 4 []))] [] (Integer 4 [])) Parameter (Integer 4 []) Source Public), kind: (ExternalSymbol 2 kind 4 kind lfortran_intrinsic_kind kind Private)}) types [lfortran_intrinsic_kind] .false.)}) []) Note that, lfortran is expected to eventually interoperate with existing mod files from compilers like gfortran.\n1lfortran mod --show-asr types.mod 2# Output 3Traceback (most recent call last): 4File \u0026#34;/build/glibc-2.32/csu/../sysdeps/x86_64/start.S\u0026#34;, line 120, in _start() 5Binary file \u0026#34;/nix/store/hp8wcylqr14hrrpqap4wdrwzq092wfln-glibc-2.32-37/lib/libc.so.6\u0026#34;, in __libc_start_main() 6File \u0026#34;$HOME/Git/Github/Fortran/mylf/src/bin/lfortran.cpp\u0026#34;, line 1076, in ?? 7asr = LFortran::mod_to_asr(al, arg_mod_file); 8File \u0026#34;$HOME/Git/Github/Fortran/mylf/src/lfortran/mod_to_asr.cpp\u0026#34;, line 361, in LFortran::mod_to_asr(Allocator\u0026amp;, std::__cxx11::basic_string\u0026lt;char, std::char_traits\u0026lt;char\u0026gt;, std::allocator\u0026lt;char\u0026gt; \u0026gt;) 9return parse_gfortran_mod_file(al, s); 10LFortranException: Only GFortran module version 14 is implemented so far Which is the source of an issue assigned to me.\nConclusions As I cross my fingers and hope my tenuously tethered wifi is equal to the task of uploading this post; I can only hope that the installation of my WiFi on Monday goes smoothly enough to be restored to full working capacity for the next week. Until then, I shall trawl the code-bases and standards for an inkling of what makes compiler design the art form as described in Cooper and Torczon (2011).\nReferences Cooper, Keith, and Linda Torczon. 2011. Engineering a Compiler. Elsevier. https://books.google.com?id=_tgh4bgQ6PAC.\n Lyon, G. E. 1980. Using Ans Fortran. National Bureau of Standards. https://books.google.com?id=8ymHAQAACAAJ.\n    I am convinced the concept behind IKEA and its Icelandic counterpart Rumfatalagerinn is that some people enjoy assembling their furniture the same way kids enjoy LEGO\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Owing to the lack of a stable internet I was reduced to text-only participation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n There were some interesting allegations of the standards committee being far too conservative, though many of the concerns reflect on the dearth of community efforts compared to other languages; Fortran is not after-all coupled to a compiler unlike modern languages like Rust and Julia\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/gsoc21-w2/","tags":["gsoc21","fortran","rambling"],"title":"GSoC21 W2: LFortran Unraveling"},{"categories":["notes"],"contents":" Charting paths towards concrete lfortran usage\n Background As mentioned in an earlier post, I have had the immense pleasure of continuing development of the disruptive lfortran compiler under the aegis of the Fortran Lang organization, financed by the Google Summer of Code and mentored ably by Ondrej Certik. A rather interesting consequence of this is that we are strongly encouraged to write a precis of our activities each week. This is mine, given that the clock starting winding down (full timeline) last Monday.\nI will workshop a few styles of getting maximal information into the post while ensuring the bulk of the technical details and other related notes makes its way into the appropriate channels, that is, into the lfortran project repository and documentation.\nSeries This post is part of a series based around my weekly GSoC21 project check-ins.\n  GSoC21 W1: LFortran Kickoff \u0026lt;\u0026ndash; You are here!\n  GSoC21 W2: LFortran Unraveling\n  GSoC21 W3: Kind, Characters, and Standards\n  GSoC21 W4: LFortran, Backends and Bugs\n  GSoC21 W5: LFortran Design Details and minidftatom\n  GSoC21 W6: LFortran ASR and Values\n  GSoC21 W7: LFortran Workflow Basics\n  Logistics  I met with Ondrej on Thursday, though our standing arrangement is for Tuesday  Meeting earlier in the week helps reduce chances of veering off course Though an equitable distribution of work is assumed, weekends tend to be more conductive to bursts of effort   We decided to release progress reports every Friday night for community oversight and approval  Slight discontinuity here, in that the weekend remains, but the idea is to take community suggestions per week as well   We discussed the work completed by my fellow GSoC student developers over the community bonding period Discussed the current minimum viable project issue relating to SNAP 1, the SN (Discrete Ordinates) Application Proxy from LANL  Also looked into starting work on the issue relating to dftatom which forms the crux of my project   Determined approaches towards the final goal  Discussed the possibility of making a first pass in the C++ backend instead of the LLVM one  Con: Is not very fast Pro: Is very easy     Notably discussed adding generic types, e.g. type(*) to internal routines for the compiler  Saves a lot of boilerplate code Could start with a simple fypp based approach (similar to fpm) Should be fine as long as users do not start using them    As per my initial project plan, the goal for the week (in progress) is the Enumeration of language features used in the dftatoms project and cross correlating it with implemented features in LFortran. To this, we decided that as the AST works well enough, it would be better to follow in the footsteps of the SNAP issue and determine the dependency tree, before working through it steadily.\nSetting up LFortran and dftatom One approach to doing this was naturally, to compile everything by hand, painstakingly using the logical dependency graph which comes with the repository. Note that for completion (and to add unnecessary gravitas to this post), we include once the setup details for lfortran and dftatom which have been included verbatim from the documentation 2.\nLFortran On Linux systems, I use nix (naturally). On a Mac, nix has unresolved issues upstream, so it is easier to use micromamba (here) or some other anaconda helper.\n1# Linux 2git clone git@gitlab.com:lfortran/lfortran 3cd lfortran 4nix-shell ci/shell.nix 5./build0.sh 6./build1.sh 7export PATH=$(pwd)/inst/bin/:$PATH 8./test_lfortran_cmdline 9./run_tests.py 10# MacOS 11micromamba create -p ./tmp llvmdev=11.0.1 bison=3.4 re2c python cmake make toml -c conda-forge 12micromamba activate tmp 13export PATH=$(pwd)/tmp/bin:$PATH 14./build0.sh 15cmake -DCMAKE_BUILD_TYPE=Debug -DWITH_LLVM=yes -DCMAKE_INSTALL_PREFIX=`pwd`/inst . 16make -j$(nproc) 17ctest 18./run_tests.py dftatom Another post can go over why dftatom was chosen as a benchmark code, but to keep things short, it is well written, has a great cython wrapper, is fast, and is useful Čertík, Pask, and Vackář (2013). Rather than write a nix shell for it though, I opted to get started with the anaconda set of instructions to test with.\n1git clone git@github.com:certik/dftatom 2cd dftatom 3micromamba create -p ./tmp cython numpy python=3.8 pytest hypothesis -c conda-forge 4cmake -DCMAKE_INSTALL_PREFIX=$CONDA_PREFIX -DCMAKE_PREFIX_PATH=$PREFIX -DWITH_PYTHON=yes . 5make 6make install 7PYTHONPATH=. python examples/atom_U.py Later in the week (over the weekend) I might add some quality of life commits like a nix environment.\nTesting the AST The logic behind the AST test is that if we can parse each file with lfortran and write it back out into the f90 files which can then be compiled by gfortran and then subsequently pass our tests; then the AST is roughly alright.\nNaturally this still strips comments and therefore openmp directives, of which dftatom thankfully has none.\nNow it so happens that this is fairly trivial with lfortran and bash.\n1cd $DFTATM/src # src under dftatom repo 2find . -name \u0026#34;*.f90\u0026#34; -exec lfortran fmt -i {} \\; 3# Rebuild and test with gfortran This worked well. No glaring errors yet. Which brings us to the ASR representation 3.\nTesting the ASR To start testing the ASR with lfortran -c fname.f90 -o fname.o and variations thereof, a good starting point is to set up a dependency graph. A logical one is provided in the repository itself as seen in Fig. 1.\n\n Figure 1: dftatom logical layout\n  However, this is not exactly the order in which things are built. Thankfully, the fantastic fortdepend tool from Peter Hill can speed things up considerably.\n1# in the conda environment 2pip install fortdepend 3cd $DFTATM/src 4fortdepend -o Makefile.dep This can and was then manually inspected and marshalled by order of dependencies into the issue. Here we will conclude with the visual details instead of dumping the entire dependency tree.\n\n Figure 2: dftatom compilation dependency graph\n  Conclusions That leads us to the end of this week\u0026rsquo;s effectively mid-week update. For the remainder of the week, until the meeting with Ondrej I shall work through parts of the dependency graph and write some more about dftatom and the lfortran terminology which will be used for the remainder of the project.\nReferences Čertík, Ondřej, John E. Pask, and Jiří Vackář. 2013. \u0026ldquo;Dftatom: A Robust and General Schrödinger and Dirac Solver for Atomic Structure Calculations.\u0026rdquo; Computer Physics Communications 184 (7): 1777\u0026ndash;91. https://doi.org/10.1016/j.cpc.2013.02.014.\n    Not the Spectral Neighbor Analysis Potential implemented in LAMMPS\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Part of which I added earlier and fixed a typo in now\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Should also add a post on standard terminology which will come up a lot\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/gsoc21-w1/","tags":["gsoc21","fortran","rambling"],"title":"GSoC21 W1: LFortran Kickoff"},{"categories":["conferences"],"contents":" A meta-post on the workshop I held for the Section Leaders of Code in Place 2021 entitled \u0026ldquo;Wrangling Pythons with Nix for Reproducible Purity\u0026rdquo;\n Background Much the same as the rationale behind my other presentation meta-posts, that is:\n I would like to preserve questions I would like to collect the video, slides and other miscellaneous stuff in one location 1 It would be nice to have my own thoughts here afterwards  Details Blurb verbatim from the spreadsheet.\n Docker. Virtual Machines. Pipenv. Virtualenv. The struggle to isolate development environments and pin package versions (both system and project) has been a defining feature of modern programming. Python packaging is a nightmare in and of itself, moving from eggs to wheels, collecting system requirements with distutils (PEP 518)\u0026hellip; The horror never ends. Indeed every language level packaging system still falls short of collecting system dependencies (MKL, LAPACK etc). This workshop will introduce the Nix ecosystem for working with Python (and system dependencies) in a lightweight, reproducible manner. We shall work through generating pure shells for development, and clean builds for PyPI. We\u0026rsquo;ll see how to package arbitrary Python code and what level of nix expression language proficiency is required for the same (spoiler: not much!).\n Other Content Anything on this site tagged with Nix or tagged with Python. Also an introduction to nix given by me (and Amrita Goswami) at CarpentryCon2020 is here:\n CarpentryCon2020 Materials A tutorial introduction to Nix and Python  Slides  Best viewed here using a browser (in a new tab) A pdf copy of the slides are embedded below The orgmode source is here on the site\u0026rsquo;s GH repo   Thoughts I gave the workshop twice and it was rather well attended both times. The speaker presentation room in Oh Yay was very enjoyable and improved the interactivity a lot.\n  One location I am going to be able to keep track of\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/cip-2021-slw-meta/","tags":["presentations","ramblings","python"],"title":"Talk Supplements for Code In Place 2021 SL Workshop"},{"categories":["notes"],"contents":"Background  Transitioning from biblatex to bibtex with biber for sphinx\n I recently started a set of notes using jupyter-book. However, in the process I ran into a horrific bibliography related SNAFU. sphinx in its infinite wisdom only accepts a rather odd subset of bibtex.\nI have been happily exporting my giant bibliography with Zotero (and better bibtex) exporting my references as biblatex while sphinx started choking dreadfully. This post describes attempts to reconcile the biblatex sources without manual intervention.\nInput The input file was an innocuous looking tiny neat biblatex file:\n1@online{grasedyckParameterdependentSmootherMultigrid2020, 2title = {A Parameter-Dependent Smoother for the Multigrid Method}, 3author = {Grasedyck, Lars and Klever, Maren and Löbbert, Christian and Werthmann, Tim A.}, 4date = {2020-08-03}, 5url = {http://arxiv.org/abs/2008.00927}, 6urldate = {2021-05-22}, 7archiveprefix = {arXiv}, 8eprint = {2008.00927}, 9eprinttype = {arxiv}, 10keywords = {65N55; 15A69,Mathematics - Numerical Analysis}, 11primaryclass = {cs, math} 12} 1314@article{grasedyckDistributedHierarchicalSVD2018, 15title = {Distributed Hierarchical {{SVD}} in the {{Hierarchical Tucker}} Format}, 16author = {Grasedyck, Lars and Löbbert, Christian}, 17date = {2018}, 18journaltitle = {Numerical Linear Algebra with Applications}, 19volume = {25}, 20pages = {e2174}, 21issn = {1099-1506}, 22doi = {10.1002/nla.2174}, 23url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nla.2174}, 24urldate = {2021-05-22}, 25annotation = {\\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nla.2174}, 26keywords = {Hierarchical Tucker,HT,multigrid,parallel algorithms,SVD,tensor arithmetic}, 27langid = {english}, 28number = {6} 29} 3031@article{grasedyckHierarchicalSingularValue2010, 32title = {Hierarchical {{Singular Value Decomposition}} of {{Tensors}}}, 33author = {Grasedyck, Lars}, 34date = {2010-01-01}, 35journaltitle = {SIAM Journal on Matrix Analysis and Applications}, 36shortjournal = {SIAM J. Matrix Anal. Appl.}, 37volume = {31}, 38pages = {2029--2054}, 39publisher = {{Society for Industrial and Applied Mathematics}}, 40issn = {0895-4798}, 41doi = {10.1137/090764189}, 42url = {https://epubs.siam.org/doi/abs/10.1137/090764189}, 43urldate = {2021-05-22}, 44number = {4} 45} Manual cleaning The first attempt was with bibutils.\n1biblatex2xml references.bib | xml2bib | tee refs.bib sphinx does not support electronic and demands misc instead.\n1biblatex2xml references.bib | xml2bib | sed -e \u0026#34;s/Electronic{/misc{/g\u0026#34; | tee refs.bib This brought us to:\n1@misc{grasedyckParameterdependentSmootherMultigrid2020, 2author=\u0026#34;Grasedyck, Lars 3and Klever, Maren 4and L{\\\u0026#34;o}bbert, Christian 5and Werthmann, Tim A.\u0026#34;, 6title=\u0026#34;A Parameter-Dependent Smoother for the Multigrid Method\u0026#34;, 7year=\u0026#34;2020\u0026#34;, 8month=\u0026#34;Aug\u0026#34;, 9day=\u0026#34;03\u0026#34;, 10archivePrefix=\u0026#34;arXiv\u0026#34;, 11eprint=\u0026#34;2008.00927\u0026#34;, 12url=\u0026#34;http://arxiv.org/abs/2008.00927\u0026#34; 13} 1415@Article{grasedyckDistributedHierarchicalSVD2018, 16author=\u0026#34;Grasedyck, Lars 17and L{\\\u0026#34;o}bbert, Christian\u0026#34;, 18title=\u0026#34;Distributed Hierarchical SVD in the Hierarchical Tucker Format\u0026#34;, 19journal=\u0026#34;Numerical Linear Algebra with Applications\u0026#34;, 20year=\u0026#34;2018\u0026#34;, 21volume=\u0026#34;25\u0026#34;, 22number=\u0026#34;6\u0026#34;, 23pages=\u0026#34;e2174\u0026#34;, 24issn=\u0026#34;1099-1506\u0026#34;, 25doi=\u0026#34;10.1002/nla.2174\u0026#34;, 26url=\u0026#34;https://onlinelibrary.wiley.com/doi/abs/10.1002/nla.2174\u0026#34;, 27url=\u0026#34;https://doi.org/10.1002/nla.2174\u0026#34; 28} 2930@Article{grasedyckHierarchicalSingularValue2010, 31author=\u0026#34;Grasedyck, Lars\u0026#34;, 32title=\u0026#34;Hierarchical Singular Value Decomposition of Tensors\u0026#34;, 33journal=\u0026#34;SIAM Journal on Matrix Analysis and Applications\u0026#34;, 34year=\u0026#34;2010\u0026#34;, 35month=\u0026#34;Jan\u0026#34;, 36day=\u0026#34;01\u0026#34;, 37volume=\u0026#34;31\u0026#34;, 38number=\u0026#34;4\u0026#34;, 39pages=\u0026#34;2029--2054\u0026#34;, 40issn=\u0026#34;0895-4798\u0026#34;, 41doi=\u0026#34;10.1137/090764189\u0026#34;, 42url=\u0026#34;https://epubs.siam.org/doi/abs/10.1137/090764189\u0026#34;, 43url=\u0026#34;https://doi.org/10.1137/090764189\u0026#34; 44} Unfortunately this also generated multiple url lines, which choke sphinx of course. Additionally, the workflow is rather fickle e.g. not starting the file with a blank line causes the first reference to be silently skipped.\nBiber The key concept here is that biber can take a configuration file to process the bib file and is also able to copy the formatted file over with tool. So our solution will boil down to writing a configuration file and then using it.\nFirst we get the location of the configuration (also check the version).\n1biber --tool-config 2biber --version Then we inspect it and extract relevant entries to a configuration file of our own, with some modifications.\n We need to reverse each entry as described here  However in practice we can get away with a minimal mapping and some command line options..   We would also like to be mindful of the mapping from online to misc   1\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding= \u0026#34;utf-8\u0026#34;?\u0026gt; 2\u0026lt;config\u0026gt; 3\u0026lt;sourcemap\u0026gt; 4\u0026lt;maps datatype=\u0026#34;bibtex\u0026#34;\u0026gt; 5\u0026lt;!-- Easy type conversions --\u0026gt; 6\u0026lt;map\u0026gt; 7\u0026lt;map_step map_type_source=\u0026#34;report\u0026#34; map_type_target=\u0026#34;techreport\u0026#34;/\u0026gt; 8\u0026lt;map_step map_type_source=\u0026#34;online\u0026#34; map_type_target=\u0026#34;misc\u0026#34;/\u0026gt; 9\u0026lt;/map\u0026gt; 10\u0026lt;/maps\u0026gt; 11\u0026lt;/sourcemap\u0026gt; 12\u0026lt;/config\u0026gt; At this stage, we are partially done, but the resulting file is rather ugly, and most importantly dates have not yet been transformed. This SE answer provided a hint, as did this gist. Effectively, we need to overwrite part of the map.\n1\u0026lt;!-- Date to year, month --\u0026gt; 2\u0026lt;map\u0026gt; 3\u0026lt;map_step map_field_source=\u0026#34;date\u0026#34; 4map_field_target=\u0026#34;year\u0026#34; /\u0026gt; 5\u0026lt;/map\u0026gt; 6\u0026lt;map\u0026gt; 7\u0026lt;map_step map_field_source=\u0026#34;year\u0026#34; 8map_match=\u0026#34;(\\d{4}|\\d{2})-(\\d{1,2})-(\\d{1,2})\u0026#34; 9map_final=\u0026#34;1\u0026#34; /\u0026gt; 10\u0026lt;map_step map_field_source=\u0026#34;year\u0026#34; 11map_match=\u0026#34;(\\d{4}|\\d{2})-(\\d{1,2})-(\\d{1,2})\u0026#34; 12map_replace=\u0026#34;$1\u0026#34; /\u0026gt; 13\u0026lt;map_step map_field_set=\u0026#34;month\u0026#34; map_origfieldval=\u0026#34;1\u0026#34; /\u0026gt; 14\u0026lt;map_step map_field_source=\u0026#34;month\u0026#34; 15map_match=\u0026#34;(\\d{4}|\\d{2})-(\\d{1,2})-(\\d{1,2})\u0026#34; 16map_replace=\u0026#34;$2\u0026#34; /\u0026gt; 17\u0026lt;/map\u0026gt; 18\u0026lt;map\u0026gt; 19\u0026lt;map_step map_field_source=\u0026#34;year\u0026#34; 20map_match=\u0026#34;(\\d{4}|\\d{2})-(\\d{1,2})\u0026#34; map_final=\u0026#34;1\u0026#34; /\u0026gt; 21\u0026lt;map_step map_field_source=\u0026#34;year\u0026#34; 22map_match=\u0026#34;(\\d{4}|\\d{2})-(\\d{1,2})\u0026#34; map_replace=\u0026#34;$1\u0026#34; /\u0026gt; 23\u0026lt;map_step map_field_set=\u0026#34;month\u0026#34; map_origfieldval=\u0026#34;1\u0026#34; /\u0026gt; 24\u0026lt;map_step map_field_source=\u0026#34;month\u0026#34; 25map_match=\u0026#34;(\\d{4}|\\d{2})-(\\d{1,2})\u0026#34; map_replace=\u0026#34;$2\u0026#34; /\u0026gt; 26\u0026lt;/map\u0026gt; Now we can also add some pretty printing (before the sourcemap):\n1\u0026lt;output_fieldcase\u0026gt;lower\u0026lt;/output_fieldcase\u0026gt; 2\u0026lt;output_resolve\u0026gt;1\u0026lt;/output_resolve\u0026gt; 3\u0026lt;output_safechars\u0026gt;1\u0026lt;/output_safechars\u0026gt; 4\u0026lt;output_format\u0026gt;bibtex\u0026lt;/output_format\u0026gt; Putting it all together:\n1\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; 2\u0026lt;!-- Got the date from https://gist.githubusercontent.com/mkouhia/f00fea7fc8d4effd9dfd/raw/500e9dbc6aa43a47e39c45ba230738ff4544709f/biblatex-to-bibtex.conf --\u0026gt; 3\u0026lt;config\u0026gt; 4\u0026lt;output_fieldcase\u0026gt;lower\u0026lt;/output_fieldcase\u0026gt; 5\u0026lt;output_resolve\u0026gt;1\u0026lt;/output_resolve\u0026gt; 6\u0026lt;output_safechars\u0026gt;1\u0026lt;/output_safechars\u0026gt; 7\u0026lt;output_format\u0026gt;bibtex\u0026lt;/output_format\u0026gt; 8\u0026lt;sourcemap\u0026gt; 9\u0026lt;maps datatype=\u0026#34;bibtex\u0026#34;\u0026gt; 10\u0026lt;!-- Easy type conversions --\u0026gt; 11\u0026lt;map\u0026gt; 12\u0026lt;map_step map_type_source=\u0026#34;report\u0026#34; map_type_target=\u0026#34;techreport\u0026#34;/\u0026gt; 13\u0026lt;map_step map_type_source=\u0026#34;online\u0026#34; map_type_target=\u0026#34;misc\u0026#34;/\u0026gt; 14\u0026lt;/map\u0026gt; 15\u0026lt;!-- Date to year, month --\u0026gt; 16\u0026lt;map\u0026gt; 17\u0026lt;map_step map_field_source=\u0026#34;date\u0026#34; 18map_field_target=\u0026#34;year\u0026#34; /\u0026gt; 19\u0026lt;/map\u0026gt; 20\u0026lt;map\u0026gt; 21\u0026lt;map_step map_field_source=\u0026#34;year\u0026#34; 22map_match=\u0026#34;(\\d{4}|\\d{2})-(\\d{1,2})-(\\d{1,2})\u0026#34; 23map_final=\u0026#34;1\u0026#34; /\u0026gt; 24\u0026lt;map_step map_field_source=\u0026#34;year\u0026#34; 25map_match=\u0026#34;(\\d{4}|\\d{2})-(\\d{1,2})-(\\d{1,2})\u0026#34; 26map_replace=\u0026#34;$1\u0026#34; /\u0026gt; 27\u0026lt;map_step map_field_set=\u0026#34;month\u0026#34; map_origfieldval=\u0026#34;1\u0026#34; /\u0026gt; 28\u0026lt;map_step map_field_source=\u0026#34;month\u0026#34; 29map_match=\u0026#34;(\\d{4}|\\d{2})-(\\d{1,2})-(\\d{1,2})\u0026#34; 30map_replace=\u0026#34;$2\u0026#34; /\u0026gt; 31\u0026lt;/map\u0026gt; 32\u0026lt;map\u0026gt; 33\u0026lt;map_step map_field_source=\u0026#34;year\u0026#34; 34map_match=\u0026#34;(\\d{4}|\\d{2})-(\\d{1,2})\u0026#34; map_final=\u0026#34;1\u0026#34; /\u0026gt; 35\u0026lt;map_step map_field_source=\u0026#34;year\u0026#34; 36map_match=\u0026#34;(\\d{4}|\\d{2})-(\\d{1,2})\u0026#34; map_replace=\u0026#34;$1\u0026#34; /\u0026gt; 37\u0026lt;map_step map_field_set=\u0026#34;month\u0026#34; map_origfieldval=\u0026#34;1\u0026#34; /\u0026gt; 38\u0026lt;map_step map_field_source=\u0026#34;month\u0026#34; 39map_match=\u0026#34;(\\d{4}|\\d{2})-(\\d{1,2})\u0026#34; map_replace=\u0026#34;$2\u0026#34; /\u0026gt; 40\u0026lt;/map\u0026gt; 41\u0026lt;/maps\u0026gt; 42\u0026lt;/sourcemap\u0026gt; 43\u0026lt;/config\u0026gt; However, we have not yet mapped journaltitle and location. These are now easier to set on the command line so we shall do so. Also note that in order to have the date logic work, we will need the --output-legacy-date option as well.\n1# Runs 2biber --tool --configfile=biberConf.xml references.bib --output-file refsTmp.bib --output-legacy-date --output-field-replace=location:address,journaltitle:journal This gives a sphinx compatible file:\n1@misc{grasedyckParameterdependentSmootherMultigrid2020, 2author = {Grasedyck, Lars and Klever, Maren and L\\\u0026#34;{o}bbert, Christian and Werthmann, Tim A.}, 3url = {http://arxiv.org/abs/2008.00927}, 4eprint = {2008.00927}, 5eprinttype = {arxiv}, 6keywords = {65N55; 15A69,Mathematics - Numerical Analysis}, 7month = {08}, 8title = {A Parameter-Dependent Smoother for the Multigrid Method}, 9urldate = {2021-05-22}, 10year = {2020}, 11} 1213@article{grasedyckDistributedHierarchicalSVD2018, 14author = {Grasedyck, Lars and L\\\u0026#34;{o}bbert, Christian}, 15url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nla.2174}, 16annotation = {\\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nla.2174}, 17doi = {10.1002/nla.2174}, 18issn = {1099-1506}, 19journal = {Numerical Linear Algebra with Applications}, 20keywords = {Hierarchical Tucker,HT,multigrid,parallel algorithms,SVD,tensor arithmetic}, 21langid = {english}, 22number = {6}, 23pages = {e2174}, 24title = {Distributed Hierarchical {{SVD}} in the {{Hierarchical Tucker}} Format}, 25urldate = {2021-05-22}, 26volume = {25}, 27year = {2018}, 28} 2930@article{grasedyckHierarchicalSingularValue2010, 31author = {Grasedyck, Lars}, 32publisher = {Society for Industrial and Applied Mathematics}, 33url = {https://epubs.siam.org/doi/abs/10.1137/090764189}, 34doi = {10.1137/090764189}, 35issn = {0895-4798}, 36journal = {SIAM Journal on Matrix Analysis and Applications}, 37month = {01}, 38number = {4}, 39pages = {2029--2054}, 40shortjournal = {SIAM J. Matrix Anal. Appl.}, 41title = {Hierarchical {{Singular Value Decomposition}} of {{Tensors}}}, 42urldate = {2021-05-22}, 43volume = {31}, 44year = {2010}, 45} Conclusions This was a rather annoying excursion into the unpleasant underbelly of bibtex and biblatex. The biber solution is actually rather elegant, but most definitely not immediately obvious.\n","permalink":"https://rgoswami.me/posts/zotero-biblatex-bibtex/","tags":["tools","workflow","tex","documentation"],"title":"Biblatex to Bibtex for Sphinx"},{"categories":["personal"],"contents":" Reminisces and prophecies brought upon by LFortran and GSoC 2021\n Background  I know not what the language of the future will look like, but I know it will be called FORTRAN\u0026hellip; \u0026lt;br\u0026gt; \u0026mdash; Charles Anthony Richard Hoare, circa 1982 \u0026lt;br\u0026gt; \u0026ndash; Daan Frenkel, 2020\n This post is a little belated, given that the GSoC announcements were a few days ago. Therein lies the future, and much digital ink, sweat and blood shall be spilled towards accomplishing the goals outlined in my accepted project proposal. However, interestingly, it was an offhand comment at a CECAM MARVEL Classics webinar on free energy of solids 1 which triggered this.\nBefore Fortran.. Formula translation, or FORTRAN was not my first programming language. I grew up with the Logo turtle, and progressed naturally into C for coursework (as was the tradition back during my schooldays) and C++ for monkeying around with my Xperia devices (mostly Cyanogenmod stuff, some LineageOS). I shifted rather awkwardly in and out of Java (also a popular mainstay in different schools) and decided at some point during high school to pick up a high level language. I choose Ruby, because it was cuter than Python (still is!) and I had just been bitten by the web-design bug (Jekyll!!! Ruby on Rails!).\nI was not unaware of Fortran, but I had little to no inclination to learn it, at the time, in my youth I was only interested in systems programming, web-design and other practical, visual activities.\nTowards Fortran As I grew older, my interest in crass physical things waned and my interest in the cool, calculating and sly discipline of simulation; fortune telling with confidence and code kept pace.\nI almost came into contact with Fortran when I mistakenly stumbled onto Quantum Espresso in the course of my first summer undergraduate research project under Prof. Rajarshi Chakrabarti at IIT Bombay back in 2014\u0026hellip;.\nAlmost, but not quite, for I was hunting models for soft matter (eventually found them too Theeyancheri et al. (2020) ), and EsPReSSO MD is not Quantum Espresso.\nFirst Encounter I finally stumbled upon FORTRAN in all its free form f90 glory in the form of BigDFT during my stint in the 2015 Students-Undergraduate Research Graduate Excellence (SURGE) program of IIT Kanpur. A mere six years ago.\nBy then I had been programming, as perhaps many do, largely on the largess of the internet. It was my source of information, and even seeded some of my opinions. I immediately set out to discover as much about Fortran as I could. Back then I failed miserably. For the first time, I was confronted only with ridiculously ugly F77 code examples, and it seemed abundantly evident that most questions and projects were clearly related to coursework (poorly taught coursework no less).\nThe only ray of sunshine in a mess of poor content was Ondřej Čertík\u0026rsquo;s fortran90.org. I knew better than to write off Fortran though, I knew even then its pedigree (impossible to configure LAMMPS without touching upon BLAS and LAPACK\u0026hellip;) 2.\nI was not a chemist, and my dealings with elegant EsPReSSo and LAMMPS in no way prepared me for the horrific mess which was (and is) the codes of ab-initio chemistry. I was however, introduced to Redwine (1995) .\nFortran the stable During my third year, I attempted to submit a project written in Julia. I spent the next couple of months convincing the instructor of the fact that Julia was a legitimate language and not a figment of my imagination. Little wonder then, when the time came to submit an undergraduate thesis, much of the computations were was inspired by the venerable Fortran codes of Kayode (1995) .\nIce, Fire and Fortran The fortunate circumstances behind my position here in Iceland, are best left to other posts. One of my tasks, was the migration of existing code for the Single Center Multipole Expansion water model Wikfeldt et al. (2013) from Fortran (free form, but not modern) to C++ or at-least to write ISO_C_BINDING wrappers for incorporation into existing C++ libraries (with an eye towards LAMMPS integration). At the time, given my projects in C++ (Goswami, Goswami, and Singh 2020) and passable familiarity with Fortran, this seemed like an optimal use of time.\nHowever, the force-field was and remains to be heavily under development Jónsson et al. (2020), Jónsson, Dohn, and Jónsson (2019), which made anything less than a hard fork unfeasible. Still on the books however, another Fortran project to look forward to!\nQuantum Chemistry and Fortran With collaborators at SURFSara I was involved in the port from MATLAB to C++, Gaussian Process Regression models Koistinen et al. (2019), Koistinen et al. (2017), Koistinen et al. (2020), Koistinen (2019) for accelerating saddle searches and coupling said methodologies to EON Chill et al. (2014) for long time scales. This was under my brilliant advisor\u0026rsquo;s ReaxPro project who also guided and encouraged me towards applying for a fellowship. As befitting a multiscale modeling project, this lead to an NDA and tweaks towards efficient workflows using potentials from the Amsterdam Modeling Suite, which naturally, is written in Fortran.\nTensors, IPAM and Fortran Much has been said about NWChem\u0026rsquo;s pivot towards C++. However, this has quite a bit to do with HiParTI (a tensor library) being in C++. To throw away Fortran simply because Tensors are as yet easier to implement in other languages is treating a symptom, not fixing the cause. In any case, over the course of an IPAM fellowship (supported by my advisor) I was able to gain a broad overview of the underlying numerical complaints driving the schism; and there are already fantastic libraries which can be leveraged for multilinear algebra. Fortran is still one of the only language to have standardized parallel programming paradigms, and in the long run there is no way that won\u0026rsquo;t count in its favor.\nFortran Lang and LFortran Around the same time frame, I was lucky enough to be involved in the documentation for SymEngine, and from there I was poised to witness the rapid rejuvenation and development of the Fortran ecosystem spearheaded in no small part by Ondřej Čertík (a fantastic person, mentor, and scientist; conversant in both algorithmic topics and quantum chemistry). LFortran is perhaps the most path-breaking of all the existing drives to fight the perceived obsolesce of Fortran. By bringing interactivity to the masses, the natural beauty of Fortran (recently reaffirmed by Milan Curic\u0026rsquo;s book) can come to the fore.\nThe Google Summer of Code project blends both my deep rooted interest in language semantics and flexible implementations; while also whetting my academic appetite by being so closely tied to the topics I love. Working on dftatom (and related quantum chemistry codes) while gaining familiarity with the LLVM based LFortran is fantastic. Being able to work closely with the rest of the team behind Fortran lang to bring about an \u0026ldquo;age of Fortran, as promised twenty years ago\u0026rdquo; is one of the most exciting premises I can imagine.\nConclusions The picture painted here, like all nostalgic reminisces, is colored and deeply biased. My love for modern C++ and elegant R, my forays into other languages, none of these are mentioned. The sheer joy of using modern Fortran, of projects with fpm; is lost in translation. It is true, that my work, has more often involved production level C++ over Fortran, however, given everything, I can be certain of one thing:\n The future is Fortran! (Or at-least involves a lot of Fortran)\n References Chill, Samuel T, Matthew Welborn, Rye Terrell, Liang Zhang, Jean-Claude Berthet, Andreas Pedersen, Hannes Jónsson, and Graeme Henkelman. 2014. \u0026ldquo;EON: Software for Long Time Simulations of Atomic Scale Systems.\u0026rdquo; Modelling and Simulation in Materials Science and Engineering 22 (5): 055002. https://doi.org/10.1088/0965-0393/22/5/055002.\n Goswami, Rohit, Amrita Goswami, and Jayant K. Singh. 2020. \u0026ldquo;D-SEAMS: Deferred Structural Elucidation Analysis for Molecular Simulations.\u0026rdquo; Journal of Chemical Information and Modeling 60 (4): 2169\u0026ndash;77. https://doi.org/10.1021/acs.jcim.0c00031.\n Jónsson, Elvar Örn, Asmus Ougaard Dohn, and Hannes Jónsson. 2019. \u0026ldquo;Polarizable Embedding with a Transferable H2o Potential Function I: Formulation and Tests on Dimer.\u0026rdquo; Journal of Chemical Theory and Computation, November. https://doi.org/10.1021/acs.jctc.9b00777.\n Jónsson, Elvar Örn, Soroush Rasti, Marta Galynska, Jörg Meyer, and Hannes Jónsson. 2020. \u0026ldquo;Transferable Potential Function for Flexible H$_2$O Molecules Based on the Single Center Multipole Expansion.\u0026rdquo; July 12, 2020. http://arxiv.org/abs/2007.06090.\n Kayode, Coker, A. 1995. Fortran Programs for Chemical Process Design, Analysis, and Simulation. Elsevier. https://doi.org/10.1016/B978-0-88415-280-4.X5000-6.\n Koistinen, Olli-Pekka. 2019. \u0026ldquo;Algorithms for Finding Saddle Points and Minimum Energy Paths Using Gaussian Process Regression.\u0026rdquo; Aalto University School of Science. https://opinvisindi.is/handle/20.500.11815/1460.\n Koistinen, Olli-Pekka, Vilhjálmur Ásgeirsson, Aki Vehtari, and Hannes Jónsson. 2019. \u0026ldquo;Nudged Elastic Band Calculations Accelerated with Gaussian Process Regression Based on Inverse Interatomic Distances.\u0026rdquo; Journal of Chemical Theory and Computation 15 (12): 6738\u0026ndash;51. https://doi.org/10.1021/acs.jctc.9b00692.\n \u0026mdash;\u0026mdash;\u0026mdash;. 2020. \u0026ldquo;Minimum Mode Saddle Point Searches Using Gaussian Process Regression with Inverse-Distance Covariance Function.\u0026rdquo; Journal of Chemical Theory and Computation 16 (1): 499\u0026ndash;509. https://doi.org/10.1021/acs.jctc.9b01038.\n Koistinen, Olli-Pekka, Freyja B. Dagbjartsdóttir, Vilhjálmur Ásgeirsson, Aki Vehtari, and Hannes Jónsson. 2017. \u0026ldquo;Nudged Elastic Band Calculations Accelerated with Gaussian Process Regression.\u0026rdquo; The Journal of Chemical Physics 147 (15): 152720. https://doi.org/10.1063/1.4986787.\n Redwine, Cooper. 1995. Upgrading to Fortran 90. New York: Springer.\n Theeyancheri, Ligesh, Subhasish Chaki, Nairhita Samanta, Rohit Goswami, Raghunath Chelakkot, and Rajarshi Chakrabarti. 2020. \u0026ldquo;Translational and Rotational Dynamics of a Self-Propelled Janus Probe in Crowded Environments.\u0026rdquo; Soft Matter, August. https://doi.org/10.1039/D0SM00339E.\n Wikfeldt, K. T., E. R. Batista, F. D. Vila, and H. Jónsson. 2013. \u0026ldquo;A Transferable H2o Interaction Potential Based on a Single Center Multipole Expansion: SCME.\u0026rdquo; Physical Chemistry Chemical Physics 15 (39): 16542. https://doi.org/f2z5vm.\n    I am after-all, actually secretly a computational chemist, whatever my degrees and blog posts might imply..\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Also a mentor and inspiration of mine (within my family) used Fortran for graduate research\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/fortran-gsoc-me/","tags":["history","ramblings","workflow","fortran"],"title":"Fortran, GSoC21 and Me"},{"categories":["notes"],"contents":" Short recollections of the second iteration of CIP; the course with the largest teaching (and voluntary) staff bringing the joy of programming to thousands\n Background Last year in 2020, as a section leader for the Stanford Code in Place initiative (which generated a series of posts), I was able to hone my teaching skills and bask in the reflected glory of many talented students who were in my Code in Place section. This year I was honored to return not only to lead a section, but also to act as a teaching mentor to a group of first time section leaders.\n\n Figure 1: 1,100 wonderful teachers across the world\u0026hellip;\n  There are still the student projects to look forward to, and like last year, there shall be some workshops, which I might also keep track of. This is then a slightly premature post, however, I just signed off on my final session (optional presentation aside) and had a chat with some returning staff, so now is a good nostalgic moment to capture.\nOnce again (Fig. 1) I had the dubious pleasure of being the only member of the teaching team (and student body) located in Iceland. This year we did also have a student on a fishing boat in Antarctica, so that was fascinating. There was a pretty fantastic age distribution (Fig. fig:agedist) as well.\n\n Figure 2: Learner distribution\n  Teaching Mentorship I was assigned a fantastic co-mentor, who turned out to be from the same undergraduate college I was at! (not during the same time period of course). Mentorship (training and delivery) was a lot of fun. Me and my co-mentor met a few times before and after to work on projects of mutual interest as well. All the section leaders had stars in their eyes and were super interactive. The sessions were packed to the gills and our Ed forum saw our shared camaraderie grow throughout the 5 week long course. I was also able to reconnect with some of the other stalwart section leaders who returned as teaching mentors.\n This CC-BY-NC-SA 4.0 licensed document sums up the goals instilled in the new section leaders. I also often plug Greg Wilson\u0026rsquo;s Teaching Tech Together  Differences  As section leaders, we were allowed to nominate up to three students who would join the program No video recordings of the section were allowed  A contentious decision at first, which may have contributed to an apparent dip in student participation Students gave permission for anonymized recordings of the OhYay sessions to allow for data mining   Instead of asking the section leaders to manage video platforms, the team set up OhYay instances (managed by Julie)  The OhYay instance for the teacher lounge was particularly enjoyable    Tools This year, coming at the tail end of an intensive year of online teaching experiences, helped me zero in on a series of tools I used throughout:\n Starts at (now Dateful) for timezone agnostic links Straw Poll for engagement Jamboard for persistent communal scribbles HackMD for collaorative notes and pseudocode JitBit for sharing screen  Didn\u0026rsquo;t get around to needing this, there was an early bug in OhYay which was ironed out    Surprises  This year\u0026rsquo;s set seemed to go by very quickly. Perhaps this can be partially attributed to the fact that the material was no longer new to me; however I did take on additional responsibilities. I can only conjecture that last year Zoom meetings were a novelty. There was a slightly grimmer tone throughout some of the forums, but the team (Chris, Mehran, Kylie, Julie, Brahm and everyone else) were as upbeat and inspiring as ever  Conclusions Last year\u0026rsquo;s Code in place mattered a lot to me. It was absolutely fantastic to see and interact with such a vibrant community again; there were some odd hiccups, including some in-sensitivities and incivilities, but nothing major. This year in fact, the program meant even more to me; my mother, who has long been (like the rest of my family) an inspiration and friend, joined and was absolutely incredibly dedicated, hardworking and creative throughout.\nI cannot speak to the future, but I know I shall probably once again have the CIP calendar sit unused till the next year, hoping to be called back to partake in the joys of teaching motivated students with brilliant colleagues.\n","permalink":"https://rgoswami.me/posts/cip2021-fin/","tags":["ideas","teaching","cs106a"],"title":"Wrapping Up Code in Place 2021"},{"categories":["code"],"contents":"Most often it makes more sense to map the same ports on every intermediate machine.\n1Host super 2Hostname super.machine.location.is 3IdentityFile ~/.ssh/mykey 4User myuser 5LocalForward 8001 localhost:8001 6LocalForward 8002 localhost:8002 7LocalForward 8003 localhost:8003 8LocalForward 8004 localhost:8004 This is good for interactive sessions with multiple servers. For single servers, reverse proxy tunnels are more efficient.\n","permalink":"https://rgoswami.me/snippets/forward-multiport/","tags":["ssh"],"title":"Forwarding Multiple Local Ports"},{"categories":["code"],"contents":"1doxygen191 = pkgs.doxygen.overrideAttrs (_: rec { 2name = \u0026#34;doxygen-1.9.1\u0026#34;; 3src = pkgs.fetchurl { 4urls = [ 5\u0026#34;mirror://sourceforge/doxygen/${name}.src.tar.gz\u0026#34; # faster, with https, etc. 6\u0026#34;http://doxygen.nl/files/${name}.src.tar.gz\u0026#34; 7]; 8sha256 = \u0026#34;1lcif1qi20gf04qyjrx7x367669g17vz2ilgi4cmamp1whdsxbk7\u0026#34;; 9}; 10}); ","permalink":"https://rgoswami.me/snippets/nix-collection-overwrite-attrs/","tags":["nix"],"title":"Overwriting Attributes"},{"categories":["personal"],"contents":" Discussion on dotfile management, a meandering path to my current setup from dotgit to bombadil. EDIT: Superseded by my chezmoi configuration described here\n Background No one gets very far working with stock one-size fits all tools in any discipline but it is especially true of working with computers. The right set of dotfiles have been compared to priming spells for invocation later, and this is probably true. More than anything else, dotfiles offer familiarity where there is none, be it from cowsay or a fancy shell prompt 1.\n1$ cowsay Welcome home $USER 2___________________________ 3\u0026lt; Welcome home rohitgoswami \u0026gt; 4--------------------------- 5\\  ^__^ 6\\  (oo)\\_______ 7(__)\\  )\\/\\ 8 ||----w | 9|| || Why Now? \n Figure 1: Home is where the dotfiles are\n  Well more prosaically, I have recently had to partially retire my beloved ThinkPad X380 Yoga 2 for a new MacBook Pro 2019 (Intel) 3. This is the single largest test of my Dotfile management system, since I now have configurations which are no longer scoped to just a Linux distribution (e.g. ArchLinux and CentOS 6), but are fundamentally not interchangeable.\nComplicating matters further, dotgit sunset the bash version I have grown to love in favor of a new python version (explained here). I also have to manage profiles on more HPC systems than before. The time seemed ripe for a re-haul.\nI\u0026rsquo;ll try to justify the Figure 1 as I dissect and rebuild my Dotfiles as I switch from dotgit to bombadil.\nDesiderata What makes a good dotfile management system? Some things which are common to most/all good systems:\n targets These are configurations which are scoped to either machines or operating systems; e.g. archlinux, colemak etc. profiles The concept of a profile is essentially a set of targets used together; e.g. mylaptop, hzhpc symlinks Most management systems use symlinks to modularly swap configurations in and out; ln -sf .... secrets Commonly implemented (with varying levels of help) in the form of a gpg encrypted file/files  All of these features are exemplefied by the fantastic dotgit and no doubt its python iteration is just as brilliant. However, I am wary of using python for my dotfile management, since I tend to use transient virtualenvs a lot and detest having a system python for anything.\nOver the years, I\u0026rsquo;ve come to also value:\n simplicity Especially true of installation proceedures, I simply need to get started quickly too often template expansion A rare feature to need, but one I\u0026rsquo;ve been addicted to since lazybones, pandoc and even orgmode brought a million snippets  On a probably technically unrelated note, I have recently been splurging on the \u0026ldquo;modern\u0026rdquo; (prettier) rust versions of standard command line tools; so I normally have cargo everywhere.\nStarting out with Bombadil Since toml-bombadil is:\n Written in Rust  installs with cargo as a single binary   Supports encryption Supports profiles Has template expansion  It was the obvious choice.\n1# Get Cargo 2curl --proto \u0026#39;=https\u0026#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh 3source $HOME/.cargo/env 4# Get toml-bombadil 5cargo install toml-bombadil For the rest of this post I\u0026rsquo;ll assume everyone is working with a set of dots like my own.\n1# Get my set 2export mydots=\u0026#34;$HOME/Git/Github/Dotfiles\u0026#34; 3mkdir -p $mydots 4git clone git@github.com/HaoZeke/Dotfiles $mydots 5cd $mydots 6git checkout bombadil 7bombadil install -c \u0026#34;$mydots/bombadil.toml\u0026#34; Now we can start defining profiles in our toml file and link them.\n1# Get colemak and mac profiles 2bombail link -p macos colemac This isn\u0026rsquo;t really meant to be a tutorial about bombadil-toml, but it might include some pointers. The remainder of the post will focus on the layout and logic of my own usage.\nShells Shells form the core of most computing environments, and typically we would like to have basic support for at least bash and one additional shell. The secondary shell (zsh for me) is the most preferred, but often might not be avaliable 4.\nCurrent Logic Since many shells are somewhat compatible with each other (especially within the POSIX family); my current setup looked a bit like:\n1. ~/.shellrc # With standard agnostic commands Which in turn loaded a bunch of conditionally symlinked files from profiles.\n1# Platform 2############# 34if [ -f ~/.shellPlatform ]; then 5. ~/.shellPlatform 6fi 78# Specifics 9############# 1011if [ -f ~/.shellSpecifics ]; then 12. ~/.shellSpecifics 13fi 1415# Wayland 16############# 1718if [ -f ~/.waylandEnv ]; then 19. ~/.waylandEnv 20fi 2122# XKB 23###### 2425if [ -f ~/.xkbEnv ]; then 26. ~/.xkbEnv 27fi 2829# Nix 30###### 3132if [ -f ~/.nixEnv ]; then 33. ~/.nixEnv 34fi Current Approach The problem with the older approach is that it isn\u0026rsquo;t always clear where different divisions should be drawn and it isn\u0026rsquo;t really flexible enough to add things arbitrarily. Basically, here there were only a few entry points. A more rational method is to emulate the work of init.d 5 scripts; where a set of files in a directory are all loaded if they exist 6.\nThis allows the previous setup to be instead refactored into the following folder setup (under $HOME/.config/shellrc):\n .login.d which has machine specific files  Spack and Lmod modulefiles on various HPC nodes Also just things which normally run only once per login (like system diagnostics) MacOS considers every terminal to be a login shell for some odd reason   .shell.d which contains POSIX compliant snippets  This is by far the longest section   .bash.d for snippets which need bashisms  Array operations   .zsh.d for snippets which are specific to zsh  Plugin management    This then flows very nicely into a smaller set of core rc files for scripts.\n1# .bashrc / .zshrc 2## Bashism (only for .bashrc) 3if [[ $- != *i* ]]; then 4# shell is non-interactive. Do nothing and return 5return 6fi 7## Zshism (only for .zshrc) 8if [[ -o interactive ]]; then 9# non-interactive, return 10return 11fi 1213export shellHome=$HOME/.config/shellrc 1415# Load all files from the shell.d directory 16if [ -d $shellHome/shell.d ]; then 17for file in $shellHome/shell.d/*.sh; do 18source $file 19done 20fi 2122# Load all files from the bashrc.d directory 23if [ -d $shellHome/bash.d ]; then 24for file in $shellHome/bash.d/*.bash; do 25source $file 26done 27fi Similarly, we can define our zshrc. For profiles (.zlogin and .bash_profile), we source the rc files along with the login.d scripts.\nPracticalities This method isn\u0026rsquo;t really very exciting at the offset. Each target has a series of scripts which are loaded in order.\n1tree . 2. 3├── bash 4│ └── bashrc 5├── posix 6│ ├── 00_warnings.sh 7│ ├── 01_alias_def.sh 8│ ├── 02_func_def.sh 9│ ├── 03_exports.sh 10│ ├── 04_sources.sh 11│ ├── 05_paths.sh 12│ └── 06_prog_conf.sh 13├── tmux 14└── zsh 15164 directories, 8 files This is correspondingly linked via the following snippet in bombadil.toml.\n1# Shells # 2# POSIX 3posix_warn = { source = \u0026#34;common/shell/posix/00_warnings.sh\u0026#34;, target = \u0026#34;.config/shellrc/shell.d/00_warnings.sh\u0026#34; } 4posix_alias = { source = \u0026#34;common/shell/posix/01_alias_def.sh\u0026#34;, target = \u0026#34;.config/shellrc/shell.d/01_alias_def.sh\u0026#34; } 5posix_func = { source = \u0026#34;common/shell/posix/02_func_def.sh\u0026#34;, target = \u0026#34;.config/shellrc/shell.d/02_func_def.sh\u0026#34; } 6posix_exports = { source = \u0026#34;common/shell/posix/03_exports.sh\u0026#34;, target = \u0026#34;.config/shellrc/shell.d/03_exports.sh\u0026#34; } 7posix_sources = { source = \u0026#34;common/shell/posix/04_sources.sh\u0026#34;, target = \u0026#34;.config/shellrc/shell.d/04_sources.sh\u0026#34; } 8posix_paths = { source = \u0026#34;common/shell/posix/05_paths.sh\u0026#34;, target = \u0026#34;.config/shellrc/shell.d/05_paths.sh\u0026#34; } 9posix_prog_conf = { source = \u0026#34;common/shell/posix/06_prog_conf.sh\u0026#34;, target = \u0026#34;.config/shellrc/shell.d/06_prog_conf.sh\u0026#34; } 10# Bash 11bashrc = { source = \u0026#34;common/shell/bash/bashrc\u0026#34;, target = \u0026#34;.bashrc\u0026#34; } The same concept (folder structure) is used for specific machines as well (e.g. archlinux).\n1[profiles.archlinux.dots] 2archlinux_warn = { source = \u0026#34;archlinux/shell/posix/07_00_warnings.sh\u0026#34;, target = \u0026#34;.config/shellrc/shell.d/07_00_warnings.sh\u0026#34; } 3archlinux_alias = { source = \u0026#34;archlinux/shell/posix/07_01_alias_def.sh\u0026#34;, target = \u0026#34;.config/shellrc/shell.d/07_01_alias_def.sh\u0026#34; } 4archlinux_func = { source = \u0026#34;archlinux/shell/posix/07_02_func_def.sh\u0026#34;, target = \u0026#34;.config/shellrc/shell.d/07_02_func_def.sh\u0026#34; } 5archlinux_exports = { source = \u0026#34;archlinux/shell/posix/07_03_exports.sh\u0026#34;, target = \u0026#34;.config/shellrc/shell.d/07_03_exports.sh\u0026#34; } 6archlinux_sources = { source = \u0026#34;archlinux/shell/posix/07_04_sources.sh\u0026#34;, target = \u0026#34;.config/shellrc/shell.d/07_04_sources.sh\u0026#34; } 7archlinux_paths = { source = \u0026#34;archlinux/shell/posix/07_05_paths.sh\u0026#34;, target = \u0026#34;.config/shellrc/shell.d/07_05_paths.sh\u0026#34; } 8archlinux_prog_conf = { source = \u0026#34;archlinux/shell/posix/07_06_prog_conf.sh\u0026#34;, target = \u0026#34;.config/shellrc/shell.d/07_06_prog_conf.sh\u0026#34; } Note the way in which the files are saved to ensure the correct loading order. For zsh the list is a little different:\n1zshrc = { source = \u0026#34;common/shell/zshrc.zsh\u0026#34;, target = \u0026#34;.zshrc\u0026#34; } 2zshenv = { source = \u0026#34;common/shell/zsh/zshenv.zsh\u0026#34;, target = \u0026#34;.zshenv\u0026#34; } 3zsh_keys = { source = \u0026#34;common/shell/zsh/01_keys.zsh\u0026#34;, target = \u0026#34;.config/shellrc/zsh.d/01_keys.zsh\u0026#34; } 4zsh_func = { source = \u0026#34;common/shell/zsh/02_func_def.zsh\u0026#34;, target = \u0026#34;.config/shellrc/zsh.d/02_func_def.zsh\u0026#34; } 5zsh_plugins = { source = \u0026#34;common/shell/zsh/03_plugins.zsh\u0026#34;, target = \u0026#34;.config/shellrc/zsh.d/03_plugins.zsh\u0026#34; } 6zsh_sources = { source = \u0026#34;common/shell/zsh/04_sources.zsh\u0026#34;, target = \u0026#34;.config/shellrc/zsh.d/04_sources.zsh\u0026#34; } 7zsh_theme = { source = \u0026#34;common/shell/zsh/04a_theme.zsh\u0026#34;, target = \u0026#34;.config/shellrc/zsh.d/04a_theme.zsh\u0026#34; } Text Editors A long time ago I switched from VIM and Sublime to Emacs. I still retain in my dotfiles enough syntactical sugar and targets to make vim and Sublime easier to use; mostly using vim-plug. Emacs has a rather complicated set of configurations which are independently managed via doom-emacs and have their own self documenting site.\nConclusions Though the shell.d setup is still overly verbose and not as flexible as I had initially hoped, this is still a few steps ahead of my previous setup. No part of this focused on the configurations themselves (more interesting examples, of switching to Colemak are here) Will be fleshing this out more with auxilliary posts on my configuration (browsers, etc.) and its management perhaps.\n  bash defaults are particularly egregious\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Water damage means I can\u0026rsquo;t type on it anymore, works as a great tablet now\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n A gift, and therefore cherished inspite of the vagrancies of Apple clang\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Even more true if one wants something elvish (details here) or other shells\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Perhaps better known in the context of udev.d startup scripts\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n A little bit of googling around showed that chris or chr4 had worked the idea trough to its logical conclusion before, so I essentially adapted it\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/dotfiles-dotgit-bombadil/","tags":["ramblings","tools","workflow"],"title":"Dotfiles from dotgit to bombadil"},{"categories":["conferences"],"contents":" A meta-post on the workshop I held for the IOP student community on Intermediate C++\n Background Much the same as the rationale behind my other presentation meta-posts, that is:\n I would like to preserve questions I would like to collect the video, slides and other miscellaneous stuff in one location 1 It would be nice to have my own thoughts here afterwards  Details Blurb verbatim from the workshop announcement.\n In the 2nd C++ workshop, we will explore the build process and its automation using CMake; before discussing continuous integration for testing on multiple operating systems. We will then round out the workshop by augmenting the particle simulater class with Python bindings using the C++ pybind11 library for visualizing results using matplotlib​, plotnine​ and other libraries.\nThis workshop will include:\nThe build process and its automation using CMake; why bind C++ to other languages Using C++ in python with bindings and using the pybind11 library Visualising results using matplotlib, plotnine and other libraries Delivered by Rohit Goswami, doctoral researcher at the University of Iceland, Reykjavik. Participation in the workshop will not require any pre-installed software. Everything demonstrated will be available for students to edit and run online.\n  Github Repository Here. This demonstrates the refactoring of a particle simulator.  Other Content Anything on this site tagged with Nix or tagged with C++. Also an introduction to nix given by me (and Amrita Goswami) at CarpentryCon2020 is here:\n CarpentryCon2020 Materials A tutorial introduction to Nix and Python  Slides  Best viewed here using a browser (in a new tab) A pdf copy of the slides are embedded below The orgmode source is here on the site\u0026rsquo;s GH repo   Video {{\u0026lt; youtube jyA13tw3VKg \u0026gt;}}\nThoughts  Using the SHA commit IDs worked really well  Perfect for demonstrating refactors   GitHub Discussions might be a good idea later too Using Powerpoint in the background to get subtitles was fantastic too  Might expand these      One location I am going to be able to keep track of\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/iop-cpp-2021-meta/","tags":["presentations","ramblings","python","cpp"],"title":"Talk Supplements for IOP's C++ Workshop"},{"categories":["personal"],"contents":" Thoughts about the FSF, toxicity, and Stallman\n First a disclaimer. Comments are not enabled for this post. This is very personal, and it is in my online space so I reserve the right to not talk about this. This is not a conversation. It is a protracted comment on recent, disappointing, news about the \u0026ldquo;tech community\u0026rdquo;. It is unlike anything I have normally published; and is hopefully the last time I will publish something like this. In general there is always something to be outraged about; and that is what Twitter is for. Most of these things are not relevant enough to me personally to write a post about.\nI do not write about my personal life and I try not to have one 1. COVID-19, earthquakes and even a volcano; have all been treated with utter and complete indifference. None of those things have defined any part of me; and will never do so. The FSF and its flip flopping with respect to Richard Stallman; does affect me in a way. It embarrasses me. I cannot imagine why people need to hero worship someone who has contributed nothing to the community for many years now, other than propagation of a toxic subculture. Even Emacs, which he \u0026ldquo;wrote\u0026rdquo; several decades ago; is being championed more by the fantastic Henrik Lissner and his doom-emacs project; proof again that the \u0026ldquo;holy wars\u0026rdquo; were pointless and toxic2.\nI got into programming over ten years ago; when I was in the tenth grade I tried to submit something silly to a mailing list. I was (rightly) shot down. That annoyed me; and I clawed my way through reams of books and eventually turned out to be a passable programmer in my own right. I joke about the way I was \u0026ldquo;guided\u0026rdquo; into open source now when I teach more inclusive introductory workshops; but in some sense the sting of that initial set of interactions has never really dulled. Nonetheless, programming remains the most open of disciplines 3, which many wrongly ascribe to Richard Stallman.\nI am not here to vilify Richard Stallman; I have never met the guy. For those interested, and from people with more of a stake in the matter Nathan Sidwell\u0026rsquo;s take is pretty solid 4. I simply do not like that I can no longer vouch for my many GPL\u0026rsquo;d projects anymore. Many will point out (have pointed out actually), that Stallman was always there; and this late stage cancel culture means nothing or is pointless posturing. I reject that stance. I have not always been a stellar human being in my personal interactions; but I strive to both make amends (personally) and also to maintain an image of a better human being. Me, online, is simply a projection of the person I wish to be. This mindset grew from a time back when it was common to have multiple online personas, before the line between one\u0026rsquo;s online identity and real identity was not so blurry. Over time; I hope to be the sort of person who can be both erudite and welcoming; inspite of the fact that it is difficult to not throw diva tantrums (which are often socially acceptable as well). This is also a stance (i.e. iterative improvement) which Stallman clearly does not share. This lack of willingness to change with the times is enough of a reason for this post.\nMy own data consisting of me and other people I have ranted to in person does not meaningfully represent a community 5; but it felt necessary to take a stand. Active rejection of the herd mentality which leads to elevating one person above the rest; and goes on to assert \u0026ldquo;proportional\u0026rdquo; justice does not involve removing such a toxic person from leadership roles is important to me. I would like to continue to be proud of my work (such as it is) and of my projects (such as they are) without having to tacitly agree with the views of Stallman or the FSF. I would prefer to wipe the names of all \u0026ldquo;icons\u0026rdquo; and \u0026ldquo;founders\u0026rdquo; from the record and deal only in terms of the concrete work they have generated. This would prevent sheeple from hero-worshiping and idolizing people (this goes for \u0026ldquo;good\u0026rdquo; and \u0026ldquo;bad\u0026rdquo; people). In my opinion; Stallman should be removed for failing to be a decent human being. So many stories are told about how \u0026ldquo;unrelenting\u0026rdquo; and \u0026ldquo;passionate\u0026rdquo; this person is about free software. It is sad then; that he feels the need to be listed so prominently. Society as a whole should reject the sort of repugnant behavior he has displayed; and he should not let that prevent him from contributing to FOSS. That there is no path to anonymous contributions to the FSF is a whole other discussion.\nThat\u0026rsquo;s the end of this rant.\nThankfully, my stance has been shared by sections of society which actually do contribute code and manage open source projects.\n Fedora Council Statement Red Hat statement GCC Steering Committee statement    Thankfully this bothers no one, me least of all.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This HN post describes other leadership failures under Stallman\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n I dare you to find open resources which teach you chemical engineering.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n He, unlike the \u0026ldquo;supporters\u0026rdquo; is an active contributor to projects which sheeple believe exist only because of Stallman\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n There is a letter against his reinstatement; and a grovelling one in support of his reinstatement.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/fsf-me-stallman/","tags":["rant"],"title":"Free Software and Me"},{"categories":["personal"],"contents":" An overview of documentation complexity and an analysis of incentives.\n Background As mentioned a few times before, last year a proposal of mine to improve the documentation for the SymEngine organization was accepted under the Google Season of Docs 2020 initiative. This is a more personal and expanded discussion on the report submitted on the SymEngine Wiki regarding the goals and completion metrics.\n Figure 1: Promotional image Google seemed to strongly suggest\n  Documentation and Me I have been a huge proponent of documentation throughout my decade long dabbling in FOSS projects. Quite bluntly, my memory is not great, and writing things down in a manner tailored to my own needs helps me reduce the amount of time it takes to do similar tasks. In many cases, the problem is not even the lack of existing documentation; it is just good to have a representation which encourages the minimum cognitive load for me.\nSymEngine and Me The SymEngine repository will turn 10 next year. There are few, if any, contenders in the field of C++ symbolic mathematics libraries; the Vienna math project seems to have stalled and does not support matrices.\nSymbolic math is one of those fields I feel should be used a lot more in the applied sciences, but somehow gets overlooked. Part of this is undoubtedly because of Mathematica, Maple and other proprietary players, which stifle innovation and FOSS development, but also the documentation and technical debt incurred is pretty large.\nMeeting the Mentors Originally, the meeting with my mentoring group was rather anxiety inducing; this was mostly due to my own unfamiliarity with symbolic algebra. Over time though, the meetings quickly became something I started to look forward to; and we were able to collectively come up with a unique; yet sustainable documentation workflow. I was also able to gain insights into the logic underlying the code which would have ordinarily taken longer to process on my own. I also ended up going over some references like Cohen (2003) and Cohen (2002).\nTimelines and Deliverable Assets One of the early issues I faced was restricting myself to documentation; with such a vibrant and fast-growing project, there were several avenues I wanted to explore and enhance, not all of which were technically under the ambit of documentation. This was an impulse which I eventually had to ignore; though the team were kind enough to strongly suggest submitting PRs in other directions after the documentation project concluded.\nDoxygen Alternatives Initially I had hoped to have an all-Sphinx documentation site, with all language bindings parsed into Sphinx. For this I explored exhale, as well as the more flexible doxyrest. None of these could match the flexibility or rich output of Doxygen for C++; so eventually I did the more rational thing and developed a nice theme for Doxygen instead, doxyYoda.\nBase Doxygen  Figure 2: Base Doxygen\n   Is ugly  Other than that, there were no standards for consistent documentation originally.\nExhale Documentation  Figure 3: Exhale example\n   Cannot include source code  That is a deal breaker, since the algorithms are often described step by step.\nDoxyrest Documentation  Figure 4: Doxyrest\n   Includes more structure than exhale Can be extended to other source languages Has a rather complicated setup  Doxygen with DoxyYoda  Figure 5: Doxygen with doxyYoda\n   All the goodness of Doxygen  Includes hierarchies   Also is now pretty  Tooling Decisions This is more of a technically specification; but after rooting around, it was decided to throw consistency of design to the wind and use native tools for each language binding as shown in Table 1.\n\nTable 1: Language bindings and tools\n  Language Package\n R pkgdown\nPython Sphinx\nC++ Doxygen + doxyYoda\nJulia Documenter.jl\n[Notebooks / Sphinx + MyST](https://symengine.org/#api-documentation) myst + jupytext Notebooks and Documentation I do not like Jupyter Notebooks. This is because, to me, a rabid org-mode fanatic 1, it seems pretty odd that they are essentially monstrous json nightmares instead of plain text. Thankfully jupytext and MyST-NB solve this problem admirably. There are still some use-cases for having pure notebooks, especially since GitHub now renders them; so a CI (Github Actions) generates a \u0026ldquo;notebooks\u0026rdquo; branch for the entrenched as well.\nCell-Tags and Disappointment One of the major disappointments faced during the project had to do with the way cell tags work; or rather, do not work. It was planned to have notebooks executed with papermill and switch between development and stable packages based on cell-tags; however, cell-tags apparently cannot be used for meta-injection; which means they are absolutely useless. A workaround of course might be using macro expansions; however, papermill did not really seem up to the challenge of injecting non-python variables either so this was abandoned.\nFuture Plans Personally, I will not be participating in further rounds in the foreseeable future, mostly because I intend to continue working on SymEngine until it is compliant with the standards I championed, and the addition of more projects to the portfolio I consider to be part of my moral responsibilities is not a good idea right now.\nConclusions It is difficult to gauge the effect of financial incentives on the quality of code. That analysis and possible rant I\u0026rsquo;ll save for another post. Projects like SymEngine attract very good proposals during high incentive development bursts like the Google Summer of Code and Season of Documentation; but these contributions tend to rot over time; which makes them of dubious use. That said, my own experience involving SymEngine was personally enriching, and I look forward to working on the project further. I believe the most useful aspect of the program is the ability to meet with the rest of the development team regularly, none of the other projects I work on have regular meetings, and it is a model I intend to carry forward in some of my projects.\nReferences Cohen, Joel S. 2002. Computer Algebra and Symbolic Computation: Elementary Algorithms. A K Peters/CRC Press. https://doi.org/10.1201/9781439863695.\n \u0026mdash;\u0026mdash;\u0026mdash;. 2003. Computer Algebra and Symbolic Computation: Mathematical Methods. A K Peters/CRC Press. https://doi.org/10.1201/9781439863701.\n    This site is written in org-mode, my doom-emacs configuration is also extensive and online\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/symengine-gsod20/","tags":["ramblings","thoughts"],"title":"SymEngine and the Season of Docs"},{"categories":["personal"],"contents":" Collection of odds and ends relating to e-readers including personal reminisces\n Background Reading has been a huge part of my life. The written word has had arguably more of an impact on my life than anything I have experienced in person. As a kid back in early 2000\u0026rsquo;s; this meant a lot of library trips and saving for paperbacks. I also caught the first wave of the e-ink revolution. Nothing beats a real book, in terms of textures and scents; but e-ink devices and the fantastic tools outlined here should make reading digital books much more palpable 1.\nI have been reading on my Kobo Aura HD for almost a decade now, ever since its release. This means my setup is about as stable as its going to get in the near future. As good a time as any to collect my thoughts 2. The focus is on e-ink devices and auxiliary tools; not on all digital content; so there are no mentions of syncing or reading on the go (with a phone) or of monitors which are good for reading on.\nThe Content In general; my e-ink reading habits can be broadly broken into the following categories:\n Light Reading Practically this includes anything I review on Goodreads; these are not often re-read; nor read very deeply; since they are read for pleasure. They are however, rarely deleted Required Reading Anything which typically requires me to take notes or practice / write out proofs; these are most often considered to be either coursework (for someone somewhere) or research monographs. These are typically large (in size) and unwieldy (in that they often lack TOCs) and are read multiple times; with a focus on highlights and notes Active Research These are the most ephemeral of my reading habits; and also the most numerous; I do not typically store these on my e-reader; and rarely need to make notes on the reader 3. These are often tiny; but require special work due to the metadata involved     Content Type Software Stack Deletion Rate     Light Reading Calibre Rare   Required Reading Calibre Never   Active Research Calibre + Zotero Frequent    Though I am a huge proponent of RSS feeds (with Newsflash) and read online content voraciously with both a Pocket and Diigo subscription4; I sincerely do not believe blog stuff or anything tailored for the web should have a presence on an e-ink device; so there shall be no mention of those parts of my reading habits5.\nHardware My primary e-reader is still my Kobo Aura HD (complete with a snazzy hemp sleep-cover), and has been my go to for almost a decade now since its release. Recently I have augmented my workflow with the reMarkable 2; though I have yet to break it in very well; mostly because I tend to gravitate towards typing out my thoughts 6 instead of writing.\nThe Kobo Aura HD is still the pinnacle of reading technology to me; mostly because the firmware is easy to bypass; and there is a vibrant community of developers on the MobileRead Forums. Display and spec aside; the biggest reason for never replacing it has been been the simple fact that most modern e-readers no longer support SD cards; and much of my workflow depends on storing insane amounts of material offline 7.\n Figure 1: Primary reading device with Koreader\n  Personally, I never use Nickel (the default Kobo interface), and it would probably choke trying to scan my 200 GB of content; so I haven\u0026rsquo;t updated the firmware in forever. My interactions are almost always in Koreader; and my launching poison of choice is the now no longer developed Kobo start menu 8.\nSoftware Broadly speaking; the main parts of the software pipeline from digital book to brain are simply the syncing mechanism and the UI/UX/OS of the device in question; though it is often best to consider pre-processing books for devices too. These are covered in the order used.\nk2pdfopt The thought of reflowing text for an optimal reading experience, especially given the slightly limited processing power of my primary reading device is an enticing prospect. K2pdfopt or the Kindle 2 PDF Optimizer is as criminally underrated as it is fantastic. An approach which works well for my device involves setting up a simple shell script (part of my Dotfiles) for optimizing files on the fly before sending them through calibre.\n1#!/usr/bin/env bash 2# Get a filename 3case \u0026#34;$#\u0026#34; in 40) 5echo \u0026#34;No arguments, so enter the filename, WITH the extension\u0026#34; 6read -p \u0026#39;Document: \u0026#39; docfile 7;; 81) 9echo \u0026#34;OK, using the filename\u0026#34; 10docfile=\u0026#34;$1\u0026#34; 11;; 12*) 13echo \u0026#34;Illegal number of parameters\u0026#34; 14exit 15;; 16esac 17# Get basename 18basename=\u0026#34;${docfile%.*}\u0026#34; 19ext=\u0026#34;${docfile##*\\.}\u0026#34; 20echo \u0026#34;Basename ${basename}with $extfrom $docfile\u0026#34; 21echo \u0026#34;Making a local store for the outputs\u0026#34; 22mkdir -p \u0026#34;$HOME/auraHDopt\u0026#34; 2324case \u0026#34;$ext\u0026#34; in 25\u0026#34;djvu\u0026#34;) 26echo \u0026#34;Converting djvu to pdf via ps and running k2pdfopt\u0026#34; 27djvups \u0026#34;${basename}.djvu\u0026#34; \u0026#34;${basename}.ps\u0026#34; 28ps2pdf \u0026#34;${basename}.ps\u0026#34; \u0026#34;${basename}.pdf\u0026#34; 29# The newline is for simulating the Enter key 30echo | k2pdfopt \u0026#34;${basename}.pdf\u0026#34; -wrap -hy -ws -0.2 -dev kbhd -x 31echo \u0026#34;Cleaning up\u0026#34; 32mv \u0026#34;${basename}_k2opt.pdf\u0026#34; \u0026#34;$HOME/auraHDopt\u0026#34; 33rm -rf \u0026#34;${basename}.{ps,pdf}\u0026#34; 34;; 35\u0026#34;pdf\u0026#34;) 36echo \u0026#34;Converting pdf with gs and running k2pdfopt\u0026#34; 37gs -sDEVICE=pdfwrite -dCompatibilityLevel=1.4 -dPDFSETTINGS=/screen \\ 38 -dNOPAUSE -dQUIET -dBATCH -sOutputFile=\u0026#34;${basename}gs.pdf\u0026#34; \u0026#34;${basename}.pdf\u0026#34; 39echo | k2pdfopt \u0026#34;${basename}gs.pdf\u0026#34; -wrap -hy -ws -0.2 -dev kbhd -x 40echo \u0026#34;Cleaning up\u0026#34; 41rm \u0026#34;${basename}gs.pdf\u0026#34; -rf 42mv \u0026#34;${basename}gs_k2opt.pdf\u0026#34; \u0026#34;$HOME/auraHDopt\u0026#34; 43;; 44*) 45echo \u0026#34;Illegal file type\u0026#34; 46exit 47;; 48esac The outputs can also be further processed with an OCR (Optical Character Recognition) script if required, and then edited in Master PDF Editor or something similar to add the table of contents interactively as well.\n1#!/bin/bash 2# Use as find . -type f -name \u0026#34;*.pdf\u0026#34; -exec isOcr \u0026#39;{}\u0026#39; \\; 34# Shamelessly kanged from here: 5# https://stackoverflow.com/questions/7997399/bash-script-to-check-pdfs-are-ocrd 6# Only searches for text on the first 5 pages 7# Modified to have red text. Also to possibly ocr the thing. 89# -*- mode: shell-script-mode -*- 1011MYFONTS=$(pdffonts -l 15 \u0026#34;$1\u0026#34; | tail -n +3 | cut -d\u0026#39; \u0026#39; -f1 | sort | uniq) 12if [ \u0026#34;$MYFONTS\u0026#34; = \u0026#39;\u0026#39; ] || [ \u0026#34;$MYFONTS\u0026#34; = \u0026#39;[none]\u0026#39; ]; then 13echo \u0026#34;$(tput setaf 1)NOT OCR\u0026#39;ed: $1\u0026#34; 14if [[ -x \u0026#34;$(which ocrmypdf)\u0026#34; ]]; then 15echo \u0026#34;$(tput setaf 4)\u0026#34; 16echo \u0026#34;Converting to ${1%.*}_ocr.pdf with ocrmypdf\u0026#34; 17echo \u0026#34;$(tput setaf 7)\u0026#34; 18ocrmypdf --deskew --clean --rotate-pages \\ 19 --jobs 4 -v --output-type pdfa \u0026#34;$1\u0026#34; \u0026#34;${1%.*}_ocr.pdf\u0026#34; 20elif [[ -x \u0026#34;$(which pypdfocr)\u0026#34; ]]; then 21echo \u0026#34;$(tput setaf 2)Looking for config files at $XDG_CONFIG_HOME/pypdfocr/config.yml\u0026#34; 22echo \u0026#34;$(tput setaf 3)\u0026#34; 23if [[ -e $XDG_CONFIG_HOME/pypdfocr/config.yml ]]; then 24echo \u0026#34;Using configuration settings\u0026#34; 25echo \u0026#34;$(tput setaf 4)\u0026#34; 26pypdfocr -c $XDG_CONFIG_HOME/pypdfocr/config.yml \u0026#34;$1\u0026#34; 27else 28echo \u0026#34;Using default settings\u0026#34; 29echo \u0026#34;$(tput setaf 4)\u0026#34; 30pypdfocr \u0026#34;$1\u0026#34; 31fi 32echo \u0026#34;$(tput setaf 2)You might want to get pypdfocr\u0026#34; 33fi 34else 35echo \u0026#34;$1is OCR\u0026#39;ed.\u0026#34; 36fi The end result is:\n A directory with perfectly pdf files re-flowed text  Possibly OCR\u0026rsquo;ed for string searches    TOC editing is still rather janky; but this is also because the OCR process is still rather spotty.\nCalibre Calibre is an excellent library software, and there are very few alternatives which offer all the salient features:\n Syncing Apart from working well with a plethora of official devices, Koreader is also pretty well supported, and mounting folders allows for easy management of a secret library (e.g. .Library) on an SD card to prevent Nickel from reading and choking on large libraries Multiple Libraries I personally keep one for fiction, one for non-fiction, and one (transiently populated) one for papers Good metadata collection Nothing beats rich metadata, and with third party plugins, all the best content providers can be leveraged for blurbs; plus most purchased books come with metadata which calibre can read  It isn\u0026rsquo;t perfect, there are far better OPDS (Open Publication Distribution System) servers like the fantastic COPS (Calibre OPDS) project, and there have been some security concerns in the past, but it is really usable and is under active development; plus it has a fun developer. I also personally find the file conversion lacking, compared to k2pdfopt, but as a library management system it is really good.\nZotero Sync Calibre provides a handy ZMI (Zotero Metadata Importer) plugin which allows for exported papers to be imported into calibre and from then into the e-reader as expected. Combined with the folder mounts facilitated by calibre this allows for a painless way to ensure a quick export; optimize; sync; read and delete workflow.\nKoreader Koreader is probably the best thing to happen to e-ink devices since sliced bread. It replaces the need to use any cables with an e-reader; since newer versions have a nice SSH server, and can also update itself. Since this is mostly used as is; and all the information required is on the Github Wiki, there\u0026rsquo;s not much else to say here.\nIt is probably worth noting that the in-built re-flow options do tend to cause major artifacts on older hardware, and is best avoided. Almost equivalently, and at a far lower cost in terms of performance, page contents can be fit to width and zoomed in automatically, which is almost as good as working with k2pdfopt in some special cases.\nConclusions Given my unfortunate separation from my library back home; it is likely that my e-ink devices will continue to be my primary source of reading material. Plus the long retarded color e-ink market finally seems to be moving out of its stupor 9. The only possible addendum to this methodology would probably involve integrating orgmode and the reMarkable 2 sometime. E-ink is here to stay. This setup would probably need revisions involving rclone or syncthing if I ever gave up and opted for a \u0026ldquo;modern\u0026rdquo; device without an SD slot.\n  They\u0026rsquo;re also more convenient than holding a torch in bed with a paperback\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n On an unrelated note, this represents a stylistic departure from older normal posts, with much more of a rambling narrative\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n These are annotated in Emacs with org-mode when necessary\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Diigo is great for saving storing sites forever and Pocket is just cheaper Instapaper\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The same can be said of Wikipedia content and tweets; spending time on the written word does not automatically worth putting on an e-reader\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n For which I have been perfecting a rather personalized workflow with Colemak\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Also why the battery lasts for days without any strain\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n NickelMenu seems to be recommended for newer devices\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n YT video on color and e-ink, YT video on recent color e-ink tech and Verge release notes for the PocketBook InkPad Color\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/my-life-in-eink/","tags":["workflow","tools","ramblings"],"title":"My Life in E-ink"},{"categories":["personal"],"contents":" Migrating Imap, Gmail and Exchange, mail accounts from GUI clients to Astroid\n Background Initially, I had planned this post to start with a brief history of the decline of email clients for Linux. That quickly got out of hand, and was therefore spun out into a post of its own (TBD). To keep things brief. Thanks to the incredible ineptitude of the Thunderbird steering committee, I ended up requiring a new mail client. Having despaired of the GUI based bloat heavy approaches of most clients, I decided to go the old fashioned route and build one up in a modular manner.\nSeries This post is part of a series on email.\n The Decline of Linux Email (TBD) Reclaiming Email with Astroid \u0026lt;\u0026ndash; You are here! Emacs and Email (TBD)  The Accounts In no order of preference, for a variety of reasons, I have 23 distinct email accounts. However, these are actually broken down into a few basic types and associated mail fetching software.\n Generic IMAP Yahoo, Mail.ru and the rest are managed with isync Exchange A school account of mine uses Office 365, and will be handled with davmail in conjunction with isync Gmail There are a few of these, two personal, and two institutional, all of which are handled with lieer  InSync For general IMAP accounts (anything which isn\u0026rsquo;t backed by gmail or outlook) an mbsync approach works best. Exhange accounts need davmail and are described in a separate sub-section. Before we get to the poll.sh script and astroid, it is a good idea to run each of these as we set them up, especially as the first run can take quite a long time (around an hour for 9205 messages).\n1mbsync -V blah General IMAP For a standard IMAP account (anything which isn\u0026rsquo;t Exchange or Gmail) the only thing we need to keep track of is a good way to obfuscate passwords. We will use pass for this 1.\n1pass git init $GPG_KEY # Optional, use a secure private git repo 2pass init $GPG_KEY 3pass add mail/rgoswami.iitk With this, we can configure a general IMAP account (an IITK webmail in this instance, but it could be anything).\n1IMAPAccount rgoswami.iitk 2Host qasid.iitk.ac.in 3User rgoswami 4PassCmd \u0026#34;pass show mail/rgoswami.iitk\u0026#34; 5AuthMechs LOGIN 6SSLType IMAPS 78IMAPStore rgoswami.iitk-remote 9Account rgoswami.iitk 1011MaildirStore rgoswami.iitk-local 12SubFolders Verbatim 13Path /run/media/Storage/.mail/rgoswami_iitk/ 14INBOX /run/media/Storage/.mail/rgoswami_iitk/Inbox 15Trash trash:/// 1617Channel rgoswami.iitk 18Master :rgoswami.iitk-remote: 19Slave :rgoswami.iitk-local: 20Patterns * 21Create Both 22SyncState * Exchange Accounts For Exchange accounts we need to setup davmail. Thankfully the process is actually excessively simple for a single account. For each account, a .properties file needs to be generated. The password section is kept blank in this case, since we will authenticate with the O365Interactive protocol.\n1# The default setup 2davmail ~/.davmail.properties Use the following properties in the .davemail.properties file to prevent timeout errors:\n1davmail.folderSizeLimit=50 2davmail.clientSoTimeout=0 3davmail.enableKeepAlive=true Further information is available at the official docs.\n Figure 1: davmail setup\n   Figure 2: Interactive login workflow\n  Now we can configure the .mbsyncrc in a manner analogous to the standard IMAP setup, but with the port and host where davmail is running.\n1IMAPAccount rog32 2Host localhost 3User rog32@hi.is 4Pass dummy 5Port 1143 6SSLType None 7AuthMechs LOGIN 89IMAPStore rog32-remote 10Account rog32 11MaildirStore rog32-local 12SubFolders Verbatim 13Path /run/media/Storage/.mail/rog32/ 14Inbox /run/media/Storage/.mail/rog32/Inbox 1516Channel rog32 17Master :rog32-remote: 18Slave :rog32-local: 19Patterns * 20Create Both 21SyncState * The SSLType and AuthMechs are important parameters. Note that the Pass is truly not important since we an OAuth token is stored in the properties file.\n Figure 3: First run of a large inbox with davmail and isync\n  Gmaileer The general setup gmi init blah@gmail.com works well for personal accounts. However, there was an additional step for the IEEE account.\nIEEE Some issues with tags led to the following changes in the .gmaileer.json file\n1{\u0026#34;replace_slash_with_dot\u0026#34;: false, \u0026#34;account\u0026#34;: \u0026#34;rgoswami@ieee.org\u0026#34;, \u0026#34;timeout\u0026#34;: 600, \u0026#34;drop_non_existing_label\u0026#34;: false, \u0026#34;ignore_empty_history\u0026#34;: false, \u0026#34;ignore_tags\u0026#34;: [\u0026#34;TODO\u0026#34;,\u0026#34;new\u0026#34;], \u0026#34;ignore_remote_labels\u0026#34;: [\u0026#34;CATEGORY_SOCIAL\u0026#34;, \u0026#34;CATEGORY_FORUMS\u0026#34;, \u0026#34;CATEGORY_PROMOTIONS\u0026#34;, \u0026#34;CATEGORY_PERSONAL\u0026#34;, \u0026#34;CATEGORY_UPDATES\u0026#34;], \u0026#34;remove_local_messages\u0026#34;: true, \u0026#34;file_extension\u0026#34;: \u0026#34;\u0026#34;} Essentially just the addition of two new ignore_tags.\nNotmuch At this point, we have all mail synced into local directories, but we have no access to view or interact with them. We will start by setting up notmuch to search and index our mail. This is pretty basic for now.\n1[database] 2path=/run/media/Storage/.mail/ 3[user] 4name=Person 5primary_email=primary@domain.com 6other_email=one@domain.com;two@domain.com 7[new] 8tags=new;unread;inbox;TODO; 9ignore=/.*[.](json|lock|bak)$/ 10[search] 11exclude_tags=deleted;spam; 12[maildir] 13synchronize_flags=true There are a lot more options which may be configured, but this is enough to get started.\nSending Email At this stage we need a way to actually send email. A simple configuration can be setup in /.config/msmtp/config is given below (where we use pass again):\n1defaults 2port 587 3tls on 4auth on 5logfile ~/.config/msmtp/.msmtp.log 6tls_trust_file /etc/ssl/certs/ca-certificates.crt 78account r95g10 9host smtp.gmail.com 10from r95g10@gmail.com 11user r95g10@gmail.com 12tls_starttls on 13tls on 14auth on 15port 587 16passwordeval pass show mail/r95g10.gmail 1718account rog32 19host localhost 20from rog32@hi.is 21user rog32@hi.is 22tls_starttls off 23tls off 24auth plain 25port 1025 26password dummy Note that the exchange account again uses a dummy password, and the real password will be prompted for on the first run. The permissions of the file above should be 600. It is prudent to test at-least the exchange section to login.\n1echo \u0026#34;Hello world\u0026#34; | msmtp --account=rog32 r95g10@gmail.com Astroid The bulk of this setup is by the numbers, with the exception of the poll script. However, minimally configure astroid with the location of our notmuch configuration.\n1astroid --new-config 2vim ~/.config/astroid/config The relevant sections are:\n1\u0026#34;notmuch_config\u0026#34;: \u0026#34;\\/home\\/whoever\\/.notmuch-config\u0026#34;, The accounts section is fairly self explanatory, but we will need to use the following sendmail line as well:\n1\u0026#34;sendmail\u0026#34;: \u0026#34;msmtp --read-envelope-from -i -t\u0026#34;, Nix Python Poll Script Natively only bash appears to be supported. However, with nix, we can use a reproducible python script with the sh library to call system functions instead.\n1from pathlib import Path 2import sh 34# For generic IMAP maildirs 56ISYNC_LABELS = [\u0026#34;rgoswami.iitk\u0026#34;, \u0026#34;rog32\u0026#34;] 78for isync in ISYNC_LABELS: 9sh.mbsync(\u0026#34;-V\u0026#34;,isync,_bg=True) 101112# Gmaileer 13GMAIL_IDENTIFIERS = [\u0026#34;gmail\u0026#34;, \u0026#34;univ\u0026#34;, \u0026#34;ieee\u0026#34;] 1415path = Path(r\u0026#34;/run/media/haozeke/Storage/.mail/\u0026#34;) 1617for dirs in path.iterdir(): 18if dirs.is_dir(): 19for gmi in GMAIL_IDENTIFIERS: 20if gmi in dirs.name: 21print(f\u0026#34;Syncing {dirs.name}\u0026#34;) 22sh.gmi(\u0026#34;sync\u0026#34;, _cwd=dirs, _fg=True) This needs to be coupled with the standard nix shebang in the poll.sh file:\n1#!/usr/bin/env nix-shell 2#!nix-shell -i python3 -p \u0026#34;python38.withPackages(ps: [ ps.numpy ps.sh ])\u0026#34; Navigation For working with HTML emails, we need to highlight the notice about potentially sketchy HTML using (defaults) j or k and then hit enter or o.\n Figure 4: Unhelpful default\n   Figure 5: After viewing the sketchy bit\n  Note that since most email is actually sent with text/html it might make more sense to configure the thread_view in the configuration file.\n1\u0026#34;thread_view\u0026#34;: { 2\u0026#34;open_html_part_external\u0026#34;: \u0026#34;false\u0026#34;, 3\u0026#34;preferred_type\u0026#34;: \u0026#34;html\u0026#34;, 4\u0026#34;preferred_html_only\u0026#34;: \u0026#34;false\u0026#34;, 5\u0026#34;allow_remote_when_encrypted\u0026#34;: \u0026#34;false\u0026#34;, 6\u0026#34;open_external_link\u0026#34;: \u0026#34;xdg-open\u0026#34;, 7\u0026#34;default_save_directory\u0026#34;: \u0026#34;~\u0026#34;, 8\u0026#34;indent_messages\u0026#34;: \u0026#34;false\u0026#34;, 9\u0026#34;gravatar\u0026#34;: { 10\u0026#34;enable\u0026#34;: \u0026#34;true\u0026#34; 11}, 12\u0026#34;mark_unread_delay\u0026#34;: \u0026#34;0.5\u0026#34;, 13\u0026#34;expand_flagged\u0026#34;: \u0026#34;true\u0026#34; 14}, Deletion There are again, two different approaches to deletion.\n Gmail For gmail accounts it is simple, just adding a trash tag will do the trick, so we can select multiple emails with t and then hit + to add trash and everything works out Other Accounts We can delete mail forever (from the server as well), by using a safe-tag and then using notmuch  The generic non-lieer method requires:\n1notmuch search --output=files --format=text0 tag:killed | xargs -r0 rm A pre-new hook will work.\nComposition Thankfully, astroid supports GPG encryption as well as markdown support. This makes for simple integration with any popular editor.\n Figure 6: Composition with emacs and astroid\n  Conclusions This is enough to get started for a while, but it isn\u0026rsquo;t yet at the stage where I can replace thunderbird unfortunately. However, there are several pain points to be addressed, which will be covered in a future post. Some of these are essentially related to network fluctuations, and the awkward deletion setup.\nUpdate  astroid appears to be having a bit of a development crisis  The sequel (ragnarok) unfortunately seems to have gone towards a browser frontend, which is quite unacceptable to me  This makes it far too similar to (in principle) Mailspring (though with less problematic connectivity issues) This means I won\u0026rsquo;t be moving forward with the next set of posts regarding astroid and will move towards emacs again     Max Mehl has a good collection of scripts for astroid    The GPG key itself can be stored with keybase\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/reclaim-email-astroid/","tags":["rant","tools","email","workflow"],"title":"Reclaiming Email with Astroid"},{"categories":["notes"],"contents":" A workflow for managing private submodules in a private repository without personal access tokens for Github actions\n Background Ever since Travis CI decided to drink corporate kool-aid, the search for a new CI has been brought to the fore again. Github Actions seem uniquely suited for private repositories, since most CIs bill for private repos. However, the basic authentication setup for the checkout action involves using one SSH key, effectively a personal access token, for both the main project and all submodules. This is untenable for anyone working with a team.\nSolution The fix, as it were, is in two steps. We will first require a deploy key to be set for the private submodule, and then store the private portion in the private repo. We will begin with a concrete setup.\nSetup Consider a standard C++ build setup:\n1name:Fake secret project23on:4push:5branches:[master, development]6pull_request:7branches:[master]89jobs:10build:11runs-on:${{ matrix.os }}1213strategy:14max-parallel:415matrix:16os:[ubuntu-18.04]17cpp-version:[g++-7, g++-9, clang++]1819steps:20- uses:actions/checkout@v221- name:build22env:23CXX:${{ matrix.cpp-version }}24run:|25mkdir build \u0026amp;\u0026amp; cd build 26cmake -DCMAKE_CXX_COMPILER=\u0026#34;$CXX\u0026#34; -DCMAKE_CXX_FLAGS=\u0026#34;-std=c++11\u0026#34; ../ 27make -j$(nproc)28- name:run29run:|30./super_secretKey Generation This section is standard. We will generate a deploy key essentially. Note that it isn\u0026rsquo;t necessary to set a password, using one would only minimally improve security and bring in some annoying script modifications.\n1# Anywhere safe 2ssh-keygen -t rsa -b 4096 -C \u0026#34;Fake Deployment Key\u0026#34; -f \u0026#39;priv_sub_a\u0026#39; -N \u0026#39;\u0026#39; Private Submodule Repo We will store the public portion of the key as a deploy key in the private submodule repository.\n1# Copy contents 2cat priv_sub_a.pub | xclip -selection clipboard Note that, as shown in Fig. 1 we do not need to give write access to this key.\n Private Project Repo Now we will need the private portion of the key as a secret in the private project repository (see Fig. 2).\n1# Copy contents of private key 2cat priv_sub_a | xclip -selection clipboard  Figure 1: Secret setup in the private project\n  Workflow Modifications Now we will simply update our workflow 1. We will simply add the following step:\n1- name:get_subm2env:3SSHK:${{ secrets.SUB_SSHK_A }}4run:|5mkdir -p $HOME/.ssh 6echo \u0026#34;$SSHK\u0026#34; \u0026gt; $HOME/.ssh/ssh.key 7chmod 600 $HOME/.ssh/ssh.key 8export GIT_SSH_COMMAND=\u0026#34;ssh -i $HOME/.ssh/ssh.key\u0026#34; 9git submodule update --init --recursiveThis will work for a single private submodule and multiple public submodules. For multiple private submodules, we would not initialize them recursively, but instead use a separate key for each.\n1- name:get_subm_a2env:3SSHK:${{ secrets.SUB_SSHK_A }}4run:|5mkdir -p $HOME/.ssh 6echo \u0026#34;$SSHK\u0026#34; \u0026gt; $HOME/.ssh/ssh.key 7chmod 600 $HOME/.ssh/ssh.key 8export GIT_SSH_COMMAND=\u0026#34;ssh -i $HOME/.ssh/ssh.key\u0026#34; 9git submodule update --init -- \u0026lt;specific relative path to submodule A\u0026gt;10- name:get_subm_b11env:12SSHK:${{ secrets.SUB_SSHK_B }}13run:|14mkdir -p $HOME/.ssh 15echo \u0026#34;$SSHK\u0026#34; \u0026gt; $HOME/.ssh/ssh.key 16chmod 600 $HOME/.ssh/ssh.key 17export GIT_SSH_COMMAND=\u0026#34;ssh -i $HOME/.ssh/ssh.key\u0026#34; 18git submodule update --init -- \u0026lt;specific relative path to submodule B\u0026gt;Note that it is not possible to use the same SSH key in multiple submodule repositories, as each deploy key can only be associated with one repository.\nComplete Workflow Putting the above steps together, for the case of a single private submodule and multiple public submodules, we have:\n1name:Fake secret project23on:4push:5branches:[master, development]6pull_request:7branches:[master]89jobs:10build:11runs-on:${{ matrix.os }}1213strategy:14max-parallel:415matrix:16os:[ubuntu-18.04]17cpp-version:[g++-7, g++-9, clang++]1819steps:20- uses:actions/checkout@v221- name:get_subm22env:23SSHK:${{ secrets.SUB_SSHK_A }}24run:|25mkdir -p $HOME/.ssh 26echo \u0026#34;$SSHK\u0026#34; \u0026gt; $HOME/.ssh/ssh.key 27chmod 600 $HOME/.ssh/ssh.key 28export GIT_SSH_COMMAND=\u0026#34;ssh -i $HOME/.ssh/ssh.key\u0026#34; 29git submodule update --init --recursive30- name:build31env:32CXX:${{ matrix.cpp-version }}33run:|34mkdir build \u0026amp;\u0026amp; cd build 35cmake -DCMAKE_CXX_COMPILER=\u0026#34;$CXX\u0026#34; -DCMAKE_CXX_FLAGS=\u0026#34;-std=c++11\u0026#34; ../ 36make -j$(nproc)37- name:run38run:|39./super_secretConclusions We have demonstrated a minimally invasive setup for working with a private submodule, which is trivially extensible to multiple such submodules. With this, it appears that GH actions might a viable option (as opposed to say, Wercker), at least for private teams. A more full comparative post might be warranted at a later date.\n  submodule-checkout uses a similar concept but unfortunately does not extend to multiple submodules and the submodules are checked out as root\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/priv-gh-actions/","tags":["tools","github","workflow"],"title":"Private Github Actions without PAT"},{"categories":["programming"],"contents":" Short post on making minimal changes to derivations in nixpkgs at a project level using callPackage() along with GH-Actions for deployment of sphinx documentation.\n Background As part of my work on the Symengine documentation1, I had originally thought of leveraging nix for reproducible builds for each of the language bindings with GH-Actions. There exists a derivation in the upstream package repository, but it was outdated (v6.0.0 instead of v6.0.1) 2. Normally this would simply require a PR to be fixed; but since documenting different language bindings requires specific flags and tests were largely not an issue, I needed to make a project level derivation for each repository.\nProject Layout The standard niv setup, as described in the longer tutorial post will suffice.\n1niv init -b master Since the bindings were for python, mach-nix was leveraged for the shell.nix; reasons for which have been outlined elsewhere. This led to the following expression:\n1let 2sources = import ./nix/sources.nix; 3pkgs = import sources.nixpkgs { }; 4nsymengine = pkgs.callPackage ./nix/nsymengine.nix { }; 5mach-nix = import (builtins.fetchGit { 6url = \u0026#34;https://github.com/DavHau/mach-nix/\u0026#34;; 7ref = \u0026#34;refs/tags/3.1.1\u0026#34;; 8}) { 9pkgs = pkgs; 10}; 11customPython = mach-nix.mkPython rec { 12requirements = builtins.readFile ./requirements.txt; 13}; 14in pkgs.mkShell { 15buildInputs = with pkgs; [ 16customPython 17cmake 18nsymengine 19bashInteractive 20sage 21which 22]; 23} The highlighted line calls nsymengine.nix which is derived from the upstream expression. Since callPackage is being used from the niv pinned sources, the expression is completely project-specific and reproducible.\nUpstream For completion; the upstream expression is reproduced. We will actually grab this directly with show-derivation:\n1nix show-derivation -f \u0026#34;\u0026lt;nixpkgs\u0026gt;\u0026#34; symengine \u0026gt; nix/nsymengine.nix Which gives us:\n1{ lib, stdenv 2, fetchFromGitHub 3, cmake 4, gmp 5, flint 6, mpfr 7, libmpc 8}: 910stdenv.mkDerivation rec { 11pname = \u0026#34;symengine\u0026#34;; 12version = \u0026#34;0.6.0\u0026#34;; 1314src = fetchFromGitHub { 15owner = \u0026#34;symengine\u0026#34;; 16repo = \u0026#34;symengine\u0026#34;; 17rev = \u0026#34;v${version}\u0026#34;; 18sha256 = \u0026#34;129iv9maabmb42ylfdv0l0g94mcbf3y4q3np175008rcqdr8z6h1\u0026#34;; 19}; 2021nativeBuildInputs = [ cmake ]; 2223buildInputs = [ gmp flint mpfr libmpc ]; 2425cmakeFlags = [ 26\u0026#34;-DWITH_FLINT=ON\u0026#34; 27\u0026#34;-DINTEGER_CLASS=flint\u0026#34; 28\u0026#34;-DWITH_SYMENGINE_THREAD_SAFE=yes\u0026#34; 29\u0026#34;-DWITH_MPC=yes\u0026#34; 30\u0026#34;-DBUILD_FOR_DISTRIBUTION=yes\u0026#34; 31]; 3233doCheck = true; 3435checkPhase = \u0026#39;\u0026#39; 36ctest 37\u0026#39;\u0026#39;; 3839meta = with lib; { 40description = \u0026#34;A fast symbolic manipulation library\u0026#34;; 41homepage = \u0026#34;https://github.com/symengine/symengine\u0026#34;; 42platforms = platforms.unix ++ platforms.windows; 43license = licenses.bsd3; 44maintainers = [ maintainers.costrouc ]; 45}; 4647} Repo-native The truncated expression meant to be bundled with the repo is reproduced below.\n1{ lib, stdenv 2, fetchFromGitHub 3, cmake 4, gmp 5, flint 6, mpfr 7, libmpc 8}: 910stdenv.mkDerivation rec { 11pname = \u0026#34;symengine\u0026#34;; 12name = \u0026#34;symengine\u0026#34;; 13version = \u0026#34;44eb47e3bbfa7e06718f2f65f3f41a0a9d133b70\u0026#34;; # From symengine-version.txt 1415src = fetchFromGitHub { 16owner = \u0026#34;symengine\u0026#34;; 17repo = \u0026#34;symengine\u0026#34;; 18rev = \u0026#34;${version}\u0026#34;; 19sha256 = \u0026#34;137cxk3x8vmr4p5x0knzjplir0slw0gmwhzi277si944i33781hd\u0026#34;; 20}; 2122nativeBuildInputs = [ cmake ]; 2324buildInputs = [ gmp flint mpfr libmpc ]; 2526cmakeFlags = [ 27\u0026#34;-DWITH_FLINT=ON\u0026#34; 28\u0026#34;-DINTEGER_CLASS=flint\u0026#34; 29\u0026#34;-DWITH_SYMENGINE_THREAD_SAFE=yes\u0026#34; 30\u0026#34;-DWITH_MPC=yes\u0026#34; 31\u0026#34;-DBUILD_TESTS=no\u0026#34; 32\u0026#34;-DBUILD_FOR_DISTRIBUTION=yes\u0026#34; 33]; 3435doCheck = false; 3637} 3839# Derived from the upstream expression : https://github.com/r-ryantm/nixpkgs/blob/34730e0640710636b15338f20836165f29b3df86/pkgs/development/libraries/symengine/default.nix Some project-specific notes:\n m2r2 was used to inject the project README.md into the Sphinx reStructuredText index file To prevent symengine from picking up the system python, a pure shell was required nix-shell --pure --run bash symengine-version.txt was already being used by the CI setup for cloning the right commit, and though a text file for hash storage is inelegant, it is a valid approach without nix  GH-Actions and Deployment Having obtained an environment; the penultimate step involved a trivial shell script which uses the fantastic nix-shell:\n1#!/usr/bin/env nix-shell 2#!nix-shell ../shell.nix -i bash 34python setup.py build_ext --inplace 5sphinx-build docs/ genDocs/ Finally an action definition in .github/workflows/mkdocs.yaml:\n1on:2push:3branches:4- main5- master67name:Make Sphinx API Docs89jobs:10mkdocs:11runs-on:ubuntu-latest12steps:13- uses:actions/checkout@v21415- uses:cachix/install-nix-action@v1216with:17nix_path:nixpkgs=channel:nixos-unstable1819- name:Build20run:./bin/docbuilder.sh2122- name:Deploy23uses:peaceiris/actions-gh-pages@v324with:25github_token:${{ secrets.GITHUB_TOKEN }}26publish_dir:./genDocsConclusion In the end, to ensure easy long-term maintenance by the other maintainers, it was decided that the nix derivations should be trashed in favor of conda madness. Though most changes should be contributed upstream, the efficient re-use of upstream expressions is still viable for certain projects, especially those related to documentation.\n  Sponsored by the Google Season of Docs initiative\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Part of the documentation design worked out involved the live at head concept (also described in this CPPCon17 talk)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/nix-prj-spec-doc/","tags":["documentation","tools","nix","workflow"],"title":"Project Specific Expressions from Nixpkgs for Sphinx documentation"},{"categories":["personal"],"contents":" An introduction to hacking keyboard layouts with X keyboard extension (XKB) and klfc, focused on Colemak and vim bindings\n Background Inspite of maximizing ergonomic bindings for most common software (e.g. Vimium, doom-emacs), every operation with the arrow keys still trouble me. Here I will lay out my experiments transitioning to a stable, uniquely defined setup with the X keyboard extension.\nSeries This post is part of a series on Colemak and keyboard management in general.\n Switching to Colemak Refactoring Dotfiles For Colemak Remapping Keys with XKB and KLFC \u0026lt;\u0026ndash; You are here! Remapping Keys for ColemakVIM on MacOS  Keyboard Basics Some terms to keep in mind for this post are1:\n Dead Keys These don\u0026rsquo;t actually output anything, but modify the next key pressed. Like applying an umlaut on the subsequent letter. Lock Keys State modifiers which are toggled, like Caps Lock Compose Key A key which interprets a series of subsequent key strokes. A dead key on steroids.  Also the different levels (from here) are concisely defined in the following table.\nTable 1: Levels for a keyboard     Level Modifier Keys     1 None Lowercase letters, numbers other symbols   2 Shift Uppercase letters, symbols placed above numbers   3 AltGr Accented characters, symbols, some dead keys   4 Shift+AltGr More dead keys and symbols   5 Extend User-defined   6 Shift+Extend User-defined    Modification Strategies Common approaches to quick remapping of keys involves xmodmap, which does not persist between reboots. Manually recreating or spinning off of XKB configuration files was also not very appealing.\nKLFC A more elegant approach is by using the excellent klfc Haskell binary. To install this from source:\n1git clone https://github.com/39aldo39/klfc 2cd klfc 3# Kanged from the AUR https://aur.archlinux.org/packages/klfc/ 4cabal v1-sandbox init 5cabal v1-update 6cabal v1-install --only-dependencies --ghc-options=-dynamic --force-reinstalls 7cabal v1-configure --prefix=/usr --ghc-options=-dynamic 8cabal v1-build The output binary is in ./dist/build/klfc/klfc.\nNote that the set of keys mapped by the json files are relative to the QWERTY layout, that is:\n This means that we have to ensure that the keys are mapped relative to QWERTY as well, not relative to the modified base layout.\nRemapping Some goals were:\n Programming (particularly in python and lisp) put a lot of stress on the right hand pinky for Colemak users2 VIM keys should be global but toggled with a lock  My primary use case is currently my ThinkPad X380, which comes with a basic QWERTY contracted layout as shown in Fig. 1.\n\n Figure 1: Basic X380 QWERTY\n  Colemak - Layers 1 and 2 The first mapping is a basic Colemak setup as shown in Fi. fig:colemak.\n\n Figure 2: Basic X380 Colemak\n  It wouldn\u0026rsquo;t make much sense to remap the first two layers. We can use the json from the examples of the klfc repository.\n1// Base Colemak layout 2// https://colemak.com 3{ 4\u0026#34;fullName\u0026#34;: \u0026#34;Colemak\u0026#34;, 5\u0026#34;name\u0026#34;: \u0026#34;colemak\u0026#34;, 6\u0026#34;localeId\u0026#34;: \u0026#34;00000409\u0026#34;, 7\u0026#34;copyright\u0026#34;: \u0026#34;Public Domain\u0026#34;, 8\u0026#34;company\u0026#34;: \u0026#34;2006-01-01 Shai Coleman\u0026#34;, 9\u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34;, 10\u0026#34;shiftlevels\u0026#34;: [ \u0026#34;None\u0026#34;, \u0026#34;Shift\u0026#34; ], 11\u0026#34;singletonKeys\u0026#34;: [ 12[ \u0026#34;CapsLock\u0026#34;, \u0026#34;Backspace\u0026#34; ] 13], 14\u0026#34;keys\u0026#34;: [ 15{ \u0026#34;pos\u0026#34;: \u0026#34;~\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;`\u0026#34;, \u0026#34;~\u0026#34; ] }, 16{ \u0026#34;pos\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;1\u0026#34;, \u0026#34;!\u0026#34; ] }, 17{ \u0026#34;pos\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;2\u0026#34;, \u0026#34;@\u0026#34; ] }, 18{ \u0026#34;pos\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;3\u0026#34;, \u0026#34;#\u0026#34; ] }, 19{ \u0026#34;pos\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;4\u0026#34;, \u0026#34;$\u0026#34; ] }, 20{ \u0026#34;pos\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;5\u0026#34;, \u0026#34;%\u0026#34; ] }, 21{ \u0026#34;pos\u0026#34;: \u0026#34;6\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;6\u0026#34;, \u0026#34;^\u0026#34; ] }, 22{ \u0026#34;pos\u0026#34;: \u0026#34;7\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;7\u0026#34;, \u0026#34;\u0026amp;\u0026#34; ] }, 23{ \u0026#34;pos\u0026#34;: \u0026#34;8\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;8\u0026#34;, \u0026#34;*\u0026#34; ] }, 24{ \u0026#34;pos\u0026#34;: \u0026#34;9\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;9\u0026#34;, \u0026#34;(\u0026#34; ] }, 25{ \u0026#34;pos\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;0\u0026#34;, \u0026#34;)\u0026#34; ] }, 26{ \u0026#34;pos\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;-\u0026#34;, \u0026#34;_\u0026#34; ] }, 27{ \u0026#34;pos\u0026#34;: \u0026#34;+\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;=\u0026#34;, \u0026#34;+\u0026#34; ] }, 28{ \u0026#34;pos\u0026#34;: \u0026#34;Q\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;q\u0026#34;, \u0026#34;Q\u0026#34; ] }, 29{ \u0026#34;pos\u0026#34;: \u0026#34;W\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;w\u0026#34;, \u0026#34;W\u0026#34; ] }, 30{ \u0026#34;pos\u0026#34;: \u0026#34;E\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;f\u0026#34;, \u0026#34;F\u0026#34; ] }, 31{ \u0026#34;pos\u0026#34;: \u0026#34;R\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;p\u0026#34;, \u0026#34;P\u0026#34; ] }, 32{ \u0026#34;pos\u0026#34;: \u0026#34;T\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;g\u0026#34;, \u0026#34;G\u0026#34; ] }, 33{ \u0026#34;pos\u0026#34;: \u0026#34;Y\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;j\u0026#34;, \u0026#34;J\u0026#34; ] }, 34{ \u0026#34;pos\u0026#34;: \u0026#34;U\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;l\u0026#34;, \u0026#34;L\u0026#34; ] }, 35{ \u0026#34;pos\u0026#34;: \u0026#34;I\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;u\u0026#34;, \u0026#34;U\u0026#34; ] }, 36{ \u0026#34;pos\u0026#34;: \u0026#34;O\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;y\u0026#34;, \u0026#34;Y\u0026#34; ] }, 37{ \u0026#34;pos\u0026#34;: \u0026#34;P\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;;\u0026#34;, \u0026#34;:\u0026#34; ] }, 38{ \u0026#34;pos\u0026#34;: \u0026#34;[\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;[\u0026#34;, \u0026#34;{\u0026#34; ] }, 39{ \u0026#34;pos\u0026#34;: \u0026#34;]\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;]\u0026#34;, \u0026#34;}\u0026#34; ] }, 40{ \u0026#34;pos\u0026#34;: \u0026#34;\\\\\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;\\\\\u0026#34;, \u0026#34;|\u0026#34; ] }, 41{ \u0026#34;pos\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;a\u0026#34;, \u0026#34;A\u0026#34; ] }, 42{ \u0026#34;pos\u0026#34;: \u0026#34;S\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;r\u0026#34;, \u0026#34;R\u0026#34; ] }, 43{ \u0026#34;pos\u0026#34;: \u0026#34;D\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;s\u0026#34;, \u0026#34;S\u0026#34; ] }, 44{ \u0026#34;pos\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;t\u0026#34;, \u0026#34;T\u0026#34; ] }, 45{ \u0026#34;pos\u0026#34;: \u0026#34;G\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;d\u0026#34;, \u0026#34;D\u0026#34; ] }, 46{ \u0026#34;pos\u0026#34;: \u0026#34;H\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;h\u0026#34;, \u0026#34;H\u0026#34; ] }, 47{ \u0026#34;pos\u0026#34;: \u0026#34;J\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;n\u0026#34;, \u0026#34;N\u0026#34; ] }, 48{ \u0026#34;pos\u0026#34;: \u0026#34;K\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;e\u0026#34;, \u0026#34;E\u0026#34; ] }, 49{ \u0026#34;pos\u0026#34;: \u0026#34;L\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;i\u0026#34;, \u0026#34;I\u0026#34; ] }, 50{ \u0026#34;pos\u0026#34;: \u0026#34;;\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;o\u0026#34;, \u0026#34;O\u0026#34; ] }, 51{ \u0026#34;pos\u0026#34;: \u0026#34;\u0026#39;\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;\u0026#39;\u0026#34;, \u0026#34;\\\u0026#34;\u0026#34; ] }, 52{ \u0026#34;pos\u0026#34;: \u0026#34;Z\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;z\u0026#34;, \u0026#34;Z\u0026#34; ] }, 53{ \u0026#34;pos\u0026#34;: \u0026#34;X\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;x\u0026#34;, \u0026#34;X\u0026#34; ] }, 54{ \u0026#34;pos\u0026#34;: \u0026#34;C\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;c\u0026#34;, \u0026#34;C\u0026#34; ] }, 55{ \u0026#34;pos\u0026#34;: \u0026#34;V\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;v\u0026#34;, \u0026#34;V\u0026#34; ] }, 56{ \u0026#34;pos\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;b\u0026#34;, \u0026#34;B\u0026#34; ] }, 57{ \u0026#34;pos\u0026#34;: \u0026#34;N\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;k\u0026#34;, \u0026#34;K\u0026#34; ] }, 58{ \u0026#34;pos\u0026#34;: \u0026#34;M\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;m\u0026#34;, \u0026#34;M\u0026#34; ] }, 59{ \u0026#34;pos\u0026#34;: \u0026#34;,\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;,\u0026#34;, \u0026#34;\u0026lt;\u0026#34; ] }, 60{ \u0026#34;pos\u0026#34;: \u0026#34;.\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;.\u0026#34;, \u0026#34;\u0026gt;\u0026#34; ] }, 61{ \u0026#34;pos\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;/\u0026#34;, \u0026#34;?\u0026#34; ] } 62], 63\u0026#34;variants\u0026#34;: [ 64{ 65\u0026#34;name\u0026#34;: \u0026#34;mod-dh\u0026#34;, 66\u0026#34;shiftlevels\u0026#34;: [ \u0026#34;None\u0026#34;, \u0026#34;Shift\u0026#34; ], 67\u0026#34;keys\u0026#34;: [ 68{ \u0026#34;pos\u0026#34;: \u0026#34;V\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;d\u0026#34;, \u0026#34;D\u0026#34; ] }, 69{ \u0026#34;pos\u0026#34;: \u0026#34;B\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;v\u0026#34;, \u0026#34;V\u0026#34; ] }, 70{ \u0026#34;pos\u0026#34;: \u0026#34;G\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;g\u0026#34;, \u0026#34;G\u0026#34; ] }, 71{ \u0026#34;pos\u0026#34;: \u0026#34;T\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;b\u0026#34;, \u0026#34;B\u0026#34; ] }, 72{ \u0026#34;pos\u0026#34;: \u0026#34;H\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;k\u0026#34;, \u0026#34;K\u0026#34; ] }, 73{ \u0026#34;pos\u0026#34;: \u0026#34;N\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;m\u0026#34;, \u0026#34;M\u0026#34; ] }, 74{ \u0026#34;pos\u0026#34;: \u0026#34;M\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;h\u0026#34;, \u0026#34;H\u0026#34; ] } 75] 76} 77] 78} VIM Extensions The additions are primarily through the Extend Layer3 (Fig. 3), with a Shift addition (Fig. 4) and more keys with AltGr (Fig. 5). As mentioned before, we have to continue mapping relative to QWERTY, so these mappings can be used by QWERTY users as well. We will essentially use the ISO_5 shift key.\n\n Figure 3: Extend layer mapping\n   Maps basic vim movement  \n Figure 4: Extend+Shift layer mapping\n   Relatively empty, just has a bracket  \n Figure 5: Extend+AltGr layer mapping\n   Includes deletions  These are defined in a single json as shown.\n1{ 2\u0026#34;filter\u0026#34;: \u0026#34;no klc,keylayout\u0026#34;, 3\u0026#34;singletonKeys\u0026#34;: [ 4[ \u0026#34;CapsLock\u0026#34;, \u0026#34;Extend\u0026#34; ], 5[ \u0026#34;Alt_L\u0026#34;, \u0026#34;AltGr\u0026#34; ] 6], 7\u0026#34;shiftlevels\u0026#34;: [ \u0026#34;Extend\u0026#34;, \u0026#34;Shift+Extend\u0026#34;, \u0026#34;AltGr+Extend\u0026#34; ], 8\u0026#34;keys\u0026#34;: [ 9{ \u0026#34;pos\u0026#34;: \u0026#34;H\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;Left\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Backspace\u0026#34; ] }, 10{ \u0026#34;pos\u0026#34;: \u0026#34;J\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;Down\u0026#34; ] }, 11{ \u0026#34;pos\u0026#34;: \u0026#34;K\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;Up\u0026#34; ] }, 12{ \u0026#34;pos\u0026#34;: \u0026#34;L\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;Right\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Delete\u0026#34; ] }, 13{ \u0026#34;pos\u0026#34;: \u0026#34;;\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;Enter\u0026#34; ] }, 14{ \u0026#34;pos\u0026#34;: \u0026#34;N\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;(\u0026#34;, \u0026#34;[\u0026#34;, \u0026#34;{\u0026#34; ] }, 15{ \u0026#34;pos\u0026#34;: \u0026#34;V\u0026#34;, \u0026#34;letters\u0026#34;: [ \u0026#34;)\u0026#34;, \u0026#34;]\u0026#34; , \u0026#34;}\u0026#34;] } 16] 17}  The key idea is to have the AltGr keys placed symmetrically One major issue is that Backspace has gone from a single stroke of CapsLock to a three-key-combo  This is the least intuitive, and might need to be changed   The third level (Extend+AltGr) is more accessible than the second in this layout  Usage To generate the files needed to load the new layout:\n1klfc colemak.json extendVIM.json -o coleVIM 2cd coleVIM/xkb 3./run-session.sh # to try them out 4./install-system.sh \u0026amp;\u0026amp; ./scripts/install-xcompose.sh # to install them Conclusions The layout takes a bit of time to get used to, but it is a lot more transparent in the end compared to manually remapping to Colemak\u0026rsquo;s NEIO instead of HJKL for movement. It is both persistent and easily extended, though it is likely that more needs to be done. Perhaps some metrics 4 might be collected as well.\n  For more details the Wikipedia article on Keyboard Layouts is useful or this file on the tmk keyboard\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Colemak, unlike Dvorak, prioritises finger rolls over alternating hands\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n DreymaR\u0026rsquo;s Extend mappings might be good for QWERTY people\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The metric collection of Michael White or the CARPALX metrics\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/remap-keys-xkb-klfc/","tags":["workflow","tools"],"title":"Remapping Keys with XKB and KLFC"},{"categories":["programming"],"contents":" Setting up unicode math and orgmode for painless Anki deck building\n Background A recent Hacker News post reminded me of Anki, and that brought back memories of my Anki orgmode setup. I thought I\u0026rsquo;d re-create and immortalize it.\nThe standard way of working with Anki, is with a pretty awkward GUI. There are changes to be made here, which make life a little easier, including the setup of custom cards, but the inherent concerns of the WYSIWYG editor are basically insurmountable.\n Figure 1: Anki GUI\n  The goal is to get this a better workflow than manual editing of Anki decks. orgmode is perfect for making cards, especially in the larger context of using it for storing images and rich pdfs.\n Figure 2: A pleasant way to make anki decks\n  Methodology To accomplish this, we basically need to have the following:\n anki-editor This emacs plugin will facilitate the conversion from our orgmode files to the Anki markup anki-connect We need a server of sorts set up to allow us to push pull and get errors from the running Anki server, this is an Anki plugin LaTeX process editor It wouldn\u0026rsquo;t be much better than manually making cards in Anki if we couldn\u0026rsquo;t leverage unicode characters, so we need to modify the internal Anki build process for TeX  Anki Editor As with all emacs related setup snippets on this site, these should be modified and adapted as needed, especially for those not using doom-emacs.\n1(use-package anki-editor 2:after org-noter 3:config 4; I like making decks 5(setq anki-editor-create-decks \u0026#39;t)) Also, my full configuration has additional non-essential quality of life keybindings amongst other things.\nAnki Connect CTRL+Shift+A will bring up the addon settings, and Anki has to be restarted after installing the addons. Anki Connect itself does not need any further configuration, though the readme is very comprehensive.\nTeX Setup The LaTeX process editor can be set in two stages, wherein we will first ensure that we can use xelatex and that we can generate an svg.\n1{ 2\u0026#34;svgCommands\u0026#34;: [ 3[ 4\u0026#34;xelatex\u0026#34;, 5\u0026#34;--no-pdf\u0026#34;, 6\u0026#34;-interaction=nonstopmode\u0026#34;, 7\u0026#34;tmp.tex\u0026#34; 8], 9[ 10\u0026#34;dvisvgm\u0026#34;, 11\u0026#34;--no-fonts\u0026#34;, 12\u0026#34;-Z\u0026#34;, 13\u0026#34;2\u0026#34;, 14\u0026#34;tmp.xdv\u0026#34;, 15\u0026#34;-o\u0026#34;, 16\u0026#34;tmp.svg\u0026#34; 17] 18] 19} The png settings can be modified in a similar manner if required, but it is better to generate svg files, which will set up in the cosmetics section. Note that we pass --no-pdf to get the xdv file which has replaced dvi files for xelatex.\nCosmetics The final aspect of this is to be configured with the GUI. The easiest option is to clone the Basic card type and customize that. CTRL+Shift+N should bring up the card editor. The relevant styles are1 (from the Cards option):\n1.card { 2font-family: Literata; 3font-size: 26px; 4text-align: center; 5color: black; 6background-color: white; 7} 8img { 9max-height:1000px; 10height: auto; 11width: auto; 12} 13img[src*=\u0026#34;latex\u0026#34;] { 14vertical-align: middle; 15} Now we need setup our TeX headers as well, and enable the Create scalable images with dvisvgm option. The header needs to have (minimally):\n1\\documentclass[12pt]{article} 2\\special{papersize=3in,5in} 3\\usepackage{geometry} 4\\usepackage{unicode-math} 5\\usepackage{mathtools} 6\\pagestyle{empty} 7\\setlength{\\parindent}{0in} 8\\begin{document} While the footer is simply \\end{document}. With this, we have achieved pretty formatting.\n Figure 3: Pretty card formatting\n  Font Locking Inspired by this post, we will also use orgcss to obtain some orgmode font-locking. We will add the following styles:\n1:not(pre) \u0026gt; code { 2padding: 2px 5px; 3margin: auto 1px; 4border: 1px solid #ddd; 5border-radius: 3px; 6background-clip: padding-box; 7color: #333; 8font-size: $code-size; 9} 1011.org-src-container { 12border: 1px solid #ccc; 13box-shadow: 3px 3px 3px #eee; 14font-family: $monospace; 15font-size: $code-size; 16margin: 1em auto; 17padding: 0.1em 0.5em; 18position: relative; 19} 2021.org-src-container \u0026gt; pre { 22overflow: auto; 23} 2425.org-src-container \u0026gt; pre:before { 26display: block; 27position: absolute; 28background-color: #b3b3b3; 29top: 0; 30right: 0; 31padding: 0 0.5em; 32border-bottom-left-radius: 8px; 33border: 0; 34color: white; 35font-size: $code-size; 36} 3738/* from http://demo.thi.ng/org-spec/ */ 3940.org-src-container \u0026gt; pre.src-sh:before { 41content: \u0026#34;sh\u0026#34;; 42} 43.org-src-container \u0026gt; pre.src-bash:before { 44content: \u0026#34;bash\u0026#34;; 45} 46.org-src-container \u0026gt; pre.src-emacs-lisp:before { 47content: \u0026#34;Emacs Lisp\u0026#34;; 48} 49.org-src-container \u0026gt; pre.src-R:before { 50content: \u0026#34;R\u0026#34;; 51} 52.org-src-container \u0026gt; pre.src-org:before { 53content: \u0026#34;Org\u0026#34;; 54} 55.org-src-container \u0026gt; pre.src-cpp:before { 56content: \u0026#34;C++\u0026#34;; 57} 58.org-src-container \u0026gt; pre.src-c:before { 59content: \u0026#34;C\u0026#34;; 60} 61.org-src-container \u0026gt; pre.src-html:before { 62content: \u0026#34;HTML\u0026#34;; 63} 64.org-src-container \u0026gt; pre.src-js:before { 65content: \u0026#34;Javascript\u0026#34;; 66} 67.org-src-container \u0026gt; pre.src-javascript:before { 68content: \u0026#34;Javascript\u0026#34;; 69} 7071// More languages from http://orgmode.org/worg/org-contrib/babel/languages.html 7273.org-src-container \u0026gt; pre.src-abc:before { 74content: \u0026#34;ABC\u0026#34;; 75} 76.org-src-container \u0026gt; pre.src-asymptote:before { 77content: \u0026#34;Asymptote\u0026#34;; 78} 79.org-src-container \u0026gt; pre.src-awk:before { 80content: \u0026#34;Awk\u0026#34;; 81} 82.org-src-container \u0026gt; pre.src-C:before { 83content: \u0026#34;C\u0026#34;; 84} 85.org-src-container \u0026gt; pre.src-calc:before { 86content: \u0026#34;Calc\u0026#34;; 87} 88.org-src-container \u0026gt; pre.src-clojure:before { 89content: \u0026#34;Clojure\u0026#34;; 90} 91.org-src-container \u0026gt; pre.src-comint:before { 92content: \u0026#34;comint\u0026#34;; 93} 94.org-src-container \u0026gt; pre.src-css:before { 95content: \u0026#34;CSS\u0026#34;; 96} 97.org-src-container \u0026gt; pre.src-D:before { 98content: \u0026#34;D\u0026#34;; 99} 100.org-src-container \u0026gt; pre.src-ditaa:before { 101content: \u0026#34;Ditaa\u0026#34;; 102} 103.org-src-container \u0026gt; pre.src-dot:before { 104content: \u0026#34;Dot\u0026#34;; 105} 106.org-src-container \u0026gt; pre.src-ebnf:before { 107content: \u0026#34;ebnf\u0026#34;; 108} 109.org-src-container \u0026gt; pre.src-forth:before { 110content: \u0026#34;Forth\u0026#34;; 111} 112.org-src-container \u0026gt; pre.src-F90:before { 113content: \u0026#34;Fortran\u0026#34;; 114} 115.org-src-container \u0026gt; pre.src-gnuplot:before { 116content: \u0026#34;Gnuplot\u0026#34;; 117} 118.org-src-container \u0026gt; pre.src-haskell:before { 119content: \u0026#34;Haskell\u0026#34;; 120} 121.org-src-container \u0026gt; pre.src-io:before { 122content: \u0026#34;Io\u0026#34;; 123} 124.org-src-container \u0026gt; pre.src-java:before { 125content: \u0026#34;Java\u0026#34;; 126} 127.org-src-container \u0026gt; pre.src-latex:before { 128content: \u0026#34;LaTeX\u0026#34;; 129} 130.org-src-container \u0026gt; pre.src-ledger:before { 131content: \u0026#34;Ledger\u0026#34;; 132} 133.org-src-container \u0026gt; pre.src-ly:before { 134content: \u0026#34;Lilypond\u0026#34;; 135} 136.org-src-container \u0026gt; pre.src-lisp:before { 137content: \u0026#34;Lisp\u0026#34;; 138} 139.org-src-container \u0026gt; pre.src-makefile:before { 140content: \u0026#34;Make\u0026#34;; 141} 142.org-src-container \u0026gt; pre.src-matlab:before { 143content: \u0026#34;Matlab\u0026#34;; 144} 145.org-src-container \u0026gt; pre.src-max:before { 146content: \u0026#34;Maxima\u0026#34;; 147} 148.org-src-container \u0026gt; pre.src-mscgen:before { 149content: \u0026#34;Mscgen\u0026#34;; 150} 151.org-src-container \u0026gt; pre.src-Caml:before { 152content: \u0026#34;Objective\u0026#34;; 153} 154.org-src-container \u0026gt; pre.src-octave:before { 155content: \u0026#34;Octave\u0026#34;; 156} 157.org-src-container \u0026gt; pre.src-org:before { 158content: \u0026#34;Org\u0026#34;; 159} 160.org-src-container \u0026gt; pre.src-perl:before { 161content: \u0026#34;Perl\u0026#34;; 162} 163.org-src-container \u0026gt; pre.src-picolisp:before { 164content: \u0026#34;Picolisp\u0026#34;; 165} 166.org-src-container \u0026gt; pre.src-plantuml:before { 167content: \u0026#34;PlantUML\u0026#34;; 168} 169.org-src-container \u0026gt; pre.src-python:before { 170content: \u0026#34;Python\u0026#34;; 171} 172.org-src-container \u0026gt; pre.src-ruby:before { 173content: \u0026#34;Ruby\u0026#34;; 174} 175.org-src-container \u0026gt; pre.src-sass:before { 176content: \u0026#34;Sass\u0026#34;; 177} 178.org-src-container \u0026gt; pre.src-scala:before { 179content: \u0026#34;Scala\u0026#34;; 180} 181.org-src-container \u0026gt; pre.src-scheme:before { 182content: \u0026#34;Scheme\u0026#34;; 183} 184.org-src-container \u0026gt; pre.src-screen:before { 185content: \u0026#34;Screen\u0026#34;; 186} 187.org-src-container \u0026gt; pre.src-sed:before { 188content: \u0026#34;Sed\u0026#34;; 189} 190.org-src-container \u0026gt; pre.src-shell:before { 191content: \u0026#34;shell\u0026#34;; 192} 193.org-src-container \u0026gt; pre.src-shen:before { 194content: \u0026#34;Shen\u0026#34;; 195} 196.org-src-container \u0026gt; pre.src-sql:before { 197content: \u0026#34;SQL\u0026#34;; 198} 199.org-src-container \u0026gt; pre.src-sqlite:before { 200content: \u0026#34;SQLite\u0026#34;; 201} 202.org-src-container \u0026gt; pre.src-stan:before { 203content: \u0026#34;Stan\u0026#34;; 204} 205.org-src-container \u0026gt; pre.src-vala:before { 206content: \u0026#34;Vala\u0026#34;; 207} 208.org-src-container \u0026gt; pre.src-axiom:before { 209content: \u0026#34;Axiom\u0026#34;; 210} 211.org-src-container \u0026gt; pre.src-browser:before { 212content: \u0026#34;HTML\u0026#34;; 213} 214.org-src-container \u0026gt; pre.src-cypher:before { 215content: \u0026#34;Neo4j\u0026#34;; 216} 217.org-src-container \u0026gt; pre.src-elixir:before { 218content: \u0026#34;Elixir\u0026#34;; 219} 220.org-src-container \u0026gt; pre.src-request:before { 221content: \u0026#34;http\u0026#34;; 222} 223.org-src-container \u0026gt; pre.src-ipython:before { 224content: \u0026#34;iPython\u0026#34;; 225} 226.org-src-container \u0026gt; pre.src-kotlin:before { 227content: \u0026#34;Kotlin\u0026#34;; 228} 229.org-src-container \u0026gt; pre.src-Flavored Erlang lfe:before { 230content: \u0026#34;Lisp\u0026#34;; 231} 232.org-src-container \u0026gt; pre.src-mongo:before { 233content: \u0026#34;MongoDB\u0026#34;; 234} 235.org-src-container \u0026gt; pre.src-prolog:before { 236content: \u0026#34;Prolog\u0026#34;; 237} 238.org-src-container \u0026gt; pre.src-rec:before { 239content: \u0026#34;rec\u0026#34;; 240} 241.org-src-container \u0026gt; pre.src-ML sml:before { 242content: \u0026#34;Standard\u0026#34;; 243} 244.org-src-container \u0026gt; pre.src-Translate translate:before { 245content: \u0026#34;Google\u0026#34;; 246} 247.org-src-container \u0026gt; pre.src-typescript:before { 248content: \u0026#34;Typescript\u0026#34;; 249} 250.org-src-container \u0026gt; pre.src-rust:before { 251content: \u0026#34;Rust\u0026#34;; 252} However, in the interests of sanity, we will leverage the Syntax Highlighting Anki plugin for managing the actual style-sheets instead of manual edits to each card type.\n Figure 4: A screencast from the plugin readme\n  At this stage, we have a card which can gracefully handle both XeLaTeX and code in an elegant manner. An example is presented in the next section.\nUsage For the sample card2 shown, the markup is dead simple.\n1*Basis Vectors:math:quantum:linear: 2:properties: 3:anki_deck: CompChem 4:anki_note_type: LaTeX 5:ANKI_NOTE_ID: 1603755931922 6🔚 7** Front 8For a three dimensional vector with components $aᵢ,i=1,2,3$ what are the basis vectors? 9** Back 10This is defined as follows: 11$$ 12\\mathbf{a}=\\mathbf{e}₁a₁+\\mathbf{e}₂a₂+\\mathbf{e}₃a₃=∑ᵢ\\mathbf{e}ᵢaᵢ 13$$ Essentially:\n Enable and load anki-editor  Add local variable section to ensure we load anki-editor. This is essentially via eval: (anki-editor-mode) in the Local variables block   Fire up Anki Export at will, and continue adding more cards or non-card details to the orgmode file  The Anki editor examples file is excellent and the issue tracker also has a ton of information.\nCode 1*Test Code:code:python: 2:properties: 3:anki_deck: CodeWiki 4:anki_note_type: myTex 5:ANKI_NOTE_ID: 1603891864091 6🔚 7** Front 8What is the definition of an inner product? What are some examples of a code block in Python and R? 9** Back 10This is essentially a *norm* with more structure. The first two properties, positive definiteness and symmetry (conjugate) defines a *norm*. 11$$ 12\\mathbf{a}=\\mathbf{e}₁a₁+\\mathbf{e}₂a₂+\\mathbf{e}₃a₃=∑ᵢ\\mathbf{e}ᵢaᵢ 13$$ 1415#+begin_src python16def test(): 17x = [1,2,3] 18for i in x: 19print(i) 20#+end_src 2122#+begin_src R23library(\u0026#34;dplyr\u0026#34;) 24x = 1 25#+end_src  Figure 5: Code card with TeX\n  More Content  Fundamental Haskell An excellent example of how a multiple frontend learning repository can be, written with org-drill3 Anki powerups with orgmode A post brought to my attention after I had published this, an excellent introduction with videos  Conclusions Some final comments:\n Screenshots and other images linked are automatically synced The TeX is best rendered on the PC first, so run through these at-least once  A missing link in this setup is the ability to use a touch screen and stylus to write proofs or skip the TeX setup altogether, but that would require another post altogether. Additionally, all the standard bells and whistles of having an orgmode document can be applied, including, crucially, the ability to have long-form notes as well, a coherent approach to this can also be covered later.\n  The alignment trick is from this post\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n It is a gag card, no judgement here\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n org-drill doesn\u0026rsquo;t support any kind of mobile synchronization\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/anki-decks-orgmode/","tags":["workflow","projects","tools","emacs","orgmode"],"title":"Anki Decks with Orgmode"},{"categories":null,"contents":" Dual screen workflows without screens across operating systems\n Background My X380 sadly has been having port issues. This meant that my M14 was no longer a viable option for my second screen needs.\nOutline The general form of the solution works in one of two ways:\n VNC Viewer Where the (second-screen) laptop connects to a VNC server on the primary laptop Peripheral Shares Where the secondary laptop runs a server to enable proxying mouse and keyboard access from the primary laptop  VNC and Windows For laptops running Windows, I personally just set up TightVNC. The standard settings work well enough for the peripheral share described below.\nComments This is best used for working with Windows only stuff like Office.\nVNC and Linux Peripheral Share For the secondary laptop we need to run a server (tigervnc) without setting an external screen.\n1x0vncserver -rfbauth ~/.vnc/passwd Now on the main laptop, we will simply leverage x2vnc to extend into the secondary laptop.\n1x2vnc $ip -west Where we can get the IP (local) by checking with ifconfig on the secondary laptop.\nMeta This works best when combined with a networked file-system, since then you can interact with files in tandem. Otherwise, there is quite a bit of git based back and forth.\nVNC and Android There are two parts to this solution. Note that, as Android devices don\u0026rsquo;t run X11 systems in a meaningful way, the direct access method is through a paid application, a2vnc server lite, which also did not work well in my tests. We will therefore focus on setting a VNC viewer up to connect to the primary laptop.\nPrimary Settings XRandR Setup For the primary laptop, we will start by obtaining our present screens configuration.\n1xrandr | grep \u0026#34; connected\u0026#34; 1eDP1 connected primary 1920x1080+1920+0 (normal left inverted right x axis y axis) 290mm x 170mm Naturally your output will differ. We also need the resolution of the Android device. In my case, they are the same. At this point we are ready to figure out the mode-line.\n1gtf 1920 1080 60 1# 1920x1080 @ 60.00 Hz (GTF) hsync: 67.08 kHz; pclk: 172.80 MHz 2Modeline \u0026#34;1920x1080_60.00\u0026#34; 172.80 1920 2040 2248 2576 1080 1081 1084 1118 -HSync +Vsync Let us now use this information to create a bunch of modelines.\n1xrandr --newmode \u0026#34;1920x1080_60.00\u0026#34; 172.80 1920 2040 2248 2576 1080 1081 1084 1118 -HSync +Vsync Note that we can create more of these in the same manner. We can now move forward with making a virtual screen.\n1xrandr --addmode VIRTUAL1 1920x1080_60.00 We can now finally set up the output.\n1xrandr --output VIRTUAL1 --mode 1920x1080_60.00 --left-of eDP1 Note that it is better to use mons to work with our newly created virtual screen.\n1mons -e left This is still a bit ugly, since the process needs to be repeated with each reboot.\nVNC Setup Now we need prepare our VNC. x11vnc is recommended at the moment.\n1x11vnc -vencrypt nodh:only-ssl -ssl SAVE -clip 1920x1080+0+0 Android Settings For this section, I personally use bVNC Pro. The setup is pretty dead simple. A basic VNC connection is all that is required.\nComments In practice, I use the x86 setup, with the secondary laptop acting as a viewer for a virtual screen, mostly because that way I can tune into multiple Zoom meetings (a bonus).\nConclusion The final setup is quite robust to changes. Future posts might go into setting up the kind of local networking tools to help move files, code and more between both machines, to improve on the peripheral share approach. Additionally, there are still some manual steps which can and should be automated. I\u0026rsquo;m not super pleased with the setup, it takes longer than a wireless screen. This post is complimented by the work and setup with touchscreens here.\n","permalink":"https://rgoswami.me/posts/laptop-as-second-screens/","tags":null,"title":"Old Laptops as Secondary Monitors"},{"categories":["conferences"],"contents":" A meta-post on my lightning talk at NixCon 2020\n Background Much the same as the rationale behind my meta-post on my talk at PyCon India 2020, that is:\n I would like to preserve questions I would like to collect the video, slides and other miscellaneous stuff in one location 1 It would be nice to have my own thoughts here afterwards  Details  Title Nix from the dark ages (without Root) Proposal See the cfp response here  Abstract  Short comments from the trenches of High Performance Clusters on working with Nix on kernel locked-in systems without proot support.\n Linked Posts  Local Nix without Root Motivation and installation  Provisioning Dotfiles on an HPC: Looks into how standard approaches fail (proot) HPC Dotfiles and LMod: All the ugly manual install steps    Other Content Anything on this site tagged with Nix. Also an introduction to nix given by me (and Amrita Goswami) at CarpentryCon2020 is here:\n CarpentryCon2020 Materials A tutorial introduction to Nix and Python  Slides  Best viewed here using a browser (in a new tab) A pdf copy of the slides are embedded below The orgmode source is here on the site\u0026rsquo;s GH repo     One location I am going to be able to keep track of\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/nixcon-in-2020-meta/","tags":["presentations","ramblings","nix","hpc"],"title":"Talk Supplements for NixCon 2020"},{"categories":["notes"],"contents":" A post on local recordings\n Background Since the advent of the COVID-19 situation, there has been an increase in the demand for recorded materials. Standard approaches involve Zoom, which is not only proprietary, but also quite a bit of a privacy nightmare. The last straw was the random placement of my speaker bauble head.\n Figure 1: Zoom webcam placement\n  At this point, given that I was to set up a pre-recorded video for PyCon India 2020, I decided to look into alternatives.\nAlternatives The search for alternative screen recording systems isn\u0026rsquo;t really a very new one. For group work (like W3cm 1), I tend to prefer Skype, since it handles speaker galleries very well. Unfortunately, Skype has no capacity for recording single person calls, at least as yet. This is not the place for an extended debate on the pros and cons of Skype, or Google Meet (only records corporate accounts), or the rest. Instead, lets sum up all these issue with the simple understanding that, if one person wants to record a webcam connected to their local computer, along with the screen, it is insane to imagine that the only way to get this is by:\n Making an account somewhere (Zoom, Meet, Skype, anything) Giving a cloud service permission to record our screens  At the same time, a lot of standard tools for screen recording do not play nice with webcam recorders (like Simple Screen Recorder and Cheese).\nOpen Broadcaster Software The OBS studio project is a godsend. It allows for simultaneously managing multiple streams, of both audio and video. Furthermore, since these are implemented as overlays, it is possible to fine-tune the positioning of each of these, which is something Zoom and friends lack.\n Figure 2: Into the matrix\n  The ability to resize the webcam is best shown in the figure below.\n Figure 3: Almost Zoom, only better\n  OBS also generates beautifully small videos and supports live-streaming.\nCommon Caveats  The standard setting is set to work with hardware acceleration, which may not be present for many users  Use the settings to change this back to the software setting    Conclusions I cannot imagine going back to Zoom to record anything local. It is an added bonus that OBS is both cross-platform and FOSS. It is only incredible more people do not use it.\n  Water, Chemicals and more with Computers for Chemistry, a computational chemistry course aimed at middle school students taught with my sister Amrita Goswami\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/rep-zoom-obs/","tags":["workflow","tools"],"title":"Replacing Zoom with Open Broadcaster Software"},{"categories":["conferences"],"contents":" A meta-post on my talk at PyCon India 2020\n Background I am to present at PyCon IN 2020. Some of the motivating reasons for having a post are:\n I would like to preserve questions I would like to collect the video, slides and other miscellaneous stuff in one location 1 It would be nice to have my own thoughts here afterwards  Details of this happy circumstance are reproduced below from the CFP here.\nDetails  Title Reproducible Scalable Workflows with Nix, Papermill and Renku  Abstract  The provenance of Jupyter notebook interfaces can no longer be denied in the data-science and analysis community. In particular, fledgling and \u0026ldquo;fresh out of school\u0026rdquo; researchers and practitioners are used to using Jupyter notebooks for their initial analysis. As might be expected, these workflows are difficult to reproduce and also store. Caching efficiency and dependency re-use are almost always sub-optimal with virtual environments, compared to native installations, and the same issues (along with additional security concerns) plague docker setups as well. There are a set of Jupyter tools which have evolved to close this gap, like JupyText. However, the fundamental aspect of reproducing workflows on high performance computing clusters, of being able to compose programmatically, compilation rules which efficiently use underlying hardware with minimal user intervention is still not a solved problem. In this talk, I will discuss packaging Python applications and workflows in an end-to-end composable manner using the Nix ecosystem, which leverages a functional programming paradigm and then show how this allows for both user-friendly low-compute analysis, while being scalable on large clusters. To that end, the tools introduced will be:\nThe Nix programming language (emphasis on developer environments for python with mkShell) Jupyter Python kernels (the Xeus kernel for Python debugging) and Jupytext Papermill for parameterizing notebooks Renku for tracing provenance The goal is to have the audience familiarized with the best practices for reproducibility and analysis. The focus will be on scientific HPC applications, though any managed cluster can and will benefit from the practices described.\n Other Content A more in-depth introductory workshop on Nix itself given by me (and Amrita Goswami) at CarpentryCon2020 is here:\n CarpentryCon2020 Materials A tutorial introduction to Nix and Python  Slides The slides are embedded below. The orgmode source is here on the site\u0026rsquo;s GH repo.\n Video {{\u0026lt; youtube 2GX9TK4uNfU \u0026gt;}}\n  One location I am going to be able to keep track of\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/pycon-in-2020-meta/","tags":["presentations","ramblings","nix","python"],"title":"Talk Supplements for PyCon India 2020"},{"categories":["programming"],"contents":" Automating documenation deployment with Travis, rake and nix\n Background In the previous post we generated documentation using Doxygen with Exhale to handle Sphinx. Now we will clean up the earlier workflow with rake and ensure the environment is reproducible with nix while deploying to Travis CI.\nSeries  Documenting C++ with Doxygen and Sphinx - Exhale Publishing Doxygen and Sphinx with Nix and Rake \u0026lt;\u0026ndash; You are here Documenting C++ with Doxygen and Sphinx - doxyrest Adding Tutorials to Sphinx Projects  Setup A quick reminder of the setup we generated in the last post:\n1tree -d $prj/ -L 2    .       ├── docs    │  ├── Doxygen   │  └── Sphinx   ├── nix    │  └── pkgs   ├── projects    │  └── symengine   └── scripts         8 directories     We had further setup files to enable documentation generation with a manual two stage process (handling doxygen and sphinx separately).\n1cd docs/Doxygen 2doxygen Doxyfile-prj.cfg 3cd ../Sphinx 4make html 5mv build/html ../../public This might be extracted into a simple build.sh script, and then we might decide to have a clean.sh script and then we might try to replicate all the functionality of a good build system with scripts.\nThankfully, we will instead start with a build script defined as above to transition to nix, before using an actual build tool for our dirty work.\nAdding Nix It wouldn\u0026rsquo;t make sense for me to not stick nix into this. I recall the dark days of setting up Dockerfiles to ensure reproducible environments on Travis.\nAt this point one might assume we will leverage the requirements.txt based workflow described earlier in Niv and Mach-Nix for Nix Python. While this would make sense, there are two barriers to its usage:\n It is slower than a poetry build, as dependency resolution is performed It does not play well with existing projects  Most python projects do not rely solely on requirements.txt 1    Poetry2Nix Recall that as sphinx is originally meant for and most often used for Python projects, we will need to consider the possibility (remote though it is) that there might be users who would like to test the documentation without setting up nix.\nThus we will look to the poetry2nix project instead. We note the following:\n The poetry2nix setup is faster (as it consumes a lockfile instead of solving dependencies from requirements.txt)  mach-nix however, is more flexible and can make use of the poetry2nix overrides   In a strange chicken and egg problem, we will have to manually generate the lockfile, thereby creating an impure poetry project for every update, though the nix setup will not need it later  This is one of the major reasons to prefer mach-nix for newer projects    Shell Environment We prep our sources in the usual way, by running niv init in the project root to generate the nix/ folder and the sources therein. With all that in mind, the shell.nix file at this point is fairly standard, keeping the general niv setup in mind (described in a previous Nix tutorial):\n1# -*- mode: nix-mode -*- 2let 3sources = import ./nix/sources.nix; 4pkgs = import sources.nixpkgs { }; 5customPython = pkgs.poetry2nix.mkPoetryEnv { projectDir = ./.; }; 6in pkgs.mkShell { 7buildInputs = with pkgs; [ doxygen customPython rake darkhttpd ]; 8} Where the most interesting aspect is that the projectDir is to be the location of the project root, though both poetrylock and pyproject variables are supported.\nRefactoring We consider the problem of refactoring the build.sh script:\n1#!/usr/bin/env bash 2cd docs/Doxygen 3doxygen Doxyfile-prj.cfg 4cd ../Sphinx 5make html 6mv build/html ../../public Without resorting to methods such as nix-shell --run build.sh --pure.\nNix Bash Script in hand, we would like to be able to run it directly in the nix environment. We modify the script as follows:\n1#! /usr/bin/env nix-shell 2#! nix-shell deps.nix -i bash 34# Build Doxygen 5cd docs/Doxygen 6doxygen Doxyfile-syme.cfg 78# Build Sphinx 9cd ../Sphinx 10make html 11mv build/html ../../public 1213# Local Variables: 14# mode: shell-script 15# End: This calls on a deps.nix2 which we shall generate in a manner very reminiscent of the shell.nix 3 as follows:\n1let 2sources = import ./../nix/sources.nix; 3pkgs = import sources.nixpkgs { }; 4customPython = pkgs.poetry2nix.mkPoetryEnv { projectDir = ./../.; }; 5in pkgs.runCommand \u0026#34;dummy\u0026#34; { 6buildInputs = with pkgs; [ doxygen customPython ]; 7} \u0026#34;\u0026#34; Only the paths have changed, and instead of creating and returning a shell environment with mkShell we instead \u0026ldquo;run\u0026rdquo; a derivation instead. At this point we can run this simply as:\n1./scripts/build.sh This is reasonably ready (as a first draft) for being incorporated into a continuous integration workflow.\nTravis CI Seeing as Travis provides first class nix support, as well as excellent integration with GitHub, we will prefer it.\nSettings A minor but necessary evil is setting up a PAP (personal access token) from here. Depending on what repositories are being used, the scope should encompass repo permissions (minimally public_repo), and admin:org permissions might be required.\nHaving obtained the token, we will need to navigate to the Settings section on the Travis web-UI and add the token as an environment variable, we might be partial to a name like GH_TOKEN.\n Figure 1: Settings at travis-ci.com/host/proj/settings\n  Build Configuration We will leverage the following configuration:\n1language:nix23before_install:4- sudo mkdir -p /etc/nix5- echo \u0026#34;substituters = https://cache.nixos.org/ file://$HOME/nix.store\u0026#34; | sudo tee -a /etc/nix/nix.conf \u0026gt; /dev/null6- echo \u0026#39;require-sigs = false\u0026#39; | sudo tee -a /etc/nix/nix.conf \u0026gt; /dev/null78before_script:9- sudo mkdir -p /etc/nix \u0026amp;\u0026amp; echo \u0026#39;sandbox = true\u0026#39; | sudo tee /etc/nix/nix.conf1011script:12- scripts/build.sh1314before_cache:15- mkdir -p $HOME/nix.store16- nix copy --to file://$HOME/nix.store -f shell.nix buildInputs1718cache:19nix:true20directories:21- $HOME/nix.store2223deploy:24provider:pages25local_dir:./public/26skip_cleanup:true27github_token:$GH_TOKEN# Set in the settings page of your repository, as a secure variable28keep_history:true29target_branch:master# Required for user pages30on:31branch:srcWhere all the action is essentially in script and deploy. Note however, that the before_cache step should change if there is a default.nix instead. We will in this case, consider the situation of having an organization or user page being the deploy target.\nRake Usable though the preceding setting is, it is still rather unwieldy in that:\n there are a bunch of artifacts which need to be cleaned manually it is fragile and tied to the folder names  We can fix this with any of the popular build systems, however here we will focus on the excellent rake 4. We shall commit to our course of action by removing make.\n1cd docs/Sphinx 2rm Makefile make.bat # other make cruft Components Variables We will begin by requiring rake and setting basic variables.\n1require \u0026#39;rake\u0026#39; 23CWD = File.expand_path(__dir__) 4DOXYFILE = \u0026#34;Doxyfile-prj.cfg\u0026#34; 5OUTDIR = File.join(CWD,\u0026#34;public\u0026#34;) 6SPHINXDIR = File.join(CWD,\u0026#34;docs/Sphinx\u0026#34;) This section should give a fairly clear idea of how the Rakefile itself is essentially pure ruby code. We are now beginning to have more holistic control of how our project is structured.\nTasks The general form of a task is simply:\n1desc \u0026#34;Blah blah\u0026#34; 2task :name do 3# Something 4end Some variations of this will be considered when appropriate.\nClean A clean task is a good first task, being as it is almost trivial in all build systems.\n1desc \u0026#34;Clean the generated content\u0026#34; 2task :clean do 3rm_rf \u0026#34;public\u0026#34; 4rm_rf \u0026#34;docs/Doxygen/gen_docs\u0026#34; 5rm_rf \u0026#34;docs/Sphinx/build\u0026#34; 6end Serve We will use the lightweight darkhttpd server for our generated documentation.\n1desc \u0026#34;Serve site with darkhttpd\u0026#34; 2task :darkServe, [:port] do |task, args| 3args.with_defaults(:port =\u0026gt; \u0026#34;1337\u0026#34;) 4sh \u0026#34;darkhttpd #{OUTDIR}--port #{args.port}\u0026#34; 5end Note that we have leveraged the args system in this case, and also used the top-level OUTDIR variable.\nDoxygen Since the doxygen output is a pre-requisite, it makes sense to set it up early on.\n1desc \u0026#34;Build doxygen\u0026#34; 2task :mkDoxy do 3Dir.chdir(to = File.join(CWD,\u0026#34;docs/Doxygen\u0026#34;)) 4system(\u0026#39;doxygen\u0026#39;, DOXYFILE) 5end Sphinx This task will depend on having the doxygen output, so we will express this idiomatically by making the doxygen task run early on.\n1desc \u0026#34;Build Sphinx\u0026#34; 2task :mkSphinx, [:builder] =\u0026gt; [\u0026#34;mkDoxy\u0026#34;] do |task, args| 3args.with_defaults(:builder =\u0026gt; \u0026#34;html\u0026#34;) 4Dir.chdir(to = File.join(CWD,\u0026#34;docs/Sphinx\u0026#34;)) 5sh \u0026#34;poetry install\u0026#34; 6sh \u0026#34;poetry run sphinx-build source #{OUTDIR}-b #{args.builder}\u0026#34; 7end There are some subtleties here, notably:\n The task is meant to run without nix We use the args setup as before  No Nix Meta With this we can now set up a task to build the documentation without having nix.\n1desc \u0026#34;Build site without Nix\u0026#34; 2task :noNixBuild =\u0026gt; \u0026#34;mkSphinx\u0026#34; do 3Rake::Task[\u0026#34;darkServe\u0026#34;].execute 4end The main take-away here is that we finally call the Rake library itself, but within the task, which means the dependency tree is respected and we get doxygen-\u0026gt;sphinx-\u0026gt;darkhttpd as required.\nNix Builder For nix use we note that we are unable to enter the nix environment from within the Rakefile itself. We work around this by being more descriptive.\n1desc \u0026#34;Build Nix Sphinx, use as nix-shell --run \u0026#39;rake mkNixDoc\u0026#39; --pure\u0026#34; 2task :mkNixDoc, [:builder] =\u0026gt; \u0026#34;mkDoxy\u0026#34; do |task, args| 3args.with_defaults(:builder =\u0026gt; \u0026#34;html\u0026#34;) 4Dir.chdir(to = SPHINXDIR) 5sh \u0026#34;sphinx-build source #{OUTDIR}-b #{args.builder}\u0026#34; 6end Final Form The final Rakefile shall be (with a default task defined):\n1require \u0026#39;rake\u0026#39; 23# Variables 4CWD = File.expand_path(__dir__) 5DOXYFILE = \u0026#34;Doxyfile-prj.cfg\u0026#34; 6OUTDIR = File.join(CWD,\u0026#34;public\u0026#34;) 7SPHINXDIR = File.join(CWD,\u0026#34;docs/Sphinx\u0026#34;) 89# Tasks 10task :default =\u0026gt; :darkServe 1112desc \u0026#34;Clean the generated content\u0026#34; 13task :clean do 14rm_rf \u0026#34;public\u0026#34; 15rm_rf \u0026#34;docs/Doxygen/gen_docs\u0026#34; 16rm_rf \u0026#34;docs/Sphinx/build\u0026#34; 17end 1819desc \u0026#34;Serve site with darkhttpd\u0026#34; 20task :darkServe, [:port] do |task, args| 21args.with_defaults(:port =\u0026gt; \u0026#34;1337\u0026#34;) 22sh \u0026#34;darkhttpd #{OUTDIR}--port #{args.port}\u0026#34; 23end 2425desc \u0026#34;Build Nix Sphinx, use as nix-shell --run \u0026#39;rake mkNixDoc\u0026#39; --pure\u0026#34; 26task :mkNixDoc, [:builder] =\u0026gt; \u0026#34;mkDoxy\u0026#34; do |task, args| 27args.with_defaults(:builder =\u0026gt; \u0026#34;html\u0026#34;) 28Dir.chdir(to = SPHINXDIR) 29sh \u0026#34;sphinx-build source #{OUTDIR}-b #{args.builder}\u0026#34; 30end 3132desc \u0026#34;Build site without Nix\u0026#34; 33task :noNixBuild =\u0026gt; \u0026#34;mkSphinx\u0026#34; do 34Rake::Task[\u0026#34;darkServe\u0026#34;].execute 35end 3637desc \u0026#34;Build doxygen\u0026#34; 38task :mkDoxy do 39Dir.chdir(to = File.join(CWD,\u0026#34;docs/Doxygen\u0026#34;)) 40system(\u0026#39;doxygen\u0026#39;, DOXYFILE) 41end 4243desc \u0026#34;Build Sphinx\u0026#34; 44task :mkSphinx, [:builder] =\u0026gt; [\u0026#34;mkDoxyRest\u0026#34;] do |task, args| 45args.with_defaults(:builder =\u0026gt; \u0026#34;html\u0026#34;) 46Dir.chdir(to = File.join(CWD,\u0026#34;docs/Sphinx\u0026#34;)) 47sh \u0026#34;poetry install\u0026#34; 48sh \u0026#34;poetry run sphinx-build source #{OUTDIR}-b #{args.builder}\u0026#34; 49end Travis We are now in a position to fix our travis build configuration. Simply replace the old and fragile build.sh script section with the following:\n1script:2- nix-shell --run \u0026#34;rake mkNixDoc\u0026#34; --show-trace --verbose --pureDirenv As a bonus section, consider the addition of the following .envrc for those who keep multiple ruby versions:\n1eval \u0026#34;$(rbenv init -)\u0026#34; 2rbenv shell 2.6.2 3rake -T Activate this with the usual direnv allow. This has the added benefit of listing the defined tasks when cd\u0026lsquo;ing into the project directory.\nConclusions A lot has happened on the tooling end, even though the documentation itself has not been updated further. We have managed to setup a robust environment which is both reproducible and also amenable to users who do not have nix. We have also setup a build system, which can help us in many more ways as well (asset optimization through the rails pipeline). In the next post, we will return to the documentation itself for further tinkering.\n  Poetry and Pipenv come to mind\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Chris Warbo has a good introduction to the nix shebang\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n In this instance, we could have simply called on shell.nix instead, but it illustrates a more general concept\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Avdi\u0026rsquo;s blog has a fantastic introduction to rake and Rakefiles\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/pub-doc-cpp-dox-sph-nix/","tags":["documentation","workflow","nix","cpp"],"title":"Publishing Doxygen and Sphinx with Nix and Rake"},{"categories":["programming"],"contents":" This post outlines a basic workflow for C++ projects using Doxygen, Sphinx, and Exhale.\n Background My project proposal for documenting Symengine was recently accepted for the Google Summer of Docs initiative. In the past I have been more than happy to document C++ code using only Doxygen (with pretty fantastic results), while keeping example usage separate (d-SEAMS wiki). Though this is still a feasible method, a monolithic multi-project setup might benefit from Sphinx, which is what will be covered.\nSeries This post is the first in a series based on best C++ documentation practices for Sphinx+Doxygen workflows.\n Documenting C++ with Doxygen and Sphinx - Exhale \u0026lt;\u0026ndash; You are here! Publishing Doxygen and Sphinx with Nix and Rake Documenting C++ with Doxygen and Sphinx - doxyrest (TBD) Adding Tutorials to Sphinx Projects (TBD)  Goals A couple of goals informed this approach:\n We expect our documentation to link to the source files We expect a lot of Python developers to contribute  Hence Sphinx   We would like to write .ipynb files into the docs  Another reason to use Sphinx (via MyST{NB})    Folder Structure 1tree -d $prj/ -L 2    .       ├── docs    │  ├── Doxygen   │  └── Sphinx   ├── nix    │  └── pkgs   ├── projects    │  └── symengine   └── scripts         8 directories     Essentially we have a scripts directory to store basic build scripts, and two kinds of documentation folders.\nBasic Doxygen The doxygen setup is beautifully simple:\n1cd docs/Doxygen 2doxygen -g 3# Easier to edit 4mv Doxyfile Doxyfile.cfg Since Doxyfile should be updated by subsequent versions of doxygen, it is best to separate the project settings. We will therefore modify some basic settings in a separate file\n1touch Doxyfile-prj.cfg 2vim Doxyfile-prj.cfg # or whatever Edit the file to be (minimally):\n1@INCLUDE = \u0026#34;./Doxyfile.cfg\u0026#34; 2GENERATE_HTML = NO 3GENERATE_XML = YES 4XML_PROGRAMLISTING = NO 56# Project Stuff 7PROJECT_NAME = \u0026#34;myProject\u0026#34; 8PROJECT_BRIEF = \u0026#34;Dev docs\u0026#34; 9OUTPUT_DIRECTORY = \u0026#34;./gen_docs\u0026#34; 1011# Inputs 12INPUT = \u0026#34;./../../projects/symengine/symengine\u0026#34; 13RECURSIVE = NO With this we will now be able to obtain the xml files for the rest of this setup.\nExhale For our first attempt, we will focus on the automation of Sphinx using the exhale tool.\n1# Basic setup 2poetry init 3poetry add exhale breathe Now we can generate the basic Sphinx structure.\n1# Separate source and build 2sphinx-quickstart --sep --makefile docs/Sphinx \\ 3 --project \u0026#34;My Proj\u0026#34; \\ 4 --author \u0026#34;Juurj\u0026#34; \\ 5 --release \u0026#34;latest\u0026#34; \\ 6 --language \u0026#34;en\u0026#34; This allows us to generate the Sphinx documentation we require, with some changes to the docs/Sphinx/source/config.py file (lifted from the exhale documentation):\n1extensions = [ 2\u0026#39;breathe\u0026#39;, 3\u0026#39;exhale\u0026#39;, 4] 56# -- Exhale configuration --------------------------------------------------- 7# Setup the breathe extension 8breathe_projects = { 9\u0026#34;My Proj\u0026#34;: \u0026#34;./../../Doxygen/gen_docs/xml\u0026#34; 10} 11breathe_default_project = \u0026#34;My Proj\u0026#34; 1213# Setup the exhale extension 14exhale_args = { 15# These arguments are required 16\u0026#34;containmentFolder\u0026#34;: \u0026#34;./api\u0026#34;, 17\u0026#34;rootFileName\u0026#34;: \u0026#34;library_root.rst\u0026#34;, 18\u0026#34;rootFileTitle\u0026#34;: \u0026#34;Library API\u0026#34;, 19\u0026#34;doxygenStripFromPath\u0026#34;: \u0026#34;..\u0026#34;, 20# Suggested optional arguments 21\u0026#34;createTreeView\u0026#34;: True, 22# TIP: if using the sphinx-bootstrap-theme, you need 23# \u0026#34;treeViewIsBootstrap\u0026#34;: True, 24} 2526# Tell sphinx what the primary language being documented is. 27primary_domain = \u0026#39;cpp\u0026#39; 2829# Tell sphinx what the pygments highlight language should be. 30highlight_language = \u0026#39;cpp\u0026#39; We also need to add the output to the index.rst use the following:\n1.. toctree::2 :maxdepth: 23 :caption: Contents:45 api/library_rootAt this point we are ready to manually build our documentation.\n1cd docs/Doxygen 2doxygen Doxyfile-prj.cfg 3cd ../Sphinx 4make html This is still pretty cumbersome though. We can view our documentation in a more pleasant manner with darkhttpd.\n1darkhttpd docs/Sphinx/build/html With this we now beautify the documentation (with the sphinx_book_theme):\n1poetry add sphinx-book-theme We need to set the theme as well (in the Sphinx config.py file):\n1html_theme = \u0026#39;sphinx_book_theme\u0026#39; This leads to some pretty documentation.\n Figure 1: Generated documentation (Exhale)\n   Figure 2: Exhale does a great job with file-based hierarchy.\n  Conclusions At this point, we have a basic setup, which we can tweak with a bunch of themes, and/or different parsers, but this is still pretty rough around the edges. However, a caveat of this setup is that the actual contents of the source are not visible in the generated documentation. In the next post, we will look at automating this setup for deploying with Travis.\n","permalink":"https://rgoswami.me/posts/doc-cpp-dox-sph-exhale/","tags":["documentation","workflow","cpp"],"title":"Documenting C++ with Doxygen and Sphinx - Exhale"},{"categories":["programming"],"contents":" Monkeying around with nix for HPC systems which have no root access and NFS filesystems.\n Background Nix is not well known for being friendly to users without root access. This is typically made worse by the \u0026ldquo;exotic\u0026rdquo; filesystem attributes common to HPC networks (this also plagues hermes). An earlier post details how and why proot failed. The short pitch is simply:\n Figure 1: Does your HPC look like this?\n   Figure 2: It really is an HPC\n  If your HPC doesn\u0026rsquo;t look that swanky and you\u0026rsquo;d like it to, then read on. Note that there are all the obvious benefits of nix as well, but this is a more eye-catchy pitch.\nSetup  The basic concept is to install nix from source, with appropriate patches, and then mess around with paths until it is ready and willing to work with stores which are not /nix 1\n This concept is strongly influenced by the work described in this repo. The premise is similar to my earlier post on HPC Dotfiles. For the purposes of this post, we will assume that all the packages in the previous post exist. lmod is not required, feel free to use an alternative path management system, or even just $HOME/.local but if lmod is present, it is highly recommended 2. We will need the following:\n Pinned set of nixpkgs We would like to be able to modify a lot of paths, which is normally a bad practice, but then we don\u0026rsquo;t normally rebuild all packages either. Grab a copy of the nixpkgs by following the instructions below. Now is also the time to fork the repo if you\u0026rsquo;d like to keep track of your changes.   1mkdir -p $HOME/Git/Github 2cd $HOME/Git/Github 3git clone https://github.com/NixOS/nixpkgs  dotgit We use the older, bash version of the excellent dotgit since python is not always present in HPC environments.   1git clone https://github.com/kobus-v-schoor/dotgit/ 2mkdir -p $HOME/.local/bin 3cp dotgit/old/bin/bash_completion dotgit/old/bin/dotgit dotgit/old/bin/dotgit_headers dotgit/old/bin/fish_completion.fish $HOME/.local/bin/ -r  lmod packages If you do not or cannot use modulefiles as described in the earlier post, inspect the module-files being loaded and set paths accordingly.   1cd $HOME/Git/Github 2git clone https://github.com/HaoZeke/hzHPC_lmod 3cd hzHPC_lmod 4$HOME/.local/bin/dotgit restore hzhpc Now we can start by obtaining the nix sources.\n1myprefix=$HOME/.hpc/nix/nix-boot 2nixdir=$HOME/.nix 3nix_version=2.3.7 4ml load gcc/9.2.0 flex bison 5ml load boost 6ml load editline 7ml load brotli/1.0.1 8ml load libseccomp/2.4.4 9ml load bdwgc/8.0.4 10ml load bzip2/1.0.8 11ml load sqlite 12ml load patch xz 13wget http://nixos.org/releases/nix/nix-${nix_version}/nix-${nix_version}.tar.bz2 14tar xfv nix-2.3.7.tar.bz2 15cd nix-2.3.7 Before actually configuring and installing from source, we need some patches.\nPatches I suggest carefully typing out the patches, though leave a comment if you want a repo with these changes (if you must star something in the meantime, star this).\n Start with this patch   1wget https://github.com/NixOS/nix/commit/8d3cb66d22f348341d7afa626acfa53b40584fdd.patch 2git apply 8d3cb66d22f348341d7afa626acfa53b40584fdd.patch  Also this one  Remove the following ifdef stuff from src/libutil/compression.cc, leaving only the contents of the else statement.\n1#ifdef HAVE_LZMA_MT 2 lzma_mt mt_options = {}; 3mt_options.flags = 0; 4mt_options.timeout = 300; // Using the same setting as the xz cmd line 5 mt_options.preset = LZMA_PRESET_DEFAULT; 6mt_options.filters = NULL; 7mt_options.check = LZMA_CHECK_CRC64; 8mt_options.threads = lzma_cputhreads(); 9mt_options.block_size = 0; 10if (mt_options.threads == 0) 11mt_options.threads = 1; 12// FIXME: maybe use lzma_stream_encoder_mt_memusage() to control the 13 // number of threads. 14 ret = lzma_stream_encoder_mt(\u0026amp;strm, \u0026amp;mt_options); 15done = true; 16#else 17 printMsg(lvlError, \u0026#34;warning: parallel XZ compression requested but not supported, falling back to single-threaded compression\u0026#34;); 18#endif If there is trouble with the bzip2 library, set $HOME/.hpc/bzip2/1.0.8/include/bzlib.h in src/libutil/compression.cc, but expand $HOME.\nFinally, you will need edit nixpkgs.\n1# vim pkgs/os-specific/linux/busybox/default.nix 2debianName = \u0026#34;busybox_1.30.1-6\u0026#34;; 3debianTarball = fetchzip { 4url = \u0026#34;http://deb.debian.org/debian/pool/main/b/busybox/${debianName}.debian.tar.xz\u0026#34;; 5sha256 = \u0026#34;05n6mxc8n4zsli4dijrr2x5c9ggwi223i5za4n0xwhgd4lkhqymw\u0026#34;; 6}; User Build We can now complete the build.\n1ml load openssl curl 2./configure --enable-gc --prefix=$myprefix --with-store-dir=$nixdir/store --localstatedir=$nixdir/var --with-boost=$BOOST_ROOT --disable-seccomp-sandboxing --disable-doc-gen --with-sandbox-shell=/usr/bin/sh CPPFLAGS=\u0026#34;-I$HOME/.hpc/bzip2/1.0.8/include\u0026#34; LDFLAGS=\u0026#34;-L$HOME/.hpc/bzip2/1.0.8/lib -Wl,-R$HOME/.hpc/bzip2/1.0.8/lib\u0026#34; 3make -j $(nproc) 4make install 5ml load nix/user # Hooray! 6ml unload openssl curl Now we still need to set a profile. Inspect .hpc/nix/nix-boot/etc/profile.d/nix.sh and check the value of NIX_PROFILES\n1chmod +x .hpc/nix/nix-boot/etc/profile.d/nix.sh 2./.hpc/nix/nix-boot/etc/profile.d/nix.sh 3# OR, and this is better 4nix-env --switch-profile .nix/var/nix/profiles/default 5mkdir -p ~/.nix/var/nix/profiles The reason why we need to switch profiles is because by default nix-env --switch-profile will use /nix/var/nix/profiles/per-user/$USER/profile in a multi-user setup, and it is better to keep this where we have control as well.\nWe also need to kill the sandbox for now, as also seen in the AUR package (and here).\n1# ~/.config/nix/nix.conf 2sandbox = false 3substituters = https://cache.nixos.org https://all-hies.cachix.org 4trusted-public-keys = cache.nixos.org-1:6NCHdD59X431o0gWypbMrAURkbJ16ZPMQFGspcDShjY= all-hies.cachix.org-1:JjrzAOEUsD9ZMt8fdFbzo3jNAyEWlPAwdVuHw4RD43k= Now we can test this before moving forward:\n1nix-channel --update 2nix-shell -p hello Rebuilding Natively The astute reader will have noticed that we glibly monkeyed around with the nix source in the previous section, but all will be made well since we can rebuild to use nix with itself. Do replace the variable with the corresponding path:\n1storeDir = \u0026#34;$HOME/.nix/store\u0026#34;; 2stateDir = \u0026#34;$HOME/.nix/var\u0026#34;; 3confDif = \u0026#34;$HOME/.nix/etc\u0026#34;; Essentially, the $HOME/.config/nixpkgs/config.nix should look like (incorporating both the patches and also the full directory we will be using):\n1{ 2packageOverrides = pkgs: 3with pkgs; { 4autogen = autogen.overrideAttrs (oldAttrs: { 5postInstall = \u0026#39;\u0026#39; 6mkdir -p $dev/bin 7mv $bin/bin/autoopts-config $dev/bin 8for f in $lib/lib/autogen/tpl-config.tlib $out/share/autogen/tpl-config.tlib; do 9sed -e \u0026#34;s|$dev/include|/no-such-autogen-include-path|\u0026#34; -i $f 10sed -e \u0026#34;s|$bin/bin|/no-such-autogen-bin-path|\u0026#34; -i $f 11sed -e \u0026#34;s|$lib/lib|/no-such-autogen-lib-path|\u0026#34; -i $f 12done 13# remove /tmp/** from RPATHs 14for f in \u0026#34;$bin\u0026#34;/bin/*; do 15local nrp=\u0026#34;$(patchelf --print-rpath \u0026#34;$f\u0026#34; | sed -E \u0026#39;s@(:|^)/tmp/[^:]*:@\\1@g\u0026#39;)\u0026#34; 16patchelf --set-rpath \u0026#34;$nrp\u0026#34; \u0026#34;$f\u0026#34; 17done 18\u0026#39;\u0026#39; + stdenv.lib.optionalString (!stdenv.hostPlatform.isDarwin) \u0026#39;\u0026#39; 19# remove /build/** from RPATHs 20for f in \u0026#34;$bin\u0026#34;/bin/*; do 21local nrp=\u0026#34;$(patchelf --print-rpath \u0026#34;$f\u0026#34; | sed -E \u0026#39;s@(:|^)/build/[^:]*:@\\1@g\u0026#39;)\u0026#34; 22patchelf --set-rpath \u0026#34;$nrp\u0026#34; \u0026#34;$f\u0026#34; 23done 24\u0026#39;\u0026#39;; 25}); 26nix = nix.overrideAttrs (oldAttrs: { 27storeDir = \u0026#34;/users/home/jdoe/.nix/store\u0026#34;; 28stateDir = \u0026#34;/users/home/jdoe/.nix/var\u0026#34;; 29confDif = \u0026#34;/users/home/jdoe/.nix/etc\u0026#34;; 30doCheck = false; 31doInstallCheck = false; 32prePatch = \u0026#39;\u0026#39; 33substituteInPlace src/libstore/local-store.cc \\ 34--replace \u0026#39;(eaName == \u0026#34;security.selinux\u0026#34;)\u0026#39; \\ 35\u0026#39;(eaName == \u0026#34;security.selinux\u0026#34; || eaName == \u0026#34;system.nfs4_acl\u0026#34;)\u0026#39; 36substituteInPlace src/libstore/gc.cc \\ 37--replace \u0026#39;auto mapLines =\u0026#39; \\ 38\u0026#39;continue; auto mapLines =\u0026#39; 39substituteInPlace src/libstore/sqlite.cc \\ 40--replace \u0026#39;SQLITE_OPEN_READWRITE | SQLITE_OPEN_CREATE, 0) != SQLITE_OK)\u0026#39; \\ 41\u0026#39;SQLITE_OPEN_READWRITE | SQLITE_OPEN_CREATE, \u0026#34;unix-dotfile\u0026#34;) != SQLITE_OK)\u0026#39; 42\u0026#39;\u0026#39;; 43}); 44}; 45} We can \u0026ldquo;speed up\u0026rdquo; our build by disabling all tests. Go to the copy of nixpkgs and run:\n1find pkgs -type f -name \u0026#39;default.nix\u0026#39; | xargs sed -i \u0026#39;s/doCheck = true/doCheck = false/\u0026#39; 1mkdir -p $HOME/.nix/var/nix/profiles/ 2nix-env -i nix -f $HOME/Git/Github/nixpkgs -j$(nproc) --keep-going --show-trace -v --cores 4 2\u0026gt;\u0026amp;1 | tee nix-no-root.log 3ml load nix/bootstrapped This will still take a couple of hours at least. Around 3-4 hours. Try to set this up on a lazy weekend to evade sysadmins.\nIf curl 429 rate limits are encountered for musl sources, the solution is to replace the source (put the following in a file, say no429.patch):\n1diff --git a/pkgs/os-specific/linux/musl/default.nix b/pkgs/os-specific/linux/musl/default.nix 2index ae175a3..1a6f6c7 100644 3--- a/pkgs/os-specific/linux/musl/default.nix 4+++ b/pkgs/os-specific/linux/musl/default.nix 5@@ -4,12 +4,12 @@ 6}: 7let 8cdefs_h = fetchurl { 9- url = \u0026#34;http://git.alpinelinux.org/cgit/aports/plain/main/libc-dev/sys-cdefs.h\u0026#34;; 10+ url = \u0026#34;https://raw.githubusercontent.com/akadata/aports/master/main/libc-dev/sys-cdefs.h\u0026#34;; 11sha256 = \u0026#34;16l3dqnfq0f20rzbkhc38v74nqcsh9n3f343bpczqq8b1rz6vfrh\u0026#34;; 12}; 13queue_h = fetchurl { 14- url = \u0026#34;http://git.alpinelinux.org/cgit/aports/plain/main/libc-dev/sys-queue.h\u0026#34;; 15- sha256 = \u0026#34;12qm82id7zys92a1qh2l1qf2wqgq6jr4qlbjmqyfffz3s3nhfd61\u0026#34;; 16+ url = \u0026#34;https://raw.githubusercontent.com/akadata/aports/master/main/libc-dev/sys-queue.h\u0026#34;; 17+ sha256 = \u0026#34;049pd547ckrsky72s18a649mz660yph14wdrlw9gnbk903skdnz4\u0026#34;; 18}; 19tree_h = fetchurl { 20url = \u0026#34;http://git.alpinelinux.org/cgit/aports/plain/main/libc-dev/sys-tree.h\u0026#34;; This can be applied with git apply.\nUsage We have finally obtained a bootstrapped nix which is bound to our set of nixpkgs. To ensure its use:\n1ml use $HOME/Modulefiles 2ml purge 3ml load nix/bootstrapped 4ml save Flakes and DevShells Newer versions of nix depend on mdbook which is meant for generating the documentation. Unfortunately, the cargo256 hashes are path dependent. A quick fix is to remove the dependency on mdbook and disable documentation generation with the following ugly patch:\n1diff --git a/pkgs/tools/package-management/nix/default.nix b/pkgs/tools/package-management/nix/default.nix 2index 7eda5ae..91bf1b8 100644 3--- a/pkgs/tools/package-management/nix/default.nix 4+++ b/pkgs/tools/package-management/nix/default.nix 5@@ -14,7 +14,7 @@ common = 6, pkg-config, boehmgc, libsodium, brotli, boost, editline, nlohmann_json 7, autoreconfHook, autoconf-archive, bison, flex 8, jq, libarchive, libcpuid 9- , lowdown, mdbook 10+ , lowdown 11# Used by tests 12, gtest 13, busybox-sandbox-shell 14@@ -36,7 +36,7 @@ common = 1516VERSION_SUFFIX = suffix; 1718- outputs = [ \u0026#34;out\u0026#34; \u0026#34;dev\u0026#34; \u0026#34;man\u0026#34; \u0026#34;doc\u0026#34; ]; 19+ outputs = [ \u0026#34;out\u0026#34; \u0026#34;dev\u0026#34; ]; 2021nativeBuildInputs = 22[ pkg-config ] 23@@ -45,7 +45,6 @@ common = 24[ autoreconfHook 25autoconf-archive 26bison flex 27- (lib.getBin lowdown) mdbook 28jq 29]; 3031@@ -119,8 +118,8 @@ common = 32[ \u0026#34;--with-store-dir=${storeDir}\u0026#34; 33\u0026#34;--localstatedir=${stateDir}\u0026#34; 34\u0026#34;--sysconfdir=${confDir}\u0026#34; 35- \u0026#34;--disable-init-state\u0026#34; 36\u0026#34;--enable-gc\u0026#34; 37+ \u0026#34;--disable-doc-gen\u0026#34; 38] 39++ lib.optionals stdenv.isLinux [ 40\u0026#34;--with-sandbox-shell=${sh}/bin/busybox\u0026#34; 41@@ -136,7 +135,8 @@ common = 4243installFlags = [ \u0026#34;sysconfdir=$(out)/etc\u0026#34; ]; 4445- doInstallCheck = true; # not cross 46+ doInstallCheck = false; # not cross 47+ doCheck = false; 4849# socket path becomes too long otherwise 50preInstallCheck = lib.optionalString stdenv.isDarwin \u0026#39;\u0026#39; 51@@ -160,7 +160,7 @@ common = 52license = lib.licenses.lgpl2Plus; 53maintainers = [ lib.maintainers.eelco ]; 54platforms = lib.platforms.unix; 55- outputsToInstall = [ \u0026#34;out\u0026#34; \u0026#34;man\u0026#34; ]; 56+ outputsToInstall = [ \u0026#34;out\u0026#34; ]; 57}; 5859passthru = { We also need to update our config.nix:\n1nixUnstable = nixUnstable.overrideAttrs (oldAttrs: { 2storeDir = \u0026#34;/users/home/rog32/.nix/store\u0026#34;; 3stateDir = \u0026#34;/users/home/rog32/.nix/var\u0026#34;; 4confDif = \u0026#34;/users/home/rog32/.nix/etc\u0026#34;; 5doCheck = false; 6doInstallCheck = false; 7prePatch = \u0026#39;\u0026#39; 8substituteInPlace src/libstore/local-store.cc \\ 9--replace \u0026#39;(eaName == \u0026#34;security.selinux\u0026#34;)\u0026#39; \\ 10\u0026#39;(eaName == \u0026#34;security.selinux\u0026#34; || eaName == \u0026#34;system.nfs4_acl\u0026#34;)\u0026#39; 11substituteInPlace src/libstore/gc.cc \\ 12--replace \u0026#39;auto mapLines =\u0026#39; \\ 13\u0026#39;continue; auto mapLines =\u0026#39; 14\u0026#39;\u0026#39;; 15}); Now we can finally get to the installation of a newer version. I prefer to live life on the edge:\n1nix-env -iA nixUnstable -f $HOME/Git/Github/nixpkgs -j$(nproc) --keep-going --show-trace --cores 4 2\u0026gt;\u0026amp;1 | tee nix-install-base.log We are now able activate flakes and other features like nix shell (note the space!).\n1# ~/.config/nix/nix.conf 2experimental-features = nix-command flakes Bonus: Fixing Documentation In order to get the original derivation working, we need to essentially modify the cargo256 hashes. Thankfully the nix build log is rather verbose.\n1installing 2error: hash mismatch in fixed-output derivation \u0026#39;/users/home/jdoen/.nix/sto$ 3e/n71nkimlbazmq1vpyyavqcxzg9c86brs-mdbook-0.4.7-vendor.tar.gz.drv\u0026#39;: 4specified: sha256-2kBJcImytsSd7Q0kj1bsP/NXxyy2Pr8gHb8iNf6h3/4= 5got: sha256-4bYLrmyI7cPUes6DYREiIB9gDze0KO2jMP/jPzvWbwQ= 6error: 1 dependencies of derivation \u0026#39;/users/home/jdoen/.nix/store/wr31pgva8a 7zn9jvvpa4bshykv80xf5qi-mdbook-0.4.7.drv\u0026#39; failed to build 8error: 1 dependencies of derivation \u0026#39;/users/home/jdoen/.nix/store/y8pkc0hhgz 9rvxgrj7c00mmsy50plya6p-nix-2.4pre20210326_dd77f71.drv\u0026#39; failed to build We need to modify pkgs/tools/text/mdbook/default.nix to update the hash; and then:\n1nix-env -iA nixUnstable -f $HOME/Git/Github/nixpkgs -j$(nproc) --keep-going --show-trace --cores 4 2\u0026gt;\u0026amp;1 | tee nix-install-base.log 2ml load nix/bootstrapped 3nix-shell --help # Works 4nix-shell -p hello # Also works Channels We would like to move away from having to constantly pass our cloned set of packages.\n1nix-channel --add https://nixos.org/channels/nixpkgs-unstable nixpkgs 2nix-channel --update Basic Packages Now we can get some basic stuff too.\n1nix-env -i tmux zsh lsof pv git -f $HOME/Git/Github/nixpkgs -j$(nproc) --keep-going --show-trace --cores 4 2\u0026gt;\u0026amp;1 | tee nix-install-base.log Ruby Caveats  No longer relevant as of April 2020\n While installing packages which depend on ruby, there will be permission errors inside the build folder. These can be \u0026ldquo;fixed\u0026rdquo; by setting very permissive controls on the build-directory in question. Do not set permissions directly on the .nix/store/$HASH folder, as doing so will make nix reject the build artifact.\n1# neovim depends on ruby 2nix-env -i neovim -v -f $HOME/Git/Github/nixpkgs A more elegant way to fix permissions involves a slightly more convoluted approach. We can note where the build is occurring (e.g. /tmp) and run a watch command to fix permissions.\n1watch -n1 -x chmod 777 -R /tmp/nix-build-ruby-2.6.6.drv-0/source/lib/ Naturally this must be run in a separate window.\nDotfiles Feel free to set up dotfiles (mine, perhaps) to profit even further. We will consider the process of obtaining my set below. Minimally, we will want to obtain tmux and zsh.\n1nix-env -i tmux zsh -v -f $HOME/Git/Github/nixpkgs Now we can set the dotfiles up.\n1git clone https://github.com/HaoZeke/Dotfiles 2cd Dotfiles 3$HOME/.local/bin/dotgit restore hzhpc The final installation configures neovim and tmux.\n1zsh 2# Should install things with zinit 3tmux 4# CTRL+b --\u0026gt; SHIFT+I to install 5nvim Misc NFS For issues concerning NFS lock files, consider simply moving the problematic file and let things sort themselves out. Consider:\n1nix-build 2# something about a .nfs lockfile in some .nix/$HASH-pkg/.nfs0234234 3mv .nix/$HASH-pkg/ .diePKGs/ 4nix-build # profit The right way to deal with this is of course:\n1nix-build 2lsof +D .nix/$HASH-pkg/.nfs0234234 3kill $whatever_blocks 4nix-build # profit Conclusions Though this is slow and seems like an inefficient use of cluster resources, the benefits of reproducible environments typically outweighs the cost. Also it is much more pleasant to have a proper package manager which can work with Dotfiles.\n  Note that this will of course entail rebuilding everything from scratch, every time, which means no binary caches. Thus there is no reasonable defence for trying this out without access to a high powered limited access machine\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The rest of the post assumes we are on the same page and working towards the same end-goal, substitute and remix at will\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/local-nix-no-root/","tags":["workflow","projects","hpc","nix","tools"],"title":"Local Nix without Root"},{"categories":["programming"],"contents":" Short post on using mach-nix with niv.\n Background In previous posts, there was a discussion on a ground up approach to adding packages which aren\u0026rsquo;t on the core nixpkgs channels using GitHub or PyPi sources. However, this lacked a way to do so programmatically, and also a way to convert existing python projects.\nPython Dependency Management This time, instead of the more pedagogical approach of building packages from PyPi or GitHub, we will use overlays and the excellent mach-nix to speed up the process. We will continue to use niv.\n1niv init 2niv update nixpkgs -b nixpkgs-unstable To leverage mach-nix we will simply need the following setup to work with niv.\n1let 2sources = import ./nix/sources.nix; 3pkgs = import sources.nixpkgs { }; 4inherit (pkgs.lib) optional optionals; 5mach-nix = import (builtins.fetchGit { 6url = \u0026#34;https://github.com/DavHau/mach-nix/\u0026#34;; 7ref = \u0026#34;refs/tags/3.1.1\u0026#34;; 8}) { 9pkgs = pkgs; 1011# optionally specify the python version 12# python = \u0026#34;python38\u0026#34;; 1314# optionally update pypi data revision from https://github.com/DavHau/pypi-deps-db 15# pypiDataRev = \u0026#34;some_revision\u0026#34;; 16# pypiDataSha256 = \u0026#34;some_sha256\u0026#34;; 17}; 18customPython = mach-nix.mkPython { 19requirements = \u0026#39;\u0026#39; 20copier 21pytest 22\u0026#39;\u0026#39;; 23providers = { 24_default = \u0026#34;nixpkgs,wheel,sdist\u0026#34;; 25pytest = \u0026#34;nixpkgs\u0026#34;; 26}; 27pkgs = pkgs; 28}; 29in pkgs.mkShell { buildInputs = with pkgs; [ customPython ]; } Note that we have essentially written out a requirements.txt and can actually pass a path there instead as well. The key point to make it work with niv is the pkgs parameter. To use the older method of overriding parts of the setup, we can use the overrides_pre hook as shown below:\n1let 2sources = import ./prjSource/nix/sources.nix; 3pkgs = import sources.nixpkgs { }; 4inherit (pkgs.lib) optional optionals; 5mach-nix = import (builtins.fetchGit { 6url = \u0026#34;https://github.com/DavHau/mach-nix/\u0026#34;; 7ref = \u0026#34;refs/tags/3.1.1\u0026#34;; 8}) { 9pkgs = pkgs; 1011# optionally specify the python version 12# python = \u0026#34;python38\u0026#34;; 1314# optionally update pypi data revision from https://github.com/DavHau/pypi-deps-db 15# pypiDataRev = \u0026#34;some_revision\u0026#34;; 16# pypiDataSha256 = \u0026#34;some_sha256\u0026#34;; 17}; 18customPython = mach-nix.mkPython { 19requirements = \u0026#39;\u0026#39; 20copier 21pytest 22f90wrap 23\u0026#39;\u0026#39;; 24providers = { 25_default = \u0026#34;nixpkgs,wheel,sdist\u0026#34;; 26pytest = \u0026#34;nixpkgs\u0026#34;; 27}; 28overrides_pre = [ 29(pythonSelf: pythonSuper: { 30pytest = pythonSuper.pytest.overrideAttrs (oldAttrs: { 31doCheck = false; 32doInstallCheck = false; 33}); 34f90wrap = pythonSelf.buildPythonPackage rec { 35pname = \u0026#34;f90wrap\u0026#34;; 36version = \u0026#34;0.2.3\u0026#34;; 37src = pkgs.fetchFromGitHub { 38owner = \u0026#34;jameskermode\u0026#34;; 39repo = \u0026#34;f90wrap\u0026#34;; 40rev = \u0026#34;master\u0026#34;; 41sha256 = \u0026#34;0d06nal4xzg8vv6sjdbmg2n88a8h8df5ajam72445mhzk08yin23\u0026#34;; 42}; 43buildInputs = with pkgs; [ gfortran stdenv ]; 44propagatedBuildInputs = with pythonSelf; [ 45setuptools 46setuptools-git 47wheel 48numpy 49]; 50preConfigure = \u0026#39;\u0026#39; 51export F90=${pkgs.gfortran}/bin/gfortran 52\u0026#39;\u0026#39;; 53doCheck = false; 54doIstallCheck = false; 55}; 56}) 57]; 58pkgs = pkgs; 59}; 60in pkgs.mkShell { buildInputs = with pkgs; [ customPython ]; } We can also pull in overrides from poetry2nix with overrides_post as described here.\nConclusion With the completion of this final remaining hurdle, nix is now fully realized as a python management system. At this point the \u0026ldquo;only\u0026rdquo; thing remaining is to find an optimal way of leveraging nix for setting up re-usable data science and scientific computing projects.\n","permalink":"https://rgoswami.me/posts/mach-nix-niv-python/","tags":["tools","nix","workflow","python"],"title":"Niv and Mach-Nix for Nix Python"},{"categories":["programming"],"contents":"Background As a prelude to writing up the details of how this site is generated, I realized I should write up a nix oriented workflow for node packages.\nTooling and Idea The basic concepts are:\n Use npm to generate a package-lock.json file Use node2nix in a shell to generate a set of nix derivations Enter a shell environment with the nix inputs Profit  However, the nuances of this are a bit annoying at first.\nPackaging Requirements We will use the standard npm installation method at first, but since we shouldn\u0026rsquo;t keep installing and removing things, so we need a way to modify package.json without running npm and will therefore add add-dependency.\n1npm install add-dependency Setting up Node2Nix We will first clean the directory of what we do not need.\n1rm -rf default.nix node-env.nix node-packages.nix node_modules Now we can enter a shell with node2nix and generate files for the node packages.\n1nix-shell -p \u0026#39;nodePackages.node2nix\u0026#39; 2node2nix -l package-lock.json A Nix Environment We will use the standard setup described in the tutorial post:\n1nix-env -i niv lorri 2niv init 3niv update nixpkgs -b nixpkgs-unstable This is to be in conjunction with the following shell.nix file 1.\n1{ sources ? import ./nix/sources.nix }: 2let 3pkgs = import sources.nixpkgs { }; 4nodeEnv = pkgs.callPackage ./node-env.nix { }; 5nodePackages = pkgs.callPackage ./node-packages.nix { 6globalBuildInputs = with pkgs; [ zsh ]; 7inherit nodeEnv; 8}; 9in nodePackages.shell Note that we have overridden the nodePackages shell which is defined in the files created by node2nix.\nWe can now enter the environment and setup node_modules2.\n1nix-shell 2ln -s $NODE_PATH node_modules Updates Unfortunately, this setup is a little fragile to updates. We will need to exit and re-create the setup. Note that we are removing the lock file now as well.\n1# In the nix-shell 2add-dependencies babel-loader @babel/core @babel/preset-env core-js @babel/plugin-transform-regenerator 3# Do not run in nix-shell 4rm -rf default.nix node-env.nix node-packages.nix node_modules package-lock.json 5# Update in a line 6nix-shell -p \u0026#39;nodePackages.node2nix\u0026#39; --run \u0026#39;node2nix package.json\u0026#39; The single line update mechanism can be run in the nix-shell itself, making things marginally less painful.\nConclusions This has been a short introduction to working with the nix-shell ecosystem. It isn\u0026rsquo;t as fast as working with the normal setup, and it is a pretty annoying workflow. Given that most CI setups have good support for caching npm dependencies, it doesn\u0026rsquo;t seem worthwhile at the moment.\n  There might be a better approach defined in this issue later\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n We can\u0026rsquo;t use lorri yet since we need to selectively add and remove the symbolic link to node_modules\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/nix-shell-node/","tags":["tools","nix","workflow","node"],"title":"Nix Shells for Node Projects"},{"categories":["programming"],"contents":" Brief introduction to a nix based project workflow.\n Background For CarpentryCon@Home 2020, along with Amrita Goswami, I am to prepare and deliver a workshop on \u0026ldquo;Reproducible Environments with the Nix Packaging System\u0026rdquo;. In particular, as a community of practice lesson, the focus is not on packaging (as is typical of most Nix tutorials) nor on the Nix expression language itself, but instead on the use of Nix as a replacement for virtual environments using mkShell.\nMaterials This is a Carpentries style single page lesson on setting up and working with Nix for reproducible environments. It was concieved to be a complimentary resource to the content of this repository, namely:\n Slides on Python packages with Nix Nix with R and devtools Statistical Rethinking and Nix An Etherpad Session recording  Ten seconds into Nix A few words to keep in mind, in no particular order.\n Nix is based of good academic principles by Dolstra, de Jonge, and Visser (2004) and Dolstra, Löh, and Pierron (2010)  It has been used in large scientific projects for reproducibility (e.g. d-SEAMS of Goswami, Goswami, and Singh (2020))   The Nix expression language is a domain specific language  Turing completeness is not a goal or a requirement   Can leverage binary caches  Not always true, only when installed in /nix    Setup For this particular tutorial, we will assume the standard Nix installation proceedure, that is, one where the installer has root access to create the initial /nix directory and set up the build users1. This follows directly from the Nix Manual:\n1# You need root permissions for this!!! 2sh \u0026lt;(curl -L https://nixos.org/nix/install) --daemon At this point we will also install the canonnical first package, the hello package, which simply outputs a friendly greeting.\n1nix-env -i hello Note that the basic package search operation is nix search and it gives outputs which look like:\n Figure 1: The nix search emacs output\n  Though this is not bad by any standard, we will try to get a more interactive management tool.\nBasic Helpers The first few things to obtain are:\n nox This is a better package management helper niv For pinning dependencies as discussed later lorri For working seamlessly with project environments  Exercise 1  Try installing these and use nox emacs to test the output\n  Figure 2: The nox emacs output\n  More Dependable Dependencies Standard Channels Nix works by searching a repository (local or online) of package derivations. Indeed, we can pass nix-env any local fork of the main nixpkgs repo as well.\n1# don\u0026#39;t run this, it is a large repo 2git clone https://github.com/NixOS/nixpkgs.git $mynixdir 3# make changes.. 4$EDITOR $nixpkgs/pkgs/applications/editors/emacs/default.nix 5nix-env -i emacs -f $nixpkgs  This might serve as a way to mass modify the nixpkgs in a pinch  However, we will almost never use this in practice The overlay approach is much better   It is also useful if we need to build local derivations  Pinning Dependencies Compared to globally tracking the branches of nixpkgs or even local changes and forks, for project oriented workflows it is better to use niv which we obtained previously. In a nutshell, niv will generate a json file to keep track of dependencies and wraps it in a nix file we can subsequently import and use.\nProject Setup We are now in a position to start working with a project oriented workflow.\n1# Make directories 2mkdir myFirstNix 3cd myFirstNix 4# Setup 5niv init 6niv update nixpkgs -b nixpkgs-unstable At this stage your project should have the following structure:\n1tree myFirstNix    myFirstNix        └── nix     ├── sources.json     └── sources.nix           1 directory, 2 files    We can now move on to the heart of this tutorial, the nix-shell. In a nutshell, running nix-shell when there is a defined shell.nix will spawn a virtual environment with the nix packages requested.\nlorri and direnv Though we haven\u0026rsquo;t as yet generated a shell.nix we should point out that writing one by hand will mean that we need to rebuild the enviroment when we make changes using nix-shell every time. A more elegant approach is to offload the rebuilding of the environment to lorri which also has a neat direnv integrration. Let\u0026rsquo;s try that out.\n1cd myFirstNix 2lorri init    Aug 18 15:24:06.524 INFO wrote file, path: ./shell.nix     Aug 18 15:24:06.524 INFO wrote file, path: ./.envrc   Aug 18 15:24:06.524 INFO done       At this point we should now have:\n1tree -a myFirstNix    myFirstNix        ├── .envrc     ├── nix     │  ├── sources.json    │  └── sources.nix    └── shell.nix           1 directory, 4 files    We might want to take a quick look at what is being loaded into the environment and the shell.nix at this point.\n1let 2pkgs = import \u0026lt;nixpkgs\u0026gt; {}; 3in 4pkgs.mkShell { 5buildInputs = [ 6pkgs.hello 7]; 8} Code Snippet 1: shell.nix\n 1eval \u0026#34;$(lorri direnv)\u0026#34; Code Snippet 2: .envrc\n The .envrc output is not very useful at a glance, however when we cd into the directory it is very verbose and explicit about what is being set up.\n Figure 3: Sample output of the evaluation\n  Note that in order to set lorri up, we will need to set up a daemon.\n1systemctl --user start lorri 2direnv allow Pinning with niv Note that inspite of having set up niv, we have not yet used the sources defined therein. We will now fix this, by modifying shell.nix.\n1let 2sources = import ./nix/sources.nix; 3pkgs = import sources.nixpkgs { }; 4inherit (pkgs.lib) optional optionals; 5in 6pkgs.mkShell { 7buildInputs = [ 8pkgs.hello 9]; 10} Code Snippet 3: shell.nix with niv\n Purity and Environments There are a couple of things to note about this setup.\n The default shell is bash On occasion, depending on your Dotfiles you might have paths overriden in an annoying way  One workaround is to use nix shell with an argument:\n1nix-shell --run \u0026#34;bash\u0026#34;  We can also pass --pure to the function, but at the cost of having to define many more dependencies for our shell  mkShell The mkShell function is the focus of our tutorial, and we will mostly work around passing in different environments and hooks. Let us start by defining a hook.\nShell Hooks Often, we will want to set an environment variable in our shell in advance. We should not use direnv for this, and instead we will focus on the shellHook option. Syntactically, we note that this is of the form:\n1let 2hook = \u0026#39;\u0026#39; 3export myvar=\u0026#34;Test\u0026#34; 4\u0026#39;\u0026#39; 5in pkgs.mkShell { 6shellHook = hook; 7} Often we will describe variables in the let section in favor of cluttering the actual function call itself.\nOverriding Global Packages For overriding global packages, it is best to leverage the config.nix (which is commonly in $HOME/.config/nixpkgs/config.nix) file instead of the current environment, though it could be managed in a per-project setup as well. Consider the case where we need to disable tests for a particular packages, say libuv.\n1{ 2packageOverrides = pkgs: 3with pkgs; { 4libuv = libuv.overrideAttrs (oldAttrs: { 5doCheck = false; 6doInstallCheck = false; 7}); 8}; 9} Code Snippet 4: A sample config.nix file\n Python Dependencies As R dependency management has been covered in an earlier post, we will focus on the management of python environments.\nGeneric Environments We can define existing packages as follows (and can check for existence with nox) using the let..in syntax.\n1let 2# Niv 3sources = import ./nix/sources.nix; 4pkgs = import sources.nixpkgs { }; 5inherit (pkgs.lib) optional optionals; 6# Python 7pythonEnv = pkgs.python38.withPackages (ps: with ps;[ 8numpy 9toolz 10]); 11in pkgs.mkShell { 12buildInputs = with pkgs; [ 13pythonEnv 1415black 16mypy 1718libffi 19openssl 20]; 21} Code Snippet 5: Shell with basic python environment\n Project Local Pip We can leverage a trick from here to set a local directory for pip installations, which boils down to some path hacking.\n1let 2# Niv 3sources = import ./nix/sources.nix; 4pkgs = import sources.nixpkgs { }; 5inherit (pkgs.lib) optional optionals; 6# Python 7pythonEnv = pkgs.python38.withPackages (ps: with ps;[ 8numpy 9toolz 10]); 11hook = \u0026#39;\u0026#39; 12export PIP_PREFIX=\u0026#34;$(pwd)/_build/pip_packages\u0026#34; 13export PYTHONPATH=\u0026#34;$(pwd)/_build/pip_packages/lib/python3.8/site-packages:$PYTHONPATH\u0026#34; 14export PATH=\u0026#34;$PIP_PREFIX/bin:$PATH\u0026#34; 15unset SOURCE_DATE_EPOCH 16\u0026#39;\u0026#39;; 17in pkgs.mkShell { 18buildInputs = with pkgs; [ 19pythonEnv 2021black 22mypy 2324libffi 25openssl 26]; 27shellHook = hook; 28} Note that this is discouraged as we will lose the caching capabilities of nix.\nNon-Standard Python For more control over the environment, we can define it in more detail with some overlays.\n1let 2python = pkgs.python38.override { 3packageOverrides = self: super: { 4pytest = super.pytest.overridePythonAttrs (old: rec { 5doCheck = false; 6doInstallCheck = false; 7}); 8}; 9}; 10myPy = python.withPackages 11(p: with p; [ numpy pip pytest ]); 12in pkgs.mkShell { 13buildInputs = with pkgs; [ 14myPy 15]; 16} We have used both overriden packages and standard packages in the above formulation.\nBuilding Packages For cases where we are certain that no existing package is present (use nox) we can also build them. Take f90wrap as an example, and we will use the Github version, rather than the PyPi version (the difference is in the source fetch function).\n1f90wrap = self.buildPythonPackage rec { 2pname = \u0026#34;f90wrap\u0026#34;; 3version = \u0026#34;0.2.3\u0026#34;; 4src = pkgs.fetchFromGitHub { 5owner = \u0026#34;jameskermode\u0026#34;; 6repo = \u0026#34;f90wrap\u0026#34;; 7rev = \u0026#34;master\u0026#34;; 8sha256 = \u0026#34;0d06nal4xzg8vv6sjdbmg2n88a8h8df5ajam72445mhzk08yin23\u0026#34;; 9}; 10buildInputs = with pkgs; [ gfortran stdenv ]; 11propagatedBuildInputs = with self; [ 12setuptools 13setuptools-git 14wheel 15numpy 16]; 17preConfigure = \u0026#39;\u0026#39; 18export F90=${pkgs.gfortran}/bin/gfortran 19\u0026#39;\u0026#39;; 20doCheck = false; 21doIstallCheck = false; 22}; This is quite involved, discuss.\nSetting Versions We can finally generalize our shell.nix to default to python 3.8 but also take a command through --argstr:\n1nix-shell --argstr pythonVersion 36 --run \u0026#34;bash\u0026#34; Where we need to simply define the option at the top of the file, with a default.\n1{ pythonVersion ? \u0026#34;38\u0026#34; }: 2# Define 3let 4sources = import ./nix/sources.nix; 5pkgs = import sources.nixpkgs { }; 6inherit (pkgs.lib) optional optionals; 7hook = \u0026#39;\u0026#39; 8# Python Stuff 9export PIP_PREFIX=\u0026#34;$(pwd)/_build/pip_packages\u0026#34; 10export PYTHONPATH=\u0026#34;$(pwd)/_build/pip_packages/lib/python3.8/site-packages:$PYTHONPATH\u0026#34; 11export PATH=\u0026#34;$PIP_PREFIX/bin:$PATH\u0026#34; 12unset SOURCE_DATE_EPOCH 13\u0026#39;\u0026#39;; 14# Apparently pip needs 1980 or above 15# https://github.com/ento/elm-doc/blob/master/shell.nix 16python = pkgs.\u0026#34;python${pythonVersion}\u0026#34;.override { 17packageOverrides = self: super: { 18pytest = super.pytest.overridePythonAttrs (old: rec { 19doCheck = false; 20doInstallCheck = false; 21}); 22ase = super.ase.overridePythonAttrs (old: rec { 23doCheck = false; 24doInstallCheck = false; 25}); 26f90wrap = self.buildPythonPackage rec { 27pname = \u0026#34;f90wrap\u0026#34;; 28version = \u0026#34;0.2.3\u0026#34;; 29src = pkgs.fetchFromGitHub { 30owner = \u0026#34;jameskermode\u0026#34;; 31repo = \u0026#34;f90wrap\u0026#34;; 32rev = \u0026#34;master\u0026#34;; 33sha256 = \u0026#34;0d06nal4xzg8vv6sjdbmg2n88a8h8df5ajam72445mhzk08yin23\u0026#34;; 34}; 35buildInputs = with pkgs; [ gfortran stdenv ]; 36propagatedBuildInputs = with self; [ 37setuptools 38setuptools-git 39wheel 40numpy 41]; 42preConfigure = \u0026#39;\u0026#39; 43export F90=${pkgs.gfortran}/bin/gfortran 44\u0026#39;\u0026#39;; 45doCheck = false; 46doInstallCheck = false; 47}; 48}; 49}; 50myPy = python.withPackages 51(p: with p; [ ase ipython ipykernel scipy numpy f90wrap pip ]); 52in pkgs.mkShell { 53buildInputs = with pkgs; [ 54# Required for the shell 55zsh 56perl 57git 58direnv 59fzf 60ag 61fd 6263# Building thigns 64gcc9 65gfortran 66openblas 6768myPy 69# https://github.com/sveitser/i-am-emotion/blob/294971493a8822940a153ba1bf211bad3ae396e6/gpt2/shell.nix 70]; 71shellHook = hook; 72} Code Snippet 6: Full shell.nix\n This is enough to cover almost all use-cases for python environments.\nBuild Helpers Note that we can speed up some aspects of fetch with the prefetch commands:\n1nix-prefetch-git $giturl 2nix-prefetch-url $url In practice, some trial and error is easier.\nSupplementary Reading Material Though these are in no means exhaustive, they may offer a slightly more advanced or different focus than the material covered here.\nCore Content  Manuals  Nix and Nixpkgs   Nix Wiki  Nix Cheatsheet   Language Sections  Learning Paths  Nix pills Official tutorials Nix dev has some nice opinionated tips  Personal Correspondence Tyson Whitehead from Compute Canada was kind enough to bring the folllowing additional training materials:\n A wiki pertaining to usage of Nix on in an HPC setting SWC style workshop materials from TECC 2018 SHARCNET live presentation materials from 2018  Conclusions The standard dive into Nix is based on building derivations and playing with language, which is in no means a bad one, just too long for the time allocated. The best way to get into Nix is to start using it for everything.\nReferences Dolstra, Eelco, Merijn de Jonge, and Eelco Visser. 2004. \u0026ldquo;Nix: A Safe and Policy-Free System for Software Deployment,\u0026rdquo; 15.\n Dolstra, Eelco, Andres Löh, and Nicolas Pierron. 2010. \u0026ldquo;NixOS: A Purely Functional Linux Distribution.\u0026rdquo; Journal of Functional Programming 20 (5-6): 577\u0026ndash;615. https://doi.org/dfrgtj.\n Goswami, Rohit, Amrita Goswami, and Jayant K. Singh. 2020. \u0026ldquo;D-SEAMS: Deferred Structural Elucidation Analysis for Molecular Simulations.\u0026rdquo; Journal of Chemical Information and Modeling 60 (4): 2169\u0026ndash;77. https://doi.org/10.1021/acs.jcim.0c00031.\n    For reasons pertaining to latency and ease-of-use, we will assume the multi-user installation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/ccon-tut-nix/","tags":["tools","nix","workflow","python"],"title":"A Tutorial Introduction to Nix"},{"categories":["programming"],"contents":"Background My move away from the powerful, but unimaginatively named HPC clusters of IITK 1 brought me in close contact with the Lua based 2 lmod module system. Rather than fall into the rabbit hole of brew we will leverage the existing system to add our new libraries. Not finding any good collections of these composable environments, and having failed once before to install Nix as a user without admin access, I decided to start my own collection of Lmod recipies. The rest of this post details the installation proceedure to be carried out in conjunction with the hzHPC_lmod repo.\nSetting Up These are reproduced from the repo for completeness.\n1git clone https://github.com/kobus-v-schoor/dotgit.git 2mkdir -p ~/.local/bin 3cp -r dotgit/old/bin/dotgit* ~/.local/bin 4cat dotgit/old/bin/bash_completion \u0026gt;\u0026gt; ~/.bash_completion 5rm -rf dotgit I actually strongly suggest using a target from my Dotfiles in conjunction with this, but it isn\u0026rsquo;t really required, so:\n1~/.local/bin/dotgit restore hzhpc Note that because of the suggested separation, I have not opted to setup a shell or even ensure that there are scripts here to help keep module in your path. Those are in my Dotfiles. If, you opt to not use these dotfiles, then do not run the ml load commands.\nBasic Setup We will first load an environment with basic packages (autotools). This is a stop-gap solution.\nMicroMamba We will set up a micromamba installation to get git and other base utilities.\n1mkdir -p ~/.local/bin 2export PATH=$HOME/.local/bin:$PATH 3wget -qO- https://micromamba.snakepit.net/api/micromamba/linux-64/latest | tar -xvj bin/micromamba 4mv bin/micromamba ~/.local/bin 5rm -rf bin 6micromamba shell init -s bash -p $HOME/micromamba 7. ~/.bashrc Now we can use mamba to get a temporary interactive environment\n1# Spack requirements 2PKGS=\u0026#34;git patch curl make tar gzip unzip bzip2 xz zstd subversion lsof m4\u0026#34; 3mkdir ~/.mamba_venvs 4micromamba create -p ~/.mamba_venvs/intBase $PKGS 5micromamba activate ~/.mamba_venvs/intBase For subsequent logins we can simply run:\n1export PATH=$HOME/.local/bin:$PATH 2. ~/.bashrc 3micromamba activate ~/.mamba_venvs/intBase Configuration A simple .condarc will suffice for the above.\n1channels:2- conda-forge3- defaults4channel_priority:disabledWarning Note that once the base packages have been installed\u0026hellip;\n We must unload our micromamba environment!!\n 1micromamba deactivate It is recommended to use a minimal set of micromamba packages.\nLMod Libraries  Note that: garpur and elja already has lmod\n The scripts in this post will also be part of the repo, but keep in mind that these are not meant to be robust ways to install anything, and every command should be run by hand because things will probably break badly. We keep all sources in $HOME/tmphpc and install everything relative to $HOME/.hpc. Paths are managed by the lmod system.\n1export hpcroot=$HOME/tmphpc 2mkdir -p $hpcroot Combining our paths with the lmod manual gives rise to the following definiton (roughly the same for each one):\n1local home = os.getenv(\u0026#34;HOME\u0026#34;) 2local version = myModuleVersion() 3local pkgName = myModuleName() 4local pkg = pathJoin(home,\u0026#34;.hpc\u0026#34;,pkgName,version,\u0026#34;bin\u0026#34;) 5prepend_path(\u0026#34;PATH\u0026#34;, pkg) We will no longer bother with the module definitions for the rest of this post, as they are handled and documented in the repo.\n1# Setting up hzhpc modules 2cd tmphpc 3git clone https://github.com/kobus-v-schoor/dotgit.git 4mkdir -p ~/.local/bin 5cp -r dotgit/old/bin/dotgit* ~/.local/bin 6cat dotgit/bin/bash_completion \u0026gt;\u0026gt; ~/.bash_completion 7rm -rf dotgit 8~/.local/bin/dotgit restore hzhpc Patch This is a basic utility we need before getting to anything else.\n1cd $hpcroot 2myprefix=$HOME/.hpc/patch/2.7.2 3wget http://ftp.gnu.org/gnu/patch/patch-2.7.2.tar.gz 4gzip -dc patch-2.7.2.tar.gz | tar xvf - 5cd patch-2.7.2 6./configure --prefix=$myprefix 7make -j$(nproc) \u0026amp;\u0026amp; make install 8ml load patch Help2man 1cd $hpcroot 2myprefix=$HOME/.hpc/help2man/1.48.3 3wget https://gnuftp.uib.no/help2man/help2man-1.48.3.tar.xz 4tar xfv help2man-1.48.3.tar.xz 5cd help2man-1.48.3 6./configure --prefix=$myprefix 7make -j$(nproc) 8make install 9ml load help2man/1.48.3 Perl This is essentially the setup from the main docs.\n1# Get Perl 2curl -L http://xrl.us/installperlnix | bash 3cpanm --local-lib=~/perl5 local::lib \u0026amp;\u0026amp; eval $(perl -I ~/perl5/lib/perl5/ -Mlocal::lib) 4cpanm ExtUtils::MakeMaker --force # For git 5cpanm Thread::Queue # For automake 6ml load perl/5.28.0 Autotools M4 This requires special considerations for glibc greater than 2.28 (true for compilers like gcc8 and above).\n1cd $HOME/tmphpc 2myprefix=$HOME/.hpc/autotools 3wget http://ftp.gnu.org/gnu/m4/m4-1.4.18.tar.gz 4gzip -dc m4-1.4.18.tar.gz | tar xvf - 5cd m4-1.4.18 6# From http://git.openembedded.org/openembedded-core/commit/meta/recipes-devtools/m4/m4/m4-1.4.18-glibc-change-work-around.patch 7# From https://askubuntu.com/a/1112101/690387 8wget http://git.openembedded.org/openembedded-core/plain/meta/recipes-devtools/m4/m4/m4-1.4.18-glibc-change-work-around.patch 9patch -p 1 \u0026lt; m4-1.4.18-glibc-change-work-around.patch 10./configure -C --prefix=$myprefix/m4/1.4.18 \u0026amp;\u0026amp; make -j$(nproc) \u0026amp;\u0026amp; make install Patch contents The patch is needed since glibc 2.28 and above have changed the nesting levels. The patch is effectively backported from the upstream repositories. Some more context is here.\n1update for glibc libio.h removal in 2.28+ 23see 4https://src.fedoraproject.org/rpms/m4/c/814d592134fad36df757f9a61422d164ea2c6c9b?branch=master 5 6Upstream-Status: Backport [https://git.savannah.gnu.org/cgit/gnulib.git/commit/?id=4af4a4a718] 7Signed-off-by: Khem Raj \u0026lt;raj.khem@gmail.com\u0026gt; 89Index: m4-1.4.18/lib/fflush.c 10=================================================================== 11--- m4-1.4.18.orig/lib/fflush.c 12+++ m4-1.4.18/lib/fflush.c 13@@ -33,7 +33,7 @@ 14#undef fflush 15 1617-#if defined _IO_ftrylockfile || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */ 18+#if defined _IO_EOF_SEEN || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */ 1920/* Clear the stream\u0026#39;s ungetc buffer, preserving the value of ftello (fp). */ 21static void 22@@ -72,7 +72,7 @@ clear_ungetc_buffer (FILE *fp) 2324#endif 25 26-#if ! (defined _IO_ftrylockfile || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */) 27+#if ! (defined _IO_EOF_SEEN || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */) 2829# if (defined __sferror || defined __DragonFly__ || defined __ANDROID__) \u0026amp;\u0026amp; defined __SNPT 30 /* FreeBSD, NetBSD, OpenBSD, DragonFly, Mac OS X, Cygwin, Android */ 31@@ -148,7 +148,7 @@ rpl_fflush (FILE *stream) 32if (stream == NULL || ! freading (stream)) 33return fflush (stream); 3435-#if defined _IO_ftrylockfile || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */ 36+#if defined _IO_EOF_SEEN || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */ 3738clear_ungetc_buffer_preserving_position (stream); 3940Index: m4-1.4.18/lib/fpending.c 41=================================================================== 42--- m4-1.4.18.orig/lib/fpending.c 43+++ m4-1.4.18/lib/fpending.c 44@@ -32,7 +32,7 @@ __fpending (FILE *fp) 45/* Most systems provide FILE as a struct and the necessary bitmask in 46\u0026lt;stdio.h\u0026gt;, because they need it for implementing getc() and putc() as 47fast macros. */ 48-#if defined _IO_ftrylockfile || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */ 49+#if defined _IO_EOF_SEEN || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */ 50return fp-\u0026gt;_IO_write_ptr - fp-\u0026gt;_IO_write_base; 51#elif defined __sferror || defined __DragonFly__ || defined __ANDROID__ 52 /* FreeBSD, NetBSD, OpenBSD, DragonFly, Mac OS X, Cygwin, Android */ 53Index: m4-1.4.18/lib/fpurge.c 54=================================================================== 55--- m4-1.4.18.orig/lib/fpurge.c 56+++ m4-1.4.18/lib/fpurge.c 57@@ -62,7 +62,7 @@ fpurge (FILE *fp) 58/* Most systems provide FILE as a struct and the necessary bitmask in 59\u0026lt;stdio.h\u0026gt;, because they need it for implementing getc() and putc() as 60fast macros. */ 61-# if defined _IO_ftrylockfile || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */ 62+# if defined _IO_EOF_SEEN || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */ 63fp-\u0026gt;_IO_read_end = fp-\u0026gt;_IO_read_ptr; 64fp-\u0026gt;_IO_write_ptr = fp-\u0026gt;_IO_write_base; 65/* Avoid memory leak when there is an active ungetc buffer. */ 66Index: m4-1.4.18/lib/freadahead.c 67=================================================================== 68--- m4-1.4.18.orig/lib/freadahead.c 69+++ m4-1.4.18/lib/freadahead.c 70@@ -25,7 +25,7 @@ 71size_t 72freadahead (FILE *fp) 73{ 74-#if defined _IO_ftrylockfile || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */ 75+#if defined _IO_EOF_SEEN || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */ 76if (fp-\u0026gt;_IO_write_ptr \u0026gt; fp-\u0026gt;_IO_write_base) 77return 0; 78return (fp-\u0026gt;_IO_read_end - fp-\u0026gt;_IO_read_ptr) 79Index: m4-1.4.18/lib/freading.c 80=================================================================== 81--- m4-1.4.18.orig/lib/freading.c 82+++ m4-1.4.18/lib/freading.c 83@@ -31,7 +31,7 @@ freading (FILE *fp) 84/* Most systems provide FILE as a struct and the necessary bitmask in 85\u0026lt;stdio.h\u0026gt;, because they need it for implementing getc() and putc() as 86fast macros. */ 87-# if defined _IO_ftrylockfile || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */ 88+# if defined _IO_EOF_SEEN || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */ 89return ((fp-\u0026gt;_flags \u0026amp; _IO_NO_WRITES) != 0 90|| ((fp-\u0026gt;_flags \u0026amp; (_IO_NO_READS | _IO_CURRENTLY_PUTTING)) == 0 91\u0026amp;\u0026amp; fp-\u0026gt;_IO_read_base != NULL)); 92Index: m4-1.4.18/lib/fseeko.c 93=================================================================== 94--- m4-1.4.18.orig/lib/fseeko.c 95+++ m4-1.4.18/lib/fseeko.c 96@@ -47,7 +47,7 @@ fseeko (FILE *fp, off_t offset, int when 97#endif 98 99/* These tests are based on fpurge.c. */ 100-#if defined _IO_ftrylockfile || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */ 101+#if defined _IO_EOF_SEEN || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */ 102if (fp-\u0026gt;_IO_read_end == fp-\u0026gt;_IO_read_ptr 103\u0026amp;\u0026amp; fp-\u0026gt;_IO_write_ptr == fp-\u0026gt;_IO_write_base 104\u0026amp;\u0026amp; fp-\u0026gt;_IO_save_base == NULL) 105@@ -123,7 +123,7 @@ fseeko (FILE *fp, off_t offset, int when 106return -1; 107} 108109-#if defined _IO_ftrylockfile || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */ 110+#if defined _IO_EOF_SEEN || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */ 111fp-\u0026gt;_flags \u0026amp;= ~_IO_EOF_SEEN; 112fp-\u0026gt;_offset = pos; 113#elif defined __sferror || defined __DragonFly__ || defined __ANDROID__ 114Index: m4-1.4.18/lib/stdio-impl.h 115=================================================================== 116--- m4-1.4.18.orig/lib/stdio-impl.h 117+++ m4-1.4.18/lib/stdio-impl.h 118@@ -18,6 +18,12 @@ 119the same implementation of stdio extension API, except that some fields 120have different naming conventions, or their access requires some casts. */ 121122+/* Glibc 2.28 made _IO_IN_BACKUP private. For now, work around this 123+ problem by defining it ourselves. FIXME: Do not rely on glibc 124+ internals. */ 125+#if !defined _IO_IN_BACKUP \u0026amp;\u0026amp; defined _IO_EOF_SEEN 126+# define _IO_IN_BACKUP 0x100 127+#endif 128129/* BSD stdio derived implementations. */ Auto{Conf,Make} and Libtool This follows the standard approach outlined in the GNU Autotools FAQ:\n1cd $hpcroot 2myprefix=$HOME/.hpc/autotools 3export PATH 4wget http://ftp.gnu.org/gnu/autoconf/autoconf-2.69.tar.gz 5wget http://ftp.gnu.org/gnu/automake/automake-1.16.2.tar.gz 6wget http://ftp.gnu.org/gnu/libtool/libtool-2.4.6.tar.gz 7wget http://ftp.gnu.org/gnu/gettext/gettext-0.20.tar.gz 8wget https://gnuftp.uib.no/autoconf-archive/autoconf-archive-2021.02.19.tar.xz 9tar xfv autoconf-archive-2021.02.19.tar.xz 10gzip -dc autoconf-2.69.tar.gz | tar xvf - 11gzip -dc automake-1.16.2.tar.gz | tar xvf - 12gzip -dc libtool-2.4.6.tar.gz | tar xvf - 13gzip -dc gettext-0.20.tar.gz | tar xvf - 14cd autoconf-2.69 15./configure -C --prefix=$myprefix/autoconf/2.69 \u0026amp;\u0026amp; make -j$(nproc) \u0026amp;\u0026amp; make install 16cd ../automake-1.16.2 17./configure -C --prefix=$myprefix/automake/1.16.2 --docdir=$myprefix/automake/1.16.2/share/doc/automake-1.16.2 \u0026amp;\u0026amp; make -j$(nproc) \u0026amp;\u0026amp; make install 18cd ../autoconf-archive-2021.02.19 19./configure -C --prefix=$myprefix/automake/1.16.2 20cd ../libtool-2.4.6 21./configure -C --disable-static --prefix=$myprefix/libtool/2.4.6 \u0026amp;\u0026amp; make -j$(nproc) \u0026amp;\u0026amp; make install 22cd ../gettext-0.20 23./configure -C --prefix=$myprefix/gettext/0.20 \u0026amp;\u0026amp; make -j$(nproc) \u0026amp;\u0026amp; make install 24ml load autotools/autotools GMP 1cd $hpcroot 2myprefix=$HOME/.hpc/gcc/gmp/6.2.0 3export PATH 4wget https://gmplib.org/download/gmp/gmp-6.2.0.tar.xz 5tar xfv gmp-6.2.0.tar.xz 6cd gmp-6.2.0 7./configure --prefix=$myprefix \\ 8 --enable-cxx \\ 9 --docdir=$myprefix/doc/gmp-6.2.0 10make -j$(nproc) 11make install 12ml load gcc/gmp/6.2.0 MPFR 1cd $hpcroot 2myprefix=$HOME/.hpc/gcc/mpfr/4.1.0 3export PATH 4wget https://www.mpfr.org/mpfr-current/mpfr-4.1.0.tar.xz 5tar xfv mpfr-4.1.0.tar.xz 6cd mpfr-4.1.0 7./configure --prefix=$myprefix \\ 8 --enable-thread-safe \\ 9 --with-gmp=$HOME/.hpc/gcc/gmp/6.2.0 \\ 10 --docdir=$myprefix/doc/mpfr-4.1.0 11make -j$(nproc) 12make install 13ml load gcc/mpfr/4.1.0 MPC 1cd $hpcroot 2myprefix=$HOME/.hpc/gcc/mpc/1.2.0 3export PATH 4wget https://ftp.gnu.org/gnu/mpc/mpc-1.2.0.tar.gz 5tar xfv mpc-1.2.0.tar.gz 6cd mpc-1.2.0 7./configure --prefix=$myprefix \\ 8 --with-gmp=$HOME/.hpc/gcc/gmp/6.2.0 \\ 9 --with-mpfr=$HOME/.hpc/gcc/mpfr/4.1.0 \\ 10 --docdir=$myprefix/doc/mpc-1.2.0 11make -j$(nproc) 12make install 13ml load gcc/mpc/1.2.0 GCC 9.2.0 1cd $hpcroot 2ml load gcc/gmp gcc/mpfr gcc/mpc 3myprefix=$HOME/.hpc/gcc/9.2.0 4wget https://ftp.gnu.org/gnu/gcc/gcc-9.2.0/gcc-9.2.0.tar.xz 5tar xfv gcc-9.2.0.tar.xz 6cd gcc-9.2.0 7case $(uname -m) in 8x86_64) 9sed -e \u0026#39;/m64=/s/lib64/lib/\u0026#39; \\ 10 -i.orig gcc/config/i386/t-linux64 11;; 12esac 13mkdir -p build \u0026amp;\u0026amp; 14cd build \u0026amp;\u0026amp; 1516SED=sed \\ 17../configure --prefix=$myprefix \\ 18 --enable-languages=c,c++,fortran \\ 19 --disable-multilib \\ 20 --with-gmp=$HOME/.hpc/gcc/gmp/6.2.0 \\ 21 --with-mpfr=$HOME/.hpc/gcc/mpfr/4.1.0 \\ 22 --with-mpc=$HOME/.hpc/gcc/mpc/1.2.0 \\ 23 --disable-bootstrap \\ 24 --with-system-zlib 25export PATH 26unset LIBRARY_PATH 27export LIBRARY_PATH=/usr/lib64/ 28mkdir -p -- .deps 29make -j$(nproc) 30make install 31ml load gcc/9.2.0 Pkg-Config 1cd $hpcroot 2myprefix=$HOME/.hpc/pkg-config/0.29.2 3wget https://pkg-config.freedesktop.org/releases/pkg-config-0.29.2.tar.gz 4gzip -dc pkg-config-0.29.2.tar.gz | tar xvf - 5cd pkg-config-0.29.2 6./configure --prefix=$myprefix --with-internal-glib --disable-host-tool --docdir=$myprefix/share/doc/pkg-config-0.29.2 7mkdir $myprefix/lib 8make -j $(nproc) 9make install 10ml load pkg-config/0.29.2 Zlib 1cd $hpcroot 2myprefix=$HOME/.hpc/zlib/1.2.11 3wget http://zlib.net/zlib-1.2.11.tar.gz 4gzip -dc zlib-1.2.11.tar.gz | tar xvf - 5cd zlib-1.2.11 6./configure --prefix=$myprefix 7make -j $(nproc) 8make install 9ml load zlib/1.2.11 XZ Utils 1cd $hpcroot 2myprefix=$HOME/.hpc/xz/5.2.5 3wget https://tukaani.org/xz/xz-5.2.5.tar.gz 4gzip -dc xz-5.2.5.tar.gz | tar xvf - 5cd xz-5.2.5 6./configure --prefix=$myprefix --enable-threads=yes 7make -j $(nproc) 8make install 9ml load xz/5.2.5 OpenSSL 1cd $hpcroot 2myprefix=$HOME/.hpc/openssl/1.1.1d 3wget https://www.openssl.org/source/openssl-1.1.1d.tar.gz 4gzip -dc openssl-1.1.1d.tar.gz | tar xvf - 5cd openssl-1.1.1d 6./config --prefix=$myprefix --openssldir=$myprefix/etc/ssl shared zlib-dynamic 7make -j $(nproc) 8make install 9ml load openssl/1.1.1d cURL 1cd $hpcroot 2myprefix=$HOME/.hpc/curl/7.76.0 3wget https://curl.haxx.se/download/curl-7.76.0.tar.xz 4tar xfv curl-7.76.0.tar.xz 5cd curl-7.76.0 6./configure --prefix=$myprefix \\ 7 --disable-static \\ 8 --enable-threaded-resolver \\ 9 --with-openssl 10make -j $(nproc) 11make install 12ml load curl/7.76.0 Git This is very similar to the previous approach. However, since by default the system perl was being picked up, some slight changes have been made.\n1cd $hpcroot 2myprefix=$HOME/.hpc/git/2.9.5 3PATH=$myprefix/bin:$PATH 4export PATH 5wget https://mirrors.edge.kernel.org/pub/software/scm/git/git-2.9.5.tar.gz 6gzip -dc git-2.9.5.tar.gz | tar xvf - 7cd git-2.9.5 8./configure --with-perl=$(which perl) --with-curl=$(which curl) -C --prefix=$myprefix 9make -j $(nproc) 10make install 11ml load git/2.9.5 12ml unload openssl # System will suffice 13ml unload curl # System will suffice Caveat Also, for TRAMP, we would prefer having a more constant path, so we can set up a symlink:\n1mkdir ~/.hpc/bin 2ln ~/.hpc/git/2.9.5/bin/git ~/.hpc/bin/git Boost The boost website is utterly incomprehensible. As is the documentation. Also, fun fact, the move from svn makes things worse. This is best installed with the standard compiler present, since the B2 engine detection seems to be very hit and miss. Thankfully, a quick dive into the slightly better Github wiki led to this nugget:\n1cd $hpcroot 2git clone --recursive https://github.com/boostorg/boost.git 3cd boost 4git checkout tags/boost-1.76.0 # or whatever branch you want to use 5./bootstrap.sh 6./b2 headers This means we\u0026rsquo;re almost done!\n1./b2 2./b2 install --prefix=$HOME/.hpc/boost/boost-1.76.0 3ml load boost/boost-1.76.0 Cmake 1cd $hpcroot 2myprefix=$HOME/.hpc/cmake/3.20.1 3wget https://github.com/Kitware/CMake/releases/download/v3.20.1/cmake-3.20.1.tar.gz 4gzip -dc cmake-3.20.1.tar.gz | tar xvf - 5cd cmake-3.20.1 6CXXFLAGS=\u0026#34;-std=c++11\u0026#34; CC=$(which gcc) CXX=$(which g++) ./bootstrap --prefix=$myprefix --system-libs 7make -j $(nproc) 8make install 9ml load cmake/3.20.1 GNU-Make 1cd $hpcroot 2myprefix=$HOME/.hpc/make/4.3 3wget http://ftp.gnu.org/gnu/make/make-4.3.tar.gz 4gzip -dc make-4.3.tar.gz | tar xvf - 5cd make-4.3 6./configure --prefix=$myprefix 7make -j $(nproc) 8make install 9ml load make/4.3 Brotli 1cd $hpcroot 2myprefix=$HOME/.hpc/brotli/1.0.1 3git clone https://github.com/bagder/libbrotli 4cd libbrotli 5libtoolize 6aclocal 7autoheader 8./autogen.sh 9./configure --prefix=$myprefix 10make -j $(nproc) 11make install 12ml load brotli/1.0.1 ncurses We will need to manually ensure the paths for pkg-config are in a feasible location.\n1cd $hpcroot 2myprefix=$HOME/.hpc/ncurses/6.2 3wget https://invisible-mirror.net/archives/ncurses/ncurses-6.2.tar.gz 4gzip -dc ncurses-6.2.tar.gz | tar xvf - 5cd ncurses-6.2 6./configure --prefix=$myprefix --enable-widec --enable-pc-files --with-shared 7make -j $(nproc) 8make install 9mkdir pkgconfig 10cp misc/formw.pc misc/menuw.pc misc/ncurses++w.pc misc/ncursesw.pc misc/panelw.pc pkgconfig/ 11mv pkgconfig $myprefix/lib/ 12ml load ncurses/6.2 texinfo 1cd $hpcroot 2myprefix=$HOME/.hpc/texinfo/6.7 3wget http://ftp.gnu.org/gnu/texinfo/texinfo-6.7.tar.gz 4gzip -dc texinfo-6.7.tar.gz | tar xvf - 5cd texinfo-6.7 6./configure --prefix=$myprefix 7make -j $(nproc) 8make install 9ml load texinfo/6.7 gperf 1cd $hpcroot 2myprefix=$HOME/.hpc/gperf/3.1 3wget http://ftp.gnu.org/gnu/gperf/gperf-3.1.tar.gz 4gzip -dc gperf-3.1.tar.gz | tar xvf - 5cd gperf-3.1 6./configure --prefix=$myprefix 7make -j $(nproc) 8make install 9ml load gperf/3.1 libseccomp There is a bug, which requires modifying src/system.c to change __NR_seccomp to _nr_seccomp.\n1cd $hpcroot 2myprefix=$HOME/.hpc/libseccomp/2.5.0 3git clone https://github.com/seccomp/libseccomp 4cd libseccomp 5git checkout tags/v2.5.0 6./autogen.sh 7./configure --prefix=$myprefix 8make -j $(nproc) 9make install 10ml load libseccomp/2.5.0 Alternatively, it is easier to work with an older version.\n1cd $hpcroot 2myprefix=$HOME/.hpc/libseccomp/2.4.4 3wget https://github.com/seccomp/libseccomp/releases/download/v2.4.4/libseccomp-2.4.4.tar.gz 4tar xfv libseccomp-2.4.4.tar.gz 5cd libseccomp-2.4.4 6./configure --prefix=$myprefix 7make -j $(nproc) 8make install 9ml load libseccomp/2.4.4 BDWGC 1cd $hpcroot 2myprefix=$HOME/.hpc/bdwgc/8.0.4 3wget https://github.com/ivmai/bdwgc/releases/download/v8.0.4/gc-8.0.4.tar.gz 4gzip -dc gc-8.0.4.tar.gz | tar xvf - 5cd gc-8.0.4 6./configure --prefix=$myprefix --enable-cplusplus 7make -j $(nproc) 8make install 9ml load bdwgc/8.0.4 pcre We will prep both pcre2 and pcre.\n1cd $hpcroot 2myprefix=$HOME/.hpc/pcre2/10.35 3wget https://ftp.pcre.org/pub/pcre/pcre2-10.35.tar.gz 4gzip -dc pcre2-10.35.tar.gz | tar xvf - 5cd pcre2-10.35 6./configure --prefix=$myprefix \\ 7 --enable-pcre2-16 \\ 8 --enable-pcre2-32 \\ 9 --enable-pcre2grep-libz 10make -j $(nproc) 11make install 12ml load pcre2/10.35 1cd $hpcroot 2myprefix=$HOME/.hpc/pcre/8.44 3wget https://ftp.pcre.org/pub/pcre/pcre-8.44.tar.gz 4gzip -dc pcre-8.44.tar.gz | tar xvf - 5cd pcre-8.44 6./configure --prefix=$myprefix \\ 7 --enable-pcre-16 \\ 8 --enable-pcre-32 \\ 9 --enable-pcregrep-libz 10make -j $(nproc) 11make install 12ml load pcre/8.44 bison 1cd $hpcroot 2myprefix=$HOME/.hpc/bison/3.7.1 3wget http://ftp.gnu.org/gnu/bison/bison-3.7.1.tar.gz 4gzip -dc bison-3.7.1.tar.gz | tar xvf - 5cd bison-3.7.1 6./configure --prefix=$myprefix 7make -j $(nproc) 8make install 9ml load bison/3.7.1 flex 1cd $hpcroot 2myprefix=$HOME/.hpc/flex/2.6.4 3wget https://github.com/westes/flex/releases/download/v2.6.4/flex-2.6.4.tar.gz 4gzip -dc flex-2.6.4.tar.gz | tar xvf - 5cd flex-2.6.4 6./configure --prefix=$myprefix 7make -j $(nproc) 8make install 9ml load flex/2.6.4 jq We first need the oniguruma regular expression library.\n1cd $hpcroot 2myprefix=$HOME/.hpc/oniguruma/6.9.7.1 3git clone git@github.com:kkos/oniguruma.git 4cd oniguruma 5git checkout tags/v6.9.7.1 6libtoolize 7autoreconf -i 8./configure --prefix=$myprefix 9make -j $(nproc) 10make install 11ml load oniguruma/6.9.7.1 We will actually use the binary.\n1cd $HOME/.local/bin 2wget https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 3chmod +x jq-linux64 4mv jq-linux64 jq bzip2 Needed to manually configure it as shown here\n1cd $hpcroot 2myprefix=$HOME/.hpc/bzip2/1.0.8 3wget https://www.sourceware.org/pub/bzip2/bzip2-1.0.8.tar.gz 4gzip -dc bzip2-1.0.8.tar.gz | tar xvf - 5cd bzip2-1.0.8 6make -f Makefile-libbz2_so 7ln -sf libbz2.so.1.0 libbz2.so 8mkdir -p $myprefix/include 9mkdir -p $myprefix/lib 10cp -avf bzlib.h $myprefix/include 11cp -avf libbz2.so* $myprefix/lib 12make install PREFIX=$myprefix 13ml load bzip2/1.0.8 Actually there are a set of patches for 1.0.6 which include pkg-config support so we will use those as well.\n1cd $hpcroot 2myprefix=$HOME/.hpc/bzip2/1.0.6 3wget ftp://sourceware.org/pub/bzip2/bzip2-1.0.6.tar.gz 4tar xfz bzip2-1.0.6.tar.gz 5cd bzip2-1.0.6 6# patches.list: https://gist.github.com/steakknife/0ee85c93495ab9f9cff5e21ee12fb25b 7wget https://gist.githubusercontent.com/steakknife/946f6ee331512a269145b293cbe898cc/raw/bzip2-1.0.6-install_docs-1.patch 8wget https://gist.githubusercontent.com/steakknife/eceda09cae0cdb4900bcd9e479bab9be/raw/bzip2recover-CVE-2016-3189.patch 9wget https://gist.githubusercontent.com/steakknife/42feaa223adb4dd7c5c85f288794973c/raw/bzip2-man-page-location.patch 10wget https://gist.githubusercontent.com/steakknife/94f8aa4bfa79a3f896a660bf4e973f72/raw/bzip2-shared-make-install.patch 11wget https://gist.githubusercontent.com/steakknife/4faee8a657db9402cbeb579279156e84/raw/bzip2-pkg-config.patch 12patch -u \u0026lt; bzip2-1.0.6-install_docs-1.patch 13patch -u \u0026lt; bzip2recover-CVE-2016-3189.patch 14patch -u \u0026lt; bzip2-man-page-location.patch 15patch -u \u0026lt; bzip2-shared-make-install.patch 16patch -u \u0026lt; bzip2-pkg-config.patch 17make 18make install PREFIX=$myprefix 19ml load bzip2/1.0.6 sqlite 1cd $hpcroot 2myprefix=$HOME/.hpc/sqlite/3.32.3 3wget https://www.sqlite.org/2020/sqlite-autoconf-3320300.tar.gz 4gzip -dc sqlite-autoconf-3320300.tar.gz | tar xvf - 5cd sqlite-autoconf-3320300 6./configure --prefix=$myprefix 7make -j $(nproc) 8make install 9ml load sqlite/3.32.3 editline 1cd $hpcroot 2myprefix=$HOME/.hpc/editline/1.17.1 3wget https://github.com/troglobit/editline/releases/download/1.17.1/editline-1.17.1.tar.gz 4gzip -dc editline-1.17.1.tar.gz | tar xvf - 5cd editline-1.17.1 6./configure --prefix=$myprefix 7make -j $(nproc) 8make install 9ml load editline/1.17.1 Miniconda We don\u0026rsquo;t need this very much, but it is still useful for some edge cases, mainly revolving around jupyter infrastructure.\n1cd $HOME 2wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh 3chmod +x Miniconda3-latest-Linux-x86_64.sh 4./Miniconda3-latest-Linux-x86_64.sh 5# Do not allow it to mess up the shell rc files 6eval \u0026#34;$($HOME/miniconda3/bin/conda shell.zsh hook)\u0026#34; Note that we will prefer the manual evaluation since it can be handled in the lmod file.\nApplications Libraries and git aside, there are some tools we might want to have.\nag The silver searcher, along with rg is very useful to have.\n1cd $hpcroot 2myprefix=$HOME/.hpc/the_silver_searcher/2.2.0 3wget https://geoff.greer.fm/ag/releases/the_silver_searcher-2.2.0.tar.gz 4gzip -dc the_silver_searcher-2.2.0.tar.gz | tar xvf - 5cd the_silver_searcher-2.2.0 6./configure --prefix=$myprefix 7make -j $(nproc) 8make install 9ml load the_silver_searcher/2.2.0 Neovim 1cd $hpcroot 2myprefix=$HOME/.hpc/nvim/0.5.0 3wget https://github.com/neovim/neovim/releases/download/nightly/nvim.appimage 4chmod +x nvim.appimage 5./nvim.appimage --appimage-extract 6mkdir -p $myprefix 7mv squashfs-root/usr/* $myprefix 8ml load nvim/0.5.0 Tmux 1cd $hpcroot 2myprefix=$HOME/.hpc/tmux/3.1b 3wget https://github.com/tmux/tmux/releases/download/3.1b/tmux-3.1b-x86_64.AppImage 4chmod +x tmux-3.1b-x86_64.AppImage 5rm -rf squashfs-root 6./tmux-3.1b-x86_64.AppImage --appimage-extract 7mkdir -p $myprefix 8mv squashfs-root/usr/bin squashfs-root/usr/lib squashfs-root/usr/share $myprefix 9ml load tmux/3.1b Zsh More of an update than a requirement.\n1cd $hpcroot 2myprefix=$HOME/.hpc/zsh/5.8 3wget https://github.com/zsh-users/zsh/archive/zsh-5.8.tar.gz 4gzip -dc zsh-5.8.tar.gz | tar xvf - 5cd zsh-zsh-5.8 6autoreconf -fiv 7./configure --prefix=$myprefix 8make -j $(nproc) 9make install 10ml load zsh/5.8 Conclusion Having composed a bunch of these, I will of course try to somehow get nix up and running so it can bootstrap itself and allow me to work in peace. I might also eventually create shell scripts to automate updating these, but hopefully I can set up nix and not re-create package manager logic in lua.\n  They were called hpc2013 and hpc2010 respectively\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n I really like Lua, enough to embed it in d-SEAMS\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/hpc-dots-lmod/","tags":["workflow","projects","hpc"],"title":"HPC Dotfiles and LMod"},{"categories":["notes"],"contents":" A short post detailing the many trials and tribulations of setting brightness on multiple monitors in tandem.\n Background As regular readers might know, I have a multi-screen setup, which accounts for having touch enabled on my primary laptop screen (detailed here). A failing of this setup was that I was not able to control the brightness of both monitors at the same time.\nExisting Stack Since I use i3, my brightness control is simply done with bindsym lines as follows1:\n1bindsym XF86MonBrightnessDown exec light -U 10 2bindsym XF86MonBrightnessUp exec light -A 10 Note that to get the right bindsym I use screenkey with the keysyms preference. My software of choice was Unfortunately, my existing setup (with light, since that is in the Arch community repo) did not actually allow dimming external screens arbitarily. To be more exact,\n1light -h 1Usage: 2light [OPTIONS] [VALUE] 34Commands: 5-H, -h Show this help and exit 6-V Show program version and exit 7-L List available devices 8-A Increase brightness by value 9-U Decrease brightness by value 10-T Multiply brightness by value (can be a non-whole number, ignores raw mode) 11-S Set brightness to value 12-G Get brightness 13-N Set minimum brightness to value 14-P Get minimum brightness 15-O Save the current brightness 16-I Restore the previously saved brightness 1718Options: 19-r Interpret input and output values in raw mode (ignored for -T) 20-s Specify device target path to use, use -L to list available 21-v Specify the verbosity level (default 0) 220: Values only 231: Values, Errors. 242: Values, Errors, Warnings. 253: Values, Errors, Warnings, Notices. 2627Copyright (C) 2012 - 2018 Fredrik Haikarainen 28This is free software, see the source for copying conditions. There is NO 29warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE Clearly it is possible to target specific devices, but for arbitrary additions this is quite tough. Additionally, the project has been more or less been stuck in maintainence mode for a while now.\nMulti-head Configurations Exposing Brightness The first hurdle faced in o was actually getting the external monitor to expose the right controls. This is accomplished neatly with ddci-driver found here. As demonstrated (courtesy of the ArchWiki):\n1# Load module 2sudo modprobe ddcci_backlight 3# Check that it worked 4sudo ddcutil capabilities | grep \u0026#34;Feature: 10\u0026#34; 5sudo ddcutil getvcp 10 6# Set brightness 7sudo ddcutil setvcp 10 70 One of the obvious caveats of this technique is that sudo access or a dedicated polkit agent is required. My preferred method of loading this comes from a comment on the ddcci-driver-linux-dkms page of the AUR:\n1# Put in /etc/systemd/system/ddcci-backlight.service: 2# https://aur.archlinux.org/packages/ddcci-driver-linux-dkms/ 3# Placing \u0026#34;ddcci_backlight\u0026#34; into /etc/modules-load.d 4# leads to hang on boot with external (HDMI) monitor 5# connected to the laptop, so we need to add the module later. 67# And ddcci_backlight can\u0026#39;t detect monitor during sddm.service startup. 89[Unit] 10After=multi-user.target 11Before=sddm.service 1213[Service] 14Type=oneshot 15ExecStart=/usr/bin/modprobe ddcci_backlight 16ExecStop=/usr/bin/modprobe --remove ddcci_backlight 17RemainAfterExit=yes 1819[Install] 20WantedBy=multi-user.target This is then activated with a standard systemctl enable ddcci_backlight.service command. At this point, the device interface should be exposed to most backlight controllers.\nXrandr This is the most obvious of all methods, and does not even require the ddcci-driver. We will simply tweak the brightness with xrandr.\n1# Get devices 2xrandr | grep \u0026#34; connected\u0026#34; 3# Tweak 4xrandr --output DP1 --brightness 0.2 Note that this is an in-exact method, since it actually adjusts the gamma value instead, and it effectively tints your screen rather than modifying the brightness.\nClight clight is an excellent, highly performant alternative to redshift, but it tends to force the main screen brightness to zero. Nothing which can\u0026rsquo;t be configured away, but in practice, I work late and tend to turn off the tint anyway. This requires a daemon to be run, as well as needing to be turned on manually for i3. A very elegant additional feature gained by using clight is that external monitors are turned off automagically when lockscreens are activated.\nBrillo brillo is one of the newer controllers, and is pretty actively developed. The interface is almost exactly like light, and unlike clight there is no need to use a daemon. It meshes perfectly with a key-press based system like i3 and also has controls for keyboard LEDS as well as for smoothed ramping up and down of the brightness. Most importantly, it features an -e flag which sets the brightness across all output devices. Essentially this means my configuration is simply modified to:\n1bindsym XF86MonBrightnessDown exec brillo -e -U 10 2bindsym XF86MonBrightnessUp exec brillo -e -A 10 Conclusions tl;dr moving from light to brillo with ddcci-driver-linux-dkms was a fantastic idea.\n  My dotfiles are here\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/expl-backlight-control/","tags":["tools","workflow"],"title":"Explorations with Backlight Controllers"},{"categories":["programming"],"contents":" A meta post introducing my solutions to the fantastic excellent second edition of \u0026ldquo;Statistical Rethinking\u0026rdquo; by Richard McElreath, a.k.a. Statistical Rethinking². Also discusses strategies to keep up with the material, mostly meant for self-study groups.\n Background As detailed previously, I recently was part of a course centered around Bayesian modeling for the Icelandic COVID-19 pandemic. The Bayesian mindset needs no introduction, and this post is completely inadequete to explain why anyone should be interested (that\u0026rsquo;s what the book is for!). That said, especially for self-paced study groups, it might help to have some structure.\nSolutions These are meant to be sample solutions, and everyone should solve these for themselves. Each solution contains the packages used, as well as a colophon in the later posts to ensure reproduciblity. Essentially this consists of four posts:\n Week I Covers the first four chapters {1,2,3,4} Week II Covers the next three chapters {5,6,7} Week III Covers five chapters {9,11,12} Week IV The last five chapters {13,14}  More concisely:\n   Chapter Solutions     1. The Golem of Prague N/A   2. Small Worlds and Large Worlds here   3. Sampling the Imaginary here   4. Geocentric Models here   5. The Many Variables \u0026amp; The Spurious Waffles here   6. The Haunted DAG \u0026amp; The Causal Terror here   7. Ulysses’ Compass here   8. Conditional Manatees N/A   9. Markov Chain Monte Carlo here   10. Big Entropy and the Generalized Linear Model N/A   11. God Spiked the Integers here   12. Monsters and Mixtures here   13. Models With Memory here   14. Adventures in Covariance here   15. Missing Data and Other Opportunities TBA   16. Generalized Linear Madness TBA   17. Horoscopes N/A    Pacing The solutions compiled here were from an accelerated 4-week course covering the Statistical Rethinking² in four weeks. The book is more traditionally used in a full-semester course, so that should be kept in mind as well.\nResources These are highly opinionated and the following list is in no way complete.\nCanonical Content  Richard\u0026rsquo;s Website The YouTube 2019 playlist Richard on Twitter The Stan page  Additional Content  An introduction to Frequentist and Bayesian statistics from LLNL by Kristin Lennox A simple COVID-19 model for Iceland More complete COVID-19 model for Iceland Convergence Diagnostics for MCMC Betancourt\u0026rsquo;s Conceptual Introduction to HMC  Follow-up Courses  Bayesian Data Analysis  Conclusions This has been a short meta post which is essentially meant to collect content posted with dates in the past. Though this is not exactly a complete reference for beginners, it might still help people.\n","permalink":"https://rgoswami.me/posts/some-sol-sr2/","tags":["R","SR2","solutions","workflow"],"title":"A Short Guide to Statistical Rethinking²"},{"categories":["personal"],"contents":" Personal recollections of the academic grant writing process.\n Background Of the many types of writing one undertakes in a typical academic career, grant writing stands out as a rather large anomaly. For the purposes of this post, we will note that an academic writing taxonomy would consist of roughly:\n Coursework and Assignments These are more or less comparative writing exercises, where the only main thing which is enforced is (or should be) plagiarism checks. In terms of locality in history, these are more or less focused on the past, with little to no original content. Peer Reviewed Articles Broadly, in this category we can lump society journals, some conference articles, and even reviews to some extent. These are hyper-local in time, with enough historical perspective to make the paper worthwhile for the journal/conference, and originality of content is a key highlight. Grants Grants are unique. They are both short (in terms of a prospectus) and also long, in that there are a huge number of auxiliary files to be added.  Grants At some stage, every researcher who doesn\u0026rsquo;t bow out of academia ends up faced with the prospect of proving to a funding agency that they are capable and well-adjusted enough to get money for an extended amount of time.\nLit Surveys Unlike papers and reviews, though recent papers are important, it is more relevant to actually project where the field will be in upcoming years to ensure the deliverables are not out-dated. Additionally, it is best to link to widely cited literature, to ensure that the reviewers believe in your holistic understanding.\nDeliverables This part is fun to write, building towards a goal, is a special kind of write up. It allows one to really flesh out a research plan with realistic goals.\nExtras  For most applications, there is a budgetary requirement, mostly with a spreadsheet component. A Gantt chart is also often required  I started with teamgantt, which was neat Of course I eventually ended up regressing to orgmode and TeX using org-gantt  This actually would need a whole other post, but it is great      Conclusions If this post seemed short, it is probably because even though a lot else went into the proposal, until I hear otherwise next year, it would be presumptous to write more. That said, it was and is an enjoyable exercise.\n","permalink":"https://rgoswami.me/posts/grant-proposals/","tags":["ramblings","thoughts","academia"],"title":"Grant Proposals - I"},{"categories":["notes"],"contents":" A short tutorial post on multiple screens for laptops with touch-support and ArchLinux. Also evolved into a long rant, with an Easter egg.\n Background Of late, I have been attempting to move away from paper, for environmental reasons1. Years of touch typing in Colemak (rationale, config changes) and a very customized Emacs setup (including mathematica, temporary latex templates, Nix, and org-roam annotations) have more or less kept me away from analog devices. One might even argue that my current website is essentially a set of tricks to move my life into orgmode completely.\nHowever, there are still a few things I cannot do without a pointing device (and some kind of canvas). Scrawl squiggly lines on papers I\u0026rsquo;m reviewing. That and, scrawl weird symbols which don\u0026rsquo;t actually follow a coherent mathematical notation but might be later written up in latex to prove a point. Also, and I haven\u0026rsquo;t mastered any of the drawing systems (like Tikz) yet, so for squiggly charts I rely on Jamboard (while teaching) and Xournal++ for collaborations.\nI also happen to have a ThinkPad X380 (try sudo dmidecode -t system | grep Version) which has an in-built stylus, and since Linux support for touchscreens from 2018 is known to be incredible, I coupled this with the ThinkVision M14 as a second screen.\nX-Windows and X-ternal Screens We will define two separate solutions:\n mons Using arbitrary external monitors autorandr Setting up profiles for specific monitors  Finally we will leverage both to ensure a constant touchscreen area.\nAutorandr I use the python rewrite simply because that\u0026rsquo;s the one which is in the ArchLinux community repo. To be honest, I came across this before I (re-)discovered mons. The most relevant aspect of autorandr is using complicated configurations for different monitors, but it also does a mean job of running generic scripts as postswitch and prefix scripts.\nMons xrandr is awesome. Unfortunately, more often than not, I forget the commands to interact with it appropriately. mons does my dirty work for me2.\n1# -e is extend 2mons -e left 3# puts screen on the left That and the similar right option, covers around 99% of all possible dual screen use-cases.\nConstant Touch The problem is that by default, the entire combined screen area is assumed to be touch-enabled, which essentially means an awkward area of the screen which is dead to all input (since it doesn\u0026rsquo;t exist). The key insight is that I never have more touch-enabled surfaces than my default screen, no matter how many extended screens are present. So the solution is:\n Make sure the touch area is constant.\n We need to figure out what the touch input devices are:\n1xinput --list    ⎡ Virtual core pointer id=2 [master pointer (3)]     ⎜ ↳ Virtual core XTEST pointer id=4 [slave pointer (2)]   ⎜ ↳ Wacom Pen and multitouch sensor Finger touch id=10 [slave pointer (2)]   ⎜ ↳ Wacom Pen and multitouch sensor Pen stylus id=11 [slave pointer (2)]   ⎜ ↳ ETPS/2 Elantech TrackPoint id=14 [slave pointer (2)]   ⎜ ↳ ETPS/2 Elantech Touchpad id=15 [slave pointer (2)]   ⎜ ↳ Wacom Pen and multitouch sensor Pen eraser id=17 [slave pointer (2)]   ⎣ Virtual core keyboard id=3 [master keyboard (2)]   ↳ Virtual core XTEST keyboard id=5 [slave keyboard (3)]   ↳ Power Button id=6 [slave keyboard (3)]   ↳ Video Bus id=7 [slave keyboard (3)]   ↳ Power Button id=8 [slave keyboard (3)]   ↳ Sleep Button id=9 [slave keyboard (3)]   ↳ Integrated Camera: Integrated C id=12 [slave keyboard (3)]   ↳ AT Translated Set 2 keyboard id=13 [slave keyboard (3)]   ↳ ThinkPad Extra Buttons id=16 [slave keyboard (3)]    At this point we will leverage autorandr to ensure that these devices are mapped to the primary (touch-enabled) screen with a postswitch script. This postswitch script needs to be:\n1#!/bin/sh 2# .config/autorandr/postswitch 3xinput --map-to-output \u0026#39;Wacom Pen and multitouch sensor Finger touch\u0026#39; eDP1 4xinput --map-to-output \u0026#39;Wacom Pen and multitouch sensor Pen stylus\u0026#39; eDP1 5xinput --map-to-output \u0026#39;Wacom Pen and multitouch sensor Pen eraser\u0026#39; eDP1 6notify-send \u0026#34;Screen configuration changed\u0026#34; The last line of course is really more of an informative boast.\nAt this stage, we have the ability to set the touchscreens up by informing autorandr that our configuration has changed, through the command line for example:\n1autorandr --change Automatic Touch Configuration Running a command manually on-change is the sort of thing which makes people think Linux is hard or un-intuitive. So we will instead make use of the incredibly powerful systemd framework for handling events.\nEssentially, we combine our existing workflow with autorandr-launcher from here, and then set it all up as follows:\n1git clone https://github.com/smac89/autorandr-launcher.git 2cd autorandr-launcher 3sudo make install 4sudo systemctl--user enable autorandr_launcher.service Conclusion We now have a setup which ensures that the touch enabled area is constant, without any explicit manual interventions for when devices are added or removed. There isn\u0026rsquo;t much else to say about this workflow here. Additional screens can be configured from older laptops described here. Separate posts can deal with how exactly I meld Zotero, org-roam and Xournal++ to wreak havoc on all kinds of documents. So, in-lieu of a conclusion, behold a recent scribble with this setup:\n Figure 1: From the planning of this voluntary course\n    Also because paper is difficult to keep track of and is essentially the antithesis of a computer oriented workflow.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n I actually planned a whole post called \u0026ldquo;An Ode to Mons\u0026rdquo;, when I first found out about it.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/multi-monitor-touch/","tags":["tools","workflow"],"title":"Multiple Monitors with Touchscreens"},{"categories":["notes"],"contents":" A follow up to my earlier post on analytics, and on migrating from Goat Counter to Clicky.\n Background A few days ago I recieved the following email:\n Hi there!\nI made some changes to the GoatCounter plans/pricing:\n  GoatCounter now has a \u0026ldquo;Starter\u0026rdquo; plan, this is €5/month, limited to 100k pageviews/month, comes with a custom domain, and allow commercial use. This is mostly the same as the \u0026ldquo;personal plus\u0026rdquo; plan there was before, except that it allows commercial use. If you had a \u0026ldquo;personal plus\u0026rdquo; for a custom domain before then you now have a Starter plan.\n  Starting on August 1st the data retention will be limited for the Free and Starter plans: the Free plan will be limited to 6 months, the Starter plan to 12 months, and the business plans remain unlimited. There is an export feature if you wish to retain your old pageviews.\n  Some background on this:\nThere seems to be a gap between \u0026ldquo;free for personal use\u0026rdquo; and \u0026ldquo;€15/month for commercial use\u0026rdquo;. I\u0026rsquo;ve gotten quite a bit of feedback of small (potential) commercial users who just run a small website, where €15/month really is prohibitively expensive.\nThe entire idea behind making it free for personal use is that I\u0026rsquo;d like GoatCounter to be usable by as many people as possible, while also ensuring commercial users pay their fair share. Redistributing software is free, but developing it is not.\nThe general thinking is that larger businesses with several employees (who can easily afford €15/month) will have more than 100k pageviews/month, whereas for most startups and the like 100k should be more than enough.\nThe entire thing is a bit of a balancing act 😅 I may tweak the pricing further in the future based on additional experience and feedback.\nAs for the data retention: the biggest issue here is that some not-especially-active sites have had short bursts of millions of pageviews in a short time because they wrote or made something that got widely shared.\nThe hits/months limit isn\u0026rsquo;t strictly enforced because I don\u0026rsquo;t want to tell people to get a plan just because they wrote a popular article that got to the front page of HN, Reddit, Twitter, etc, and GoatCounter has no problem handling these levels of pageviews, so that\u0026rsquo;s all fine.\nBut on the other hand, a million pageviews currently takes up about 400M of disk space including backups (although this could probably be reduced a bit with a more clever backup strategy). Disk space is pretty cheap, but it does add up.\nIt also means more effort on scaling GoatCounter; limiting the data retention is an easy way to reduce the pressure on this. It also gives people a bit more incentive to get a plan 😅\nAs always, self-hosting isn\u0026rsquo;t affected by any of this. This just applies to the goatcounter.com service.\nFeel free to let me know if you\u0026rsquo;ve got any questions or feedback.\nCheers, Martin\n Personal Repercussions I should point out that I unequivocally support Martin\u0026rsquo;s decision. It is fair and equitable. That said, continuity is super important to me. I\u0026rsquo;ve mentioned earlier that for me, the daily limit is not much of an issue but I do like to look back at my collective history.\nGoat Counter Goat Counter is still definitely my go-to option for both self-hosting and shelling out, if the 15 euro fee is acceptable. Honestly, the best option is probably opening a PR or making a tool to aggregate metrics offline, since it is still possible to export the data. The major drawback is the six month window.\nClicky Clicky is pretty great, and they have a good example of what works out in their favor compared to Google Analytics.\nPros  Free tier has no time limit Has a nice dark theme  Cons  Not open source Capped at 3K daily views Blocked by some VPN providers (Windscribe)  Conclusions It is unfortunate to have had to move, since this does imply losing the past eight months of metrics. Eventually I might even go back to steady dependable Google Analytics. Until this, Clicky will do.\n","permalink":"https://rgoswami.me/posts/goat-clicky/","tags":["tools","rationale","workflow","webdev"],"title":"Analytics II: Goat to Clicky"},{"categories":["personal"],"contents":" A post on a surprisingly heartwarming community appreciation effort.\n Background As probably anyone who has asked me about my programming experience has heard, my first real foray into the FOSS community was being a LineageOS co-maintainer (as HaoZeke) for the Xperia Z5 Dual. I haven\u0026rsquo;t thought about the community all that much for a few years, mostly since XDA became pretty toxic, and Android development just got, less exciting.\nThe Email I recieved two of these from different accounts:\n Thank you so much for your contribution to making my phone the phone it is!\nHave a great day!\n  Conclusion This was completely unexpected, and really made my day. In reterospect this seems like something which should be made more explicit, more often.\n","permalink":"https://rgoswami.me/posts/linos-maintainer-appre/","tags":["ramblings","thoughts"],"title":"LineageOS Maintainer Appreciation"},{"categories":["programming"],"contents":" Setup details are described here, and the meta-post about these solutions is here.\n Materials The summmer course1 is based off of the second edition of Statistical Rethinking by Richard McElreath.\n Chapter 13  E{1,2,3,4,5}   Chapter 14  E{1,2,3}    Packages A colophon with details is provided at the end, but the following packages and theme parameters are used throughout.\n1libsUsed\u0026lt;-c(\u0026#34;tidyverse\u0026#34;,\u0026#34;tidybayes\u0026#34;,\u0026#34;orgutils\u0026#34;,\u0026#34;dagitty\u0026#34;, 2\u0026#34;rethinking\u0026#34;,\u0026#34;tidybayes.rethinking\u0026#34;, 3\u0026#34;ggplot2\u0026#34;,\u0026#34;kableExtra\u0026#34;,\u0026#34;dplyr\u0026#34;,\u0026#34;glue\u0026#34;, 4\u0026#34;latex2exp\u0026#34;,\u0026#34;data.table\u0026#34;,\u0026#34;printr\u0026#34;,\u0026#34;devtools\u0026#34;) 5invisible(lapply(libsUsed, library, character.only = TRUE)); 6theme_set(theme_grey(base_size=24)) 7set.seed(1995) Chapter XIII: Models With Memory Easy Questions (Ch13) HOLD 13E1 Which of the following priors will produce more shrinkage in the estimates? (a) \\(α_{\\mathrm{TANK}}∼\\mathrm{Normal}(0,1)\\); (b) \\(α_{\\mathrm{TANK}}∼\\mathrm{Normal}(0,2)\\).\nSolution The normal distribution fits a probability distribution centered around the mean and the spread is given by the standard deviation. Thus the first option, (a) will produce more shrinkage in the estimates, as the prior will be more concentrated.\n1curve(dnorm(x,0,1),from=-10,to=10,col=\u0026#34;red\u0026#34;,ylab=\u0026#34;density\u0026#34;) 2curve(dnorm(x,0,2),add=TRUE) 3legend(\u0026#34;topright\u0026#34;, 4col = c(\u0026#34;red\u0026#34;,\u0026#34;black\u0026#34;), 5pch = 19, 6legend = c(\u0026#34;Normal(0,1)\u0026#34;,\u0026#34;Normal(0,2)\u0026#34;))  13E2 Rewrite the following model as a multilevel model.\nSolution The model can be expressed as:\nThe priors have been chosen to be essentially uninformative, as is appropriate for a situation where no further insight is present for the hyperparameters.\n13E3 Rewrite the following model as a multilevel model.\nSolution The model can be defined as:\nHOLD 13E4 Write a mathematical model formula for a Poisson regression with varying intercepts.\nSolution 13E5 Write a mathematical model formula for a Poisson regression with two different kinds of varying intercepts, a cross-classified model.\nSolution We will use the non-centered form for the cross-classified model.\nChapter XIV: Adventures in Covariance Easy Questions (Ch14) HOLD 14E1 Add to the following model varying slopes on the predictor \\(x\\).\nSolution Following the convention in the physical sciences, I will use square brackets for matrices and parenthesis for vectors.\nWhere we do not have any information so have used a standard weakly informative LKJcorr prior for correlation matrices which is flat for all valid correlation matrices. We also use weakly uninformative priors for the standard deviations among slopes and intercepts.\nHOLD 14E2 Think up a context in which varying intercepts will be positively correlated with varying slopes. Provide a mechanistic explanation for the correlation.\nSolution We note at the onset that the concept of varying intercepts is to account for blocks or sub-groups in our problem. This means that the clusters in our data which have higher average values will show a stronger positive association with predictor variables. To augment the example of the tadpoles in the book, if the data is arranged as:\n Tadpoles in tanks Some tanks have larger tadpoles (different species) which grow faster  For a repeated measurement in an interval of time, there will be a positive correlation between the initial height and the slope.\nHOLD 14E3 When is it possible for a varying slopes model to have fewer effective parameters (as estimated by WAIC or PSIS) than the corresponding model with fixed (unpooled) slopes? Explain.\nSolution The varying effects essentially causes regularization or shrinkage towards the global mean to prevent overfitting to the individual data-points. Consider the example from the text, for the chimpanzee experiment.\n1data(chimpanzees) 2d \u0026lt;- chimpanzees 3d$block_id \u0026lt;- d$block 4d$treatment \u0026lt;- 1L + d$prosoc_left + 2L*d$condition 5dat \u0026lt;- list( 6L = d$pulled_left, 7tid = as.integer(d$treatment), 8actor = d$actor ) We will set up a simple fixed effects model.\n1m14fix \u0026lt;- ulam( 2alist( 3L ~ dbinom( 1 , p ) , 4logit(p) \u0026lt;- alpha[actor] + beta[tid] , 5alpha[actor] ~ dnorm( 0 , 5 ), 6beta[tid] ~ dnorm( 0 , 0.5 ) 7) , data=dat , chains=4 , log_lik=TRUE ) 1SAMPLING FOR MODEL \u0026#39;90fe1cae14bc2bf32f08b4d71c2d1f0d\u0026#39; NOW (CHAIN 1). 2Chain 1: 3Chain 1: Gradient evaluation took 9.2e-05 seconds 4Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.92 seconds. 5Chain 1: Adjust your expectations accordingly! 6Chain 1: 7Chain 1: 8Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) 9Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) 10Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) 11Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) 12Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) 13Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) 14Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) 15Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) 16Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) 17Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) 18Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) 19Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) 20Chain 1: 21Chain 1: Elapsed Time: 0.335488 seconds (Warm-up) 22Chain 1: 0.228533 seconds (Sampling) 23Chain 1: 0.564021 seconds (Total) 24Chain 1: 2526SAMPLING FOR MODEL \u0026#39;90fe1cae14bc2bf32f08b4d71c2d1f0d\u0026#39; NOW (CHAIN 2). 27Chain 2: 28Chain 2: Gradient evaluation took 3.4e-05 seconds 29Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.34 seconds. 30Chain 2: Adjust your expectations accordingly! 31Chain 2: 32Chain 2: 33Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup) 34Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) 35Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) 36Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) 37Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) 38Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) 39Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) 40Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) 41Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) 42Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) 43Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) 44Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) 45Chain 2: 46Chain 2: Elapsed Time: 0.365676 seconds (Warm-up) 47Chain 2: 0.330942 seconds (Sampling) 48Chain 2: 0.696618 seconds (Total) 49Chain 2: 5051SAMPLING FOR MODEL \u0026#39;90fe1cae14bc2bf32f08b4d71c2d1f0d\u0026#39; NOW (CHAIN 3). 52Chain 3: 53Chain 3: Gradient evaluation took 4.3e-05 seconds 54Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.43 seconds. 55Chain 3: Adjust your expectations accordingly! 56Chain 3: 57Chain 3: 58Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) 59Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) 60Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) 61Chain 3: Iteration: 300 / 1000 [ 30%] (Warmup) 62Chain 3: Iteration: 400 / 1000 [ 40%] (Warmup) 63Chain 3: Iteration: 500 / 1000 [ 50%] (Warmup) 64Chain 3: Iteration: 501 / 1000 [ 50%] (Sampling) 65Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) 66Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) 67Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) 68Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) 69Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) 70Chain 3: 71Chain 3: Elapsed Time: 0.356879 seconds (Warm-up) 72Chain 3: 0.352045 seconds (Sampling) 73Chain 3: 0.708924 seconds (Total) 74Chain 3: 7576SAMPLING FOR MODEL \u0026#39;90fe1cae14bc2bf32f08b4d71c2d1f0d\u0026#39; NOW (CHAIN 4). 77Chain 4: 78Chain 4: Gradient evaluation took 4.5e-05 seconds 79Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.45 seconds. 80Chain 4: Adjust your expectations accordingly! 81Chain 4: 82Chain 4: 83Chain 4: Iteration: 1 / 1000 [ 0%] (Warmup) 84Chain 4: Iteration: 100 / 1000 [ 10%] (Warmup) 85Chain 4: Iteration: 200 / 1000 [ 20%] (Warmup) 86Chain 4: Iteration: 300 / 1000 [ 30%] (Warmup) 87Chain 4: Iteration: 400 / 1000 [ 40%] (Warmup) 88Chain 4: Iteration: 500 / 1000 [ 50%] (Warmup) 89Chain 4: Iteration: 501 / 1000 [ 50%] (Sampling) 90Chain 4: Iteration: 600 / 1000 [ 60%] (Sampling) 91Chain 4: Iteration: 700 / 1000 [ 70%] (Sampling) 92Chain 4: Iteration: 800 / 1000 [ 80%] (Sampling) 93Chain 4: Iteration: 900 / 1000 [ 90%] (Sampling) 94Chain 4: Iteration: 1000 / 1000 [100%] (Sampling) 95Chain 4: 96Chain 4: Elapsed Time: 0.296276 seconds (Warm-up) 97Chain 4: 0.261692 seconds (Sampling) 98Chain 4: 0.557968 seconds (Total) 99Chain 4: Now we can extend this to a varying slopes model, where we will consider varying slopes for actors.\n1m14ppool \u0026lt;- ulam( 2alist( 3L ~ dbinom( 1 , p ) , 4logit(p) \u0026lt;- alpha + a[actor]*vary_id + beta[tid], 5alpha ~ dnorm( 0 , 5 ), 6a[actor] ~ dnorm( 0 , 1 ), 7beta[tid] ~ dnorm( 0 , 0.5 ), 8vary_id ~ dexp( 1 ) 9) , data=dat , chains=4 , log_lik=TRUE ) 1SAMPLING FOR MODEL \u0026#39;d14d0bbe9399ac4de917b8e279a6e9e5\u0026#39; NOW (CHAIN 1). 2Chain 1: 3Chain 1: Gradient evaluation took 0.000184 seconds 4Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.84 seconds. 5Chain 1: Adjust your expectations accordingly! 6Chain 1: 7Chain 1: 8Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) 9Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) 10Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) 11Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) 12Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) 13Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) 14Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) 15Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) 16Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) 17Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) 18Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) 19Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) 20Chain 1: 21Chain 1: Elapsed Time: 1.63295 seconds (Warm-up) 22Chain 1: 1.18708 seconds (Sampling) 23Chain 1: 2.82003 seconds (Total) 24Chain 1: 2526SAMPLING FOR MODEL \u0026#39;d14d0bbe9399ac4de917b8e279a6e9e5\u0026#39; NOW (CHAIN 2). 27Chain 2: 28Chain 2: Gradient evaluation took 6.3e-05 seconds 29Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.63 seconds. 30Chain 2: Adjust your expectations accordingly! 31Chain 2: 32Chain 2: 33Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup) 34Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) 35Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) 36Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) 37Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) 38Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) 39Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) 40Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) 41Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) 42Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) 43Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) 44Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) 45Chain 2: 46Chain 2: Elapsed Time: 1.58303 seconds (Warm-up) 47Chain 2: 1.30156 seconds (Sampling) 48Chain 2: 2.88459 seconds (Total) 49Chain 2: 5051SAMPLING FOR MODEL \u0026#39;d14d0bbe9399ac4de917b8e279a6e9e5\u0026#39; NOW (CHAIN 3). 52Chain 3: 53Chain 3: Gradient evaluation took 6.4e-05 seconds 54Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.64 seconds. 55Chain 3: Adjust your expectations accordingly! 56Chain 3: 57Chain 3: 58Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) 59Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) 60Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) 61Chain 3: Iteration: 300 / 1000 [ 30%] (Warmup) 62Chain 3: Iteration: 400 / 1000 [ 40%] (Warmup) 63Chain 3: Iteration: 500 / 1000 [ 50%] (Warmup) 64Chain 3: Iteration: 501 / 1000 [ 50%] (Sampling) 65Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) 66Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) 67Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) 68Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) 69Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) 70Chain 3: 71Chain 3: Elapsed Time: 1.37077 seconds (Warm-up) 72Chain 3: 1.12109 seconds (Sampling) 73Chain 3: 2.49186 seconds (Total) 74Chain 3: 7576SAMPLING FOR MODEL \u0026#39;d14d0bbe9399ac4de917b8e279a6e9e5\u0026#39; NOW (CHAIN 4). 77Chain 4: 78Chain 4: Gradient evaluation took 5.9e-05 seconds 79Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.59 seconds. 80Chain 4: Adjust your expectations accordingly! 81Chain 4: 82Chain 4: 83Chain 4: Iteration: 1 / 1000 [ 0%] (Warmup) 84Chain 4: Iteration: 100 / 1000 [ 10%] (Warmup) 85Chain 4: Iteration: 200 / 1000 [ 20%] (Warmup) 86Chain 4: Iteration: 300 / 1000 [ 30%] (Warmup) 87Chain 4: Iteration: 400 / 1000 [ 40%] (Warmup) 88Chain 4: Iteration: 500 / 1000 [ 50%] (Warmup) 89Chain 4: Iteration: 501 / 1000 [ 50%] (Sampling) 90Chain 4: Iteration: 600 / 1000 [ 60%] (Sampling) 91Chain 4: Iteration: 700 / 1000 [ 70%] (Sampling) 92Chain 4: Iteration: 800 / 1000 [ 80%] (Sampling) 93Chain 4: Iteration: 900 / 1000 [ 90%] (Sampling) 94Chain 4: Iteration: 1000 / 1000 [100%] (Sampling) 95Chain 4: 96Chain 4: Elapsed Time: 1.48561 seconds (Warm-up) 97Chain 4: 1.38809 seconds (Sampling) 98Chain 4: 2.8737 seconds (Total) 99Chain 4: Now we can test the number of parameters.\n1compare(m14ppool,m14fix) %\u0026gt;% toOrg 1| row.names | WAIC | SE | dWAIC | dSE | pWAIC | weight | 2|-----------+------------------+------------------+------------------+------------------+------------------+-------------------| 3| m14ppool | 532.211705503729 | 19.5177343184252 | 0 | NA | 9.12911563615787 | 0.796616007296431 | 4| m14fix | 534.942259470042 | 18.0912938913487 | 2.73055396631241 | 1.66292448384092 | 8.10370201332515 | 0.203383992703568 | As we can see, the model with partial pooling has only one effective additional parameter, even though the model without pooling has \\(n(\\mathrm{actor})\\) intercepts (one per actor) with a standard deviation, while the partial pooling parameter has an additional average intercept and a standard deviation parameter.\nBoth the models have around the same number of effective parameters, which mean that the additional parameters do not actually cause additional overfitting. This simply implies that for Bayesian models, the raw number of model parameters does not correspond necessarily to a model with more overfitting.\nIn general, we should keep in mind that the effective number of parameters, when the variation among clusters is high, is probably going to be lower than the total number of parameters, due to adaptive regularization.\nA: Colophon To ensure that this document is fully reproducible at a later date, we will record the session info.\n1devtools::session_info() 1─ Session info ─────────────────────────────────────────────────────────────── 2setting value 3version R version 4.0.0 (2020-04-24) 4os Arch Linux 5system x86_64, linux-gnu 6ui X11 7language (EN) 8collate en_US.UTF-8 9ctype en_US.UTF-8 10tz Iceland 11date 2020-06-27 1213─ Packages ─────────────────────────────────────────────────────────────────── 14package * version date lib source 15arrayhelpers 1.1-0 2020-02-04 [167] CRAN (R 4.0.0) 16assertthat 0.2.1 2019-03-21 [34] CRAN (R 4.0.0) 17backports 1.1.6 2020-04-05 [68] CRAN (R 4.0.0) 18boot 1.3-24 2019-12-20 [5] CRAN (R 4.0.0) 19broom 0.5.6 2020-04-20 [67] CRAN (R 4.0.0) 20callr 3.4.3 2020-03-28 [87] CRAN (R 4.0.0) 21cellranger 1.1.0 2016-07-27 [55] CRAN (R 4.0.0) 22cli 2.0.2 2020-02-28 [33] CRAN (R 4.0.0) 23coda 0.19-3 2019-07-05 [169] CRAN (R 4.0.0) 24colorspace 1.4-1 2019-03-18 [97] CRAN (R 4.0.0) 25crayon 1.3.4 2017-09-16 [35] CRAN (R 4.0.0) 26curl 4.3 2019-12-02 [26] CRAN (R 4.0.0) 27dagitty * 0.2-2 2016-08-26 [244] CRAN (R 4.0.0) 28data.table * 1.12.8 2019-12-09 [27] CRAN (R 4.0.0) 29DBI 1.1.0 2019-12-15 [77] CRAN (R 4.0.0) 30dbplyr 1.4.3 2020-04-19 [76] CRAN (R 4.0.0) 31desc 1.2.0 2018-05-01 [84] CRAN (R 4.0.0) 32devtools * 2.3.0 2020-04-10 [219] CRAN (R 4.0.0) 33digest 0.6.25 2020-02-23 [42] CRAN (R 4.0.0) 34dplyr * 0.8.5 2020-03-07 [69] CRAN (R 4.0.0) 35ellipsis 0.3.0 2019-09-20 [30] CRAN (R 4.0.0) 36evaluate 0.14 2019-05-28 [82] CRAN (R 4.0.0) 37fansi 0.4.1 2020-01-08 [36] CRAN (R 4.0.0) 38forcats * 0.5.0 2020-03-01 [29] CRAN (R 4.0.0) 39fs 1.4.1 2020-04-04 [109] CRAN (R 4.0.0) 40generics 0.0.2 2018-11-29 [71] CRAN (R 4.0.0) 41ggplot2 * 3.3.0 2020-03-05 [78] CRAN (R 4.0.0) 42glue * 1.4.0 2020-04-03 [37] CRAN (R 4.0.0) 43gridExtra 2.3 2017-09-09 [123] CRAN (R 4.0.0) 44gtable 0.3.0 2019-03-25 [79] CRAN (R 4.0.0) 45haven 2.2.0 2019-11-08 [28] CRAN (R 4.0.0) 46hms 0.5.3 2020-01-08 [44] CRAN (R 4.0.0) 47htmltools 0.4.0 2019-10-04 [112] CRAN (R 4.0.0) 48httr 1.4.1 2019-08-05 [100] CRAN (R 4.0.0) 49inline 0.3.15 2018-05-18 [162] CRAN (R 4.0.0) 50jsonlite 1.6.1 2020-02-02 [101] CRAN (R 4.0.0) 51kableExtra * 1.1.0 2019-03-16 [212] CRAN (R 4.0.0) 52knitr 1.28 2020-02-06 [113] CRAN (R 4.0.0) 53latex2exp * 0.4.0 2015-11-30 [211] CRAN (R 4.0.0) 54lattice 0.20-41 2020-04-02 [6] CRAN (R 4.0.0) 55lifecycle 0.2.0 2020-03-06 [38] CRAN (R 4.0.0) 56loo 2.2.0 2019-12-19 [163] CRAN (R 4.0.0) 57lubridate 1.7.8 2020-04-06 [106] CRAN (R 4.0.0) 58magrittr 1.5 2014-11-22 [21] CRAN (R 4.0.0) 59MASS 7.3-51.5 2019-12-20 [7] CRAN (R 4.0.0) 60matrixStats 0.56.0 2020-03-13 [164] CRAN (R 4.0.0) 61memoise 1.1.0 2017-04-21 [229] CRAN (R 4.0.0) 62modelr 0.1.6 2020-02-22 [107] CRAN (R 4.0.0) 63munsell 0.5.0 2018-06-12 [96] CRAN (R 4.0.0) 64mvtnorm 1.1-0 2020-02-24 [243] CRAN (R 4.0.0) 65nlme 3.1-147 2020-04-13 [11] CRAN (R 4.0.0) 66orgutils * 0.4-1 2017-03-21 [209] CRAN (R 4.0.0) 67pillar 1.4.3 2019-12-20 [39] CRAN (R 4.0.0) 68pkgbuild 1.0.6 2019-10-09 [86] CRAN (R 4.0.0) 69pkgconfig 2.0.3 2019-09-22 [43] CRAN (R 4.0.0) 70pkgload 1.0.2 2018-10-29 [83] CRAN (R 4.0.0) 71plyr 1.8.6 2020-03-03 [73] CRAN (R 4.0.0) 72prettyunits 1.1.1 2020-01-24 [58] CRAN (R 4.0.0) 73printr * 0.1 2017-05-19 [214] CRAN (R 4.0.0) 74processx 3.4.2 2020-02-09 [88] CRAN (R 4.0.0) 75ps 1.3.2 2020-02-13 [89] CRAN (R 4.0.0) 76purrr * 0.3.4 2020-04-17 [50] CRAN (R 4.0.0) 77R6 2.4.1 2019-11-12 [48] CRAN (R 4.0.0) 78Rcpp 1.0.4.6 2020-04-09 [10] CRAN (R 4.0.0) 79readr * 1.3.1 2018-12-21 [45] CRAN (R 4.0.0) 80readxl 1.3.1 2019-03-13 [54] CRAN (R 4.0.0) 81remotes 2.1.1 2020-02-15 [233] CRAN (R 4.0.0) 82reprex 0.3.0 2019-05-16 [108] CRAN (R 4.0.0) 83rethinking * 2.01 2020-06-06 [242] local 84rlang 0.4.5 2020-03-01 [31] CRAN (R 4.0.0) 85rmarkdown 2.1 2020-01-20 [110] CRAN (R 4.0.0) 86rprojroot 1.3-2 2018-01-03 [85] CRAN (R 4.0.0) 87rstan * 2.19.3 2020-02-11 [161] CRAN (R 4.0.0) 88rstudioapi 0.11 2020-02-07 [91] CRAN (R 4.0.0) 89rvest 0.3.5 2019-11-08 [120] CRAN (R 4.0.0) 90scales 1.1.0 2019-11-18 [93] CRAN (R 4.0.0) 91sessioninfo 1.1.1 2018-11-05 [231] CRAN (R 4.0.0) 92shape 1.4.4 2018-02-07 [193] CRAN (R 4.0.0) 93StanHeaders * 2.19.2 2020-02-11 [165] CRAN (R 4.0.0) 94stringi 1.4.6 2020-02-17 [52] CRAN (R 4.0.0) 95stringr * 1.4.0 2019-02-10 [74] CRAN (R 4.0.0) 96svUnit 1.0.3 2020-04-20 [168] CRAN (R 4.0.0) 97testthat 2.3.2 2020-03-02 [81] CRAN (R 4.0.0) 98textutils 0.2-0 2020-01-07 [210] CRAN (R 4.0.0) 99tibble * 3.0.1 2020-04-20 [32] CRAN (R 4.0.0) 100tidybayes * 2.0.3 2020-04-04 [166] CRAN (R 4.0.0) 101tidybayes.rethinking * 2.0.3.9000 2020-06-07 [246] local 102tidyr * 1.0.2 2020-01-24 [75] CRAN (R 4.0.0) 103tidyselect 1.0.0 2020-01-27 [49] CRAN (R 4.0.0) 104tidyverse * 1.3.0 2019-11-21 [66] CRAN (R 4.0.0) 105usethis * 1.6.0 2020-04-09 [238] CRAN (R 4.0.0) 106V8 3.0.2 2020-03-14 [245] CRAN (R 4.0.0) 107vctrs 0.2.4 2020-03-10 [41] CRAN (R 4.0.0) 108viridisLite 0.3.0 2018-02-01 [99] CRAN (R 4.0.0) 109webshot 0.5.2 2019-11-22 [213] CRAN (R 4.0.0) 110withr 2.2.0 2020-04-20 [90] CRAN (R 4.0.0) 111xfun 0.13 2020-04-13 [116] CRAN (R 4.0.0) 112xml2 1.3.2 2020-04-23 [122] CRAN (R 4.0.0) 113114[1] /nix/store/xzd8h53xkyvfm3kvj5ab6znp685wi04w-r-car-3.0-7/library 115[2] /nix/store/mhr8zw9bmxarc3n821b83i0gz2j9zlrq-r-abind-1.4-5/library 116[3] /nix/store/hp86nhr0787vib3l8mkw0gf9nxwb45im-r-carData-3.0-3/library 117[4] /nix/store/vhw7s2h5ds6sp110z2yvilchv8j9jch5-r-lme4-1.1-23/library 118[5] /nix/store/987n8g0zy9sjvfvnsck1bkkcknw05yvb-r-boot-1.3-24/library 119[6] /nix/store/jxxxxyz4c1k5g3drd35gsrbjdg028d11-r-lattice-0.20-41/library 120[7] /nix/store/q9zfm5h53m8rd08xcsdcwaag31k4z1pf-r-MASS-7.3-51.5/library 121[8] /nix/store/kjkm50sr144yvrhl5axfgykbiy13pbmg-r-Matrix-1.2-18/library 122[9] /nix/store/8786z5lgy8h3akfjgj3yq5yq4s17rhjy-r-minqa-1.2.4/library 123[10] /nix/store/93wv3j0z1nzqp6fjsm9v7v8bf8d1xkm2-r-Rcpp-1.0.4.6/library 124[11] /nix/store/akfw6zsmawmz8lmjkww0rnqrazm4mqp0-r-nlme-3.1-147/library 125[12] /nix/store/rxs0d9bbn8qhw7wmkfb21yk5abp6lpq1-r-nloptr-1.2.2.1/library 126[13] /nix/store/8n0jfiqn4275i58qgld0dv8zdaihdzrk-r-RcppEigen-0.3.3.7.0/library 127[14] /nix/store/8vxrma33rhc96260zsi1jiw7dy3v2mm4-r-statmod-1.4.34/library 128[15] /nix/store/2y46pb5x9lh8m0hdmzajnx7sc1bk9ihl-r-maptools-0.9-9/library 129[16] /nix/store/iwf9nxx1v883wlv0p88q947hpz5lhfh7-r-foreign-0.8-78/library 130[17] /nix/store/rl9sjqply6rjbnz5k792ghm62ybv76px-r-sp-1.4-1/library 131[18] /nix/store/ws4bkzyv2vj5pyn1hgwyy6nlp48arz0n-r-mgcv-1.8-31/library 132[19] /nix/store/307dzxrmnqk4p86560a02r64x1fhhmxb-r-nnet-7.3-13/library 133[20] /nix/store/g2zpzkdb9hzkza1wpcbrk58119v1wyaf-r-pbkrtest-0.4-8.6/library 134[21] /nix/store/p0l503fr8960vld70w6ilmknxs5qwq77-r-magrittr-1.5/library 135[22] /nix/store/rmjpcaw3i446kwnjgcxcaid0yac36cj2-r-quantreg-5.55/library 136[23] /nix/store/10mzmnvc5jjgk2xzasia522pk60a30qz-r-MatrixModels-0.4-1/library 137[24] /nix/store/6qwdzvmnnmhjwdnvg2zmvv6wafd1vf91-r-SparseM-1.78/library 138[25] /nix/store/aa9c39a3yiqkh1h7pbngjlbr7czvc7yi-r-rio-0.5.16/library 139[26] /nix/store/2fx4vqlybgwp5rhhy6pssqx7h1a927fn-r-curl-4.3/library 140[27] /nix/store/k4m3fn1kqvvvn8y33kd57gq49hr3ar8y-r-data.table-1.12.8/library 141[28] /nix/store/651hfjylqzmsf565wyx474vyjny771gy-r-haven-2.2.0/library 142[29] /nix/store/a3rnz28irmqvmj8axj5x5j1am2c3gzs4-r-forcats-0.5.0/library 143[30] /nix/store/j8v4gzib137q2cml31hvvfkrc0f60pp5-r-ellipsis-0.3.0/library 144[31] /nix/store/xaswqlnamf4k8vwx0x3wav3l0x60sag0-r-rlang-0.4.5/library 145[32] /nix/store/dqm3xpix2jwhhhr67s6fgrwbw7hizap7-r-tibble-3.0.1/library 146[33] /nix/store/v7xfsq6d97wpn6m0hjrac78w5xawbr8a-r-cli-2.0.2/library 147[34] /nix/store/fikjasr98klhk9cf44x4lhi57vh3pmkg-r-assertthat-0.2.1/library 148[35] /nix/store/3fya6cd38vsqdj0gjb7bcsy00sirlyw1-r-crayon-1.3.4/library 149[36] /nix/store/payqi9bwh216rwhaq07jgc26l4fv1zsb-r-fansi-0.4.1/library 150[37] /nix/store/h6a61ghws7yrdxlg412xl1im37z5r28i-r-glue-1.4.0/library 151[38] /nix/store/y8mjbia1wbnq26dkigr0p3xxwrbzsc2r-r-lifecycle-0.2.0/library 152[39] /nix/store/kwaghh12cnifgvcbvlv2anx0hd5f4ild-r-pillar-1.4.3/library 153[40] /nix/store/k1phn8j10nni7gzvcgp0vc25dby6bb77-r-utf8-1.1.4/library 154[41] /nix/store/k3b77y8v7zsshpp1ccs8jwk2i2g4rm9a-r-vctrs-0.2.4/library 155[42] /nix/store/iibjmbh7vj0d0bfafz98yn29ymg43gkw-r-digest-0.6.25/library 156[43] /nix/store/aqsj4k3pgm80qk4jjg7sh3ac28n6alv0-r-pkgconfig-2.0.3/library 157[44] /nix/store/i7c5v8s4hd9rlqah3bbvy06yywjqwdgk-r-hms-0.5.3/library 158[45] /nix/store/2fyrk58cmcbrxid66rbwjli7y114lvrm-r-readr-1.3.1/library 159[46] /nix/store/163xq2g5nblqgh7qhvzb6mvgg6qdrirj-r-BH-1.72.0-3/library 160[47] /nix/store/dr27b6k49prwgrjs0v30b6mf5lxa36pk-r-clipr-0.7.0/library 161[48] /nix/store/bghvqg9mcaj2jkbwpy0di6c563v24acz-r-R6-2.4.1/library 162[49] /nix/store/nq8jdq7nlg9xns4xpgyj6sqv8p4ny1wz-r-tidyselect-1.0.0/library 163[50] /nix/store/zlwhf75qld7vmwx3d4bdws057ld4mqbp-r-purrr-0.3.4/library 164[51] /nix/store/0gbmmnbpqlr69l573ymkcx8154fvlaca-r-openxlsx-4.1.4/library 165[52] /nix/store/1m1q4rmwx56dvx9rdzfsfq0jpw3hw0yx-r-stringi-1.4.6/library 166[53] /nix/store/mhy5vnvbsl4q7dcinwx3vqlyywxphbfd-r-zip-2.0.4/library 167[54] /nix/store/88sp7f7q577i6l5jjanqiv5ak6nv5357-r-readxl-1.3.1/library 168[55] /nix/store/6q9zwivzalhmzdracc8ma932wirq8rl5-r-cellranger-1.1.0/library 169[56] /nix/store/jh2n6k2ancdzqych5ix8n4rq9w514qq9-r-rematch-1.0.1/library 170[57] /nix/store/22xjqikqd6q556absb5224sbx6q0kp0c-r-progress-1.2.2/library 171[58] /nix/store/9vp32wa1qvv6lkq6p70qlli5whrxzfbi-r-prettyunits-1.1.1/library 172[59] /nix/store/r9rhqb6fsk75shihmb7nagqb51pqwp0y-r-class-7.3-16/library 173[60] /nix/store/z1kad071y43wij1ml9lpghh7jbimmcli-r-cluster-2.1.0/library 174[61] /nix/store/i8wr965caf6j1rxs2dsvpzhlh4hyyb4y-r-codetools-0.2-16/library 175[62] /nix/store/8iglq3zr68a39hzswvzxqi2ffhpw9p51-r-KernSmooth-2.23-16/library 176[63] /nix/store/n3k50zv40i40drpdf8npbmy2y08gkr6w-r-rpart-4.1-15/library 177[64] /nix/store/b4r6adzcvpm8ivflsmis7ja7q4r5hkjy-r-spatial-7.3-11/library 178[65] /nix/store/zqg6hmrncl8ax3vn7z5drf4csddwnhcx-r-survival-3.1-12/library 179[66] /nix/store/4anrihkx11h8mzb269xdyi84yp5v7grl-r-tidyverse-1.3.0/library 180[67] /nix/store/945haq0w8nfm9ib7r0nfngn5lk2i15ix-r-broom-0.5.6/library 181[68] /nix/store/52viqxzrmxl7dk0zji293g5b0b9grwh8-r-backports-1.1.6/library 182[69] /nix/store/zp1k42sw2glqy51w4hnzsjs8rgi8xzx2-r-dplyr-0.8.5/library 183[70] /nix/store/mkjd98mnshch2pwnj6h31czclqdaph3f-r-plogr-0.2.0/library 184[71] /nix/store/kflrzax6y5pwfqwzgfvqz433a3q3hnhn-r-generics-0.0.2/library 185[72] /nix/store/xi1n5h5w17c33y6ax3dfhg2hgzjl9bxz-r-reshape2-1.4.4/library 186[73] /nix/store/vn63z92zkpbaxmmhzpb6mq2fvg0xa26h-r-plyr-1.8.6/library 187[74] /nix/store/wmpyxss67bj44rin7hlnr9qabx66p5hj-r-stringr-1.4.0/library 188[75] /nix/store/330qbgbvllwz3h0i2qidrlk50y0mbgph-r-tidyr-1.0.2/library 189[76] /nix/store/cx3x4pqb65l1mhss65780hbzv9jdrzl6-r-dbplyr-1.4.3/library 190[77] /nix/store/gsj49bp3hpw9jlli3894c49amddryqsq-r-DBI-1.1.0/library 191[78] /nix/store/kvymhwp4gac0343c2yi1qvdpavx4gdn2-r-ggplot2-3.3.0/library 192[79] /nix/store/knv51jvpairvibrkkq48b6f1l2pa1cv8-r-gtable-0.3.0/library 193[80] /nix/store/158dx0ddv20ikwag2860nlg9p3hbh1zc-r-isoband-0.2.1/library 194[81] /nix/store/fprs9rp1jlhxzj7fp6l79akyf8k3p7zd-r-testthat-2.3.2/library 195[82] /nix/store/0pmlnkyn0ir3k9bvxihi1r06jyl64w3i-r-evaluate-0.14/library 196[83] /nix/store/7210bjjqn5cjndxn5isnd4vip00xhkhy-r-pkgload-1.0.2/library 197[84] /nix/store/9a12ybd74b7dns40gcfs061wv7913qjy-r-desc-1.2.0/library 198[85] /nix/store/na9pb1apa787zp7vvyz1kzym0ywjwbj0-r-rprojroot-1.3-2/library 199[86] /nix/store/pa2n7bh61qxyarn5i2ynd62k6knb1np1-r-pkgbuild-1.0.6/library 200[87] /nix/store/1hxm1m7h4272zxk9bpsaq46mvnl0dbss-r-callr-3.4.3/library 201[88] /nix/store/bigvyk6ipglbiil93zkf442nv4y3xa1x-r-processx-3.4.2/library 202[89] /nix/store/370lr0wf7qlq0m72xnmasg2iahkp2n52-r-ps-1.3.2/library 203[90] /nix/store/rr72q61d8mkd42zc5fhcd2rqjghvc141-r-withr-2.2.0/library 204[91] /nix/store/9gw77p7fmz89fa8wi1d9rvril6hd4sxy-r-rstudioapi-0.11/library 205[92] /nix/store/9x4v4pbrgmykbz2801h77yz2l0nmm5nb-r-praise-1.0.0/library 206[93] /nix/store/pf8ssb0dliw5bzsncl227agc8przb7ic-r-scales-1.1.0/library 207[94] /nix/store/095z4wgjrxn63ixvyzrj1fm1rdv6ci95-r-farver-2.0.3/library 208[95] /nix/store/5aczj4s7i9prf5i32ik5ac5baqvjwdb1-r-labeling-0.3/library 209[96] /nix/store/wch26phipzz9gxd4vbr4fynh7v28349j-r-munsell-0.5.0/library 210[97] /nix/store/3w8fh756mszhsjx5fwgwydcpn8vkwady-r-colorspace-1.4-1/library 211[98] /nix/store/8cmaj81v2vm4f8p59ylbnsby8adkbmhd-r-RColorBrewer-1.1-2/library 212[99] /nix/store/h4x4ygax7gpz6f0c2v0xacr62080qwb8-r-viridisLite-0.3.0/library 213[100] /nix/store/qhx0i2nn5syb6vygdn8fdxgl7k56yj81-r-httr-1.4.1/library 214[101] /nix/store/lxnb4aniv02i4jhdvz02aaql1kznbpxb-r-jsonlite-1.6.1/library 215[102] /nix/store/13dcry4gad3vfwqzqb0ii4n06ybrxybr-r-mime-0.9/library 216[103] /nix/store/2can5l8gscc92a3bqlak8hfcg96v5hvf-r-openssl-1.4.1/library 217[104] /nix/store/piwsgxdz5w2ak8c6fcq0lc978qbxwdp1-r-askpass-1.1/library 218[105] /nix/store/3sj5h6dwa1l27d2hvdchclygk0pgffsr-r-sys-3.3/library 219[106] /nix/store/2z0p88g0c03gigl2ip60dlsfkdv1k30h-r-lubridate-1.7.8/library 220[107] /nix/store/1pkmj8nqjg2iinrkg2w0zkwq0ldc01za-r-modelr-0.1.6/library 221[108] /nix/store/bswkzvn8lczwbyw3y7n0p0qp2q472s0g-r-reprex-0.3.0/library 222[109] /nix/store/yid22gad8z49q52d225vfba2m4cgj2lx-r-fs-1.4.1/library 223[110] /nix/store/d185qiqaplm5br9fk1pf29y0srlabw83-r-rmarkdown-2.1/library 224[111] /nix/store/iszqviydsdj31c3ww095ndqy1ld3cibs-r-base64enc-0.1-3/library 225[112] /nix/store/i89wfw4cr0fz3wbd7cg44fk4dwz8b6h1-r-htmltools-0.4.0/library 226[113] /nix/store/qrl28laqwmhpwg3dpcf4nca8alv0px0g-r-knitr-1.28/library 227[114] /nix/store/jffaxc4a3bbf2g6ip0gdcya73dmg53mb-r-highr-0.8/library 228[115] /nix/store/717srph13qpnbzmgsvhx25q8pl51ivpj-r-markdown-1.1/library 229[116] /nix/store/mxqmyq3ybdfyc6p0anhfy2kfw0iz5k4n-r-xfun-0.13/library 230[117] /nix/store/b8g6hadva0359l6j1aq4dbvxlqf1acxc-r-yaml-2.2.1/library 231[118] /nix/store/rrl05vpv7cw58zi0k9ykm7m4rjb9gjv3-r-tinytex-0.22/library 232[119] /nix/store/2ziq8nzah6xy3dgmxgim9h2wszz1f89f-r-whisker-0.4/library 233[120] /nix/store/540wbw4p1g2qmnmbfk0rhvwvfnf657sj-r-rvest-0.3.5/library 234[121] /nix/store/n3prn77gd9sf3z4whqp86kghr55bf5w8-r-selectr-0.4-2/library 235[122] /nix/store/gv28yjk5isnglq087y7767xw64qa40cw-r-xml2-1.3.2/library 236[123] /nix/store/693czdcvkp6glyir0mi8cqvdc643whvc-r-gridExtra-2.3/library 237[124] /nix/store/3sykinp7lyy70dgzr0fxjb195nw864dv-r-future-1.17.0/library 238[125] /nix/store/bqi2l53jfxncks6diy0hr34bw8f86rvk-r-globals-0.12.5/library 239[126] /nix/store/dydyl209klklzh69w9q89f2dym9xycnp-r-listenv-0.8.0/library 240[127] /nix/store/lni0bi36r4swldkx7g4hql7gfz9b121b-r-gganimate-1.0.5/library 241[128] /nix/store/hh92jxs79kx7vxrxr6j6vin1icscl4k7-r-tweenr-1.0.1/library 242[129] /nix/store/0npx3srjnqgh7bib80xscjqvfyzjvimq-r-GGally-1.5.0/library 243[130] /nix/store/x5nzxklmacj6l162g7kg6ln9p25r3f17-r-reshape-0.8.8/library 244[131] /nix/store/q29z7ckdyhfmg1zlzrrg1nrm36ax756j-r-ggfortify-0.4.9/library 245[132] /nix/store/1rvm1w9iv2c5n22p4drbjq8lr9wa2q2r-r-cowplot-1.0.0/library 246[133] /nix/store/rp8jhnasaw1vbv5ny5zx0mw30zgcp796-r-ggrepel-0.8.2/library 247[134] /nix/store/wb7y931mm8nsj7w9xin83bvbaq8wvi4d-r-corrplot-0.84/library 248[135] /nix/store/gdzcqivfvgdrsz247v5kmnnw1v6p9c1p-r-rpart.plot-3.0.8/library 249[136] /nix/store/6yqg37108r0v22476cm2kv0536wyilki-r-caret-6.0-86/library 250[137] /nix/store/6fjdgcwgisiqz451sg5fszxnn9z8vxg6-r-foreach-1.5.0/library 251[138] /nix/store/c3ph5i341gk7jdinrkkqf6y631xli424-r-iterators-1.0.12/library 252[139] /nix/store/sjm1rxshlpakpxbrynfhsjnnp1sjvc3r-r-ModelMetrics-1.2.2.2/library 253[140] /nix/store/vgk4m131d057xglmrrb9rijhzdr2qhhp-r-pROC-1.16.2/library 254[141] /nix/store/bv1kvy1wc2jx3v55rzn3cg2qjbv7r8zp-r-recipes-0.1.10/library 255[142] /nix/store/001h42q4za01gli7avjxhq7shpv73n9k-r-gower-0.2.1/library 256[143] /nix/store/ssffpl6ydffqyn9phscnccxnj71chnzg-r-ipred-0.9-9/library 257[144] /nix/store/baliqip8m6p0ylqhqcgqak29d8ghral1-r-prodlim-2019.11.13/library 258[145] /nix/store/j4n2wsv98asw83qiffg6a74dymk8r2hl-r-lava-1.6.7/library 259[146] /nix/store/hf5wq5kpsf6p9slglq5iav09s4by0y5i-r-numDeriv-2016.8-1.1/library 260[147] /nix/store/s58hm38078mx4gyqffvv09zn575xn648-r-SQUAREM-2020.2/library 261[148] /nix/store/g63ydzd53586pvr9kdgk8kf5szq5f2bc-r-timeDate-3043.102/library 262[149] /nix/store/0jkarmlf1kjv4g8a3svkc7jfarpp77ny-r-mlr3-0.2.0/library 263[150] /nix/store/g1m0n1w7by213v773iyn7vnxr25pkf56-r-checkmate-2.0.0/library 264[151] /nix/store/fc2ah8cz2sj6j2jk7zldvjmsjn1yakpn-r-lgr-0.3.4/library 265[152] /nix/store/0i2hs088j1s0a6i61124my6vnzq8l27m-r-mlbench-2.1-1/library 266[153] /nix/store/vzcs6k21pqrli3ispqnvj5qwkv14srf5-r-mlr3measures-0.1.3/library 267[154] /nix/store/h2yqqaia46bk3b1d1a7bq35zf09p1b1a-r-mlr3misc-0.2.0/library 268[155] /nix/store/c9mrkc928cmsvvnib50l0jb8lsz59nyk-r-paradox-0.2.0/library 269[156] /nix/store/vqpbdipi4p4advl2vxrn765mmgcrabvk-r-uuid-0.1-4/library 270[157] /nix/store/xpclynxnfq4h9218gk4y62nmgyyga6zl-r-mlr3viz-0.1.1/library 271[158] /nix/store/7w6pld5vir3p9bybay67kq0qwl0gnx17-r-mlr3learners-0.2.0/library 272[159] /nix/store/ca50rp6ha5s51qmhb1gjlj62r19xfzxs-r-mlr3pipelines-0.1.3/library 273[160] /nix/store/9hg0xap4pir64mhbgq8r8cgrfjn8aiz5-r-mlr3filters-0.2.0/library 274[161] /nix/store/jgqcmfix0xxm3y90m8wy3xkgmqf2b996-r-rstan-2.19.3/library 275[162] /nix/store/mvv1gjyrrpvf47fn7a8x722wdwrf5azk-r-inline-0.3.15/library 276[163] /nix/store/zmkw51x4w4d1v1awcws0xihj4hnxfr09-r-loo-2.2.0/library 277[164] /nix/store/30xxalfwzxl05bbfvj5sy8k3ysys6z5y-r-matrixStats-0.56.0/library 278[165] /nix/store/fhkww2l0izx87bjnf0pl9ydl1wprp0xv-r-StanHeaders-2.19.2/library 279[166] /nix/store/aflck5pzxa8ym5q1dxchx5hisfmfghkr-r-tidybayes-2.0.3/library 280[167] /nix/store/jhlbhiv4fg0wsbxwjz8igc4hcg79vw94-r-arrayhelpers-1.1-0/library 281[168] /nix/store/fv089zrnvicnavbi08hnzqpi9g1z4inj-r-svUnit-1.0.3/library 282[169] /nix/store/xci2rgjizx1fyb33818jx5s1bgn8v8k6-r-coda-0.19-3/library 283[170] /nix/store/dch9asd38yldz0sdn8nsgk9ivjrkbhva-r-HDInterval-0.2.0/library 284[171] /nix/store/rs8dri2m5cqdmpiw187rvl4yhjn0jg2v-r-e1071-1.7-3/library 285[172] /nix/store/qs1zyh3sbvccgnqjzas3br6pak399zgc-r-pvclust-2.2-0/library 286[173] /nix/store/sh3zxvdazp7rkjn1iczrag1h2358ifm1-r-forecast-8.12/library 287[174] /nix/store/h67kaxqr2ppdpyj77wg5hm684jypznji-r-fracdiff-1.5-1/library 288[175] /nix/store/fh0z465ligbpqyam5l1fwiijc7334kbk-r-lmtest-0.9-37/library 289[176] /nix/store/0lnsbwfg0axr80h137q52pa50cllbjpf-r-zoo-1.8-7/library 290[177] /nix/store/p7k4s3ivf83dp2kcxr1cr0wlc1rfk6jx-r-RcppArmadillo-0.9.860.2.0/library 291[178] /nix/store/ssnxv5x6zid2w11v8k5yvnyxis6n1qfk-r-tseries-0.10-47/library 292[179] /nix/store/zrbskjwaz0bzz4v76j044d771m24g6h8-r-quadprog-1.5-8/library 293[180] /nix/store/2x3w5sjalrfm6hf1dxd951j8y94nh765-r-quantmod-0.4.17/library 294[181] /nix/store/7g55xshf49s9379ijm1zi1qnh1vbsifq-r-TTR-0.23-6/library 295[182] /nix/store/6ilyzph46q6ijyanq4p7f0ccyni0d7j0-r-xts-0.12-0/library 296[183] /nix/store/17xhqghcnqha7pwbf98dxsq1729slqd5-r-urca-1.3-0/library 297[184] /nix/store/722lyn0k8y27pj1alik56r4vpjnncd9z-r-swdft-1.0.0/library 298[185] /nix/store/36n0zgy10fsqcq76n0qmdwjxrwh7pn9n-r-xgboost-1.0.0.2/library 299[186] /nix/store/ac0ar7lf75qx84xsdjv6j02rkdgnhybz-r-ranger-0.12.1/library 300[187] /nix/store/i1ighkq42x10dirqmzgbx2mhbnz1ynkb-r-DALEX-1.2.0/library 301[188] /nix/store/28fqnhsfng1bkphl0wvr7lg5y3p6va46-r-iBreakDown-1.2.0/library 302[189] /nix/store/dpym77x9qc2ksr4mwjm3pb9ar1kvwhdl-r-ingredients-1.2.0/library 303[190] /nix/store/sp4d281w6dpr31as0xdjqizdx8hhb01q-r-DALEXtra-0.2.1/library 304[191] /nix/store/ckhp9kpmjcs0wxb113pxn25c2wip2d0n-r-ggdendro-0.1-20/library 305[192] /nix/store/f3k7dxj1dsmqri2gn0svq4c9fvvl9g7q-r-glmnet-3.0-2/library 306[193] /nix/store/l6ccj6mwkqybjvh6dr8qzalygp0i7jyb-r-shape-1.4.4/library 307[194] /nix/store/418mqfwlafh6984xld8lzhl7rv29qw68-r-reticulate-1.15/library 308[195] /nix/store/qwh982mgxd2mzrgbjk14irqbasywa1jk-r-rappdirs-0.3.1/library 309[196] /nix/store/6sxs76abll23c6372h6nf101wi8fcr4c-r-FactoMineR-2.3/library 310[197] /nix/store/39d2va10ydgyzddwr07xwdx11fwk191i-r-ellipse-0.4.1/library 311[198] /nix/store/4lxym5nxdn8hb7l8a566n5vg9paqcfi2-r-flashClust-1.01-2/library 312[199] /nix/store/wp161zbjjs41fq4kn4k3m244c7b8l2l2-r-leaps-3.1/library 313[200] /nix/store/irghsaplrpb3hg3y7j831bbklf2cqs6d-r-scatterplot3d-0.3-41/library 314[201] /nix/store/09ahkf50g1q9isxanbdykqgcdrp8mxl1-r-factoextra-1.0.7/library 315[202] /nix/store/zi9bq7amsgc6w2x7fvd62g9qxz69vjfm-r-dendextend-1.13.4/library 316[203] /nix/store/wcywb7ydglzlxg57jf354x31nmy63923-r-viridis-0.5.1/library 317[204] /nix/store/pvnpg4vdvv93pmwrlgmy51ihrb68j55f-r-ggpubr-0.2.5/library 318[205] /nix/store/qpapsc4l9pylzfhc72ha9d82hcbac41z-r-ggsci-2.9/library 319[206] /nix/store/h0zg4x3bmkc82ggx8h4q595ffckcqgx5-r-ggsignif-0.6.0/library 320[207] /nix/store/vn5svgbf8vsgv8iy8fdzlj0izp279q15-r-polynom-1.4-0/library 321[208] /nix/store/mc1mlsjx5h3gc8nkl7jlpd4vg145nk1z-r-lindia-0.9/library 322[209] /nix/store/z1k4c8lhabp9niwfg1xylg58pf99ld9r-r-orgutils-0.4-1/library 323[210] /nix/store/ybj4538v74wx4f1l064m0qn589vyjmzg-r-textutils-0.2-0/library 324[211] /nix/store/hhm5j0wvzjc0bfd53170bw8w7mij2wnh-r-latex2exp-0.4.0/library 325[212] /nix/store/njlv5mkxgjyx3x8p984nr84dwa2v1iqp-r-kableExtra-1.1.0/library 326[213] /nix/store/lf2sb84ylh259m421ljbj731a4prjhsl-r-webshot-0.5.2/library 327[214] /nix/store/n6b8ap54b78h8l70kyx9nvayp44rnfzf-r-printr-0.1/library 328[215] /nix/store/02g1v6d3ly8zylpckigwk6w3l1mx2i9d-r-microbenchmark-1.4-7/library 329[216] /nix/store/ri6qm0fp8cyx2qnysxjv2wsk0nndl1x9-r-webchem-0.5.0/library 330[217] /nix/store/cg95rqc1gmaqxf5kxja3cz8m5w4vl76l-r-RCurl-1.98-1.2/library 331[218] /nix/store/qbpinv148778fzdz8372x8gp34hspvy1-r-bitops-1.0-6/library 332[219] /nix/store/1g0lbrx6si76k282sxr9cj0mgknrw0lx-r-devtools-2.3.0/library 333[220] /nix/store/hnvww0128czlx6w8aipjn0zs7nvmvak9-r-covr-3.5.0/library 334[221] /nix/store/p4nv59przmb14sxi49jwqarkv0l40jsp-r-rex-1.2.0/library 335[222] /nix/store/vnysmc3vkgkligwah1zh9l4sahr533a8-r-lazyeval-0.2.2/library 336[223] /nix/store/d638w33ahybsa3sqr52fafvxs2b7w9x3-r-DT-0.13/library 337[224] /nix/store/35nqc34wy2nhd9bl7lv6wriw0l3cghsw-r-crosstalk-1.1.0.1/library 338[225] /nix/store/03838i63x5irvgmpgwj67ah0wi56k9d7-r-htmlwidgets-1.5.1/library 339[226] /nix/store/l4640jxlsjzqhw63c18fziar5vc0xyhk-r-promises-1.1.0/library 340[227] /nix/store/rxrb8p3dxzsg10v7yqaq5pi3y3gk6nqh-r-later-1.0.0/library 341[228] /nix/store/giprr32bl6k18b9n4qjckpf102flarly-r-git2r-0.26.1/library 342[229] /nix/store/bbkpkf44b13ig1pkz7af32kw5dzp12vb-r-memoise-1.1.0/library 343[230] /nix/store/m31vzssnfzapsapl7f8v4m15003lcc8r-r-rcmdcheck-1.3.3/library 344[231] /nix/store/hbiylknhxsin9hp9zaa6dwc2c9ai1mqx-r-sessioninfo-1.1.1/library 345[232] /nix/store/8vwlbx3s345gjccrkiqa6h1bm9wq4s9q-r-xopen-1.0.0/library 346[233] /nix/store/mjnwnlv60cn56ap0rrzvrkqlh5qisszx-r-remotes-2.1.1/library 347[234] /nix/store/1rq4zyzqymml7cc11q89rl5g514ml9na-r-roxygen2-7.1.0/library 348[235] /nix/store/2658mrn1hpkq0fv629rvags91qg65pbn-r-brew-1.0-6/library 349[236] /nix/store/nvjalws9lzva4pd4nz1z2131xsb9b5p6-r-commonmark-1.7/library 350[237] /nix/store/qx900vivd9s2zjrxc6868s92ljfwj5dv-r-rversions-2.0.1/library 351[238] /nix/store/1drg446wilq5fjnxkglxnnv8pbp1hllg-r-usethis-1.6.0/library 352[239] /nix/store/p3f3wa41d304zbs5cwvw7vy4j17zd6nq-r-gh-1.1.0/library 353[240] /nix/store/769g7jh93da8w15ad0wsbn2aqziwwx56-r-ini-0.3.1/library 354[241] /nix/store/p7kifw1l6z2zg68a71s4sdbfj8gdmnv5-r-rematch2-2.1.1/library 355[242] /nix/store/6zhdqip9ld9vl6pvifqcf4gsqy2f5wix-r-rethinking/library 356[243] /nix/store/496p28klmflihdkc83c8p1cywg85mgk4-r-mvtnorm-1.1-0/library 357[244] /nix/store/xb1zn7ab4nka7h1vm678ginzfwg4w9wf-r-dagitty-0.2-2/library 358[245] /nix/store/3zj4dkjbdwgf3mdsl9nf9jkicpz1nwgc-r-V8-3.0.2/library 359[246] /nix/store/qiqsh62w69b5xgj2i4wjamibzxxji0mf-r-tidybayes.rethinking/library 360[247] /nix/store/4j6byy1klyk4hm2k6g3657682cf3wxcj-R-4.0.0/lib/R/library   Summer of 2020\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/sr2-ch13-ch14/","tags":["solutions","R","SR2"],"title":"SR2 :: Solutions for Chapters {13,14}"},{"categories":["programming"],"contents":" Setup details are described here, and the meta-post about these solutions is here.\n Materials The summmer course1 is based off of the second edition of Statistical Rethinking by Richard McElreath. This submission covers the following exercise questions:\n Chapter 9  E{3,4,5,6} M{1,2,3}   Chapter 11  E{1,2,3,4} M{2,3,4,5,6,8}   Chapter 12  E{4} H{1,2}    Packages A colophon with details is provided at the end, but the following packages and theme parameters are used throughout.\n1libsUsed\u0026lt;-c(\u0026#34;tidyverse\u0026#34;,\u0026#34;tidybayes\u0026#34;,\u0026#34;orgutils\u0026#34;,\u0026#34;dagitty\u0026#34;, 2\u0026#34;rethinking\u0026#34;,\u0026#34;tidybayes.rethinking\u0026#34;, 3\u0026#34;ggplot2\u0026#34;,\u0026#34;kableExtra\u0026#34;,\u0026#34;dplyr\u0026#34;,\u0026#34;glue\u0026#34;, 4\u0026#34;latex2exp\u0026#34;,\u0026#34;data.table\u0026#34;,\u0026#34;printr\u0026#34;,\u0026#34;devtools\u0026#34;) 5invisible(lapply(libsUsed, library, character.only = TRUE)); 6theme_set(theme_grey(base_size=24)) 7set.seed(1995) Chapter IX: Markov Chain Monte Carlo Easy Questions (Ch9) 9E3 Which sort of parameters can Hamiltonian Monte Carlo not handle? Can you explain why?\nSolution Hamiltonian Monte Carlo is derived by adding the concept of momentum which requires that the Hessian is non-negative, which in term requires a continuous smooth function. Thus HMC cannot handle discrete parameters by construction. More formally, the HMC requires a transform from the D-dimensional parameter space to a 2D-dimensional phase space cite:betancourtConceptualIntroductionHamiltonian2018.\n9E4 Explain the difference between the effective number of samples, n_eff as calculated by Stan, and the actual number of samples.\nSolution We will invoke the precise definition of the effective sample size cite:betancourtConceptualIntroductionHamiltonian2018\n\\[ ESS = \\frac{N}{1+2\\sum_{l=1}^{\\infty}\\rho_{l}} \\]\nWhere we note that \\(\\rho_{l}\\) is the lag-l autocorrelation of \\(f\\) over the Markov chain (in time). In essence, this is the number of independent samples which have equivalent information of the posterior. This is relevant, because the samples from a Marko chain are sequentially correlated (autocorrelated).\n9E5 Which value should Rhat approach, when a chain is sampling the posterior distribution correctly?\nSolution The literature cite:gelmanBayesianDataAnalysis2014 often cites a value of \\(1.01\\) for convergence. However, newer versions of Stan tend are documented to suggest \\(1.05\\) since they use newer formulations of the Rhat value cite:vehtariRanknormalizationFoldingLocalization2020. It should also be noted that cite:royConvergenceDiagnosticsMarkov2020 the Rhat value does not necessarily indicate convergence, it is not a necessary and sufficient condition, but a heuristic, and should be understood as such.\nHOLD 9E6 Sketch a good trace plot for a Markov chain, one that is effectively sampling from the posterior distribution. What is good about its shape? Then sketch a trace plot for a malfunctioning Markov chain. What about its shape indicates malfunction?\nSolution Recall that the \u0026ldquo;health\u0026rdquo; of a chain can be determined by the following qualities in the trace plot.\n Stationarity This ensures that the chain is sampling the high probability portion of the posterior distribution Mixing This ensures that the chain explores the full region Convergence Convergence implies that independent chains agree on the same region of high probability  We will require a sample model to plot.\n1data(rugged) 2rugDat\u0026lt;-rugged 3rugDat\u0026lt;-rugDat %\u0026gt;% dplyr::mutate(logGDP=log(rgdppc_2000)) %\u0026gt;% tidyr::drop_na() %\u0026gt;% dplyr::mutate(logGDP_std=logGDP/mean(logGDP), 4rugged_std=rugged/max(rugged), 5cid=ifelse(cont_africa==1,1,2)) 6datList\u0026lt;-list( 7logGDP_std=rugDat$logGDP_std, 8rugged_std=rugDat$rugged_std, 9cid=as.integer(rugDat$cid) 10) 1m91unif\u0026lt;-ulam( 2alist( 3logGDP_std ~ dnorm(mu,sigma), 4mu\u0026lt;-a[cid] + b[cid]*(rugged_std-0.215), 5a[cid]~dnorm(1,0.1), 6b[cid]~dnorm(0,0.3), 7sigma~dunif(0,1) 8), data=datList, chains=4, cores=4 9) We would like to check the trace and trace rank plots.\n1m91unif %\u0026gt;% traceplot  1m91unif %\u0026gt;% trankplot  Clearly this is a good model, with well mixed chains, as can be seen in the trank and trace plots.\nWe will now check the plots for the unhealthy chain described in the chapter.\n1m9e4un\u0026lt;-ulam( 2alist( 3y ~ dnorm(mu,sigma), 4mu\u0026lt;-alpha, 5alpha ~ dnorm(0,1000), 6sigma~dexp(0.0001) 7),data=list(y=c(-1,1)),chains=4,cores=4 8) 1SAMPLING FOR MODEL \u0026#39;726d002e27cec1633082261fcfedb813\u0026#39; NOW (CHAIN 1). 2Chain 1: 3Chain 1: Gradient evaluation took 8.1e-05 seconds 4Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.81 seconds. 5Chain 1: Adjust your expectations accordingly! 6Chain 1: 7Chain 1: 8Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) 9Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) 1011SAMPLING FOR MODEL \u0026#39;726d002e27cec1633082261fcfedb813\u0026#39; NOW (CHAIN 2). 12Chain 2: 13Chain 2: Gradient evaluation took 3.6e-05 seconds 14Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.36 seconds. 15Chain 2: Adjust your expectations accordingly! 16Chain 2: 17Chain 2: 18Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup) 19Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) 20Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) 21Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) 22Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) 23Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) 24Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) 2526SAMPLING FOR MODEL \u0026#39;726d002e27cec1633082261fcfedb813\u0026#39; NOW (CHAIN 3). 27Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) 28Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) 29Chain 3: 30Chain 3: Gradient evaluation took 2.5e-05 seconds 31Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds. 32Chain 3: Adjust your expectations accordingly! 33Chain 3: 34Chain 3: 35Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) 36Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) 37Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) 38Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) 39Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) 40Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) 41Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) 42Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) 43Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) 44Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) 45Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) 46Chain 2: 47Chain 2: Elapsed Time: 0.069084 seconds (Warm-up) 48Chain 2: 0.023083 seconds (Sampling) 49Chain 2: 0.092167 seconds (Total) 50Chain 2: 51Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) 5253SAMPLING FOR MODEL \u0026#39;726d002e27cec1633082261fcfedb813\u0026#39; NOW (CHAIN 4). 54Chain 4: 55Chain 4: Gradient evaluation took 3.1e-05 seconds 56Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. 57Chain 4: Adjust your expectations accordingly! 58Chain 4: 59Chain 4: 60Chain 4: Iteration: 1 / 1000 [ 0%] (Warmup) 61Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) 62Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) 63Chain 3: Iteration: 300 / 1000 [ 30%] (Warmup) 64Chain 3: Iteration: 400 / 1000 [ 40%] (Warmup) 65Chain 4: Iteration: 100 / 1000 [ 10%] (Warmup) 66Chain 3: Iteration: 500 / 1000 [ 50%] (Warmup) 67Chain 3: Iteration: 501 / 1000 [ 50%] (Sampling) 68Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) 69Chain 4: Iteration: 200 / 1000 [ 20%] (Warmup) 70Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) 71Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) 72Chain 1: 73Chain 1: Elapsed Time: 0.082116 seconds (Warm-up) 74Chain 1: 0.095511 seconds (Sampling) 75Chain 1: 0.177627 seconds (Total) 76Chain 1: 77Chain 4: Iteration: 300 / 1000 [ 30%] (Warmup) 78Chain 4: Iteration: 400 / 1000 [ 40%] (Warmup) 79Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) 80Chain 4: Iteration: 500 / 1000 [ 50%] (Warmup) 81Chain 4: Iteration: 501 / 1000 [ 50%] (Sampling) 82Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) 83Chain 4: Iteration: 600 / 1000 [ 60%] (Sampling) 84Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) 85Chain 4: Iteration: 700 / 1000 [ 70%] (Sampling) 86Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) 87Chain 3: 88Chain 3: Elapsed Time: 0.073606 seconds (Warm-up) 89Chain 3: 0.076388 seconds (Sampling) 90Chain 3: 0.149994 seconds (Total) 91Chain 3: 92Chain 4: Iteration: 800 / 1000 [ 80%] (Sampling) 93Chain 4: Iteration: 900 / 1000 [ 90%] (Sampling) 94Chain 4: Iteration: 1000 / 1000 [100%] (Sampling) 95Chain 4: 96Chain 4: Elapsed Time: 0.064213 seconds (Warm-up) 97Chain 4: 0.091667 seconds (Sampling) 98Chain 4: 0.15588 seconds (Total) 99Chain 4: 100Warning messages: 1011: There were 55 divergent transitions after warmup. Increasing adapt_delta above 0.95 may help. See 102http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 1032: Examine the pairs() plot to diagnose sampling problems 1041053: The largest R-hat is 1.08, indicating chains have not mixed. 106Running the chains for more iterations may help. See 107http://mc-stan.org/misc/warnings.html#r-hat 1084: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. 109Running the chains for more iterations may help. See 110http://mc-stan.org/misc/warnings.html#bulk-ess 1115: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. 112Running the chains for more iterations may help. See 113http://mc-stan.org/misc/warnings.html#tail-ess 1m9e4un %\u0026gt;% traceplot  1m9e4un %\u0026gt;% trankplot  Clearly these plots show a model which is unable to converge.\n1m9e4un %\u0026gt;% precis 1mean sd 5.5% 94.5% n_eff Rhat4 2alpha -22.01 267.51 -414.95 316.61 154 1.02 3sigma 420.97 983.52 5.39 1894.87 151 1.04 This has clear repercussions on the actual predictions as well.\nQuestions of Medium Complexity (Ch9) HOLD 9M1 Re-estimate the terrain ruggedness model from the chapter, but now using a uniform prior for the standard deviation, sigma. The uniform prior should be dunif(0,1). Use ulam to estimate the posterior. Does the different prior have any detectable influence on the posterior distribution of sigma? What or why not?\nSolution Instead of using the complete.cases formulation in the book, we will instead use a more tidyverse friendly approach.\n1data(rugged) 2rugDat\u0026lt;-rugged 3rugDat\u0026lt;-rugDat %\u0026gt;% dplyr::mutate(logGDP=log(rgdppc_2000)) %\u0026gt;% tidyr::drop_na() %\u0026gt;% dplyr::mutate(logGDP_std=logGDP/mean(logGDP), 4rugged_std=rugged/max(rugged), 5cid=ifelse(cont_africa==1,1,2)) 6datList\u0026lt;-list( 7logGDP_std=rugDat$logGDP_std, 8rugged_std=rugDat$rugged_std, 9cid=as.integer(rugDat$cid) 10) We can now formulate a model with a uniform prior on sigma.\n1m91unif\u0026lt;-ulam( 2alist( 3logGDP_std ~ dnorm(mu,sigma), 4mu\u0026lt;-a[cid] + b[cid]*(rugged_std-0.215), 5a[cid]~dnorm(1,0.1), 6b[cid]~dnorm(0,0.3), 7sigma~dunif(0,1) 8), data=datList, chains=4, cores=4 9) 1m91exp\u0026lt;-ulam( 2alist( 3logGDP_std ~ dnorm(mu,sigma), 4mu\u0026lt;-a[cid] + b[cid]*(rugged_std-0.215), 5a[cid]~dnorm(1,0.1), 6b[cid]~dnorm(0,0.3), 7sigma~dexp(1) 8), data=datList, chains=4, cores=4 9) 1SAMPLING FOR MODEL \u0026#39;9b462775c5cc2badb2b667c53f2020c8\u0026#39; NOW (CHAIN 1). 2Chain 1: 3Chain 1: Gradient evaluation took 2.8e-05 seconds 4Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. 5Chain 1: Adjust your expectations accordingly! 6Chain 1: 7Chain 1: 8Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) 910SAMPLING FOR MODEL \u0026#39;9b462775c5cc2badb2b667c53f2020c8\u0026#39; NOW (CHAIN 2). 11Chain 2: 12Chain 2: Gradient evaluation took 3.3e-05 seconds 13Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.33 seconds. 14Chain 2: Adjust your expectations accordingly! 15Chain 2: 16Chain 2: 17Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup) 1819SAMPLING FOR MODEL \u0026#39;9b462775c5cc2badb2b667c53f2020c8\u0026#39; NOW (CHAIN 3). 20Chain 3: 21Chain 3: Gradient evaluation took 2.6e-05 seconds 22Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds. 23Chain 3: Adjust your expectations accordingly! 24Chain 3: 25Chain 3: 26Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) 27Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) 28Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) 29Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) 30Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) 3132SAMPLING FOR MODEL \u0026#39;9b462775c5cc2badb2b667c53f2020c8\u0026#39; NOW (CHAIN 4). 33Chain 4: 34Chain 4: Gradient evaluation took 2.5e-05 seconds 35Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds. 36Chain 4: Adjust your expectations accordingly! 37Chain 4: 38Chain 4: 39Chain 3: Iteration: 300 / 1000 [ 30%] (Warmup) 40Chain 4: Iteration: 1 / 1000 [ 0%] (Warmup) 41Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) 42Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) 43Chain 3: Iteration: 400 / 1000 [ 40%] (Warmup) 44Chain 3: Iteration: 500 / 1000 [ 50%] (Warmup) 45Chain 3: Iteration: 501 / 1000 [ 50%] (Sampling) 46Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) 47Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) 48Chain 4: Iteration: 100 / 1000 [ 10%] (Warmup) 49Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) 50Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) 51Chain 4: Iteration: 200 / 1000 [ 20%] (Warmup) 52Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) 53Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) 54Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) 55Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) 56Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) 57Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) 58Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) 59Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) 60Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) 61Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) 62Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) 63Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) 64Chain 4: Iteration: 300 / 1000 [ 30%] (Warmup) 65Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) 66Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) 67Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) 68Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) 69Chain 4: Iteration: 400 / 1000 [ 40%] (Warmup) 70Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) 71Chain 1: 72Chain 1: Elapsed Time: 0.048459 seconds (Warm-up) 73Chain 1: 0.017728 seconds (Sampling) 74Chain 1: 0.066187 seconds (Total) 75Chain 1: 76Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) 77Chain 2: 78Chain 2: Elapsed Time: 0.043574 seconds (Warm-up) 79Chain 2: 0.018569 seconds (Sampling) 80Chain 2: 0.062143 seconds (Total) 81Chain 2: 82Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) 83Chain 3: 84Chain 3: Elapsed Time: 0.03201 seconds (Warm-up) 85Chain 3: 0.024666 seconds (Sampling) 86Chain 3: 0.056676 seconds (Total) 87Chain 3: 88Chain 4: Iteration: 500 / 1000 [ 50%] (Warmup) 89Chain 4: Iteration: 501 / 1000 [ 50%] (Sampling) 90Chain 4: Iteration: 600 / 1000 [ 60%] (Sampling) 91Chain 4: Iteration: 700 / 1000 [ 70%] (Sampling) 92Chain 4: Iteration: 800 / 1000 [ 80%] (Sampling) 93Chain 4: Iteration: 900 / 1000 [ 90%] (Sampling) 94Chain 4: Iteration: 1000 / 1000 [100%] (Sampling) 95Chain 4: 96Chain 4: Elapsed Time: 0.040804 seconds (Warm-up) 97Chain 4: 0.018479 seconds (Sampling) 98Chain 4: 0.059283 seconds (Total) 99Chain 4: The posterior distributions are simply:\n1m91exp %\u0026gt;% extract.samples %\u0026gt;% .$sigma %\u0026gt;% dens(.,xlab=\u0026#34;sigma\u0026#34;) 2m91unif %\u0026gt;% extract.samples %\u0026gt;% .$sigma %\u0026gt;% dens(.,add=TRUE,col=\u0026#34;blue\u0026#34;) 3mtext(\u0026#34;posterior\u0026#34;)  With the priors being:\n1m91exp %\u0026gt;% extract.prior %\u0026gt;% .$sigma %\u0026gt;% dens(.,xlab=\u0026#34;sigma\u0026#34;) 2m91unif %\u0026gt;% extract.prior %\u0026gt;% .$sigma %\u0026gt;% dens(.,add=TRUE,col=\u0026#34;blue\u0026#34;) 3mtext(\u0026#34;prior\u0026#34;)  This makes sense, since we know that uniform prior is essentially a step function between 0 and 1 with a value of 1, while the exponential function decays normally, but should actually be spiked upwards to 1 as well.\nHOLD 9M2 Modify the terrain ruggedness model again. This times, change the prior for b[cid] to dexp(0.3). What does this do to the posterior distribution? Can you explain it?\nSolution 1m92exp\u0026lt;-ulam( 2alist( 3logGDP_std ~ dnorm(mu,sigma), 4mu\u0026lt;-a[cid] + b[cid]*(rugged_std-0.215), 5a[cid]~dnorm(1,0.1), 6b[cid]~dexp(0.3), 7sigma~dexp(1) 8),data=datList, chains=4, cores=4 9) Priors:\n1m92exp %\u0026gt;% extract.prior %\u0026gt;% .$sigma %\u0026gt;% dens(.,xlab=\u0026#34;sigma\u0026#34;,col=\u0026#34;blue\u0026#34;) 2mtext(\u0026#34;prior\u0026#34;)  Posterior:\n1m92exp %\u0026gt;% extract.samples %\u0026gt;% .$sigma %\u0026gt;% dens(.,xlab=\u0026#34;sigma\u0026#34;,col=\u0026#34;blue\u0026#34;) 2mtext(\u0026#34;posterior\u0026#34;)  1m92exp %\u0026gt;% precis(.,depth = 2) 2m91exp %\u0026gt;% precis(.,depth = 2) 3m91unif %\u0026gt;% precis(.,depth = 2) 1mean sd 5.5% 94.5% n_eff Rhat4 2a[1] 1.01 0.02 0.98 1.03 1161 1 3b[1] 0.13 0.08 0.02 0.27 821 1 4sigma 0.12 0.01 0.10 0.15 1097 1 56mean sd 5.5% 94.5% n_eff Rhat4 7a[1] 1.01 0.02 0.98 1.04 1609 1 8b[1] 0.09 0.10 -0.06 0.24 1518 1 9sigma 0.12 0.01 0.10 0.15 1493 1 1011mean sd 5.5% 94.5% n_eff Rhat4 12a[1] 1.01 0.02 0.97 1.04 1744 1 13b[1] 0.10 0.09 -0.05 0.25 1874 1 14sigma 0.12 0.01 0.10 0.15 1824 1 We can see that there isn\u0026rsquo;t much difference, however, the main difference is in the b parameter, which seems to have fewer samples, and is also no longer takes any negative values.\nHOLD 9M3 Re-estimate one of the Stan models from the chapter, but at different numbers of warmup iterations. Be sure to use the same number of sampling iterations in each case. Compare the n_eff values. How much warmup is enough?\nSolution For brevity, we will re-use the same data and model as used in the previous questions.\n1warmTrial\u0026lt;-seq.int(10,10000,length.out = 10) 2nSampleEff\u0026lt;-matrix(NA,nrow=length(warmTrial),ncol=3) 3nSampleEffExp\u0026lt;-matrix(NA,nrow=length(warmTrial),ncol=3)    Uniform Model\n1for(i in 1:length(warmTrial)){ 2tmp\u0026lt;-ulam(m91unif,chains=4,cores=4,refresh=-1,warmup=warmTrial[i],iter=1000+warmTrial[i]) 3nSampleEff[i,]\u0026lt;-precis(tmp,2)$n_eff 4} 12Chain 1: 3Chain 1: Gradient evaluation took 9.7e-05 seconds 4Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.97 seconds. 5Chain 1: Adjust your expectations accordingly! 6Chain 1: 7Chain 1: 8Chain 1: WARNING: No variance estimation is 9Chain 1: performed for num_warmup \u0026lt; 20 10Chain 1: 11Chain 2: 12Chain 2: Gradient evaluation took 9.6e-05 seconds 13Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.96 seconds. 14Chain 2: Adjust your expectations accordingly! 15Chain 2: 16Chain 2: 17Chain 2: WARNING: No variance estimation is 18Chain 2: performed for num_warmup \u0026lt; 20 19Chain 2: 20Chain 3: 21Chain 3: Gradient evaluation took 0.000107 seconds 22Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.07 seconds. 23Chain 3: Adjust your expectations accordingly! 24Chain 3: 25Chain 3: 26Chain 3: WARNING: No variance estimation is 27Chain 3: performed for num_warmup \u0026lt; 20 28Chain 3: 29Chain 1: 30Chain 1: Elapsed Time: 0.00162 seconds (Warm-up) 31Chain 1: 0.1738 seconds (Sampling) 32Chain 1: 0.17542 seconds (Total) 33Chain 1: 34Chain 3: 35Chain 3: Elapsed Time: 0.003755 seconds (Warm-up) 36Chain 3: 0.059869 seconds (Sampling) 37Chain 3: 0.063624 seconds (Total) 38Chain 3: 39Chain 4: 40Chain 4: Gradient evaluation took 9.2e-05 seconds 41Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.92 seconds. 42Chain 4: Adjust your expectations accordingly! 43Chain 4: 44Chain 4: 45Chain 4: WARNING: No variance estimation is 46Chain 4: performed for num_warmup \u0026lt; 20 47Chain 4: 48Chain 2: 49Chain 2: Elapsed Time: 0.003924 seconds (Warm-up) 50Chain 2: 0.257815 seconds (Sampling) 51Chain 2: 0.261739 seconds (Total) 52Chain 2: 53Chain 4: 54Chain 4: Elapsed Time: 0.003735 seconds (Warm-up) 55Chain 4: 0.127719 seconds (Sampling) 56Chain 4: 0.131454 seconds (Total) 57Chain 4: 58Chain 1: 59Chain 1: Gradient evaluation took 0.000209 seconds 60Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 2.09 seconds. 61Chain 1: Adjust your expectations accordingly! 62Chain 1: 63Chain 1: 64Chain 2: 65Chain 2: Gradient evaluation took 7.8e-05 seconds 66Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.78 seconds. 67Chain 2: Adjust your expectations accordingly! 68Chain 2: 69Chain 2: 70Chain 3: 71Chain 3: Gradient evaluation took 6.2e-05 seconds 72Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.62 seconds. 73Chain 3: Adjust your expectations accordingly! 74Chain 3: 75Chain 3: 76Chain 4: 77Chain 4: Gradient evaluation took 8.2e-05 seconds 78Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.82 seconds. 79Chain 4: Adjust your expectations accordingly! 80Chain 4: 81Chain 4: 82Chain 1: 83Chain 1: Elapsed Time: 0.118609 seconds (Warm-up) 84Chain 1: 0.075057 seconds (Sampling) 85Chain 1: 0.193666 seconds (Total) 86Chain 1: 87Chain 2: 88Chain 2: Elapsed Time: 0.122167 seconds (Warm-up) 89Chain 2: 0.05402 seconds (Sampling) 90Chain 2: 0.176187 seconds (Total) 91Chain 2: 92Chain 3: 93Chain 3: Elapsed Time: 0.118156 seconds (Warm-up) 94Chain 3: 0.035528 seconds (Sampling) 95Chain 3: 0.153684 seconds (Total) 96Chain 3: 97Chain 4: 98Chain 4: Elapsed Time: 0.073505 seconds (Warm-up) 99Chain 4: 0.040445 seconds (Sampling) 100Chain 4: 0.11395 seconds (Total) 101Chain 4: 102Chain 1: 103Chain 1: Gradient evaluation took 9.8e-05 seconds 104Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.98 seconds. 105Chain 1: Adjust your expectations accordingly! 106Chain 1: 107Chain 1: 108Chain 2: 109Chain 2: Gradient evaluation took 7.3e-05 seconds 110Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.73 seconds. 111Chain 2: Adjust your expectations accordingly! 112Chain 2: 113Chain 2: 114Chain 3: 115Chain 3: Gradient evaluation took 0.000107 seconds 116Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.07 seconds. 117Chain 3: Adjust your expectations accordingly! 118Chain 3: 119Chain 3: 120Chain 4: 121Chain 4: Gradient evaluation took 0.000109 seconds 122Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 1.09 seconds. 123Chain 4: Adjust your expectations accordingly! 124Chain 4: 125Chain 4: 126Chain 1: 127Chain 1: Elapsed Time: 0.23286 seconds (Warm-up) 128Chain 1: 0.032903 seconds (Sampling) 129Chain 1: 0.265763 seconds (Total) 130Chain 1: 131Chain 2: 132Chain 2: Elapsed Time: 0.196139 seconds (Warm-up) 133Chain 2: 0.032946 seconds (Sampling) 134Chain 2: 0.229085 seconds (Total) 135Chain 2: 136Chain 3: 137Chain 3: Elapsed Time: 0.15298 seconds (Warm-up) 138Chain 3: 0.042238 seconds (Sampling) 139Chain 3: 0.195218 seconds (Total) 140Chain 3: 141Chain 4: 142Chain 4: Elapsed Time: 0.109015 seconds (Warm-up) 143Chain 4: 0.041119 seconds (Sampling) 144Chain 4: 0.150134 seconds (Total) 145Chain 4: 146Chain 1: 147Chain 1: Gradient evaluation took 7.4e-05 seconds 148Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.74 seconds. 149Chain 1: Adjust your expectations accordingly! 150Chain 1: 151Chain 1: 152Chain 2: 153Chain 2: Gradient evaluation took 7.9e-05 seconds 154Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.79 seconds. 155Chain 2: Adjust your expectations accordingly! 156Chain 2: 157Chain 2: 158Chain 3: 159Chain 3: Gradient evaluation took 0.000109 seconds 160Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.09 seconds. 161Chain 3: Adjust your expectations accordingly! 162Chain 3: 163Chain 3: 164Chain 4: 165Chain 4: Gradient evaluation took 0.000101 seconds 166Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 1.01 seconds. 167Chain 4: Adjust your expectations accordingly! 168Chain 4: 169Chain 4: 170Chain 1: 171Chain 1: Elapsed Time: 0.304209 seconds (Warm-up) 172Chain 1: 0.038004 seconds (Sampling) 173Chain 1: 0.342213 seconds (Total) 174Chain 1: 175Chain 2: 176Chain 2: Elapsed Time: 0.274087 seconds (Warm-up) 177Chain 2: 0.034831 seconds (Sampling) 178Chain 2: 0.308918 seconds (Total) 179Chain 2: 180Chain 3: 181Chain 3: Elapsed Time: 0.231719 seconds (Warm-up) 182Chain 3: 0.038131 seconds (Sampling) 183Chain 3: 0.26985 seconds (Total) 184Chain 3: 185Chain 4: 186Chain 4: Elapsed Time: 0.177718 seconds (Warm-up) 187Chain 4: 0.038546 seconds (Sampling) 188Chain 4: 0.216264 seconds (Total) 189Chain 4: 190Chain 1: 191Chain 1: Gradient evaluation took 0.000117 seconds 192Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.17 seconds. 193Chain 1: Adjust your expectations accordingly! 194Chain 1: 195Chain 1: 196Chain 2: 197Chain 2: Gradient evaluation took 7.3e-05 seconds 198Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.73 seconds. 199Chain 2: Adjust your expectations accordingly! 200Chain 2: 201Chain 2: 202Chain 3: 203Chain 3: Gradient evaluation took 7.2e-05 seconds 204Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.72 seconds. 205Chain 3: Adjust your expectations accordingly! 206Chain 3: 207Chain 3: 208Chain 4: 209Chain 4: Gradient evaluation took 7.3e-05 seconds 210Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.73 seconds. 211Chain 4: Adjust your expectations accordingly! 212Chain 4: 213Chain 4: 214Chain 1: 215Chain 1: Elapsed Time: 0.322041 seconds (Warm-up) 216Chain 1: 0.048995 seconds (Sampling) 217Chain 1: 0.371036 seconds (Total) 218Chain 1: 219Chain 2: 220Chain 2: Elapsed Time: 0.293325 seconds (Warm-up) 221Chain 2: 0.032541 seconds (Sampling) 222Chain 2: 0.325866 seconds (Total) 223Chain 2: 224Chain 3: 225Chain 3: Elapsed Time: 0.264383 seconds (Warm-up) 226Chain 3: 0.04051 seconds (Sampling) 227Chain 3: 0.304893 seconds (Total) 228Chain 3: 229Chain 4: 230Chain 4: Elapsed Time: 0.220301 seconds (Warm-up) 231Chain 4: 0.040218 seconds (Sampling) 232Chain 4: 0.260519 seconds (Total) 233Chain 4: 234Chain 1: 235Chain 1: Gradient evaluation took 4.9e-05 seconds 236Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.49 seconds. 237Chain 1: Adjust your expectations accordingly! 238Chain 1: 239Chain 1: 240Chain 2: 241Chain 2: Gradient evaluation took 5.1e-05 seconds 242Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.51 seconds. 243Chain 2: Adjust your expectations accordingly! 244Chain 2: 245Chain 2: 246Chain 3: 247Chain 3: Gradient evaluation took 3.9e-05 seconds 248Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds. 249Chain 3: Adjust your expectations accordingly! 250Chain 3: 251Chain 3: 252Chain 4: 253Chain 4: Gradient evaluation took 3.9e-05 seconds 254Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds. 255Chain 4: Adjust your expectations accordingly! 256Chain 4: 257Chain 4: 258Chain 1: 259Chain 1: Elapsed Time: 0.306918 seconds (Warm-up) 260Chain 1: 0.048194 seconds (Sampling) 261Chain 1: 0.355112 seconds (Total) 262Chain 1: 263Chain 2: 264Chain 2: Elapsed Time: 0.299256 seconds (Warm-up) 265Chain 2: 0.052776 seconds (Sampling) 266Chain 2: 0.352032 seconds (Total) 267Chain 2: 268Chain 3: 269Chain 3: Elapsed Time: 0.300093 seconds (Warm-up) 270Chain 3: 0.052132 seconds (Sampling) 271Chain 3: 0.352225 seconds (Total) 272Chain 3: 273Chain 4: 274Chain 4: Elapsed Time: 0.278897 seconds (Warm-up) 275Chain 4: 0.053532 seconds (Sampling) 276Chain 4: 0.332429 seconds (Total) 277Chain 4: 278Chain 1: 279Chain 1: Gradient evaluation took 9.4e-05 seconds 280Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.94 seconds. 281Chain 1: Adjust your expectations accordingly! 282Chain 1: 283Chain 1: 284Chain 2: 285Chain 2: Gradient evaluation took 8.4e-05 seconds 286Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.84 seconds. 287Chain 2: Adjust your expectations accordingly! 288Chain 2: 289Chain 2: 290Chain 3: 291Chain 3: Gradient evaluation took 7.1e-05 seconds 292Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.71 seconds. 293Chain 3: Adjust your expectations accordingly! 294Chain 3: 295Chain 3: 296Chain 4: 297Chain 4: Gradient evaluation took 6.2e-05 seconds 298Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.62 seconds. 299Chain 4: Adjust your expectations accordingly! 300Chain 4: 301Chain 4: 302Chain 1: 303Chain 1: Elapsed Time: 0.437519 seconds (Warm-up) 304Chain 1: 0.04052 seconds (Sampling) 305Chain 1: 0.478039 seconds (Total) 306Chain 1: 307Chain 3: 308Chain 3: Elapsed Time: 0.3479 seconds (Warm-up) 309Chain 3: 0.033007 seconds (Sampling) 310Chain 3: 0.380907 seconds (Total) 311Chain 3: 312Chain 2: 313Chain 2: Elapsed Time: 0.412403 seconds (Warm-up) 314Chain 2: 0.052948 seconds (Sampling) 315Chain 2: 0.465351 seconds (Total) 316Chain 2: 317Chain 4: 318Chain 4: Elapsed Time: 0.315601 seconds (Warm-up) 319Chain 4: 0.038006 seconds (Sampling) 320Chain 4: 0.353607 seconds (Total) 321Chain 4: 322Chain 1: 323Chain 1: Gradient evaluation took 8.8e-05 seconds 324Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.88 seconds. 325Chain 1: Adjust your expectations accordingly! 326Chain 1: 327Chain 1: 328Chain 2: 329Chain 2: Gradient evaluation took 5.8e-05 seconds 330Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.58 seconds. 331Chain 2: Adjust your expectations accordingly! 332Chain 2: 333Chain 2: 334Chain 3: 335Chain 3: Gradient evaluation took 5.9e-05 seconds 336Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.59 seconds. 337Chain 3: Adjust your expectations accordingly! 338Chain 3: 339Chain 3: 340Chain 4: 341Chain 4: Gradient evaluation took 4.4e-05 seconds 342Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.44 seconds. 343Chain 4: Adjust your expectations accordingly! 344Chain 4: 345Chain 4: 346Chain 1: 347Chain 1: Elapsed Time: 0.387624 seconds (Warm-up) 348Chain 1: 0.033915 seconds (Sampling) 349Chain 1: 0.421539 seconds (Total) 350Chain 1: 351Chain 2: 352Chain 2: Elapsed Time: 0.385989 seconds (Warm-up) 353Chain 2: 0.044089 seconds (Sampling) 354Chain 2: 0.430078 seconds (Total) 355Chain 2: 356Chain 3: 357Chain 3: Elapsed Time: 0.409957 seconds (Warm-up) 358Chain 3: 0.039642 seconds (Sampling) 359Chain 3: 0.449599 seconds (Total) 360Chain 3: 361Chain 4: 362Chain 4: Elapsed Time: 0.363549 seconds (Warm-up) 363Chain 4: 0.042624 seconds (Sampling) 364Chain 4: 0.406173 seconds (Total) 365Chain 4: 366Chain 1: 367Chain 1: Gradient evaluation took 7.8e-05 seconds 368Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.78 seconds. 369Chain 1: Adjust your expectations accordingly! 370Chain 1: 371Chain 1: 372Chain 2: 373Chain 2: Gradient evaluation took 5.1e-05 seconds 374Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.51 seconds. 375Chain 2: Adjust your expectations accordingly! 376Chain 2: 377Chain 2: 378Chain 3: 379Chain 3: Gradient evaluation took 4.7e-05 seconds 380Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.47 seconds. 381Chain 3: Adjust your expectations accordingly! 382Chain 3: 383Chain 3: 384Chain 4: 385Chain 4: Gradient evaluation took 5.1e-05 seconds 386Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.51 seconds. 387Chain 4: Adjust your expectations accordingly! 388Chain 4: 389Chain 4: 390Chain 2: 391Chain 2: Elapsed Time: 0.355205 seconds (Warm-up) 392Chain 2: 0.051537 seconds (Sampling) 393Chain 2: 0.406742 seconds (Total) 394Chain 2: 395Chain 1: 396Chain 1: Elapsed Time: 0.394264 seconds (Warm-up) 397Chain 1: 0.037432 seconds (Sampling) 398Chain 1: 0.431696 seconds (Total) 399Chain 1: 400Chain 3: 401Chain 3: Elapsed Time: 0.383287 seconds (Warm-up) 402Chain 3: 0.036322 seconds (Sampling) 403Chain 3: 0.419609 seconds (Total) 404Chain 3: 405Chain 4: 406Chain 4: Elapsed Time: 0.335487 seconds (Warm-up) 407Chain 4: 0.048271 seconds (Sampling) 408Chain 4: 0.383758 seconds (Total) 409Chain 4: 410Chain 1: 411Chain 1: Gradient evaluation took 5e-05 seconds 412Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.5 seconds. 413Chain 1: Adjust your expectations accordingly! 414Chain 1: 415Chain 1: 416Chain 2: 417Chain 2: Gradient evaluation took 5.5e-05 seconds 418Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.55 seconds. 419Chain 2: Adjust your expectations accordingly! 420Chain 2: 421Chain 2: 422Chain 3: 423Chain 3: Gradient evaluation took 6.3e-05 seconds 424Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.63 seconds. 425Chain 3: Adjust your expectations accordingly! 426Chain 3: 427Chain 3: 428Chain 4: 429Chain 4: Gradient evaluation took 5.1e-05 seconds 430Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.51 seconds. 431Chain 4: Adjust your expectations accordingly! 432Chain 4: 433Chain 4: 434Chain 1: 435Chain 1: Elapsed Time: 0.454863 seconds (Warm-up) 436Chain 1: 0.037816 seconds (Sampling) 437Chain 1: 0.492679 seconds (Total) 438Chain 1: 439Chain 2: 440Chain 2: Elapsed Time: 0.415499 seconds (Warm-up) 441Chain 2: 0.037093 seconds (Sampling) 442Chain 2: 0.452592 seconds (Total) 443Chain 2: 444Chain 3: 445Chain 3: Elapsed Time: 0.363295 seconds (Warm-up) 446Chain 3: 0.061002 seconds (Sampling) 447Chain 3: 0.424297 seconds (Total) 448Chain 3: 449Chain 4: 450Chain 4: Elapsed Time: 0.462541 seconds (Warm-up) 451Chain 4: 0.043532 seconds (Sampling) 452Chain 4: 0.506073 seconds (Total) 453Chain 4: 454Warning messages: 4551: There were 470 divergent transitions after warmup. Increasing adapt_delta above 0.95 may help. See 456http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 4572: There were 1 chains where the estimated Bayesian Fraction of Missing Information was low. See 458http://mc-stan.org/misc/warnings.html#bfmi-low 4593: Examine the pairs() plot to diagnose sampling problems 4604614: The largest R-hat is 1.05, indicating chains have not mixed. 462Running the chains for more iterations may help. See 463http://mc-stan.org/misc/warnings.html#r-hat 4645: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. 465Running the chains for more iterations may help. See 466http://mc-stan.org/misc/warnings.html#bulk-ess 4676: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. 468Running the chains for more iterations may help. See 469http://mc-stan.org/misc/warnings.html#tail-ess 1nSampleEff %\u0026gt;% tibble(nWarmup=warmTrial) 1\u001b[90m# A tibble: 10 x 2\u001b[39m 2.[,\u0026#34;a[1]\u0026#34;] [,\u0026#34;b[1]\u0026#34;] [,\u0026#34;sigma\u0026#34;] nWarmup 3\u001b[3m\u001b[90m\u0026lt;dbl\u0026gt;\u001b[39m\u001b[23m \u001b[3m\u001b[90m\u0026lt;dbl\u0026gt;\u001b[39m\u001b[23m \u001b[3m\u001b[90m\u0026lt;dbl\u0026gt;\u001b[39m\u001b[23m \u001b[3m\u001b[90m\u0026lt;int\u0026gt;\u001b[39m\u001b[23m 4\u001b[90m 1\u001b[39m \u001b[4m1\u001b[24m051. 399. 45.8 10 5\u001b[90m 2\u001b[39m \u001b[4m3\u001b[24m101. \u001b[4m3\u001b[24m085. \u001b[4m3\u001b[24m135. \u001b[4m1\u001b[24m120 6\u001b[90m 3\u001b[39m \u001b[4m3\u001b[24m515. \u001b[4m3\u001b[24m529. \u001b[4m3\u001b[24m214. \u001b[4m2\u001b[24m230 7\u001b[90m 4\u001b[39m \u001b[4m3\u001b[24m122. \u001b[4m3\u001b[24m277. \u001b[4m3\u001b[24m522. \u001b[4m3\u001b[24m340 8\u001b[90m 5\u001b[39m \u001b[4m3\u001b[24m145. \u001b[4m3\u001b[24m382. \u001b[4m3\u001b[24m322. \u001b[4m4\u001b[24m450 9\u001b[90m 6\u001b[39m \u001b[4m3\u001b[24m378. \u001b[4m3\u001b[24m193. \u001b[4m3\u001b[24m701. \u001b[4m5\u001b[24m560 10\u001b[90m 7\u001b[39m \u001b[4m3\u001b[24m299. \u001b[4m3\u001b[24m539. \u001b[4m3\u001b[24m149. \u001b[4m6\u001b[24m670 11\u001b[90m 8\u001b[39m \u001b[4m3\u001b[24m570. \u001b[4m3\u001b[24m050. \u001b[4m3\u001b[24m079. \u001b[4m7\u001b[24m780 12\u001b[90m 9\u001b[39m \u001b[4m3\u001b[24m247. \u001b[4m3\u001b[24m148. \u001b[4m3\u001b[24m340. \u001b[4m8\u001b[24m890 13\u001b[90m10\u001b[39m \u001b[4m3\u001b[24m159. \u001b[4m2\u001b[24m929. \u001b[4m2\u001b[24m960. \u001b[4m1\u001b[24m\u001b[4m0\u001b[24m000      Exponential Model\n1for(i in 1:length(warmTrial)){ 2tmp\u0026lt;-ulam(m91exp,chains=4,cores=4,refresh=-1,warmup=warmTrial[i],iter=1000+warmTrial[i]) 3nSampleEffExp[i,]\u0026lt;-precis(tmp,2)$n_eff 4} 12Chain 1: 3Chain 1: Gradient evaluation took 3e-05 seconds 4Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds. 5Chain 1: Adjust your expectations accordingly! 6Chain 1: 7Chain 1: 8Chain 1: WARNING: No variance estimation is 9Chain 1: performed for num_warmup \u0026lt; 20 10Chain 1: 11Chain 2: 12Chain 2: Gradient evaluation took 2.8e-05 seconds 13Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. 14Chain 2: Adjust your expectations accordingly! 15Chain 2: 16Chain 2: 17Chain 2: WARNING: No variance estimation is 18Chain 2: performed for num_warmup \u0026lt; 20 19Chain 2: 20Chain 3: 21Chain 3: Gradient evaluation took 2.7e-05 seconds 22Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. 23Chain 3: Adjust your expectations accordingly! 24Chain 3: 25Chain 3: 26Chain 3: WARNING: No variance estimation is 27Chain 3: performed for num_warmup \u0026lt; 20 28Chain 3: 29Chain 4: 30Chain 4: Gradient evaluation took 2.5e-05 seconds 31Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds. 32Chain 4: Adjust your expectations accordingly! 33Chain 4: 34Chain 4: 35Chain 4: WARNING: No variance estimation is 36Chain 4: performed for num_warmup \u0026lt; 20 37Chain 4: 38Chain 1: 39Chain 1: Elapsed Time: 0.000825 seconds (Warm-up) 40Chain 1: 0.074412 seconds (Sampling) 41Chain 1: 0.075237 seconds (Total) 42Chain 1: 43Chain 3: 44Chain 3: Elapsed Time: 0.000433 seconds (Warm-up) 45Chain 3: 0.05124 seconds (Sampling) 46Chain 3: 0.051673 seconds (Total) 47Chain 3: 48Chain 4: 49Chain 4: Elapsed Time: 0.001827 seconds (Warm-up) 50Chain 4: 0.059028 seconds (Sampling) 51Chain 4: 0.060855 seconds (Total) 52Chain 4: 53Chain 2: 54Chain 2: Elapsed Time: 0.005092 seconds (Warm-up) 55Chain 2: 0.111894 seconds (Sampling) 56Chain 2: 0.116986 seconds (Total) 57Chain 2: 58Chain 1: 59Chain 1: Gradient evaluation took 2.7e-05 seconds 60Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. 61Chain 1: Adjust your expectations accordingly! 62Chain 1: 63Chain 1: 64Chain 2: 65Chain 2: Gradient evaluation took 2.7e-05 seconds 66Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. 67Chain 2: Adjust your expectations accordingly! 68Chain 2: 69Chain 2: 70Chain 3: 71Chain 3: Gradient evaluation took 2.7e-05 seconds 72Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. 73Chain 3: Adjust your expectations accordingly! 74Chain 3: 75Chain 3: 76Chain 4: 77Chain 4: Gradient evaluation took 3.3e-05 seconds 78Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.33 seconds. 79Chain 4: Adjust your expectations accordingly! 80Chain 4: 81Chain 4: 82Chain 1: 83Chain 1: Elapsed Time: 0.048625 seconds (Warm-up) 84Chain 1: 0.03552 seconds (Sampling) 85Chain 1: 0.084145 seconds (Total) 86Chain 1: 87Chain 2: 88Chain 2: Elapsed Time: 0.045476 seconds (Warm-up) 89Chain 2: 0.03068 seconds (Sampling) 90Chain 2: 0.076156 seconds (Total) 91Chain 2: 92Chain 3: 93Chain 3: Elapsed Time: 0.059723 seconds (Warm-up) 94Chain 3: 0.034697 seconds (Sampling) 95Chain 3: 0.09442 seconds (Total) 96Chain 3: 97Chain 4: 98Chain 4: Elapsed Time: 0.055658 seconds (Warm-up) 99Chain 4: 0.030508 seconds (Sampling) 100Chain 4: 0.086166 seconds (Total) 101Chain 4: 102Chain 1: 103Chain 1: Gradient evaluation took 3.7e-05 seconds 104Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds. 105Chain 1: Adjust your expectations accordingly! 106Chain 1: 107Chain 1: 108Chain 2: 109Chain 2: Gradient evaluation took 2.8e-05 seconds 110Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. 111Chain 2: Adjust your expectations accordingly! 112Chain 2: 113Chain 2: 114Chain 3: 115Chain 3: Gradient evaluation took 2.6e-05 seconds 116Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds. 117Chain 3: Adjust your expectations accordingly! 118Chain 3: 119Chain 3: 120Chain 4: 121Chain 4: Gradient evaluation took 2.6e-05 seconds 122Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds. 123Chain 4: Adjust your expectations accordingly! 124Chain 4: 125Chain 4: 126Chain 1: 127Chain 1: Elapsed Time: 0.10462 seconds (Warm-up) 128Chain 1: 0.035886 seconds (Sampling) 129Chain 1: 0.140506 seconds (Total) 130Chain 1: 131Chain 3: 132Chain 3: Elapsed Time: 0.08377 seconds (Warm-up) 133Chain 3: 0.036938 seconds (Sampling) 134Chain 3: 0.120708 seconds (Total) 135Chain 3: 136Chain 2: 137Chain 2: Elapsed Time: 0.096442 seconds (Warm-up) 138Chain 2: 0.0419 seconds (Sampling) 139Chain 2: 0.138342 seconds (Total) 140Chain 2: 141Chain 4: 142Chain 4: Elapsed Time: 0.091513 seconds (Warm-up) 143Chain 4: 0.062068 seconds (Sampling) 144Chain 4: 0.153581 seconds (Total) 145Chain 4: 146Chain 1: 147Chain 1: Gradient evaluation took 3.1e-05 seconds 148Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. 149Chain 1: Adjust your expectations accordingly! 150Chain 1: 151Chain 1: 152Chain 2: 153Chain 2: Gradient evaluation took 3.1e-05 seconds 154Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. 155Chain 2: Adjust your expectations accordingly! 156Chain 2: 157Chain 2: 158Chain 3: 159Chain 3: Gradient evaluation took 2.6e-05 seconds 160Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds. 161Chain 3: Adjust your expectations accordingly! 162Chain 3: 163Chain 3: 164Chain 4: 165Chain 4: Gradient evaluation took 2.4e-05 seconds 166Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds. 167Chain 4: Adjust your expectations accordingly! 168Chain 4: 169Chain 4: 170Chain 1: 171Chain 1: Elapsed Time: 0.12662 seconds (Warm-up) 172Chain 1: 0.033816 seconds (Sampling) 173Chain 1: 0.160436 seconds (Total) 174Chain 1: 175Chain 2: 176Chain 2: Elapsed Time: 0.119929 seconds (Warm-up) 177Chain 2: 0.048545 seconds (Sampling) 178Chain 2: 0.168474 seconds (Total) 179Chain 2: 180Chain 4: 181Chain 4: Elapsed Time: 0.118087 seconds (Warm-up) 182Chain 4: 0.040127 seconds (Sampling) 183Chain 4: 0.158214 seconds (Total) 184Chain 4: 185Chain 3: 186Chain 3: Elapsed Time: 0.131859 seconds (Warm-up) 187Chain 3: 0.053882 seconds (Sampling) 188Chain 3: 0.185741 seconds (Total) 189Chain 3: 190Chain 1: 191Chain 1: Gradient evaluation took 2.8e-05 seconds 192Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. 193Chain 1: Adjust your expectations accordingly! 194Chain 1: 195Chain 1: 196Chain 2: 197Chain 2: Gradient evaluation took 3.1e-05 seconds 198Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. 199Chain 2: Adjust your expectations accordingly! 200Chain 2: 201Chain 2: 202Chain 3: 203Chain 3: Gradient evaluation took 3.8e-05 seconds 204Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.38 seconds. 205Chain 3: Adjust your expectations accordingly! 206Chain 3: 207Chain 3: 208Chain 4: 209Chain 4: Gradient evaluation took 2.8e-05 seconds 210Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. 211Chain 4: Adjust your expectations accordingly! 212Chain 4: 213Chain 4: 214Chain 1: 215Chain 1: Elapsed Time: 0.178377 seconds (Warm-up) 216Chain 1: 0.051505 seconds (Sampling) 217Chain 1: 0.229882 seconds (Total) 218Chain 1: 219Chain 2: 220Chain 2: Elapsed Time: 0.19769 seconds (Warm-up) 221Chain 2: 0.028853 seconds (Sampling) 222Chain 2: 0.226543 seconds (Total) 223Chain 2: 224Chain 3: 225Chain 3: Elapsed Time: 0.209976 seconds (Warm-up) 226Chain 3: 0.049889 seconds (Sampling) 227Chain 3: 0.259865 seconds (Total) 228Chain 3: 229Chain 4: 230Chain 4: Elapsed Time: 0.233406 seconds (Warm-up) 231Chain 4: 0.03065 seconds (Sampling) 232Chain 4: 0.264056 seconds (Total) 233Chain 4: 234Chain 1: 235Chain 1: Gradient evaluation took 3.1e-05 seconds 236Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. 237Chain 1: Adjust your expectations accordingly! 238Chain 1: 239Chain 1: 240Chain 2: 241Chain 2: Gradient evaluation took 3e-05 seconds 242Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds. 243Chain 2: Adjust your expectations accordingly! 244Chain 2: 245Chain 2: 246Chain 3: 247Chain 3: Gradient evaluation took 2.9e-05 seconds 248Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.29 seconds. 249Chain 3: Adjust your expectations accordingly! 250Chain 3: 251Chain 3: 252Chain 4: 253Chain 4: Gradient evaluation took 3.3e-05 seconds 254Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.33 seconds. 255Chain 4: Adjust your expectations accordingly! 256Chain 4: 257Chain 4: 258Chain 1: 259Chain 1: Elapsed Time: 0.226122 seconds (Warm-up) 260Chain 1: 0.031202 seconds (Sampling) 261Chain 1: 0.257324 seconds (Total) 262Chain 1: 263Chain 2: 264Chain 2: Elapsed Time: 0.229103 seconds (Warm-up) 265Chain 2: 0.028798 seconds (Sampling) 266Chain 2: 0.257901 seconds (Total) 267Chain 2: 268Chain 3: 269Chain 3: Elapsed Time: 0.238441 seconds (Warm-up) 270Chain 3: 0.03459 seconds (Sampling) 271Chain 3: 0.273031 seconds (Total) 272Chain 3: 273Chain 4: 274Chain 4: Elapsed Time: 0.228212 seconds (Warm-up) 275Chain 4: 0.036574 seconds (Sampling) 276Chain 4: 0.264786 seconds (Total) 277Chain 4: 278Chain 1: 279Chain 1: Gradient evaluation took 3e-05 seconds 280Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds. 281Chain 1: Adjust your expectations accordingly! 282Chain 1: 283Chain 1: 284Chain 2: 285Chain 2: Gradient evaluation took 2.7e-05 seconds 286Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. 287Chain 2: Adjust your expectations accordingly! 288Chain 2: 289Chain 2: 290Chain 3: 291Chain 3: Gradient evaluation took 2.8e-05 seconds 292Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. 293Chain 3: Adjust your expectations accordingly! 294Chain 3: 295Chain 3: 296Chain 4: 297Chain 4: Gradient evaluation took 2.4e-05 seconds 298Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds. 299Chain 4: Adjust your expectations accordingly! 300Chain 4: 301Chain 4: 302Chain 1: 303Chain 1: Elapsed Time: 0.285094 seconds (Warm-up) 304Chain 1: 0.048246 seconds (Sampling) 305Chain 1: 0.33334 seconds (Total) 306Chain 1: 307Chain 4: 308Chain 4: Elapsed Time: 0.263814 seconds (Warm-up) 309Chain 4: 0.034411 seconds (Sampling) 310Chain 4: 0.298225 seconds (Total) 311Chain 4: 312Chain 2: 313Chain 2: Elapsed Time: 0.305613 seconds (Warm-up) 314Chain 2: 0.039627 seconds (Sampling) 315Chain 2: 0.34524 seconds (Total) 316Chain 2: 317Chain 3: 318Chain 3: Elapsed Time: 0.314974 seconds (Warm-up) 319Chain 3: 0.040995 seconds (Sampling) 320Chain 3: 0.355969 seconds (Total) 321Chain 3: 322Chain 1: 323Chain 1: Gradient evaluation took 3e-05 seconds 324Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds. 325Chain 1: Adjust your expectations accordingly! 326Chain 1: 327Chain 1: 328Chain 2: 329Chain 2: Gradient evaluation took 2.8e-05 seconds 330Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. 331Chain 2: Adjust your expectations accordingly! 332Chain 2: 333Chain 2: 334Chain 3: 335Chain 3: Gradient evaluation took 3e-05 seconds 336Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds. 337Chain 3: Adjust your expectations accordingly! 338Chain 3: 339Chain 3: 340Chain 4: 341Chain 4: Gradient evaluation took 2.7e-05 seconds 342Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. 343Chain 4: Adjust your expectations accordingly! 344Chain 4: 345Chain 4: 346Chain 1: 347Chain 1: Elapsed Time: 0.293873 seconds (Warm-up) 348Chain 1: 0.028447 seconds (Sampling) 349Chain 1: 0.32232 seconds (Total) 350Chain 1: 351Chain 3: 352Chain 3: Elapsed Time: 0.288431 seconds (Warm-up) 353Chain 3: 0.04518 seconds (Sampling) 354Chain 3: 0.333611 seconds (Total) 355Chain 3: 356Chain 2: 357Chain 2: Elapsed Time: 0.36752 seconds (Warm-up) 358Chain 2: 0.027453 seconds (Sampling) 359Chain 2: 0.394973 seconds (Total) 360Chain 2: 361Chain 4: 362Chain 4: Elapsed Time: 0.383786 seconds (Warm-up) 363Chain 4: 0.032154 seconds (Sampling) 364Chain 4: 0.41594 seconds (Total) 365Chain 4: 366Chain 1: 367Chain 1: Gradient evaluation took 3.1e-05 seconds 368Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. 369Chain 1: Adjust your expectations accordingly! 370Chain 1: 371Chain 1: 372Chain 2: 373Chain 2: Gradient evaluation took 2.7e-05 seconds 374Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. 375Chain 2: Adjust your expectations accordingly! 376Chain 2: 377Chain 2: 378Chain 3: 379Chain 3: Gradient evaluation took 2.7e-05 seconds 380Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. 381Chain 3: Adjust your expectations accordingly! 382Chain 3: 383Chain 3: 384Chain 4: 385Chain 4: Gradient evaluation took 3.1e-05 seconds 386Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. 387Chain 4: Adjust your expectations accordingly! 388Chain 4: 389Chain 4: 390Chain 1: 391Chain 1: Elapsed Time: 0.354637 seconds (Warm-up) 392Chain 1: 0.027996 seconds (Sampling) 393Chain 1: 0.382633 seconds (Total) 394Chain 1: 395Chain 2: 396Chain 2: Elapsed Time: 0.339298 seconds (Warm-up) 397Chain 2: 0.030313 seconds (Sampling) 398Chain 2: 0.369611 seconds (Total) 399Chain 2: 400Chain 4: 401Chain 4: Elapsed Time: 0.319312 seconds (Warm-up) 402Chain 4: 0.027904 seconds (Sampling) 403Chain 4: 0.347216 seconds (Total) 404Chain 4: 405Chain 3: 406Chain 3: Elapsed Time: 0.333814 seconds (Warm-up) 407Chain 3: 0.032747 seconds (Sampling) 408Chain 3: 0.366561 seconds (Total) 409Chain 3: 410Chain 1: 411Chain 1: Gradient evaluation took 3.8e-05 seconds 412Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.38 seconds. 413Chain 1: Adjust your expectations accordingly! 414Chain 1: 415Chain 1: 416Chain 2: 417Chain 2: Gradient evaluation took 3.7e-05 seconds 418Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds. 419Chain 2: Adjust your expectations accordingly! 420Chain 2: 421Chain 2: 422Chain 3: 423Chain 3: Gradient evaluation took 2.6e-05 seconds 424Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds. 425Chain 3: Adjust your expectations accordingly! 426Chain 3: 427Chain 3: 428Chain 4: 429Chain 4: Gradient evaluation took 3.1e-05 seconds 430Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. 431Chain 4: Adjust your expectations accordingly! 432Chain 4: 433Chain 4: 434Chain 1: 435Chain 1: Elapsed Time: 0.343002 seconds (Warm-up) 436Chain 1: 0.028223 seconds (Sampling) 437Chain 1: 0.371225 seconds (Total) 438Chain 1: 439Chain 3: 440Chain 3: Elapsed Time: 0.305501 seconds (Warm-up) 441Chain 3: 0.027824 seconds (Sampling) 442Chain 3: 0.333325 seconds (Total) 443Chain 3: 444Chain 2: 445Chain 2: Elapsed Time: 0.358116 seconds (Warm-up) 446Chain 2: 0.02975 seconds (Sampling) 447Chain 2: 0.387866 seconds (Total) 448Chain 2: 449Chain 4: 450Chain 4: Elapsed Time: 0.330673 seconds (Warm-up) 451Chain 4: 0.030407 seconds (Sampling) 452Chain 4: 0.36108 seconds (Total) 453Chain 4: 454Warning messages: 4551: There were 14 divergent transitions after warmup. Increasing adapt_delta above 0.95 may help. See 456http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 4572: Examine the pairs() plot to diagnose sampling problems 1nSampleEffExp %\u0026gt;% tibble(nWarmup=warmTrial) 1\u001b[90m# A tibble: 10 x 2\u001b[39m 2.[,1] [,2] [,3] nWarmup 3\u001b[3m\u001b[90m\u0026lt;dbl\u0026gt;\u001b[39m\u001b[23m \u001b[3m\u001b[90m\u0026lt;dbl\u0026gt;\u001b[39m\u001b[23m \u001b[3m\u001b[90m\u0026lt;dbl\u0026gt;\u001b[39m\u001b[23m \u001b[3m\u001b[90m\u0026lt;int\u0026gt;\u001b[39m\u001b[23m 4\u001b[90m 1\u001b[39m \u001b[4m3\u001b[24m063. \u001b[4m1\u001b[24m265. \u001b[4m1\u001b[24m045. 10 5\u001b[90m 2\u001b[39m \u001b[4m3\u001b[24m238. \u001b[4m2\u001b[24m909. \u001b[4m3\u001b[24m425. \u001b[4m1\u001b[24m120 6\u001b[90m 3\u001b[39m \u001b[4m3\u001b[24m268. \u001b[4m2\u001b[24m949. \u001b[4m2\u001b[24m966. \u001b[4m2\u001b[24m230 7\u001b[90m 4\u001b[39m \u001b[4m3\u001b[24m123. \u001b[4m3\u001b[24m257. \u001b[4m3\u001b[24m218. \u001b[4m3\u001b[24m340 8\u001b[90m 5\u001b[39m \u001b[4m3\u001b[24m227. \u001b[4m3\u001b[24m449. \u001b[4m3\u001b[24m554. \u001b[4m4\u001b[24m450 9\u001b[90m 6\u001b[39m \u001b[4m3\u001b[24m494. \u001b[4m3\u001b[24m743. \u001b[4m3\u001b[24m343. \u001b[4m5\u001b[24m560 10\u001b[90m 7\u001b[39m \u001b[4m3\u001b[24m200. \u001b[4m3\u001b[24m012. \u001b[4m2\u001b[24m879. \u001b[4m6\u001b[24m670 11\u001b[90m 8\u001b[39m \u001b[4m3\u001b[24m210. \u001b[4m3\u001b[24m207. \u001b[4m3\u001b[24m125. \u001b[4m7\u001b[24m780 12\u001b[90m 9\u001b[39m \u001b[4m2\u001b[24m919. \u001b[4m3\u001b[24m329. \u001b[4m3\u001b[24m060. \u001b[4m8\u001b[24m890 13\u001b[90m10\u001b[39m \u001b[4m2\u001b[24m951. \u001b[4m3\u001b[24m461. \u001b[4m3\u001b[24m078. \u001b[4m1\u001b[24m\u001b[4m0\u001b[24m000 It is important to note that the divergent transitions are probably why the number of effective samples decrease in the last two rows.\n     Results\nWe can see that the number of effective samples increases almost constantly. This is probably due to correlations in the chain, which are removed during the warmup period.\n  Chapter XI: God Spiked The Integers Easy Questions (Ch11) 11E1 If an event has probability \\(0.35\\), what are the log-odds of this event?\nSolution 1log(0.35/(1-0.35)) 1[1] -0.6190392 11E2 If an event has log-odds \\(3.2\\), what is the probability of this event?\nSolution 1logistic(3.2) 1[1] 0.9608343 11E3 Suppose that a coefficient in a logistic regression has value \\(1.7\\). What does this imply about the proportional change in odds of the outcome?\nSolution 1exp(1.7) 1[1] 5.473947 Note that this is not really the change in the variable, but the proportional odds.\nHOLD 11E4 Why do Poisson regressions sometimes require the use of an offset? Provide an example.\nSolution The Poisson distribution is often understood as a limiting distribution of the Binomial where \\(λ=np\\) as \\(n→∞\\) and \\(p→0\\). The single parameter thus expresses the expected value, but is often used to encode different time-steps as well. Essentially, the distribution assumes a constant rate in time or space, and thus the change in exposure, is expressed by the offset, which is the logarithm of the exposure.\nFor any case where samples are drawn from populations which have different aggregation time periods but are still within the purview of a Poisson distribution, the offset is a natural way of expressing these.\nTo leverage the example of the book, when constructing a model to account for the fact that one Monastery calculates their averages on a weekly basis, while the other averages by day, this constraint should be modeled by having differing offsets.\nQuestions of Medium Complexity (Ch11) HOLD 11M2 If a coefficient in a Poisson regression has values \\(1.7\\), what does this imply about the change in the outcome?\nSolution 1exp(1.7) 1[1] 5.473947 The coefficient in a Poisson regression implies that the proportional change in the count will be ~5.474 when the predictor variable increase by one unit.\nHOLD 11M3 Explain why the logit link is appropriate for a binomial generalized linear model.\nSolution The logit link essentially connects a parameter constrained between zero and one and the real space. The logit function is defined as:\n\\[\\mathrm{logit}(pᵢ)=\\log{\\frac{pᵢ}{1-pᵢ}}\\]\nWhere \\(pᵢ\\) is a probability mass. This link makes sense for a GLM since the predicted value is a probability distribution parameter, and we would like to obtain this from a linear model which spans the entire set of real numbers.\n1curve(logit,from=-0.5,to=1.5)  The link function maps a parameter onto a linear model.\nHOLD 11M4 Explain why the log is appropriate for a Poisson generalized linear model.\nSolution The log function ensures that the parameter cannot take values which are less than zero. This is a natural consequence of the function definition.\n1curve(log,from=-0.5,to=100000)  The log link assumes that the parameter value is the exponentiation of the linear model.\nThis makes sense for a Poisson GLM as the Poisson distribution does not accept negative values.\nHOLD 11M5 What would it imply to use a logit link for the mean of a Poisson generalized linear model? Can you think of a real research problem for which this would make sense?\nSolution We should write this out more explicitly.\nThis implies that the mean μ lies between zero and one. Since the Poisson distribution is defined by a single parameter, this does limit the model outputs. The premise of a Poisson regression problem is that the GLM models a count with an unknown maximum, so it does seem to be a very severe restriction.\nTo my mind this is feasible for constrained problems, where the Poisson distribution is to be followed but only within a particular range for some reason, and when the Binomial (of which the Poisson is a special case), decreases too slowly.\nIt was mentioned on the class forums, that the COVID-19 problem was modeled with a two parameter generalized link function, i.e. \\(\\log{\\frac{p}{S-p}}\\) which essentially constrains the model to have Poisson dynamics but with an output mean between 0 and S.\nHOLD 11M6 State the constraints for which the binomial and Poisson distributions have maximum entropy. Are the constraints different at all for binomial and Poisson? Why or why not?\nSolution The Binomial distribution is defined to be the maximum entropy distributions are:\n Discrete binary outcomes Constant probability (or expectation)  This is defined by the number of outcomes (n) as well as the probability (p). The experiment is essentially reduced to a series of independent and identical Bernoulli trials with only two outcomes. The Poisson distribution is derived as a limiting form of the Binomial, where \\(n→∞\\) and \\(p→0\\). Since this does not change the underlying constraints, this is still a maximum entropy distribution.\nHOLD 11M8 Revisit the data(Kline) islands example. This time drop Hawaii from the sample and refit the models. What changes do you observe?\nSolution 1data(Kline) 2kDat\u0026lt;-Kline 3kDat\u0026lt;-kDat %\u0026gt;% dplyr::mutate(cid=ifelse(contact==\u0026#34;high\u0026#34;,2,1), 4stdPop=standardize(log(population))) %\u0026gt;% filter(culture!=\u0026#34;Hawaii\u0026#34;) 5datList\u0026lt;-list( 6totTools=kDat$total_tools, 7stdPop=kDat$stdPop, 8cid=as.integer(kDat$cid) 9) We can now fit this.\n1m11m10res\u0026lt;-ulam( 2alist( 3totTools ~ dpois(lambda), 4log(lambda)\u0026lt;-a[cid]+b[cid]*stdPop, 5a[cid] ~ dnorm(3,0.5), 6b[cid] ~ dnorm(0,0.2) 7),data=datList, chains=4, cores=4 8) 1m11m10res %\u0026gt;% precis(2) 1mean sd 5.5% 94.5% n_eff Rhat4 2a[1] 3.18 0.12 2.99 3.37 1621 1 3a[2] 3.61 0.08 3.48 3.73 1962 1 4b[1] 0.19 0.13 -0.01 0.39 1639 1 5b[2] 0.19 0.16 -0.06 0.44 1830 1 We see that the slopes are now the same, which makes sense since in this data-set Hawaii was the only outlier.\nChapter XII: Monsters and Mixtures Easy Questions (Ch12) HOLD 12E4 Over-dispersion is common in count data. Give an example of a natural process that might produce over-dispersed counts. Can you also give an example of a process that might produce /under-/dispersed counts?\nSolution    Over-dispersion\nOver dispersion is essentially the occurrence of greater variability than accounted for based on the statistical model. The presence of over-dispersion is typically due to heterogeneity in populations. This heterogeneity may arise from simple aggregation issues like in the case considered in the text, of Monasteries which accumulate data weekly or daily, in-spite of following the same Poisson model.\n     Under-dispersion\nUnder dispersion is essentially the occurrence of less variability than accounted for based on the statistical model. The clearest example of under-dispersion is from the draws of an MCMC sampler. The number of effective samples is typically lower than the number of samples, as the data is highly correlated (autocorrelated) as the sampler draws sequential samples. For a count model, if a hidden rate limiting variable exists and has not been accounted for, then the variation in counts is lowered, and will show up as under-dispersion.\n  Hard Questions (Ch12) 12H1 In 2014, a paper was published that was entitle \u0026ldquo;Female hurricanes are deadlier than male hurricanes.\u0026rdquo; As the title suggests, the paper claimed that hurricanes with female names have caused greater loss of life, and the explanation given is that people unconsciously rate female hurricanes as less dangerous and so are less likely to evacuate. Statisticians severely criticized the paper after publication. Here, you\u0026rsquo;ll explore the complete data used in the paper and consider the hypothesis that hurricanes with female names are deadlier. Load the data with:\n1library(rethinking) 2data(Hurricanes) Acquaint yourself with the columns by inspecting the help ?Hurricanes. In this problem, you\u0026rsquo;ll focus on predicting deaths using feminity as a predictor. You can use quap or ulam. Compare the model to an intercept-only Poisson model of deaths. How strong is the association between feminity of name and deaths? Which storms does the model fit (retrodict) well? Which storms does it fit poorly?\nSolution Since I have no understanding of hurricanes except that it is unlikely to have too much of an effect. I will run through some sample priors. Presumably, most hurricanes do not kill over a thousand people. Furthermore, a-priori, I would not like to assume that femininity is positive or negative, so I will instead encode a belief that it shouldn\u0026rsquo;t matter much either way, ergo a Gaussian.\n1N\u0026lt;-100 2a\u0026lt;-rnorm(N,1,0.5) 3bF\u0026lt;-rnorm(N,0.5,2) 4seqF\u0026lt;-seq(from=-2,to=2,length.out=100) 5plot(NULL,xlim=c(-2,2),ylim=c(0,1000),xlab=\u0026#34;Femininity\u0026#34;,ylab=\u0026#34;deaths\u0026#34;) 6for(i in 1:N) lines(seqF,exp(a[i]+bF[i]*seqF),col=grau())  This seems to be reasonable to me. It does have a bit of an unreasonable focus on 0, but it does also seem to mostly hug the x-axis in a way indicating my prior belief that it should not matter all that much. There is enough diversity in the priors to allow for stronger trends, but they are by and large unlikely.\nNow we can actually use these in a model.\n1data(Hurricanes) 2hurDat\u0026lt;-Hurricanes %\u0026gt;% as.data.frame 3hurDat\u0026lt;-hurDat %\u0026gt;% dplyr::mutate(femStd=standardize(femininity)) 4datListH\u0026lt;-list(deaths=hurDat$deaths,femStd=hurDat$femStd) 5m12h1norm\u0026lt;-ulam(alist(deaths ~ dpois(lambda), 6log(lambda) \u0026lt;- a+bF*femStd, 7a ~ dnorm(1,0.5), 8bF ~ dnorm(0.5,2) 9),data=datListH, chains=4, cores=4,log_lik = TRUE) 1SAMPLING FOR MODEL \u0026#39;bd16fb771b491de48a3f8ce09fc68301\u0026#39; NOW (CHAIN 1). 23SAMPLING FOR MODEL \u0026#39;bd16fb771b491de48a3f8ce09fc68301\u0026#39; NOW (CHAIN 2). 4Chain 2: 5Chain 1: 6Chain Chain 12: : Gradient evaluation took 5.1e-05 secondsGradient evaluation took 3.9e-05 seconds 78Chain Chain 12: : 1000 transitions using 10 leapfrog steps per transition would take 0.51 seconds.1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds. 910Chain Chain 12: : Adjust your expectations accordingly!Adjust your expectations accordingly! 1112Chain Chain 12: : 1314Chain Chain 12: : 1516Chain Chain 21: Iteration: 1 / 1000 [ 0%] (Warmup) 17: Iteration: 1 / 1000 [ 0%] (Warmup) 18Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) 19Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) 2021SAMPLING FOR MODEL \u0026#39;bd16fb771b491de48a3f8ce09fc68301\u0026#39; NOW (CHAIN 3). 22Chain 3: 23Chain 3: Gradient evaluation took 4.3e-05 seconds 24Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.43 seconds. 25Chain 3: Adjust your expectations accordingly! 26Chain 3: 27Chain 3: 28Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) 29Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) 30Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) 31Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) 32Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) 3334SAMPLING FOR MODEL \u0026#39;bd16fb771b491de48a3f8ce09fc68301\u0026#39; NOW (CHAIN 4). 35Chain 4: 36Chain 4: Gradient evaluation took 3.1e-05 seconds 37Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. 38Chain 4: Adjust your expectations accordingly! 39Chain 4: 40Chain 4: 41Chain 4: Iteration: 1 / 1000 [ 0%] (Warmup) 42Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) 43Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) 44Chain 4: Iteration: 100 / 1000 [ 10%] (Warmup) 45Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) 46Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) 47Chain 4: Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) 48Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) 49Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) 50Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) 51Chain 3: Iteration: 300 / 1000 [ 30%] (Warmup) 52Iteration: 200 / 1000 [ 20%] (Warmup) 53Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) 54Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) 55Chain 3: Iteration: 400 / 1000 [ 40%] (Warmup) 56Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) 57Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) 58Chain 4: Iteration: 300 / 1000 [ 30%] (Warmup) 59Chain 3: Iteration: 500 / 1000 [ 50%] (Warmup) 60Chain 3: Iteration: 501 / 1000 [ 50%] (Sampling) 61Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) 62Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) 63Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) 64Chain 4: Iteration: 400 / 1000 [ 40%] (Warmup) 65Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) 66Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) 67Chain 4: Iteration: 500 / 1000 [ 50%] (Warmup) 68Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) 69Chain 4: Iteration: 501 / 1000 [ 50%] (Sampling) 70Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) 71Chain 1: 72Chain 1: Elapsed Time: 0.055115 seconds (Warm-up) 73Chain 1: 0.042775 seconds (Sampling) 74Chain 1: 0.09789 seconds (Total) 75Chain 1: 76Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) 77Chain 4: Iteration: 600 / 1000 [ 60%] (Sampling) 78Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) 79Chain 2: 80Chain 2: Elapsed Time: 0.050636 seconds (Warm-up) 81Chain 2: 0.049372 seconds (Sampling) 82Chain 2: 0.100008 seconds (Total) 83Chain 2: 84Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) 85Chain 4: Iteration: 700 / 1000 [ 70%] (Sampling) 86Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) 87Chain 3: 88Chain 3: Elapsed Time: 0.056256 seconds (Warm-up) 89Chain 3: 0.039069 seconds (Sampling) 90Chain 3: 0.095325 seconds (Total) 91Chain 3: 92Chain 4: Iteration: 800 / 1000 [ 80%] (Sampling) 93Chain 4: Iteration: 900 / 1000 [ 90%] (Sampling) 94Chain 4: Iteration: 1000 / 1000 [100%] (Sampling) 95Chain 4: 96Chain 4: Elapsed Time: 0.054509 seconds (Warm-up) 97Chain 4: 0.036572 seconds (Sampling) 98Chain 4: 0.091081 seconds (Total) 99Chain 4: 1m12h1norm %\u0026gt;% precis 1mean sd 5.5% 94.5% n_eff Rhat4 2a 3.00 0.02 2.96 3.04 1334 1 3bF 0.24 0.02 0.20 0.28 1361 1 There are several points to be noted:\n The number of effective samples is far lower than the number simulated (4000) The bounds are quite tight for the values  All told, we can see that the model is quite certain of the values, but given the low number of effective samples it would make sense to run this model longer.\nFurthermore, the model infers that from our data, there is a positive correlation (quite a high one) between femininity and deaths.\nThis is best seen by actually visualizing the results.\n1m12h1norm %\u0026gt;% pairs  We should check the posterior as well.\n1k\u0026lt;-PSIS(m12h1norm,pointwise=TRUE)$k 2plot(hurDat$femStd,hurDat$deaths,xlab=\u0026#34;Standardized Femininity\u0026#34;,ylab=\u0026#34;Deaths\u0026#34;,col=rangi2,pch=hurDat$female, lwd=2,cex=1+normalize(k)) 3## Axis for predictions 4ns\u0026lt;-500 5femininity\u0026lt;-seq(from=min(hurDat$femStd), to=max(hurDat$femStd),length.out = ns) 6## Female 7lambda\u0026lt;-link(m12h1norm,data=data.frame(femStd=femininity)) 8lmu\u0026lt;-apply(lambda,2,mean) 9lci\u0026lt;-apply(lambda,2,PI) 10lines(femininity,lmu,lty=2,lwd=1.5) 11shade(lci,femininity,xpd=TRUE)  We can see that the model prediction does not actually handle the data very well, in that it is evident the model simply cannot account for the high death rate values. We have plotted the 89% interval as well (the default for PI).\nWe can also inspect the expect PSISk values.\n1m12h1norm %\u0026gt;% PSISk %\u0026gt;% summary 1Some Pareto k values are very high (\u0026gt;1). Set pointwise=TRUE to inspect individual points. 2Min. 1st Qu. Median Mean 3rd Qu. Max. 3-0.1800 -0.0500 0.0600 0.1521 0.1500 2.5800 Clearly, there are some points with high leverage.\nFinally we can plot the posterior distribution.\n1posterior\u0026lt;-m12h1norm %\u0026gt;% extract.samples 2fem\u0026lt;-sample(hurDat$femStd,3) 3for(i in 1:10){ 4curve(dgamma(x,exp(posterior$a[i]+posterior$bF[i]*(fem[1]))), from=0,to=100, col=\u0026#34;red\u0026#34;,ylab=\u0026#34;Density\u0026#34;,xlab=\u0026#34;Average deaths\u0026#34;,add=ifelse(i==1,FALSE,TRUE)) 5curve(dgamma(x,exp(posterior$a[i]+posterior$bF[i]*(fem[2]))), from=0,to=100, col=\u0026#34;blue\u0026#34;,add=TRUE) 6curve(dgamma(x,exp(posterior$a[i]+posterior$bF[i]*(fem[3]))), from=0,to=100, col=\u0026#34;black\u0026#34;,add=TRUE)} 7legend(\u0026#34;topright\u0026#34;, 8legend=c(sprintf(\u0026#34;Femininity=%.3f\u0026#34;,fem[1]), sprintf(\u0026#34;Femininity=%.3f\u0026#34;,fem[2]), sprintf(\u0026#34;Femininity=%.3f\u0026#34;,fem[3])), 9col=c(\u0026#34;red\u0026#34;,\u0026#34;blue\u0026#34;,\u0026#34;black\u0026#34;), 10pch=19 11)  As is to be expected, for each value of femininity, we have one family of gamma distributions.\nHOLD 12H2 Counts are nearly always over-dispersed relative to Poisson. So fit a gamma-Poisson (aka negative-binomial) model to predict deaths using feminity. Show that the over-dispersed model no longer shows as precise a positive association between feminity and deaths, with an $89$% interval that overlaps zero. Can you explain why the association diminished in strength?\nSolution Recall that the gamma-Poisson has two parameters, one for the rate, and the other for the dispersion of rates. Larger values of the dispersion imply that the distribution is more similar to a pure Poisson process. For ensuring meaningful comparisons, we will keep the same priors as before. We will need a scale parameter, but we will postulate a simple exponential prior for that.\n1data(Hurricanes) 2hurDat\u0026lt;-Hurricanes %\u0026gt;% as.data.frame 3hurDat\u0026lt;-hurDat %\u0026gt;% dplyr::mutate(femStd=standardize(femininity)) 4datListH\u0026lt;-list(deaths=hurDat$deaths,femStd=hurDat$femStd) 5m12h2norm\u0026lt;-ulam(alist(deaths ~ dgampois(lambda,scale), 6log(lambda) \u0026lt;- a+bF*femStd, 7a ~ dnorm(1,0.5), 8bF ~ dnorm(0.5,2), 9scale ~ dexp(1) 10),data=datListH, chains=4, cores=4,log_lik = TRUE) 1SAMPLING FOR MODEL \u0026#39;5dc94bd836781d34a208695cf643c56c\u0026#39; NOW (CHAIN 1). 2Chain 1: 3Chain 1: Gradient evaluation took 0.00012 seconds 4Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.2 seconds. 5Chain 1: Adjust your expectations accordingly! 6Chain 1: 7Chain 1: 89SAMPLING FOR MODEL \u0026#39;5dc94bd836781d34a208695cf643c56c\u0026#39; NOW (CHAIN 2). 10Chain 2: 11Chain 2: Gradient evaluation took 9.6e-05 seconds 12Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.96 seconds. 13Chain 2: Adjust your expectations accordingly! 14Chain 2: 15Chain 2: 16Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) 17Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup) 1819SAMPLING FOR MODEL \u0026#39;5dc94bd836781d34a208695cf643c56c\u0026#39; NOW (CHAIN 3). 20Chain 3: 21Chain 3: Gradient evaluation took 9.1e-05 seconds 22Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.91 seconds. 23Chain 3: Adjust your expectations accordingly! 24Chain 3: 25Chain 3: 26Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) 2728SAMPLING FOR MODEL \u0026#39;5dc94bd836781d34a208695cf643c56c\u0026#39; NOW (CHAIN 4). 29Chain 4: 30Chain 4: Gradient evaluation took 8.2e-05 seconds 31Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.82 seconds. 32Chain 4: Adjust your expectations accordingly! 33Chain 4: 34Chain 4: 35Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) 36Chain 4: Iteration: 1 / 1000 [ 0%] (Warmup) 37Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) 38Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) 39Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) 40Chain 4: Iteration: 100 / 1000 [ 10%] (Warmup) 41Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) 42Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) 43Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) 44Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) 45Chain 4: Iteration: 200 / 1000 [ 20%] (Warmup) 46Chain 3: Iteration: 300 / 1000 [ 30%] (Warmup) 47Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) 48Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) 49Chain 3: Iteration: 400 / 1000 [ 40%] (Warmup) 50Chain 4: Iteration: 300 / 1000 [ 30%] (Warmup) 51Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) 52Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) 53Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) 54Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) 55Chain 3: Iteration: 500 / 1000 [ 50%] (Warmup) 56Chain 3: Iteration: 501 / 1000 [ 50%] (Sampling) 57Chain 4: Iteration: 400 / 1000 [ 40%] (Warmup) 58Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) 59Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) 60Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) 61Chain 4: Iteration: 500 / 1000 [ 50%] (Warmup) 62Chain 4: Iteration: 501 / 1000 [ 50%] (Sampling) 63Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) 64Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) 65Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) 66Chain 4: Iteration: 600 / 1000 [ 60%] (Sampling) 67Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) 68Chain 4: Iteration: 700 / 1000 [ 70%] (Sampling) 69Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) 70Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) 71Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) 72Chain 4: Iteration: 800 / 1000 [ 80%] (Sampling) 73Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) 74Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) 75Chain 2: 76Chain 2: Elapsed Time: 0.119773 seconds (Warm-up) 77Chain 2: 0.091705 seconds (Sampling) 78Chain 2: 0.211478 seconds (Total) 79Chain 2: 80Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) 81Chain 4: Iteration: 900 / 1000 [ 90%] (Sampling) 82Chain 4: Iteration: 1000 / 1000 [100%] (Sampling) 83Chain 4: 84Chain 4: Elapsed Time: 0.122953 seconds (Warm-up) 85Chain 4: 0.082244 seconds (Sampling) 86Chain 4: 0.205197 seconds (Total) 87Chain 4: 88Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) 89Chain 3: 90Chain 3: Elapsed Time: 0.115457 seconds (Warm-up) 91Chain 3: 0.114422 seconds (Sampling) 92Chain 3: 0.229879 seconds (Total) 93Chain 3: 94Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) 95Chain 1: 96Chain 1: Elapsed Time: 0.110931 seconds (Warm-up) 97Chain 1: 0.129622 seconds (Sampling) 98Chain 1: 0.240553 seconds (Total) 99Chain 1: 1m12h2norm %\u0026gt;% precis 2m12h1norm %\u0026gt;% precis 1mean sd 5.5% 94.5% n_eff Rhat4 2a 2.86 0.14 2.64 3.08 1705 1 3bF 0.21 0.14 -0.01 0.44 1946 1 4scale 0.45 0.06 0.35 0.55 1957 1 56mean sd 5.5% 94.5% n_eff Rhat4 7a 3.00 0.02 2.96 3.04 1334 1 8bF 0.24 0.02 0.20 0.28 1361 1 We note that the effective number of samples in the second model are greater, which implies that this model is less prone to correlations. We can quantify this with the WAIC as well.\n1WAIC(m12h2norm) %\u0026gt;% rbind(WAIC(m12h1norm)) %\u0026gt;% tibble(model=c(\u0026#34;Gamma-Poisson\u0026#34;,\u0026#34;Poisson\u0026#34;)) %\u0026gt;% toOrg 1| WAIC | lppd | penalty | std_err | model | 2|-----------------+-------------------+------------------+------------------+---------------| 3| 710.78471929582 | -351.457619486557 | 3.93474016135277 | 34.6128979470592 | Gamma-Poisson | 4| 4427.5667952452 | -2080.92835360918 | 132.855044013418 | 1009.13483879188 | Poisson | The WAIC values show that the gamma-Poisson model is less likely to over-fit.\nWe would like to see the models together.\n1m12h1norm %\u0026gt;% precis(pars=c(\u0026#34;a\u0026#34;,\u0026#34;bF\u0026#34;)) %\u0026gt;% plot(col=\u0026#34;blue\u0026#34;) 2m12h2norm %\u0026gt;% precis(pars=c(\u0026#34;a\u0026#34;,\u0026#34;bF\u0026#34;)) %\u0026gt;% plot(add=TRUE,col=\u0026#34;red\u0026#34;)  We can see that there is little to no difference in the means, though the intervals seem wider than before. This is more clear in the coeftab plot.\n1plot(coeftab(m12h1norm,m12h2norm))  An important consequence of this is that the model is no longer completely sure that there is any effect of femininity on the death count, as can be seen from the wider uncertainty interval, which includes 0.\nWe can also visualize the model with pairs.\n1pairs(m12h2norm)  1k\u0026lt;-PSIS(m12h2norm,pointwise=TRUE)$k 2plot(hurDat$femStd,hurDat$deaths,xlab=\u0026#34;Standardized Femininity\u0026#34;,ylab=\u0026#34;Deaths\u0026#34;,col=rangi2,pch=hurDat$female, lwd=2,cex=1+normalize(k)) 3## Axis for predictions 4ns\u0026lt;-500 5femininity\u0026lt;-seq(from=min(hurDat$femStd),to=max(hurDat$femStd),length.out = ns) 6## Gamma Poisson 7lambda\u0026lt;-link(m12h2norm,data=data.frame(femStd=femininity)) 8lmu\u0026lt;-apply(lambda,2,mean) 9lci\u0026lt;-apply(lambda,2,PI) 10shade(lci,femininity,xpd=TRUE,col=\u0026#34;red\u0026#34;) 11lines(femininity,lmu,lty=2,lwd=1.5,col=\u0026#34;white\u0026#34;) 12## Poisson 13lambda\u0026lt;-link(m12h1norm,data=data.frame(femStd=femininity)) 14lmu\u0026lt;-apply(lambda,2,mean) 15lci\u0026lt;-apply(lambda,2,PI) 16shade(lci,femininity,xpd=TRUE,col=\u0026#34;blue\u0026#34;) 17lines(femininity,lmu,lty=2,lwd=1.5,col=\u0026#34;white\u0026#34;)  Clearly, the uncertainty of the newer model is much greater, even though the predictions do not differ much. Unfortunately, both models fail to account for storms with high death counts.\nWe would also like to plot the predicted distributions.\n1posterior\u0026lt;-m12h2norm %\u0026gt;% extract.samples 2fem\u0026lt;-sample(hurDat$femStd,2) 3for(i in 1:100){ 4curve(dgamma2(x,exp(posterior$a[i]+posterior$bF[i]*(fem[1])),posterior$scale[i]), from=0,to=100,col=\u0026#34;red\u0026#34;,ylab=\u0026#34;Density\u0026#34;,xlab=\u0026#34;Average deaths\u0026#34;,add=ifelse(i==1,FALSE,TRUE)) 5curve(dgamma2(x,exp(posterior$a[i]+posterior$bF[i]*(fem[2])),posterior$scale[i]), from=0,to=100,col=\u0026#34;blue\u0026#34;,add=TRUE) 6} 7legend(\u0026#34;topright\u0026#34;, 8legend=c(sprintf(\u0026#34;Femininity=%.3f\u0026#34;,fem[1]), sprintf(\u0026#34;Femininity=%.3f\u0026#34;,fem[2])), 9col=c(\u0026#34;red\u0026#34;,\u0026#34;blue\u0026#34;), 10pch=19 11)  This clearly has more spread than the previous predictions. By definition, the dispersion term tends to spread the distribution out, with higher values of the dispersion corresponding to a \u0026ldquo;true\u0026rdquo; Poisson distribution.\nA: Colophon To ensure that this document is fully reproducible at a later date, we will record the session info.\n1devtools::session_info() 1─ Session info ─────────────────────────────────────────────────────────────── 2setting value 3version R version 4.0.0 (2020-04-24) 4os Arch Linux 5system x86_64, linux-gnu 6ui X11 7language (EN) 8collate en_US.UTF-8 9ctype en_US.UTF-8 10tz Iceland 11date 2020-06-21 1213─ Packages ─────────────────────────────────────────────────────────────────── 14package * version date lib source 15arrayhelpers 1.1-0 2020-02-04 [167] CRAN (R 4.0.0) 16assertthat 0.2.1 2019-03-21 [34] CRAN (R 4.0.0) 17backports 1.1.6 2020-04-05 [68] CRAN (R 4.0.0) 18boot 1.3-24 2019-12-20 [5] CRAN (R 4.0.0) 19broom 0.5.6 2020-04-20 [67] CRAN (R 4.0.0) 20callr 3.4.3 2020-03-28 [87] CRAN (R 4.0.0) 21cellranger 1.1.0 2016-07-27 [55] CRAN (R 4.0.0) 22cli 2.0.2 2020-02-28 [33] CRAN (R 4.0.0) 23coda 0.19-3 2019-07-05 [169] CRAN (R 4.0.0) 24colorspace 1.4-1 2019-03-18 [97] CRAN (R 4.0.0) 25crayon 1.3.4 2017-09-16 [35] CRAN (R 4.0.0) 26curl 4.3 2019-12-02 [26] CRAN (R 4.0.0) 27dagitty * 0.2-2 2016-08-26 [244] CRAN (R 4.0.0) 28data.table * 1.12.8 2019-12-09 [27] CRAN (R 4.0.0) 29DBI 1.1.0 2019-12-15 [77] CRAN (R 4.0.0) 30dbplyr 1.4.3 2020-04-19 [76] CRAN (R 4.0.0) 31desc 1.2.0 2018-05-01 [84] CRAN (R 4.0.0) 32devtools * 2.3.0 2020-04-10 [219] CRAN (R 4.0.0) 33digest 0.6.25 2020-02-23 [42] CRAN (R 4.0.0) 34dplyr * 0.8.5 2020-03-07 [69] CRAN (R 4.0.0) 35ellipsis 0.3.0 2019-09-20 [30] CRAN (R 4.0.0) 36evaluate 0.14 2019-05-28 [82] CRAN (R 4.0.0) 37fansi 0.4.1 2020-01-08 [36] CRAN (R 4.0.0) 38forcats * 0.5.0 2020-03-01 [29] CRAN (R 4.0.0) 39fs 1.4.1 2020-04-04 [109] CRAN (R 4.0.0) 40generics 0.0.2 2018-11-29 [71] CRAN (R 4.0.0) 41ggplot2 * 3.3.0 2020-03-05 [78] CRAN (R 4.0.0) 42glue * 1.4.0 2020-04-03 [37] CRAN (R 4.0.0) 43gridExtra 2.3 2017-09-09 [123] CRAN (R 4.0.0) 44gtable 0.3.0 2019-03-25 [79] CRAN (R 4.0.0) 45haven 2.2.0 2019-11-08 [28] CRAN (R 4.0.0) 46hms 0.5.3 2020-01-08 [44] CRAN (R 4.0.0) 47htmltools 0.4.0 2019-10-04 [112] CRAN (R 4.0.0) 48httr 1.4.1 2019-08-05 [100] CRAN (R 4.0.0) 49inline 0.3.15 2018-05-18 [162] CRAN (R 4.0.0) 50jsonlite 1.6.1 2020-02-02 [101] CRAN (R 4.0.0) 51kableExtra * 1.1.0 2019-03-16 [212] CRAN (R 4.0.0) 52knitr 1.28 2020-02-06 [113] CRAN (R 4.0.0) 53latex2exp * 0.4.0 2015-11-30 [211] CRAN (R 4.0.0) 54lattice 0.20-41 2020-04-02 [6] CRAN (R 4.0.0) 55lifecycle 0.2.0 2020-03-06 [38] CRAN (R 4.0.0) 56loo 2.2.0 2019-12-19 [163] CRAN (R 4.0.0) 57lubridate 1.7.8 2020-04-06 [106] CRAN (R 4.0.0) 58magrittr 1.5 2014-11-22 [21] CRAN (R 4.0.0) 59MASS 7.3-51.5 2019-12-20 [7] CRAN (R 4.0.0) 60matrixStats 0.56.0 2020-03-13 [164] CRAN (R 4.0.0) 61memoise 1.1.0 2017-04-21 [229] CRAN (R 4.0.0) 62modelr 0.1.6 2020-02-22 [107] CRAN (R 4.0.0) 63munsell 0.5.0 2018-06-12 [96] CRAN (R 4.0.0) 64mvtnorm 1.1-0 2020-02-24 [243] CRAN (R 4.0.0) 65nlme 3.1-147 2020-04-13 [11] CRAN (R 4.0.0) 66orgutils * 0.4-1 2017-03-21 [209] CRAN (R 4.0.0) 67pillar 1.4.3 2019-12-20 [39] CRAN (R 4.0.0) 68pkgbuild 1.0.6 2019-10-09 [86] CRAN (R 4.0.0) 69pkgconfig 2.0.3 2019-09-22 [43] CRAN (R 4.0.0) 70pkgload 1.0.2 2018-10-29 [83] CRAN (R 4.0.0) 71plyr 1.8.6 2020-03-03 [73] CRAN (R 4.0.0) 72prettyunits 1.1.1 2020-01-24 [58] CRAN (R 4.0.0) 73printr * 0.1 2017-05-19 [214] CRAN (R 4.0.0) 74processx 3.4.2 2020-02-09 [88] CRAN (R 4.0.0) 75ps 1.3.2 2020-02-13 [89] CRAN (R 4.0.0) 76purrr * 0.3.4 2020-04-17 [50] CRAN (R 4.0.0) 77R6 2.4.1 2019-11-12 [48] CRAN (R 4.0.0) 78Rcpp 1.0.4.6 2020-04-09 [10] CRAN (R 4.0.0) 79readr * 1.3.1 2018-12-21 [45] CRAN (R 4.0.0) 80readxl 1.3.1 2019-03-13 [54] CRAN (R 4.0.0) 81remotes 2.1.1 2020-02-15 [233] CRAN (R 4.0.0) 82reprex 0.3.0 2019-05-16 [108] CRAN (R 4.0.0) 83rethinking * 2.01 2020-06-06 [242] local 84rlang 0.4.5 2020-03-01 [31] CRAN (R 4.0.0) 85rmarkdown 2.1 2020-01-20 [110] CRAN (R 4.0.0) 86rprojroot 1.3-2 2018-01-03 [85] CRAN (R 4.0.0) 87rstan * 2.19.3 2020-02-11 [161] CRAN (R 4.0.0) 88rstudioapi 0.11 2020-02-07 [91] CRAN (R 4.0.0) 89rvest 0.3.5 2019-11-08 [120] CRAN (R 4.0.0) 90scales 1.1.0 2019-11-18 [93] CRAN (R 4.0.0) 91sessioninfo 1.1.1 2018-11-05 [231] CRAN (R 4.0.0) 92shape 1.4.4 2018-02-07 [193] CRAN (R 4.0.0) 93StanHeaders * 2.19.2 2020-02-11 [165] CRAN (R 4.0.0) 94stringi 1.4.6 2020-02-17 [52] CRAN (R 4.0.0) 95stringr * 1.4.0 2019-02-10 [74] CRAN (R 4.0.0) 96svUnit 1.0.3 2020-04-20 [168] CRAN (R 4.0.0) 97testthat 2.3.2 2020-03-02 [81] CRAN (R 4.0.0) 98textutils 0.2-0 2020-01-07 [210] CRAN (R 4.0.0) 99tibble * 3.0.1 2020-04-20 [32] CRAN (R 4.0.0) 100tidybayes * 2.0.3 2020-04-04 [166] CRAN (R 4.0.0) 101tidybayes.rethinking * 2.0.3.9000 2020-06-07 [246] local 102tidyr * 1.0.2 2020-01-24 [75] CRAN (R 4.0.0) 103tidyselect 1.0.0 2020-01-27 [49] CRAN (R 4.0.0) 104tidyverse * 1.3.0 2019-11-21 [66] CRAN (R 4.0.0) 105usethis * 1.6.0 2020-04-09 [238] CRAN (R 4.0.0) 106V8 3.0.2 2020-03-14 [245] CRAN (R 4.0.0) 107vctrs 0.2.4 2020-03-10 [41] CRAN (R 4.0.0) 108viridisLite 0.3.0 2018-02-01 [99] CRAN (R 4.0.0) 109webshot 0.5.2 2019-11-22 [213] CRAN (R 4.0.0) 110withr 2.2.0 2020-04-20 [90] CRAN (R 4.0.0) 111xfun 0.13 2020-04-13 [116] CRAN (R 4.0.0) 112xml2 1.3.2 2020-04-23 [122] CRAN (R 4.0.0) 113114[1] /nix/store/xzd8h53xkyvfm3kvj5ab6znp685wi04w-r-car-3.0-7/library 115[2] /nix/store/mhr8zw9bmxarc3n821b83i0gz2j9zlrq-r-abind-1.4-5/library 116[3] /nix/store/hp86nhr0787vib3l8mkw0gf9nxwb45im-r-carData-3.0-3/library 117[4] /nix/store/vhw7s2h5ds6sp110z2yvilchv8j9jch5-r-lme4-1.1-23/library 118[5] /nix/store/987n8g0zy9sjvfvnsck1bkkcknw05yvb-r-boot-1.3-24/library 119[6] /nix/store/jxxxxyz4c1k5g3drd35gsrbjdg028d11-r-lattice-0.20-41/library 120[7] /nix/store/q9zfm5h53m8rd08xcsdcwaag31k4z1pf-r-MASS-7.3-51.5/library 121[8] /nix/store/kjkm50sr144yvrhl5axfgykbiy13pbmg-r-Matrix-1.2-18/library 122[9] /nix/store/8786z5lgy8h3akfjgj3yq5yq4s17rhjy-r-minqa-1.2.4/library 123[10] /nix/store/93wv3j0z1nzqp6fjsm9v7v8bf8d1xkm2-r-Rcpp-1.0.4.6/library 124[11] /nix/store/akfw6zsmawmz8lmjkww0rnqrazm4mqp0-r-nlme-3.1-147/library 125[12] /nix/store/rxs0d9bbn8qhw7wmkfb21yk5abp6lpq1-r-nloptr-1.2.2.1/library 126[13] /nix/store/8n0jfiqn4275i58qgld0dv8zdaihdzrk-r-RcppEigen-0.3.3.7.0/library 127[14] /nix/store/8vxrma33rhc96260zsi1jiw7dy3v2mm4-r-statmod-1.4.34/library 128[15] /nix/store/2y46pb5x9lh8m0hdmzajnx7sc1bk9ihl-r-maptools-0.9-9/library 129[16] /nix/store/iwf9nxx1v883wlv0p88q947hpz5lhfh7-r-foreign-0.8-78/library 130[17] /nix/store/rl9sjqply6rjbnz5k792ghm62ybv76px-r-sp-1.4-1/library 131[18] /nix/store/ws4bkzyv2vj5pyn1hgwyy6nlp48arz0n-r-mgcv-1.8-31/library 132[19] /nix/store/307dzxrmnqk4p86560a02r64x1fhhmxb-r-nnet-7.3-13/library 133[20] /nix/store/g2zpzkdb9hzkza1wpcbrk58119v1wyaf-r-pbkrtest-0.4-8.6/library 134[21] /nix/store/p0l503fr8960vld70w6ilmknxs5qwq77-r-magrittr-1.5/library 135[22] /nix/store/rmjpcaw3i446kwnjgcxcaid0yac36cj2-r-quantreg-5.55/library 136[23] /nix/store/10mzmnvc5jjgk2xzasia522pk60a30qz-r-MatrixModels-0.4-1/library 137[24] /nix/store/6qwdzvmnnmhjwdnvg2zmvv6wafd1vf91-r-SparseM-1.78/library 138[25] /nix/store/aa9c39a3yiqkh1h7pbngjlbr7czvc7yi-r-rio-0.5.16/library 139[26] /nix/store/2fx4vqlybgwp5rhhy6pssqx7h1a927fn-r-curl-4.3/library 140[27] /nix/store/k4m3fn1kqvvvn8y33kd57gq49hr3ar8y-r-data.table-1.12.8/library 141[28] /nix/store/651hfjylqzmsf565wyx474vyjny771gy-r-haven-2.2.0/library 142[29] /nix/store/a3rnz28irmqvmj8axj5x5j1am2c3gzs4-r-forcats-0.5.0/library 143[30] /nix/store/j8v4gzib137q2cml31hvvfkrc0f60pp5-r-ellipsis-0.3.0/library 144[31] /nix/store/xaswqlnamf4k8vwx0x3wav3l0x60sag0-r-rlang-0.4.5/library 145[32] /nix/store/dqm3xpix2jwhhhr67s6fgrwbw7hizap7-r-tibble-3.0.1/library 146[33] /nix/store/v7xfsq6d97wpn6m0hjrac78w5xawbr8a-r-cli-2.0.2/library 147[34] /nix/store/fikjasr98klhk9cf44x4lhi57vh3pmkg-r-assertthat-0.2.1/library 148[35] /nix/store/3fya6cd38vsqdj0gjb7bcsy00sirlyw1-r-crayon-1.3.4/library 149[36] /nix/store/payqi9bwh216rwhaq07jgc26l4fv1zsb-r-fansi-0.4.1/library 150[37] /nix/store/h6a61ghws7yrdxlg412xl1im37z5r28i-r-glue-1.4.0/library 151[38] /nix/store/y8mjbia1wbnq26dkigr0p3xxwrbzsc2r-r-lifecycle-0.2.0/library 152[39] /nix/store/kwaghh12cnifgvcbvlv2anx0hd5f4ild-r-pillar-1.4.3/library 153[40] /nix/store/k1phn8j10nni7gzvcgp0vc25dby6bb77-r-utf8-1.1.4/library 154[41] /nix/store/k3b77y8v7zsshpp1ccs8jwk2i2g4rm9a-r-vctrs-0.2.4/library 155[42] /nix/store/iibjmbh7vj0d0bfafz98yn29ymg43gkw-r-digest-0.6.25/library 156[43] /nix/store/aqsj4k3pgm80qk4jjg7sh3ac28n6alv0-r-pkgconfig-2.0.3/library 157[44] /nix/store/i7c5v8s4hd9rlqah3bbvy06yywjqwdgk-r-hms-0.5.3/library 158[45] /nix/store/2fyrk58cmcbrxid66rbwjli7y114lvrm-r-readr-1.3.1/library 159[46] /nix/store/163xq2g5nblqgh7qhvzb6mvgg6qdrirj-r-BH-1.72.0-3/library 160[47] /nix/store/dr27b6k49prwgrjs0v30b6mf5lxa36pk-r-clipr-0.7.0/library 161[48] /nix/store/bghvqg9mcaj2jkbwpy0di6c563v24acz-r-R6-2.4.1/library 162[49] /nix/store/nq8jdq7nlg9xns4xpgyj6sqv8p4ny1wz-r-tidyselect-1.0.0/library 163[50] /nix/store/zlwhf75qld7vmwx3d4bdws057ld4mqbp-r-purrr-0.3.4/library 164[51] /nix/store/0gbmmnbpqlr69l573ymkcx8154fvlaca-r-openxlsx-4.1.4/library 165[52] /nix/store/1m1q4rmwx56dvx9rdzfsfq0jpw3hw0yx-r-stringi-1.4.6/library 166[53] /nix/store/mhy5vnvbsl4q7dcinwx3vqlyywxphbfd-r-zip-2.0.4/library 167[54] /nix/store/88sp7f7q577i6l5jjanqiv5ak6nv5357-r-readxl-1.3.1/library 168[55] /nix/store/6q9zwivzalhmzdracc8ma932wirq8rl5-r-cellranger-1.1.0/library 169[56] /nix/store/jh2n6k2ancdzqych5ix8n4rq9w514qq9-r-rematch-1.0.1/library 170[57] /nix/store/22xjqikqd6q556absb5224sbx6q0kp0c-r-progress-1.2.2/library 171[58] /nix/store/9vp32wa1qvv6lkq6p70qlli5whrxzfbi-r-prettyunits-1.1.1/library 172[59] /nix/store/r9rhqb6fsk75shihmb7nagqb51pqwp0y-r-class-7.3-16/library 173[60] /nix/store/z1kad071y43wij1ml9lpghh7jbimmcli-r-cluster-2.1.0/library 174[61] /nix/store/i8wr965caf6j1rxs2dsvpzhlh4hyyb4y-r-codetools-0.2-16/library 175[62] /nix/store/8iglq3zr68a39hzswvzxqi2ffhpw9p51-r-KernSmooth-2.23-16/library 176[63] /nix/store/n3k50zv40i40drpdf8npbmy2y08gkr6w-r-rpart-4.1-15/library 177[64] /nix/store/b4r6adzcvpm8ivflsmis7ja7q4r5hkjy-r-spatial-7.3-11/library 178[65] /nix/store/zqg6hmrncl8ax3vn7z5drf4csddwnhcx-r-survival-3.1-12/library 179[66] /nix/store/4anrihkx11h8mzb269xdyi84yp5v7grl-r-tidyverse-1.3.0/library 180[67] /nix/store/945haq0w8nfm9ib7r0nfngn5lk2i15ix-r-broom-0.5.6/library 181[68] /nix/store/52viqxzrmxl7dk0zji293g5b0b9grwh8-r-backports-1.1.6/library 182[69] /nix/store/zp1k42sw2glqy51w4hnzsjs8rgi8xzx2-r-dplyr-0.8.5/library 183[70] /nix/store/mkjd98mnshch2pwnj6h31czclqdaph3f-r-plogr-0.2.0/library 184[71] /nix/store/kflrzax6y5pwfqwzgfvqz433a3q3hnhn-r-generics-0.0.2/library 185[72] /nix/store/xi1n5h5w17c33y6ax3dfhg2hgzjl9bxz-r-reshape2-1.4.4/library 186[73] /nix/store/vn63z92zkpbaxmmhzpb6mq2fvg0xa26h-r-plyr-1.8.6/library 187[74] /nix/store/wmpyxss67bj44rin7hlnr9qabx66p5hj-r-stringr-1.4.0/library 188[75] /nix/store/330qbgbvllwz3h0i2qidrlk50y0mbgph-r-tidyr-1.0.2/library 189[76] /nix/store/cx3x4pqb65l1mhss65780hbzv9jdrzl6-r-dbplyr-1.4.3/library 190[77] /nix/store/gsj49bp3hpw9jlli3894c49amddryqsq-r-DBI-1.1.0/library 191[78] /nix/store/kvymhwp4gac0343c2yi1qvdpavx4gdn2-r-ggplot2-3.3.0/library 192[79] /nix/store/knv51jvpairvibrkkq48b6f1l2pa1cv8-r-gtable-0.3.0/library 193[80] /nix/store/158dx0ddv20ikwag2860nlg9p3hbh1zc-r-isoband-0.2.1/library 194[81] /nix/store/fprs9rp1jlhxzj7fp6l79akyf8k3p7zd-r-testthat-2.3.2/library 195[82] /nix/store/0pmlnkyn0ir3k9bvxihi1r06jyl64w3i-r-evaluate-0.14/library 196[83] /nix/store/7210bjjqn5cjndxn5isnd4vip00xhkhy-r-pkgload-1.0.2/library 197[84] /nix/store/9a12ybd74b7dns40gcfs061wv7913qjy-r-desc-1.2.0/library 198[85] /nix/store/na9pb1apa787zp7vvyz1kzym0ywjwbj0-r-rprojroot-1.3-2/library 199[86] /nix/store/pa2n7bh61qxyarn5i2ynd62k6knb1np1-r-pkgbuild-1.0.6/library 200[87] /nix/store/1hxm1m7h4272zxk9bpsaq46mvnl0dbss-r-callr-3.4.3/library 201[88] /nix/store/bigvyk6ipglbiil93zkf442nv4y3xa1x-r-processx-3.4.2/library 202[89] /nix/store/370lr0wf7qlq0m72xnmasg2iahkp2n52-r-ps-1.3.2/library 203[90] /nix/store/rr72q61d8mkd42zc5fhcd2rqjghvc141-r-withr-2.2.0/library 204[91] /nix/store/9gw77p7fmz89fa8wi1d9rvril6hd4sxy-r-rstudioapi-0.11/library 205[92] /nix/store/9x4v4pbrgmykbz2801h77yz2l0nmm5nb-r-praise-1.0.0/library 206[93] /nix/store/pf8ssb0dliw5bzsncl227agc8przb7ic-r-scales-1.1.0/library 207[94] /nix/store/095z4wgjrxn63ixvyzrj1fm1rdv6ci95-r-farver-2.0.3/library 208[95] /nix/store/5aczj4s7i9prf5i32ik5ac5baqvjwdb1-r-labeling-0.3/library 209[96] /nix/store/wch26phipzz9gxd4vbr4fynh7v28349j-r-munsell-0.5.0/library 210[97] /nix/store/3w8fh756mszhsjx5fwgwydcpn8vkwady-r-colorspace-1.4-1/library 211[98] /nix/store/8cmaj81v2vm4f8p59ylbnsby8adkbmhd-r-RColorBrewer-1.1-2/library 212[99] /nix/store/h4x4ygax7gpz6f0c2v0xacr62080qwb8-r-viridisLite-0.3.0/library 213[100] /nix/store/qhx0i2nn5syb6vygdn8fdxgl7k56yj81-r-httr-1.4.1/library 214[101] /nix/store/lxnb4aniv02i4jhdvz02aaql1kznbpxb-r-jsonlite-1.6.1/library 215[102] /nix/store/13dcry4gad3vfwqzqb0ii4n06ybrxybr-r-mime-0.9/library 216[103] /nix/store/2can5l8gscc92a3bqlak8hfcg96v5hvf-r-openssl-1.4.1/library 217[104] /nix/store/piwsgxdz5w2ak8c6fcq0lc978qbxwdp1-r-askpass-1.1/library 218[105] /nix/store/3sj5h6dwa1l27d2hvdchclygk0pgffsr-r-sys-3.3/library 219[106] /nix/store/2z0p88g0c03gigl2ip60dlsfkdv1k30h-r-lubridate-1.7.8/library 220[107] /nix/store/1pkmj8nqjg2iinrkg2w0zkwq0ldc01za-r-modelr-0.1.6/library 221[108] /nix/store/bswkzvn8lczwbyw3y7n0p0qp2q472s0g-r-reprex-0.3.0/library 222[109] /nix/store/yid22gad8z49q52d225vfba2m4cgj2lx-r-fs-1.4.1/library 223[110] /nix/store/d185qiqaplm5br9fk1pf29y0srlabw83-r-rmarkdown-2.1/library 224[111] /nix/store/iszqviydsdj31c3ww095ndqy1ld3cibs-r-base64enc-0.1-3/library 225[112] /nix/store/i89wfw4cr0fz3wbd7cg44fk4dwz8b6h1-r-htmltools-0.4.0/library 226[113] /nix/store/qrl28laqwmhpwg3dpcf4nca8alv0px0g-r-knitr-1.28/library 227[114] /nix/store/jffaxc4a3bbf2g6ip0gdcya73dmg53mb-r-highr-0.8/library 228[115] /nix/store/717srph13qpnbzmgsvhx25q8pl51ivpj-r-markdown-1.1/library 229[116] /nix/store/mxqmyq3ybdfyc6p0anhfy2kfw0iz5k4n-r-xfun-0.13/library 230[117] /nix/store/b8g6hadva0359l6j1aq4dbvxlqf1acxc-r-yaml-2.2.1/library 231[118] /nix/store/rrl05vpv7cw58zi0k9ykm7m4rjb9gjv3-r-tinytex-0.22/library 232[119] /nix/store/2ziq8nzah6xy3dgmxgim9h2wszz1f89f-r-whisker-0.4/library 233[120] /nix/store/540wbw4p1g2qmnmbfk0rhvwvfnf657sj-r-rvest-0.3.5/library 234[121] /nix/store/n3prn77gd9sf3z4whqp86kghr55bf5w8-r-selectr-0.4-2/library 235[122] /nix/store/gv28yjk5isnglq087y7767xw64qa40cw-r-xml2-1.3.2/library 236[123] /nix/store/693czdcvkp6glyir0mi8cqvdc643whvc-r-gridExtra-2.3/library 237[124] /nix/store/3sykinp7lyy70dgzr0fxjb195nw864dv-r-future-1.17.0/library 238[125] /nix/store/bqi2l53jfxncks6diy0hr34bw8f86rvk-r-globals-0.12.5/library 239[126] /nix/store/dydyl209klklzh69w9q89f2dym9xycnp-r-listenv-0.8.0/library 240[127] /nix/store/lni0bi36r4swldkx7g4hql7gfz9b121b-r-gganimate-1.0.5/library 241[128] /nix/store/hh92jxs79kx7vxrxr6j6vin1icscl4k7-r-tweenr-1.0.1/library 242[129] /nix/store/0npx3srjnqgh7bib80xscjqvfyzjvimq-r-GGally-1.5.0/library 243[130] /nix/store/x5nzxklmacj6l162g7kg6ln9p25r3f17-r-reshape-0.8.8/library 244[131] /nix/store/q29z7ckdyhfmg1zlzrrg1nrm36ax756j-r-ggfortify-0.4.9/library 245[132] /nix/store/1rvm1w9iv2c5n22p4drbjq8lr9wa2q2r-r-cowplot-1.0.0/library 246[133] /nix/store/rp8jhnasaw1vbv5ny5zx0mw30zgcp796-r-ggrepel-0.8.2/library 247[134] /nix/store/wb7y931mm8nsj7w9xin83bvbaq8wvi4d-r-corrplot-0.84/library 248[135] /nix/store/gdzcqivfvgdrsz247v5kmnnw1v6p9c1p-r-rpart.plot-3.0.8/library 249[136] /nix/store/6yqg37108r0v22476cm2kv0536wyilki-r-caret-6.0-86/library 250[137] /nix/store/6fjdgcwgisiqz451sg5fszxnn9z8vxg6-r-foreach-1.5.0/library 251[138] /nix/store/c3ph5i341gk7jdinrkkqf6y631xli424-r-iterators-1.0.12/library 252[139] /nix/store/sjm1rxshlpakpxbrynfhsjnnp1sjvc3r-r-ModelMetrics-1.2.2.2/library 253[140] /nix/store/vgk4m131d057xglmrrb9rijhzdr2qhhp-r-pROC-1.16.2/library 254[141] /nix/store/bv1kvy1wc2jx3v55rzn3cg2qjbv7r8zp-r-recipes-0.1.10/library 255[142] /nix/store/001h42q4za01gli7avjxhq7shpv73n9k-r-gower-0.2.1/library 256[143] /nix/store/ssffpl6ydffqyn9phscnccxnj71chnzg-r-ipred-0.9-9/library 257[144] /nix/store/baliqip8m6p0ylqhqcgqak29d8ghral1-r-prodlim-2019.11.13/library 258[145] /nix/store/j4n2wsv98asw83qiffg6a74dymk8r2hl-r-lava-1.6.7/library 259[146] /nix/store/hf5wq5kpsf6p9slglq5iav09s4by0y5i-r-numDeriv-2016.8-1.1/library 260[147] /nix/store/s58hm38078mx4gyqffvv09zn575xn648-r-SQUAREM-2020.2/library 261[148] /nix/store/g63ydzd53586pvr9kdgk8kf5szq5f2bc-r-timeDate-3043.102/library 262[149] /nix/store/0jkarmlf1kjv4g8a3svkc7jfarpp77ny-r-mlr3-0.2.0/library 263[150] /nix/store/g1m0n1w7by213v773iyn7vnxr25pkf56-r-checkmate-2.0.0/library 264[151] /nix/store/fc2ah8cz2sj6j2jk7zldvjmsjn1yakpn-r-lgr-0.3.4/library 265[152] /nix/store/0i2hs088j1s0a6i61124my6vnzq8l27m-r-mlbench-2.1-1/library 266[153] /nix/store/vzcs6k21pqrli3ispqnvj5qwkv14srf5-r-mlr3measures-0.1.3/library 267[154] /nix/store/h2yqqaia46bk3b1d1a7bq35zf09p1b1a-r-mlr3misc-0.2.0/library 268[155] /nix/store/c9mrkc928cmsvvnib50l0jb8lsz59nyk-r-paradox-0.2.0/library 269[156] /nix/store/vqpbdipi4p4advl2vxrn765mmgcrabvk-r-uuid-0.1-4/library 270[157] /nix/store/xpclynxnfq4h9218gk4y62nmgyyga6zl-r-mlr3viz-0.1.1/library 271[158] /nix/store/7w6pld5vir3p9bybay67kq0qwl0gnx17-r-mlr3learners-0.2.0/library 272[159] /nix/store/ca50rp6ha5s51qmhb1gjlj62r19xfzxs-r-mlr3pipelines-0.1.3/library 273[160] /nix/store/9hg0xap4pir64mhbgq8r8cgrfjn8aiz5-r-mlr3filters-0.2.0/library 274[161] /nix/store/jgqcmfix0xxm3y90m8wy3xkgmqf2b996-r-rstan-2.19.3/library 275[162] /nix/store/mvv1gjyrrpvf47fn7a8x722wdwrf5azk-r-inline-0.3.15/library 276[163] /nix/store/zmkw51x4w4d1v1awcws0xihj4hnxfr09-r-loo-2.2.0/library 277[164] /nix/store/30xxalfwzxl05bbfvj5sy8k3ysys6z5y-r-matrixStats-0.56.0/library 278[165] /nix/store/fhkww2l0izx87bjnf0pl9ydl1wprp0xv-r-StanHeaders-2.19.2/library 279[166] /nix/store/aflck5pzxa8ym5q1dxchx5hisfmfghkr-r-tidybayes-2.0.3/library 280[167] /nix/store/jhlbhiv4fg0wsbxwjz8igc4hcg79vw94-r-arrayhelpers-1.1-0/library 281[168] /nix/store/fv089zrnvicnavbi08hnzqpi9g1z4inj-r-svUnit-1.0.3/library 282[169] /nix/store/xci2rgjizx1fyb33818jx5s1bgn8v8k6-r-coda-0.19-3/library 283[170] /nix/store/dch9asd38yldz0sdn8nsgk9ivjrkbhva-r-HDInterval-0.2.0/library 284[171] /nix/store/rs8dri2m5cqdmpiw187rvl4yhjn0jg2v-r-e1071-1.7-3/library 285[172] /nix/store/qs1zyh3sbvccgnqjzas3br6pak399zgc-r-pvclust-2.2-0/library 286[173] /nix/store/sh3zxvdazp7rkjn1iczrag1h2358ifm1-r-forecast-8.12/library 287[174] /nix/store/h67kaxqr2ppdpyj77wg5hm684jypznji-r-fracdiff-1.5-1/library 288[175] /nix/store/fh0z465ligbpqyam5l1fwiijc7334kbk-r-lmtest-0.9-37/library 289[176] /nix/store/0lnsbwfg0axr80h137q52pa50cllbjpf-r-zoo-1.8-7/library 290[177] /nix/store/p7k4s3ivf83dp2kcxr1cr0wlc1rfk6jx-r-RcppArmadillo-0.9.860.2.0/library 291[178] /nix/store/ssnxv5x6zid2w11v8k5yvnyxis6n1qfk-r-tseries-0.10-47/library 292[179] /nix/store/zrbskjwaz0bzz4v76j044d771m24g6h8-r-quadprog-1.5-8/library 293[180] /nix/store/2x3w5sjalrfm6hf1dxd951j8y94nh765-r-quantmod-0.4.17/library 294[181] /nix/store/7g55xshf49s9379ijm1zi1qnh1vbsifq-r-TTR-0.23-6/library 295[182] /nix/store/6ilyzph46q6ijyanq4p7f0ccyni0d7j0-r-xts-0.12-0/library 296[183] /nix/store/17xhqghcnqha7pwbf98dxsq1729slqd5-r-urca-1.3-0/library 297[184] /nix/store/722lyn0k8y27pj1alik56r4vpjnncd9z-r-swdft-1.0.0/library 298[185] /nix/store/36n0zgy10fsqcq76n0qmdwjxrwh7pn9n-r-xgboost-1.0.0.2/library 299[186] /nix/store/ac0ar7lf75qx84xsdjv6j02rkdgnhybz-r-ranger-0.12.1/library 300[187] /nix/store/i1ighkq42x10dirqmzgbx2mhbnz1ynkb-r-DALEX-1.2.0/library 301[188] /nix/store/28fqnhsfng1bkphl0wvr7lg5y3p6va46-r-iBreakDown-1.2.0/library 302[189] /nix/store/dpym77x9qc2ksr4mwjm3pb9ar1kvwhdl-r-ingredients-1.2.0/library 303[190] /nix/store/sp4d281w6dpr31as0xdjqizdx8hhb01q-r-DALEXtra-0.2.1/library 304[191] /nix/store/ckhp9kpmjcs0wxb113pxn25c2wip2d0n-r-ggdendro-0.1-20/library 305[192] /nix/store/f3k7dxj1dsmqri2gn0svq4c9fvvl9g7q-r-glmnet-3.0-2/library 306[193] /nix/store/l6ccj6mwkqybjvh6dr8qzalygp0i7jyb-r-shape-1.4.4/library 307[194] /nix/store/418mqfwlafh6984xld8lzhl7rv29qw68-r-reticulate-1.15/library 308[195] /nix/store/qwh982mgxd2mzrgbjk14irqbasywa1jk-r-rappdirs-0.3.1/library 309[196] /nix/store/6sxs76abll23c6372h6nf101wi8fcr4c-r-FactoMineR-2.3/library 310[197] /nix/store/39d2va10ydgyzddwr07xwdx11fwk191i-r-ellipse-0.4.1/library 311[198] /nix/store/4lxym5nxdn8hb7l8a566n5vg9paqcfi2-r-flashClust-1.01-2/library 312[199] /nix/store/wp161zbjjs41fq4kn4k3m244c7b8l2l2-r-leaps-3.1/library 313[200] /nix/store/irghsaplrpb3hg3y7j831bbklf2cqs6d-r-scatterplot3d-0.3-41/library 314[201] /nix/store/09ahkf50g1q9isxanbdykqgcdrp8mxl1-r-factoextra-1.0.7/library 315[202] /nix/store/zi9bq7amsgc6w2x7fvd62g9qxz69vjfm-r-dendextend-1.13.4/library 316[203] /nix/store/wcywb7ydglzlxg57jf354x31nmy63923-r-viridis-0.5.1/library 317[204] /nix/store/pvnpg4vdvv93pmwrlgmy51ihrb68j55f-r-ggpubr-0.2.5/library 318[205] /nix/store/qpapsc4l9pylzfhc72ha9d82hcbac41z-r-ggsci-2.9/library 319[206] /nix/store/h0zg4x3bmkc82ggx8h4q595ffckcqgx5-r-ggsignif-0.6.0/library 320[207] /nix/store/vn5svgbf8vsgv8iy8fdzlj0izp279q15-r-polynom-1.4-0/library 321[208] /nix/store/mc1mlsjx5h3gc8nkl7jlpd4vg145nk1z-r-lindia-0.9/library 322[209] /nix/store/z1k4c8lhabp9niwfg1xylg58pf99ld9r-r-orgutils-0.4-1/library 323[210] /nix/store/ybj4538v74wx4f1l064m0qn589vyjmzg-r-textutils-0.2-0/library 324[211] /nix/store/hhm5j0wvzjc0bfd53170bw8w7mij2wnh-r-latex2exp-0.4.0/library 325[212] /nix/store/njlv5mkxgjyx3x8p984nr84dwa2v1iqp-r-kableExtra-1.1.0/library 326[213] /nix/store/lf2sb84ylh259m421ljbj731a4prjhsl-r-webshot-0.5.2/library 327[214] /nix/store/n6b8ap54b78h8l70kyx9nvayp44rnfzf-r-printr-0.1/library 328[215] /nix/store/02g1v6d3ly8zylpckigwk6w3l1mx2i9d-r-microbenchmark-1.4-7/library 329[216] /nix/store/ri6qm0fp8cyx2qnysxjv2wsk0nndl1x9-r-webchem-0.5.0/library 330[217] /nix/store/cg95rqc1gmaqxf5kxja3cz8m5w4vl76l-r-RCurl-1.98-1.2/library 331[218] /nix/store/qbpinv148778fzdz8372x8gp34hspvy1-r-bitops-1.0-6/library 332[219] /nix/store/1g0lbrx6si76k282sxr9cj0mgknrw0lx-r-devtools-2.3.0/library 333[220] /nix/store/hnvww0128czlx6w8aipjn0zs7nvmvak9-r-covr-3.5.0/library 334[221] /nix/store/p4nv59przmb14sxi49jwqarkv0l40jsp-r-rex-1.2.0/library 335[222] /nix/store/vnysmc3vkgkligwah1zh9l4sahr533a8-r-lazyeval-0.2.2/library 336[223] /nix/store/d638w33ahybsa3sqr52fafvxs2b7w9x3-r-DT-0.13/library 337[224] /nix/store/35nqc34wy2nhd9bl7lv6wriw0l3cghsw-r-crosstalk-1.1.0.1/library 338[225] /nix/store/03838i63x5irvgmpgwj67ah0wi56k9d7-r-htmlwidgets-1.5.1/library 339[226] /nix/store/l4640jxlsjzqhw63c18fziar5vc0xyhk-r-promises-1.1.0/library 340[227] /nix/store/rxrb8p3dxzsg10v7yqaq5pi3y3gk6nqh-r-later-1.0.0/library 341[228] /nix/store/giprr32bl6k18b9n4qjckpf102flarly-r-git2r-0.26.1/library 342[229] /nix/store/bbkpkf44b13ig1pkz7af32kw5dzp12vb-r-memoise-1.1.0/library 343[230] /nix/store/m31vzssnfzapsapl7f8v4m15003lcc8r-r-rcmdcheck-1.3.3/library 344[231] /nix/store/hbiylknhxsin9hp9zaa6dwc2c9ai1mqx-r-sessioninfo-1.1.1/library 345[232] /nix/store/8vwlbx3s345gjccrkiqa6h1bm9wq4s9q-r-xopen-1.0.0/library 346[233] /nix/store/mjnwnlv60cn56ap0rrzvrkqlh5qisszx-r-remotes-2.1.1/library 347[234] /nix/store/1rq4zyzqymml7cc11q89rl5g514ml9na-r-roxygen2-7.1.0/library 348[235] /nix/store/2658mrn1hpkq0fv629rvags91qg65pbn-r-brew-1.0-6/library 349[236] /nix/store/nvjalws9lzva4pd4nz1z2131xsb9b5p6-r-commonmark-1.7/library 350[237] /nix/store/qx900vivd9s2zjrxc6868s92ljfwj5dv-r-rversions-2.0.1/library 351[238] /nix/store/1drg446wilq5fjnxkglxnnv8pbp1hllg-r-usethis-1.6.0/library 352[239] /nix/store/p3f3wa41d304zbs5cwvw7vy4j17zd6nq-r-gh-1.1.0/library 353[240] /nix/store/769g7jh93da8w15ad0wsbn2aqziwwx56-r-ini-0.3.1/library 354[241] /nix/store/p7kifw1l6z2zg68a71s4sdbfj8gdmnv5-r-rematch2-2.1.1/library 355[242] /nix/store/6zhdqip9ld9vl6pvifqcf4gsqy2f5wix-r-rethinking/library 356[243] /nix/store/496p28klmflihdkc83c8p1cywg85mgk4-r-mvtnorm-1.1-0/library 357[244] /nix/store/xb1zn7ab4nka7h1vm678ginzfwg4w9wf-r-dagitty-0.2-2/library 358[245] /nix/store/3zj4dkjbdwgf3mdsl9nf9jkicpz1nwgc-r-V8-3.0.2/library 359[246] /nix/store/qiqsh62w69b5xgj2i4wjamibzxxji0mf-r-tidybayes.rethinking/library 360[247] /nix/store/4j6byy1klyk4hm2k6g3657682cf3wxcj-R-4.0.0/lib/R/library   Summer of 2020\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/sr2-ch9-ch11-ch12/","tags":["solutions","R","SR2"],"title":"SR2 :: Solutions for Chapters {9,11,12}"},{"categories":["programming"],"contents":" A post on working with transient TeX templates in orgmode without modifying global configurations. This will also serve as a rudimentary introduction to TeX in orgmode.\n Background The sad reality of working in a field dominated by institutional actors which do not care for recognizing software development as a skill is that there are often a lot of ugly LaTeX templates1. In particular, often Universities have arbitrary LaTeX templates from the the dark days of 2010 something, which include gratuitous usage of say, natbib instead of biblatex. In other situations, .cls files define separate document classes which are not covered by the orgmode defaults and need to be accounted for.\nStandard methods Essentially for the exporter, the document is broken into2:\n document_class This cannot be changed arbitrarily and has to be a valid element of org-latex-classes preamble This section of the document is essentially everything before \\begin{document} and after \\documentclass{...} body The rest of the document  We will briefly cover the standard methods of entering TeX in each of these sections, in reverse order since that is the direction in which the intuitive aspect decreases.\nIn-Body TeX The method of writing TeX in orgmode for the document involves simply writing TeX directly, or wrapping the TeX markup in an export TeX block for font locking3. Essentially, for a document snippet:\n1% #+BEGIN_SRC latex :exports code 2\\begin{align} 3\\pi(x) \u0026amp;= \\sum_{n=1}^{\\infty}\\frac{\\mu(n)}{n}\\Pi(x^{\\frac{1}{n}}) \\\\ 4\u0026amp;= \\Pi(x) -\\frac{1}{2}\\Pi(x^{\\frac{1}{2}}) - \\frac{1}{3}\\Pi(x^{\\frac{1}{3}}) - \\frac{1}{5}\\Pi(x^{\\frac{1}{5}}) + \\frac{1}{6} \\Pi(x^{\\frac{1}{6}}) -\\cdots, 5\\end{align} 6% #+END_SRC Which will actually be rendered in a real document of course4:\n\\begin{align} \\pi(x) \u0026amp;= \\sum_{n=1}^{\\infty}\\frac{\\mu(n)}{n}\\Pi(x^{\\frac{1}{n}}) \\\\ \u0026amp;= \\Pi(x) -\\frac{1}{2}\\Pi(x^{\\frac{1}{2}}) - \\frac{1}{3}\\Pi(x^{\\frac{1}{3}}) - \\frac{1}{5}\\Pi(x^{\\frac{1}{5}}) + \\frac{1}{6} \\Pi(x^{\\frac{1}{6}}) -\\cdots, \\end{align}\nThere is also the inline form of writing LaTeX with @@\\sin{x}@@ which is essentially \\(\\sin{x}\\).\nPreamble The main use of the preamble is to either add classes or modify class options for loaded packages like geometry. Essentially, for orgmode, anything prefixed with #+LATEX_HEADER: gets inserted in the preamble.\n1#+LATEX_HEADER: \\usepackage{amssymb,amsmath,MnSymbol} 2#+LATEX_HEADER: \\usepackage{unicode-math} 3#+LATEX_HEADER: \\usepackage{mathtools} For larger documents, this gets quite annoying for loading packages. We will demonstrate a more aesthetically pleasant form later in this post.\nLaTeX classes Working with document classes is the least intuitive of all TeX manipulations, because for some reason, #+LATEX_CLASS: only accepts values defined in org-latex-classes.\nThe standard approach to extending the orgmode TeX backend is to add lines like the following in init.el or, in my case5, config.org:\n1(add-to-list \u0026#39;org-latex-classes 2\u0026#39;(\u0026#34;koma-article\u0026#34; \u0026#34;\\\\documentclass{scrartcl}\u0026#34; 3(\u0026#34;\\\\section{%s}\u0026#34; . \u0026#34;\\\\section*{%s}\u0026#34;) 4(\u0026#34;\\\\subsection{%s}\u0026#34; . \u0026#34;\\\\subsection*{%s}\u0026#34;) 5(\u0026#34;\\\\subsubsection{%s}\u0026#34; . \u0026#34;\\\\subsubsection*{%s}\u0026#34;) 6(\u0026#34;\\\\paragraph{%s}\u0026#34; . \u0026#34;\\\\paragraph*{%s}\u0026#34;) 7(\u0026#34;\\\\subparagraph{%s}\u0026#34; . \u0026#34;\\\\subparagraph*{%s}\u0026#34;))) This is alright for often used classes like the koma-⋆ family of LaTeX document-classes, but it is hardly ideal for one-off TeX templates which are meant for say, grant proposals6.\nElisp to the rescue The core idea is quite simple.\n Since orgmode files are literate documents, and emacs is self-documenting and completely programmable, it should be possible to execute code to deterministically set the state of emacs before exporting the document to TeX.\n Practically this has a few moving parts. In the following sections, assume that we have a .cls file which defines a document-class foo with a bunch of packages which conflict with our global configuration.\nAdding Document Classes Instead of adding the code snippet to our global configuration, we will now add it to the document directly with the comments indicating the appropriate environment.\n1;; #+BEGIN_SRC emacs-lisp :exports none :results none :eval always 2(add-to-list \u0026#39;org-latex-classes 3\u0026#39;(\u0026#34;foo\u0026#34; \u0026#34;\\\\documentclass{foo}\u0026#34; 4(\u0026#34;\\\\section{%s}\u0026#34; . \u0026#34;\\\\section*{%s}\u0026#34;) 5(\u0026#34;\\\\subsection{%s}\u0026#34; . \u0026#34;\\\\subsection*{%s}\u0026#34;) 6(\u0026#34;\\\\subsubsection{%s}\u0026#34; . \u0026#34;\\\\subsubsection*{%s}\u0026#34;) 7(\u0026#34;\\\\paragraph{%s}\u0026#34; . \u0026#34;\\\\paragraph*{%s}\u0026#34;) 8(\u0026#34;\\\\subparagraph{%s}\u0026#34; . \u0026#34;\\\\subparagraph*{%s}\u0026#34;))) 9;; #+END_SRC Where the header arguments simply ensure that the code and result do not show up in the document, and that the chunk is always evaluated by org-babel.\nEnsuring Purity Since we want to stick to an external template defined in foo and no other packages we will need to clear the defaults we lovingly set globally for our convenience.\n1;; #+BEGIN_SRC emacs-lisp :exports none :results none :eval always 2(setq org-latex-packages-alist \u0026#39;nil) 3(setq org-latex-minted-options \u0026#39;nil) ;; Separately nullify minted 4;; #+END_SRC Pretty Packages For packages we really would like to add, we can now leverage the elisp code instead of the ugly #+LATEX_HEADER: lines.\n1;; #+BEGIN_SRC emacs-lisp :exports none :results none :eval always 2(setq org-latex-default-packages-alist 3\u0026#39;((\u0026#34;utf8\u0026#34; \u0026#34;inputenc\u0026#34; t) 4(\u0026#34;normalem\u0026#34; \u0026#34;ulem\u0026#34; t) 5(\u0026#34;\u0026#34; \u0026#34;mathtools\u0026#34; t) 6)) 7;; #+END_SRC Note that for setting options, we will still need to use the #+LATEX_HEADER: syntax.\nAutomating With Hooks At this stage, we have a chunk of elisp we can manually evaluate with org-babel before exporting with org-latex-export-to-pdf or org-latex-export-to-latex. However, this can get old quickly, so we will instead have a before-save-hook to do this for us.\n1# Local Variables: 2# before-save-hook: org-babel-execute-buffer 3# End: Bonus Hook In my own configuration, I have a function defined for an after-save-hook which generates the TeX file without having me deal with it. For a per-file configuration of this, or globally, the elisp is:\n1(defun haozeke/org-save-and-export-latex () 2(interactive) 3(if (eq major-mode \u0026#39;org-mode) 4(org-latex-export-to-latex t))) This indirection is required to call the function as a hook. The additional t allows for asynchronous non blocking exports. Now this can be used as:\n1# Local Variables: 2# after-save-hook: haozeke/org-save-and-export-latex 3# End: Conclusions The entire file would look something like this (the elisp can be anywhere in the orgmode file):\n1;; #+BEGIN_SRC emacs-lisp :exports none :results none :eval always 2(add-to-list \u0026#39;org-latex-classes 3\u0026#39;(\u0026#34;foo\u0026#34; \u0026#34;\\\\documentclass{foo}\u0026#34; 4(\u0026#34;\\\\section{%s}\u0026#34; . \u0026#34;\\\\section*{%s}\u0026#34;) 5(\u0026#34;\\\\subsection{%s}\u0026#34; . \u0026#34;\\\\subsection*{%s}\u0026#34;) 6(\u0026#34;\\\\subsubsection{%s}\u0026#34; . \u0026#34;\\\\subsubsection*{%s}\u0026#34;) 7(\u0026#34;\\\\paragraph{%s}\u0026#34; . \u0026#34;\\\\paragraph*{%s}\u0026#34;) 8(\u0026#34;\\\\subparagraph{%s}\u0026#34; . \u0026#34;\\\\subparagraph*{%s}\u0026#34;))) 9(setq org-latex-packages-alist \u0026#39;nil) 10(setq org-latex-default-packages-alist 11\u0026#39;((\u0026#34;utf8\u0026#34; \u0026#34;inputenc\u0026#34; t) 12(\u0026#34;\u0026#34; \u0026#34;minted\u0026#34; t) 13(\u0026#34;\u0026#34; \u0026#34;rotating\u0026#34; nil) 14(\u0026#34;normalem\u0026#34; \u0026#34;ulem\u0026#34; t) 15(\u0026#34;\u0026#34; \u0026#34;mathtools\u0026#34; t) 16)) 17;; #+END_SRC 1#+TITLE: Something 2#+AUTHOR: Rohit Goswami 3#+OPTIONS: toc:nil \\n:nil 4#+STARTUP: fninline 5#+LATEX_COMPILER: xelatex 6#+LATEX_CLASS: foo 7#+LATEX_HEADER: \\setlength\\parindent{0pt} 8#+LATEX_HEADER: \\addbibresource{./biblio/refs.bib} 910Blah blah document $\\sin{x}$ stuff 1112# Local Variables: 13# before-save-hook: org-babel-execute-buffer 14# after-save-hook: haozeke/org-save-and-export-latex 15# End: This method could be extended to essentially resetting all emacs variables on a per-file basis (without file-local-variables and dir-local-variables) or to potentially execute any elisp to make emacs do things, though I cannot really think of another realistic use-case. The method presented here is really general enough to work with any arbitrary LaTeX .cls file or other draconian measures.\n  Of course there are more issues stemming from this toxic practice, but that\u0026rsquo;s for another rant\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n For more on document structure in TeX read the wikibook\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Or syntax highlighting for most people\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Props to anyone who recognizes that formula\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n I use doom-emacs with my own literate configuration\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n I haven\u0026rsquo;t seen a grant proposal template I\u0026rsquo;d like to store for later, ever\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/org-arb-tex/","tags":["tools","emacs","workflow","orgmode"],"title":"Temporary LaTeX Documents with Orgmode"},{"categories":["programming"],"contents":" Setup details are described here, and the meta-post about these solutions is here.\n Materials The summmer course1 is based off of the second edition of Statistical Rethinking by Richard McElreath. This post covers the following exercise questions:\n Chapter 5  E{1,2,3,4} M{1,2,3,5}   Chapter 6  E{1,2,3,4} M{1,2,3}   Chapter 7  E{1,2,3,4} M{1,2,3,4,5,6}    Packages A colophon with details is provided at the end, but the following packages and theme parameters are used throughout.\n1libsUsed\u0026lt;-c(\u0026#34;tidyverse\u0026#34;,\u0026#34;tidybayes\u0026#34;,\u0026#34;orgutils\u0026#34;,\u0026#34;dagitty\u0026#34;, 2\u0026#34;rethinking\u0026#34;,\u0026#34;tidybayes.rethinking\u0026#34;, 3\u0026#34;ggplot2\u0026#34;,\u0026#34;kableExtra\u0026#34;,\u0026#34;dplyr\u0026#34;,\u0026#34;glue\u0026#34;, 4\u0026#34;latex2exp\u0026#34;,\u0026#34;data.table\u0026#34;,\u0026#34;printr\u0026#34;,\u0026#34;devtools\u0026#34;) 5invisible(lapply(libsUsed, library, character.only = TRUE)); 6theme_set(theme_grey(base_size=24)) Chapter V: The Many Variables \u0026amp; The Spurious Waffles Easy Questions (Ch5) 5E1 Which of the linear models below are multiple linear regressions?\n \\(μᵢ=α+βxᵢ\\) \\(μᵢ=βₓxᵢ+β_{z}zᵢ\\) \\(μᵢ=α+β(xᵢ-zᵢ)\\) \\(μᵢ=α+βₓxᵢ+β_{z}zᵢ\\)  Solution A multiple regression problem is one with more than one predictor and corresponding coefficients in an additive (hence \u0026ldquo;linear\u0026rdquo;) manner. By this logic, we can analyze the options as follows:\n Has one predictor variable, \\(x\\) thus is not a multiple regression Is a multiple linear regression since there are two independent variables, \\(x\\) and \\(z\\) Is not a multiple regression model, since only the difference of \\(x\\) and \\(z\\) enters the model (with slope \\(\\beta\\)) This is a multiple linear regression problem, since there are two predictor variables \\(x\\) and \\(z\\)  Thus options two and four are correct.\n5E2 Write down a multiple regression to evaluate the claim: Animal diversity is linearly related to latitude, but only after controlling for plant diversity. You just need to write down the model definition.\nSolution Without any further information, we can simply write a model for diversity as:\n\\[D_{A}\\sim\\mathrm{Log-Normal}(μᵢ,σ)\\] \\[μᵢ=α+β_{L}Lᵢ+β_{D_P}D_{Pᵢ}\\]\nWhere:\n \\(D_{A}\\) is the animal diversity \\(D_{P}\\) is the plant diversity \\(L\\) is the latitude  We have used a log-normal distribution for the animal diversity, since negative values for diversity are meaningless. This arises from the understanding that the diversity is on an ordinal scale with classes. The linear model posits a linear model which has two predictors, the latitude and plant diversity. Thus this model allows for \u0026ldquo;control\u0026rdquo; of the plant diversity.\nFurther details would be relegated to the choice of priors instead of the model.\n5E3 Write down a multiple regression to evaluate the claim: Neither amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree. Write down the model definition and indicate which side of zero each slope parameter should be on.\nSolution Without considering priors, we would like to write a linear model with two variables, funding and the lab size. To allow for extensions later regarding the type of funding, we will use \u0026ldquo;money\u0026rdquo; and \u0026ldquo;time\u0026rdquo; as inputs for the model. Again, since the time to a PhD cannot be negative, we will posit a log-normal distribution.\n\\[Tᵢ∼\\mathrm{Log-Normal}(μᵢ,σ)\\] \\[μᵢ=α+β_{M}M_{i}+β_{S}Sᵢ\\]\nWhere:\n \\(Tᵢ\\) is the time to completion \\(M\\) corresponds to money \\(S\\) corresponds to the size of the lab  Since we are told that the variables considered jointly have a positive association with the time, we note that the slope parameters for both should be positive.\n5E4 Suppose you have a single categorical predictor with 4 levels (unique values), labeled A,B,C and D. Let \\(Aᵢ\\) be an indicator variable that is \\(1\\) where case \\(i\\) is in category A. Also suppose \\(Bᵢ\\), \\(Cᵢ\\) and \\(Dᵢ\\) for the other categories. Now which of the following linear models are inferentially equivalent ways to include the categorical variable in a regression? Models are inferentially equivalent when it\u0026rsquo;s possible to compute one posterior distribution from the posterior distribution of another model.\n \\(μᵢ=α+β_{A}Aᵢ+β_{B}Bᵢ+β_{D}Dᵢ\\) \\(μᵢ=α+β_{A}Aᵢ+β_{B}Bᵢ+β_{C}Cᵢ+β_{D}Dᵢ\\) \\(μᵢ=α+β_{A}Aᵢ+β_{C}Cᵢ+β_{D}Dᵢ\\) \\(μᵢ=α_{A}Aᵢ+α_{B}Bᵢ+α_{C}Cᵢ+α_{D}Dᵢ\\) \\(μᵢ=α(1-Bᵢ-Cᵢ-Dᵢ)+α_{B}Bᵢ+α_{C}Cᵢ+α_{D}Dᵢ\\)  Solution Without the priors, it is difficult to infer much from these models. For the rest of the answer to make sense, we can assume indifferent priors, and enough data to overwhelm our priors (i.e., they are weakly informative).\nAll the models listed have an intercept term, and several variables. We will therefore only consider the number of independent variables and their nature.\n   Model Variables     (1) \\(μᵢ=α+β_{A}Aᵢ+β_{B}Bᵢ+β_{D}Dᵢ\\) 4   (2) \\(μᵢ=α+β_{A}Aᵢ+β_{B}Bᵢ+β_{C}Cᵢ+β_{D}Dᵢ\\) 5   (3) \\(μᵢ=α+β_{A}Aᵢ+β_{C}Cᵢ+β_{D}Dᵢ\\) 4   (4) \\(μᵢ=α_{A}Aᵢ+α_{B}Bᵢ+α_{C}Cᵢ+α_{D}Dᵢ\\) 4   (5) \\(μᵢ=α(1-Bᵢ-Cᵢ-Dᵢ)+α_{B}Bᵢ+α_{C}Cᵢ+α_{D}Dᵢ\\) 4    Thus we can infer that of the models, after fitting, only option two will have inferences which cannot be computed from the others.\nQuestions of Medium Complexity (Ch5) HOLD 5M1 Invent your own example of a spurious correlation. An outcome variable should be correlated with both predictor variables. But when both predictors are entered in the same model, the correlation between the outcome and one of the predictors should mostly vanish (or at least be greatly reduced).\nSolution For this example, consider the total potential energy of a molecular system. We will recall that this can be written as follows: \\[ E_{total}=E_{electrostatics}+E_{1B}+E_{2B} + \\cdots\\] Where the \\(B\\) terms indicate the correction terms. When predicting the total energy, if the electrostatic energy is a function of the atomic descriptors, and is entered in a model, then it masks the effect of the correction terms which also rely on the atomic descriptors. This means that correction terms to the total energy can also be thought of as a correction to the electrostatics, thus following the pattern of the divorce rate and waffles example in the chapter.\nTo put this is more context, let us introduce more explicit variables.\n\\[ E_{T}=E_{Elec}(\\theta)+E_{1B}(\\theta)+E_{2B}(\\theta)+\\cdots\\]\nIn this setting it is clear to see that the masking of variables is artificially induced.\nAnother possible example is from textcite:wainerMostDangerousEquation, where the utility of having smaller schools is a function of school size and the average number of achievements. The school size also affects the average number of achievements, as well as the actual utility. This then implies that there is a spurious correlation which does not exist when the variances are taken into account.\nHOLD 5M2 Invent your own example of a masked relationship. An outcome variable should be correlated with both predictor variables, but in opposite directions. And the two predictor variables should be correlated with one another.\nSolution Let us consider a simple case of student completion rate based on the influences of college tuition and faculty members. Assuming that college tuition is negatively correlated, and the number of faculty is positively correlated. However, since there are more wealthy people who can afford college, a chosen sample may show a spurious where examining either variable shows a weak correlation with completion rate, due to the positive association in the wealthy population.\nIt is important to note that masked relationships usually arise when the population is incorrectly sampled.\n5M3 It is sometimes observed that the best predictor of fire risk is the presence of firefighters-States and localities with many firefighters also have more fires. Presumably firefighters do not cause fires. Nevertheless, this is not a spurious correlation. Instead fires cause firefighters. Consider the same reversal of causal inferences in the context of the divorce and marriage data. How might a high divorce rate cause a higher marriage rate? Can you think of a way to evaluate this relationship, using multiple regression?\nSolution The example given simply allows for the inference that areas with a higher incidence of fires do tend to allocated more money and resources to prevent them, hence the observed larger number of firefighters. Similarly, a reversal of the divorce and marriage data might be focused on the possibility that divorcees tend to get married more often than other singles. However, to understand this further, more categorical variables would be required, though this information might also be best represented by a time series of life events. We can posit the following:\n\\[M\\sim\\mathrm{Normal}(μᵢ,σ)\\] \\[μᵢ=α+β_{L}Lᵢ+β_{R}Rᵢ\\]\nWhere:\n \\(M\\) is the marriage rate \\(L\\) is the probability of being married based on \u0026ldquo;love\u0026rdquo; \\(R\\) is the variable accounting for remarriage  5M5 One way to reason through multiple causation hypotheses is to imagine detailed mechanisms through which predictor variables may influence outcomes. For example, it is sometimes argued that the price of gasoline (predictor variable) is positively associated with lower obesity rates (outcome variable). However, there are at least two important mechanisms by which the price of gas could reduce obesity. First, it could lead to less driving and therefore more exercise. Second, it could lead to less driving, which leads to less eating out, which leads to less consumption of huge restaurant meals. Can you outline one or more multiple regressions that address these two mechanisms? Assume you can have any predictor data you need.\nSolution We adopt the following notation:\n \\(P\\) is price (predictor) \\(O\\) is obesity (outcome) \\(D\\) is for driving \\(E\\) for eating out \\(E_{x}\\) for exercise  Let us try to put this in the form of a DAG.\n1dag5m5\u0026lt;- dagitty(\u0026#34;dag{ 2P -\u0026gt; D -\u0026gt; E -\u0026gt; O 3P -\u0026gt; D -\u0026gt; Ex -\u0026gt; O 4}\u0026#34;) 1dag5m5 %\u0026gt;% graphLayout %\u0026gt;% plot  We should note that it seems straightforward, but it is nice to check as well.\n1dag5m5 %\u0026gt;% adjustmentSets(exposure=\u0026#34;P\u0026#34;,outcome=\u0026#34;O\u0026#34;) %\u0026gt;% print 12{} Now we can start working our way through the set of regressions by the most basic walk through the DAG.\n   Path One\n \\(D(P)\\) decreases \\(E_{x}(D)\\) increases \\(O(E_{x})\\) decreases       Path Two\n \\(D(P)\\) decreases \\(E(D)\\) decreases \\(O(E)\\) decreases    Chapter VI: The Haunted DAG \u0026amp; The Causal Terror Easy Questions (Ch6) 6E1 List three mechanisms by which multiple regression can produce false inferences about causal effects.\nSolution As per chapter five and six, we have three mechanisms:\n Confounding Where there exists an additional variable which influences exposure and outcome values Multicollinearity Strong associations between two or more predictor variables, which will cause the posterior distribution to suggest that none of variables are associated with the outcome even if they all actually are Post-treatment variables This is a form of included variable bias Collider Bias Conditioning on collider variables creates statistical but not causal associations between its causes  HOLD 6E2 For one of the mechanisms in the previous problem, provide an example of your choice, perhaps from your own research.\nSolution One of the core tenets of the field of computational chemistry is the act of fitting empirical potential models to more accurate potential data (or even experiments).\n Multicollinearity When dealing with decreasing effects, then using strongly correlated variables (like distance and effective distance measures like centeroid densities) cause the overall model to suggest that none of the measures are useful Post-treatment variables Often while finding minima and saddle points on a potential energy surface, adding information of the existing minima values will impede training a model which actually fits to the whole potential energy surface instead of being concentrated around the known minima  6E3 List the four elemental confounds. Can you explain the conditional dependencies of each?\nSolution The four elemental confounds are enumerated in Figure 1.\n\n Figure 1: The four elemental confounds\n  In symbolic notation, we can express this as:\n   Confound Symbolic Form Conditional Independencies     Forks \\(X←Z→Y\\) \\(Y⫫ X\\vert Z\\)   Pipes \\(X → Z → Y\\) \\(Y⫫ X\\vert Z\\)   Colliders \\(X→Z←Y\\) \\(Y \\not⫫ X\\vert Z\\)   Descendants See Figure 1 Weakly conditions on parent    6E4 How is a biased sample like conditioning on a collider? Think of the example at the open of the chapter.\nSolution Recall that the biased sample in the introduction to the chapter was:\n It seems like the most newsworthy scientific studies are the least trustworthy. The more likely it is to kill you, if true, the less likely it is to be true. The more boring the topic, the more rigorous the results. How could this widely believed negative correlation exist? There doesn’t seem to be any reason for studies of topics that people care about to produce less reliable results. Maybe popular topics attract more and worse researchers, like flies drawn to the smell of honey?\n Note that this can also be expressed as a collider in a causal DAG as:\n\\[\\mathrm{newsworthiness}→\\mathrm{acceptance}←\\mathrm{trustworthiness}\\]\nThe idea is that a proposal will be accepted if either the newsworthiness or the trustworthiness is high. There is thus on average a negative association between these criteria among the selected set of proposals.\nIn essence the association in the sub-samples is not the same as the total sample, and this causes wrong inferences on the total sample set, when conditioning on collider variables.\nQuestions of Medium Complexity (Ch6) 6M1 Modify the DAG on page \\(186\\) to include the variable \\(V\\), an unobserved cause of \\(C\\) and \\(Y:C\\gets V \\to Y\\). Reanalyze the DAG. How many paths connect \\(X\\) to \\(Y\\)? Which must be closed? Which variables should you condition on now?\nSolution Let us outline this DAG.\n1dag6m1\u0026lt;- dagitty(\u0026#34;dag{ 2U [unobserved] 3V [unobserved] 4X -\u0026gt; Y 5X \u0026lt;- U -\u0026gt; B \u0026lt;- C -\u0026gt; Y 6U \u0026lt;- A -\u0026gt; C 7C \u0026lt;- V -\u0026gt; Y 8}\u0026#34;) 9coordinates(dag6m1)\u0026lt;-list( 10x=c(X=0,Y=2,U=0,A=1,B=1,C=2,V=2.5), 11y=c(X=2,Y=2,U=1,A=0.2,B=1.5,C=1,V=1.5) 12) We can visualize this with:\n1dag6m1 %\u0026gt;% drawdag  The paths between \\(X\\) and \\(Y\\) are:\n \\(X→Y\\) \\(X←U→A←C→Y\\) \\(X←U→A←C←V→Y\\) \\(X←U→B←C→Y\\) \\(X←U→B←C←V→Y\\)  We can leverage dagitty to check which paths should be closed.\n1dag6m1 %\u0026gt;% adjustmentSets(exposure=\u0026#34;X\u0026#34;,outcome=\u0026#34;Y\u0026#34;) %\u0026gt;% print 1{ A } Logically, conditioning on \\(A\\) to close non-causal paths makes sense as it consistent with the understanding that only (1) is a causal path, and the rest will confound paths.\n6M2 Sometimes in order to avoid multicollinearity, people inspect pairwise correlations among predictors before including them in a model. This is a bad procedure, because what matters is the conditional association, not the association before the variables are included in the model. To highlight this, consider the DAG \\(X\\to Z\\to Y\\). Simulate data from this DAG so that the correlation between \\(X\\) and \\(Z\\) is very large. Then include both in a model prediction \\(Y\\). Do you observe any multicollinearity? Why or why not? What is different from the legs example in the chapter?\nSolution The DAG under consideration is: \\[ X\\to Z\\to Y \\] We will simulate data first.\n1N\u0026lt;-5000 2X\u0026lt;-N %\u0026gt;% rnorm(mean=0,sd=1) 3Z\u0026lt;-N %\u0026gt;% rnorm(mean=X,sd=0.5) 4Y\u0026lt;-N %\u0026gt;% rnorm(mean=Z,sd=1) 5cor(X,Z) %\u0026gt;% print 12[1] 0.9987166 The variables \\(X\\) and \\(Z\\) are highly correlated. We can check with a regression model for this.\n1m6m2\u0026lt;-quap( 2alist( 3Y ~ dnorm(mu,sigma), 4mu\u0026lt;-a+bX*X+bZ*Z, 5c(a,bX,bZ)~dnorm(0,1), 6sigma~dexp(1) 7), data=list(X=X,Y=Y,Z=Z) 8) The regression fit is essentially.\n1m6m2 %\u0026gt;% precis 1mean sd 5.5% 94.5% 2a -0.01 0.01 -0.03 0.01 3bX 0.06 0.03 0.00 0.11 4bZ 0.95 0.03 0.90 1.00 5sigma 1.02 0.01 1.00 1.04 The fit shows how \\(X\\) is not a useful variable, due to the addition of \\(Z\\), which is a post-treatment variable, and thus should not have been included. In effect, we also realize from this that multicollinearity is a data-driven property, and has no interpretation outside specific model instances.\n6M3 Learning to analyze DAGs requires practice. For each of the four DAGs below, state which variables, if any, you must adjust for (condition on) to estimate the total causal influence of \\(X\\) on \\(Y\\).\n Solution We can leverage the dagitty package as well to figure out which variables should be conditioned on.\n1dag6m3a\u0026lt;- dagitty(\u0026#34;dag{ 2X -\u0026gt; Y 3X \u0026lt;- Z -\u0026gt; Y 4X \u0026lt;- Z \u0026lt;- A -\u0026gt; Y 5}\u0026#34;) 6dag6m3b\u0026lt;- dagitty(\u0026#34;dag{ 7X -\u0026gt; Y 8X -\u0026gt; Z -\u0026gt; Y 9X -\u0026gt; Z \u0026lt;- A -\u0026gt; Y 10}\u0026#34;) 11dag6m3c\u0026lt;- dagitty(\u0026#34;dag{ 12X -\u0026gt; Y 13X -\u0026gt; Z \u0026lt;- Y 14X \u0026lt;- A -\u0026gt; Z \u0026lt;- Y 15}\u0026#34;) 16dag6m3d\u0026lt;- dagitty(\u0026#34;dag{ 17X -\u0026gt; Y 18X -\u0026gt; Z -\u0026gt; Y 19X \u0026lt;- A -\u0026gt; Z -\u0026gt; Y 20}\u0026#34;) 1dag6m3a %\u0026gt;% adjustmentSets(exposure=\u0026#34;X\u0026#34;,outcome=\u0026#34;Y\u0026#34;) %\u0026gt;% print 2dag6m3b %\u0026gt;% adjustmentSets(exposure=\u0026#34;X\u0026#34;,outcome=\u0026#34;Y\u0026#34;) %\u0026gt;% print 3dag6m3c %\u0026gt;% adjustmentSets(exposure=\u0026#34;X\u0026#34;,outcome=\u0026#34;Y\u0026#34;) %\u0026gt;% print 4dag6m3d %\u0026gt;% adjustmentSets(exposure=\u0026#34;X\u0026#34;,outcome=\u0026#34;Y\u0026#34;) %\u0026gt;% print 1{ Z } 23{} 45{} 67{ A } Clearly the upper left and lower right DAGs need to be conditioned on Z and A respectively to close non-causal paths.\nWe can further rationalize this as follows:\n Upper Left \\(X\\gets Z\\to Y\\) and \\(X\\gets Z \\gets A \\to Y\\) are open, non-causal paths which need to be closed Upper Right \\(Z\\) is a collider which ensures that only causal paths are open Lower Left There is a collider \\(Z\\) which ensures that the non-causal paths are closed Lower Right This figure is more complicated, so we will consider all the paths, i.e. \\(X \\to Y\\), \\(X \\to Z \\to Y\\), \\(X\\gets A \\to Z\\to Y\\), and we clearly need to condition on either \\(A\\) or \\(Z\\). \\(Z\\) is also part of a causal path, so only \\(A\\) is to be conditioned on  A more canonical way to do this is to enumerate all paths for every option, but dagitty is more elegant.\n1dag6m3d %\u0026gt;% graphLayout %\u0026gt;% plot Chapter VII: Ulysses\u0026rsquo; Compass Easy Questions (Ch7) HOLD 7E1 State the three motivating criteria that define information entropy. Try to express each in your own words.\nSolution The motivating criteria for defining informational entropy or \u0026ldquo;uncertainity\u0026rdquo; are:\n Continuity It is preferable to have a continuous function to define our informational criteria, since we can always discretize a continuous function (by binning) later, but a discrete function does not have a full range of values which can correspond to all the possible models. As a metric then, it is preferable to have a minimum and maximum bound, but define it such that it is continuous for representing arbitrary models Positive and Monotonic The monotonicity constraint is simply to ensure that as the number of events increases, given no other changes in the system, the uncertainity will increase. Since the function is already continuous, the incerasing nature is really by construction. It should be noted that a monotonously decreasing function would also satisfy the motivating criteria, but will change the interpretation completely Additivity As mentioned for continuity, it is possible always to bin continuous functions or discretize it. Similarly, it is desirable to keep the amount of uncertainity constant and add or subtract values to redefine categories  7E2 Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads \\(70\\%\\) of the time. What is the entropy of this coin?\nSolution We can simulate this system easily.\n1p\u0026lt;-c(0.7,0.3) 2-sum(p*log(p)) %\u0026gt;% print 1[1] 0.6108643 7E3 Suppose a four-sided die is loaded such that, when tossed onto a table, it shows \u0026ldquo;1\u0026rdquo; \\(20\\%\\), \u0026ldquo;2\u0026rdquo;, \\(25\\%\\), and \u0026ldquo;4\u0026rdquo; \\(30\\%\\) of the time. What is the entropy of this die?\nSolution 1p\u0026lt;-c(0.2,0.25,0.25,0.3) 2-sum(p*log(p)) %\u0026gt;% print 1[1] 1.376227 7E4 Suppose another four-sided die is loaded such that it never shows \u0026ldquo;4\u0026rdquo;. The other three sides show equally often. What is the entropy of this die?\nSolution We will not consider impossible events in our simulation.\n1p\u0026lt;- c(1/3,1/3,1/3) 2-sum(p*log(p)) %\u0026gt;% print 1[1] 1.098612 Questions of Medium Complexity (Ch7) HOLD 7M1 Write down and compare the definitions of AIC and WAIC. Which of these criteria is most general? Which assumptions are required to transform the more general criterion into a less general one?\nSolution We know that AIC or \u0026ldquo;Akaike Information Criterion\u0026rdquo; is defined as:\nWhere \\(k\\) is the number of parameters in the model.\nThe WAIC or \u0026ldquo;Widely Applicable Information Criterion\u0026rdquo; is given by: \\[\\mathrm{WAIC}=-2\\left(\\sum_{i}\\log\\Pr(y_{i})-\\sum_{i}V(y_{i})\\right)\\]\nWAIC is more general than the AIC. WAIC and AIC will be approximately equivalent when the priors are effectively flat or when there is enough data to render the priors redundant. This is because the WAIC makes no assumptions about the shape of the posterior, while AIC is an approximation depending on:\n A flat prior (or one overwhelmed by the likelihood) A posterior distribution which is approximately a multivariate Gaussian Sample size \\(N\\) with more parameters (\\(p\\))  Furthermore, we note that the AIC simply estimates that the penalty term is twice the number of parameters, while the WAIC fits uses the lppd or the sum of variances of each log-likelihood.\nHOLD 7M2 Explain the difference between model selection and model comparison. What information is lost under model selection?\nSolution Model selection involves choosing one model over the others. Ideally this occurs after appropriate model comparision. However, the chapter does mention that it is common to use heuristics like \u0026ldquo;stargazing\u0026rdquo; which uses frequentist tools to estimate which variables are important, then choose a model (or causal salad)a which has the highest number of significant variables.\nModel comparision in theory should be based off entropic measures for the information used. The models should be trained on the same data-set for the metrics to be meaningful.\nModel selection loses information regarding the uncertainity quantifications of the models which do not necessarily have the (relatively) optimal values of the metric used for comparision. This is important, especially since models which are parameterized for prediction, often perform better without being useful for causal analysis.\n7M3 When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure.\nSolution When using an information criterion, it is important to understand that different values define different \u0026ldquo;small worlds\u0026rdquo;.\nThis is why when working on gauging the information criterion, which work on the basis of the accumulated deviance values, having a varying number of training values will effectively be comparing apples and oranges. Each training data-set essentially fits one model, and comparing models trained on different data-sets (even subsets of the same data) will not lead to a fundamentally sound comparison.\nWe also know that in general, fewer data-points will have fewer deviance terms, and therefore artificially seem to be better.\nWe will prove this with an artificial data-set.\n1ySmallDat \u0026lt;- rnorm(100) 2yLargeDat \u0026lt;- rnorm(1000) 3m7m3S \u0026lt;- quap( 4alist( 5y ~ dnorm(mu,1), 6mu ~ dnorm(0,sigma) 7), data=list(y=ySmallDat,sigma=1) 8) 9m7m3L \u0026lt;- quap( 10alist( 11y ~ dnorm(mu,1), 12mu ~ dnorm(0,sigma) 13),data=list(y=yLargeDat,sigma=1) 14) 1WAIC(m7m3S) %\u0026gt;% rbind(WAIC(m7m3L)) %\u0026gt;% mutate(numSamples=c(100,1000)) %\u0026gt;% toOrg    WAIC lppd penalty std_err numSamples     278.876677095335 -138.576629006818 0.861709540849766 11.1055429875975 100   2898.5831283182 -1448.20174278015 1.08982137894866 49.5298847525459 1000    We see that apparently, the model with fewer data-points is superior, but from the discussion above, as well as by construction, we know that the models are the same, so the effect is clearly spurious, and caused by training on different data-sets.\n7M4 What happens to the effective number of parameters as measured by PSIS or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure.\nSolution Since a strength of a prior is directly related to the process of regularization, it is clear that as a prior becomes more concentrated, the model tends to be more critical of new data, and therefore the effective number of parameters will drop proportionately. Another approach to the same problem is to understand that the prior encodes our previous beliefs which in effect represents additional data which the model a-priori has been trained with.\nWe can test this simply by re-using the models we defined for 7M3.\n1yDat \u0026lt;- rnorm(5) 2sigL\u0026lt;-1000 3sigS\u0026lt;-1 4m7m4S \u0026lt;- quap( 5alist( 6y ~ dnorm(mu,1), 7mu ~ dnorm(0,sigma) 8), data=list(y=yDat,sigma=sigS) 9) 10m7m4L \u0026lt;- quap( 11alist( 12y ~ dnorm(mu,1), 13mu ~ dnorm(0,sigma) 14),data=list(y=yDat,sigma=sigL) 15) Recall that the WAIC is defined by:\n\\[ WAIC = -2(lppd-pWAIC) \\]\nWhere pWAIC is the effective number of parameters. So we note that:\n\\[ pWAIC=lppd-0.5*WAIC \\]\nThis is reported by WAIC as the penalty parameter.\n1WAIC(m7m4S) %\u0026gt;% rbind(WAIC(m7m4L)) %\u0026gt;% mutate(sigma=c(sigS,sigL)) %\u0026gt;% toOrg    WAIC lppd penalty std_err sigma     16.4098404638955 -7.31440324407321 0.890516987874561 2.31086161575483 1   16.9915093637752 -7.3011838990595 1.19457078282808 2.5268591022024 1000    Though the effect is not too strong, it is clear that having a denser prior (a.k.a smaller sigma) has a smaller number of effective paramters, as expected.\nHOLD 7M5 Provide an informal explanation of why informative priors reduce overfitting.\nSolution Overfitting is easier to understand in the context of data-compression. Essentially, when overfitting occurs, the data is represented in a different encoding, instead of being compressed.\nWe can also look at the overfitting process to be a trade off between simply fitting to every data-point (low bias, high variance) and being completely oblivious to the data (high bias, low variance). In another sense, overfitting occurs when the model is \u0026ldquo;overly eager\u0026rdquo; to learn from the data.\nGiven this understanding, informative priors essentially regularize the model, by ensuring that the likelihood is closer to the posterior, and hence prevents the model from \u0026ldquo;learning\u0026rdquo; from data-points which are not actually relevant to the prior.\nThis implies that overfitting reduces the model by lowering the sensitivity of the model to a sample, which implicitly implies that the data contains points which are not actually a feature of the process which will generate future data.\nHOLD 7M6 Provide an informal explanation of why overly informative priors result in underfitting.\nSolution Underfitting occurs when the model is insensitive to newer samples of the data. In classical terms, this means that the model has a very high bias, and typically has a correspondingly low variance.\nWith the understanding that priors cause regularization, which enforces sparsity of features, it is easier to see that very strong priors ensure that the model is overly sparse and incapable of picking up relevant trends in the training data.\nOverly informative priors, essentially imply that the model has \u0026ldquo;seen\u0026rdquo; a large amount of data previously, which then means that it is less sensitive to newer samples of data. This means that features present in the training data which are relevant to future data will be ignored in favor of the prior predictions.\nA: Colophon To ensure that this document is fully reproducible at a later date, we will record the session info.\n1devtools::session_info() 1- Session info --------------------------------------------------------------- 2setting value 3version R version 4.0.0 (2020-04-24) 4os Arch Linux 5system x86_64, linux-gnu 6ui X11 7language (EN) 8collate C 9ctype C 10tz Iceland 11date 2020-06-13 1213- Packages ------------------------------------------------------------------- 14package * version date lib source 15arrayhelpers 1.1-0 2020-02-04 [167] CRAN (R 4.0.0) 16assertthat 0.2.1 2019-03-21 [34] CRAN (R 4.0.0) 17backports 1.1.6 2020-04-05 [68] CRAN (R 4.0.0) 18boot 1.3-24 2019-12-20 [5] CRAN (R 4.0.0) 19broom 0.5.6 2020-04-20 [67] CRAN (R 4.0.0) 20callr 3.4.3 2020-03-28 [87] CRAN (R 4.0.0) 21cellranger 1.1.0 2016-07-27 [55] CRAN (R 4.0.0) 22cli 2.0.2 2020-02-28 [33] CRAN (R 4.0.0) 23coda 0.19-3 2019-07-05 [169] CRAN (R 4.0.0) 24colorspace 1.4-1 2019-03-18 [97] CRAN (R 4.0.0) 25crayon 1.3.4 2017-09-16 [35] CRAN (R 4.0.0) 26curl 4.3 2019-12-02 [26] CRAN (R 4.0.0) 27dagitty * 0.2-2 2016-08-26 [244] CRAN (R 4.0.0) 28data.table * 1.12.8 2019-12-09 [27] CRAN (R 4.0.0) 29DBI 1.1.0 2019-12-15 [77] CRAN (R 4.0.0) 30dbplyr 1.4.3 2020-04-19 [76] CRAN (R 4.0.0) 31desc 1.2.0 2018-05-01 [84] CRAN (R 4.0.0) 32devtools * 2.3.0 2020-04-10 [219] CRAN (R 4.0.0) 33digest 0.6.25 2020-02-23 [42] CRAN (R 4.0.0) 34dplyr * 0.8.5 2020-03-07 [69] CRAN (R 4.0.0) 35ellipsis 0.3.0 2019-09-20 [30] CRAN (R 4.0.0) 36evaluate 0.14 2019-05-28 [82] CRAN (R 4.0.0) 37fansi 0.4.1 2020-01-08 [36] CRAN (R 4.0.0) 38forcats * 0.5.0 2020-03-01 [29] CRAN (R 4.0.0) 39fs 1.4.1 2020-04-04 [109] CRAN (R 4.0.0) 40generics 0.0.2 2018-11-29 [71] CRAN (R 4.0.0) 41ggplot2 * 3.3.0 2020-03-05 [78] CRAN (R 4.0.0) 42glue * 1.4.0 2020-04-03 [37] CRAN (R 4.0.0) 43gridExtra 2.3 2017-09-09 [123] CRAN (R 4.0.0) 44gtable 0.3.0 2019-03-25 [79] CRAN (R 4.0.0) 45haven 2.2.0 2019-11-08 [28] CRAN (R 4.0.0) 46hms 0.5.3 2020-01-08 [44] CRAN (R 4.0.0) 47htmltools 0.4.0 2019-10-04 [112] CRAN (R 4.0.0) 48httr 1.4.1 2019-08-05 [100] CRAN (R 4.0.0) 49inline 0.3.15 2018-05-18 [162] CRAN (R 4.0.0) 50jsonlite 1.6.1 2020-02-02 [101] CRAN (R 4.0.0) 51kableExtra * 1.1.0 2019-03-16 [212] CRAN (R 4.0.0) 52knitr 1.28 2020-02-06 [113] CRAN (R 4.0.0) 53latex2exp * 0.4.0 2015-11-30 [211] CRAN (R 4.0.0) 54lattice 0.20-41 2020-04-02 [6] CRAN (R 4.0.0) 55lifecycle 0.2.0 2020-03-06 [38] CRAN (R 4.0.0) 56loo 2.2.0 2019-12-19 [163] CRAN (R 4.0.0) 57lubridate 1.7.8 2020-04-06 [106] CRAN (R 4.0.0) 58magrittr 1.5 2014-11-22 [21] CRAN (R 4.0.0) 59MASS 7.3-51.5 2019-12-20 [7] CRAN (R 4.0.0) 60matrixStats 0.56.0 2020-03-13 [164] CRAN (R 4.0.0) 61memoise 1.1.0 2017-04-21 [229] CRAN (R 4.0.0) 62modelr 0.1.6 2020-02-22 [107] CRAN (R 4.0.0) 63munsell 0.5.0 2018-06-12 [96] CRAN (R 4.0.0) 64mvtnorm 1.1-0 2020-02-24 [243] CRAN (R 4.0.0) 65nlme 3.1-147 2020-04-13 [11] CRAN (R 4.0.0) 66orgutils * 0.4-1 2017-03-21 [209] CRAN (R 4.0.0) 67pillar 1.4.3 2019-12-20 [39] CRAN (R 4.0.0) 68pkgbuild 1.0.6 2019-10-09 [86] CRAN (R 4.0.0) 69pkgconfig 2.0.3 2019-09-22 [43] CRAN (R 4.0.0) 70pkgload 1.0.2 2018-10-29 [83] CRAN (R 4.0.0) 71plyr 1.8.6 2020-03-03 [73] CRAN (R 4.0.0) 72prettyunits 1.1.1 2020-01-24 [58] CRAN (R 4.0.0) 73printr * 0.1 2017-05-19 [214] CRAN (R 4.0.0) 74processx 3.4.2 2020-02-09 [88] CRAN (R 4.0.0) 75ps 1.3.2 2020-02-13 [89] CRAN (R 4.0.0) 76purrr * 0.3.4 2020-04-17 [50] CRAN (R 4.0.0) 77R6 2.4.1 2019-11-12 [48] CRAN (R 4.0.0) 78Rcpp 1.0.4.6 2020-04-09 [10] CRAN (R 4.0.0) 79readr * 1.3.1 2018-12-21 [45] CRAN (R 4.0.0) 80readxl 1.3.1 2019-03-13 [54] CRAN (R 4.0.0) 81remotes 2.1.1 2020-02-15 [233] CRAN (R 4.0.0) 82reprex 0.3.0 2019-05-16 [108] CRAN (R 4.0.0) 83rethinking * 2.01 2020-06-06 [242] local 84rlang 0.4.5 2020-03-01 [31] CRAN (R 4.0.0) 85rmarkdown 2.1 2020-01-20 [110] CRAN (R 4.0.0) 86rprojroot 1.3-2 2018-01-03 [85] CRAN (R 4.0.0) 87rstan * 2.19.3 2020-02-11 [161] CRAN (R 4.0.0) 88rstudioapi 0.11 2020-02-07 [91] CRAN (R 4.0.0) 89rvest 0.3.5 2019-11-08 [120] CRAN (R 4.0.0) 90scales 1.1.0 2019-11-18 [93] CRAN (R 4.0.0) 91sessioninfo 1.1.1 2018-11-05 [231] CRAN (R 4.0.0) 92shape 1.4.4 2018-02-07 [193] CRAN (R 4.0.0) 93StanHeaders * 2.19.2 2020-02-11 [165] CRAN (R 4.0.0) 94stringi 1.4.6 2020-02-17 [52] CRAN (R 4.0.0) 95stringr * 1.4.0 2019-02-10 [74] CRAN (R 4.0.0) 96svUnit 1.0.3 2020-04-20 [168] CRAN (R 4.0.0) 97testthat 2.3.2 2020-03-02 [81] CRAN (R 4.0.0) 98textutils 0.2-0 2020-01-07 [210] CRAN (R 4.0.0) 99tibble * 3.0.1 2020-04-20 [32] CRAN (R 4.0.0) 100tidybayes * 2.0.3 2020-04-04 [166] CRAN (R 4.0.0) 101tidybayes.rethinking * 2.0.3.9000 2020-06-07 [246] local 102tidyr * 1.0.2 2020-01-24 [75] CRAN (R 4.0.0) 103tidyselect 1.0.0 2020-01-27 [49] CRAN (R 4.0.0) 104tidyverse * 1.3.0 2019-11-21 [66] CRAN (R 4.0.0) 105usethis * 1.6.0 2020-04-09 [238] CRAN (R 4.0.0) 106V8 3.0.2 2020-03-14 [245] CRAN (R 4.0.0) 107vctrs 0.2.4 2020-03-10 [41] CRAN (R 4.0.0) 108viridisLite 0.3.0 2018-02-01 [99] CRAN (R 4.0.0) 109webshot 0.5.2 2019-11-22 [213] CRAN (R 4.0.0) 110withr 2.2.0 2020-04-20 [90] CRAN (R 4.0.0) 111xfun 0.13 2020-04-13 [116] CRAN (R 4.0.0) 112xml2 1.3.2 2020-04-23 [122] CRAN (R 4.0.0) 113114[1] /nix/store/xzd8h53xkyvfm3kvj5ab6znp685wi04w-r-car-3.0-7/library 115[2] /nix/store/mhr8zw9bmxarc3n821b83i0gz2j9zlrq-r-abind-1.4-5/library 116[3] /nix/store/hp86nhr0787vib3l8mkw0gf9nxwb45im-r-carData-3.0-3/library 117[4] /nix/store/vhw7s2h5ds6sp110z2yvilchv8j9jch5-r-lme4-1.1-23/library 118[5] /nix/store/987n8g0zy9sjvfvnsck1bkkcknw05yvb-r-boot-1.3-24/library 119[6] /nix/store/jxxxxyz4c1k5g3drd35gsrbjdg028d11-r-lattice-0.20-41/library 120[7] /nix/store/q9zfm5h53m8rd08xcsdcwaag31k4z1pf-r-MASS-7.3-51.5/library 121[8] /nix/store/kjkm50sr144yvrhl5axfgykbiy13pbmg-r-Matrix-1.2-18/library 122[9] /nix/store/8786z5lgy8h3akfjgj3yq5yq4s17rhjy-r-minqa-1.2.4/library 123[10] /nix/store/93wv3j0z1nzqp6fjsm9v7v8bf8d1xkm2-r-Rcpp-1.0.4.6/library 124[11] /nix/store/akfw6zsmawmz8lmjkww0rnqrazm4mqp0-r-nlme-3.1-147/library 125[12] /nix/store/rxs0d9bbn8qhw7wmkfb21yk5abp6lpq1-r-nloptr-1.2.2.1/library 126[13] /nix/store/8n0jfiqn4275i58qgld0dv8zdaihdzrk-r-RcppEigen-0.3.3.7.0/library 127[14] /nix/store/8vxrma33rhc96260zsi1jiw7dy3v2mm4-r-statmod-1.4.34/library 128[15] /nix/store/2y46pb5x9lh8m0hdmzajnx7sc1bk9ihl-r-maptools-0.9-9/library 129[16] /nix/store/iwf9nxx1v883wlv0p88q947hpz5lhfh7-r-foreign-0.8-78/library 130[17] /nix/store/rl9sjqply6rjbnz5k792ghm62ybv76px-r-sp-1.4-1/library 131[18] /nix/store/ws4bkzyv2vj5pyn1hgwyy6nlp48arz0n-r-mgcv-1.8-31/library 132[19] /nix/store/307dzxrmnqk4p86560a02r64x1fhhmxb-r-nnet-7.3-13/library 133[20] /nix/store/g2zpzkdb9hzkza1wpcbrk58119v1wyaf-r-pbkrtest-0.4-8.6/library 134[21] /nix/store/p0l503fr8960vld70w6ilmknxs5qwq77-r-magrittr-1.5/library 135[22] /nix/store/rmjpcaw3i446kwnjgcxcaid0yac36cj2-r-quantreg-5.55/library 136[23] /nix/store/10mzmnvc5jjgk2xzasia522pk60a30qz-r-MatrixModels-0.4-1/library 137[24] /nix/store/6qwdzvmnnmhjwdnvg2zmvv6wafd1vf91-r-SparseM-1.78/library 138[25] /nix/store/aa9c39a3yiqkh1h7pbngjlbr7czvc7yi-r-rio-0.5.16/library 139[26] /nix/store/2fx4vqlybgwp5rhhy6pssqx7h1a927fn-r-curl-4.3/library 140[27] /nix/store/k4m3fn1kqvvvn8y33kd57gq49hr3ar8y-r-data.table-1.12.8/library 141[28] /nix/store/651hfjylqzmsf565wyx474vyjny771gy-r-haven-2.2.0/library 142[29] /nix/store/a3rnz28irmqvmj8axj5x5j1am2c3gzs4-r-forcats-0.5.0/library 143[30] /nix/store/j8v4gzib137q2cml31hvvfkrc0f60pp5-r-ellipsis-0.3.0/library 144[31] /nix/store/xaswqlnamf4k8vwx0x3wav3l0x60sag0-r-rlang-0.4.5/library 145[32] /nix/store/dqm3xpix2jwhhhr67s6fgrwbw7hizap7-r-tibble-3.0.1/library 146[33] /nix/store/v7xfsq6d97wpn6m0hjrac78w5xawbr8a-r-cli-2.0.2/library 147[34] /nix/store/fikjasr98klhk9cf44x4lhi57vh3pmkg-r-assertthat-0.2.1/library 148[35] /nix/store/3fya6cd38vsqdj0gjb7bcsy00sirlyw1-r-crayon-1.3.4/library 149[36] /nix/store/payqi9bwh216rwhaq07jgc26l4fv1zsb-r-fansi-0.4.1/library 150[37] /nix/store/h6a61ghws7yrdxlg412xl1im37z5r28i-r-glue-1.4.0/library 151[38] /nix/store/y8mjbia1wbnq26dkigr0p3xxwrbzsc2r-r-lifecycle-0.2.0/library 152[39] /nix/store/kwaghh12cnifgvcbvlv2anx0hd5f4ild-r-pillar-1.4.3/library 153[40] /nix/store/k1phn8j10nni7gzvcgp0vc25dby6bb77-r-utf8-1.1.4/library 154[41] /nix/store/k3b77y8v7zsshpp1ccs8jwk2i2g4rm9a-r-vctrs-0.2.4/library 155[42] /nix/store/iibjmbh7vj0d0bfafz98yn29ymg43gkw-r-digest-0.6.25/library 156[43] /nix/store/aqsj4k3pgm80qk4jjg7sh3ac28n6alv0-r-pkgconfig-2.0.3/library 157[44] /nix/store/i7c5v8s4hd9rlqah3bbvy06yywjqwdgk-r-hms-0.5.3/library 158[45] /nix/store/2fyrk58cmcbrxid66rbwjli7y114lvrm-r-readr-1.3.1/library 159[46] /nix/store/163xq2g5nblqgh7qhvzb6mvgg6qdrirj-r-BH-1.72.0-3/library 160[47] /nix/store/dr27b6k49prwgrjs0v30b6mf5lxa36pk-r-clipr-0.7.0/library 161[48] /nix/store/bghvqg9mcaj2jkbwpy0di6c563v24acz-r-R6-2.4.1/library 162[49] /nix/store/nq8jdq7nlg9xns4xpgyj6sqv8p4ny1wz-r-tidyselect-1.0.0/library 163[50] /nix/store/zlwhf75qld7vmwx3d4bdws057ld4mqbp-r-purrr-0.3.4/library 164[51] /nix/store/0gbmmnbpqlr69l573ymkcx8154fvlaca-r-openxlsx-4.1.4/library 165[52] /nix/store/1m1q4rmwx56dvx9rdzfsfq0jpw3hw0yx-r-stringi-1.4.6/library 166[53] /nix/store/mhy5vnvbsl4q7dcinwx3vqlyywxphbfd-r-zip-2.0.4/library 167[54] /nix/store/88sp7f7q577i6l5jjanqiv5ak6nv5357-r-readxl-1.3.1/library 168[55] /nix/store/6q9zwivzalhmzdracc8ma932wirq8rl5-r-cellranger-1.1.0/library 169[56] /nix/store/jh2n6k2ancdzqych5ix8n4rq9w514qq9-r-rematch-1.0.1/library 170[57] /nix/store/22xjqikqd6q556absb5224sbx6q0kp0c-r-progress-1.2.2/library 171[58] /nix/store/9vp32wa1qvv6lkq6p70qlli5whrxzfbi-r-prettyunits-1.1.1/library 172[59] /nix/store/r9rhqb6fsk75shihmb7nagqb51pqwp0y-r-class-7.3-16/library 173[60] /nix/store/z1kad071y43wij1ml9lpghh7jbimmcli-r-cluster-2.1.0/library 174[61] /nix/store/i8wr965caf6j1rxs2dsvpzhlh4hyyb4y-r-codetools-0.2-16/library 175[62] /nix/store/8iglq3zr68a39hzswvzxqi2ffhpw9p51-r-KernSmooth-2.23-16/library 176[63] /nix/store/n3k50zv40i40drpdf8npbmy2y08gkr6w-r-rpart-4.1-15/library 177[64] /nix/store/b4r6adzcvpm8ivflsmis7ja7q4r5hkjy-r-spatial-7.3-11/library 178[65] /nix/store/zqg6hmrncl8ax3vn7z5drf4csddwnhcx-r-survival-3.1-12/library 179[66] /nix/store/4anrihkx11h8mzb269xdyi84yp5v7grl-r-tidyverse-1.3.0/library 180[67] /nix/store/945haq0w8nfm9ib7r0nfngn5lk2i15ix-r-broom-0.5.6/library 181[68] /nix/store/52viqxzrmxl7dk0zji293g5b0b9grwh8-r-backports-1.1.6/library 182[69] /nix/store/zp1k42sw2glqy51w4hnzsjs8rgi8xzx2-r-dplyr-0.8.5/library 183[70] /nix/store/mkjd98mnshch2pwnj6h31czclqdaph3f-r-plogr-0.2.0/library 184[71] /nix/store/kflrzax6y5pwfqwzgfvqz433a3q3hnhn-r-generics-0.0.2/library 185[72] /nix/store/xi1n5h5w17c33y6ax3dfhg2hgzjl9bxz-r-reshape2-1.4.4/library 186[73] /nix/store/vn63z92zkpbaxmmhzpb6mq2fvg0xa26h-r-plyr-1.8.6/library 187[74] /nix/store/wmpyxss67bj44rin7hlnr9qabx66p5hj-r-stringr-1.4.0/library 188[75] /nix/store/330qbgbvllwz3h0i2qidrlk50y0mbgph-r-tidyr-1.0.2/library 189[76] /nix/store/cx3x4pqb65l1mhss65780hbzv9jdrzl6-r-dbplyr-1.4.3/library 190[77] /nix/store/gsj49bp3hpw9jlli3894c49amddryqsq-r-DBI-1.1.0/library 191[78] /nix/store/kvymhwp4gac0343c2yi1qvdpavx4gdn2-r-ggplot2-3.3.0/library 192[79] /nix/store/knv51jvpairvibrkkq48b6f1l2pa1cv8-r-gtable-0.3.0/library 193[80] /nix/store/158dx0ddv20ikwag2860nlg9p3hbh1zc-r-isoband-0.2.1/library 194[81] /nix/store/fprs9rp1jlhxzj7fp6l79akyf8k3p7zd-r-testthat-2.3.2/library 195[82] /nix/store/0pmlnkyn0ir3k9bvxihi1r06jyl64w3i-r-evaluate-0.14/library 196[83] /nix/store/7210bjjqn5cjndxn5isnd4vip00xhkhy-r-pkgload-1.0.2/library 197[84] /nix/store/9a12ybd74b7dns40gcfs061wv7913qjy-r-desc-1.2.0/library 198[85] /nix/store/na9pb1apa787zp7vvyz1kzym0ywjwbj0-r-rprojroot-1.3-2/library 199[86] /nix/store/pa2n7bh61qxyarn5i2ynd62k6knb1np1-r-pkgbuild-1.0.6/library 200[87] /nix/store/1hxm1m7h4272zxk9bpsaq46mvnl0dbss-r-callr-3.4.3/library 201[88] /nix/store/bigvyk6ipglbiil93zkf442nv4y3xa1x-r-processx-3.4.2/library 202[89] /nix/store/370lr0wf7qlq0m72xnmasg2iahkp2n52-r-ps-1.3.2/library 203[90] /nix/store/rr72q61d8mkd42zc5fhcd2rqjghvc141-r-withr-2.2.0/library 204[91] /nix/store/9gw77p7fmz89fa8wi1d9rvril6hd4sxy-r-rstudioapi-0.11/library 205[92] /nix/store/9x4v4pbrgmykbz2801h77yz2l0nmm5nb-r-praise-1.0.0/library 206[93] /nix/store/pf8ssb0dliw5bzsncl227agc8przb7ic-r-scales-1.1.0/library 207[94] /nix/store/095z4wgjrxn63ixvyzrj1fm1rdv6ci95-r-farver-2.0.3/library 208[95] /nix/store/5aczj4s7i9prf5i32ik5ac5baqvjwdb1-r-labeling-0.3/library 209[96] /nix/store/wch26phipzz9gxd4vbr4fynh7v28349j-r-munsell-0.5.0/library 210[97] /nix/store/3w8fh756mszhsjx5fwgwydcpn8vkwady-r-colorspace-1.4-1/library 211[98] /nix/store/8cmaj81v2vm4f8p59ylbnsby8adkbmhd-r-RColorBrewer-1.1-2/library 212[99] /nix/store/h4x4ygax7gpz6f0c2v0xacr62080qwb8-r-viridisLite-0.3.0/library 213[100] /nix/store/qhx0i2nn5syb6vygdn8fdxgl7k56yj81-r-httr-1.4.1/library 214[101] /nix/store/lxnb4aniv02i4jhdvz02aaql1kznbpxb-r-jsonlite-1.6.1/library 215[102] /nix/store/13dcry4gad3vfwqzqb0ii4n06ybrxybr-r-mime-0.9/library 216[103] /nix/store/2can5l8gscc92a3bqlak8hfcg96v5hvf-r-openssl-1.4.1/library 217[104] /nix/store/piwsgxdz5w2ak8c6fcq0lc978qbxwdp1-r-askpass-1.1/library 218[105] /nix/store/3sj5h6dwa1l27d2hvdchclygk0pgffsr-r-sys-3.3/library 219[106] /nix/store/2z0p88g0c03gigl2ip60dlsfkdv1k30h-r-lubridate-1.7.8/library 220[107] /nix/store/1pkmj8nqjg2iinrkg2w0zkwq0ldc01za-r-modelr-0.1.6/library 221[108] /nix/store/bswkzvn8lczwbyw3y7n0p0qp2q472s0g-r-reprex-0.3.0/library 222[109] /nix/store/yid22gad8z49q52d225vfba2m4cgj2lx-r-fs-1.4.1/library 223[110] /nix/store/d185qiqaplm5br9fk1pf29y0srlabw83-r-rmarkdown-2.1/library 224[111] /nix/store/iszqviydsdj31c3ww095ndqy1ld3cibs-r-base64enc-0.1-3/library 225[112] /nix/store/i89wfw4cr0fz3wbd7cg44fk4dwz8b6h1-r-htmltools-0.4.0/library 226[113] /nix/store/qrl28laqwmhpwg3dpcf4nca8alv0px0g-r-knitr-1.28/library 227[114] /nix/store/jffaxc4a3bbf2g6ip0gdcya73dmg53mb-r-highr-0.8/library 228[115] /nix/store/717srph13qpnbzmgsvhx25q8pl51ivpj-r-markdown-1.1/library 229[116] /nix/store/mxqmyq3ybdfyc6p0anhfy2kfw0iz5k4n-r-xfun-0.13/library 230[117] /nix/store/b8g6hadva0359l6j1aq4dbvxlqf1acxc-r-yaml-2.2.1/library 231[118] /nix/store/rrl05vpv7cw58zi0k9ykm7m4rjb9gjv3-r-tinytex-0.22/library 232[119] /nix/store/2ziq8nzah6xy3dgmxgim9h2wszz1f89f-r-whisker-0.4/library 233[120] /nix/store/540wbw4p1g2qmnmbfk0rhvwvfnf657sj-r-rvest-0.3.5/library 234[121] /nix/store/n3prn77gd9sf3z4whqp86kghr55bf5w8-r-selectr-0.4-2/library 235[122] /nix/store/gv28yjk5isnglq087y7767xw64qa40cw-r-xml2-1.3.2/library 236[123] /nix/store/693czdcvkp6glyir0mi8cqvdc643whvc-r-gridExtra-2.3/library 237[124] /nix/store/3sykinp7lyy70dgzr0fxjb195nw864dv-r-future-1.17.0/library 238[125] /nix/store/bqi2l53jfxncks6diy0hr34bw8f86rvk-r-globals-0.12.5/library 239[126] /nix/store/dydyl209klklzh69w9q89f2dym9xycnp-r-listenv-0.8.0/library 240[127] /nix/store/lni0bi36r4swldkx7g4hql7gfz9b121b-r-gganimate-1.0.5/library 241[128] /nix/store/hh92jxs79kx7vxrxr6j6vin1icscl4k7-r-tweenr-1.0.1/library 242[129] /nix/store/0npx3srjnqgh7bib80xscjqvfyzjvimq-r-GGally-1.5.0/library 243[130] /nix/store/x5nzxklmacj6l162g7kg6ln9p25r3f17-r-reshape-0.8.8/library 244[131] /nix/store/q29z7ckdyhfmg1zlzrrg1nrm36ax756j-r-ggfortify-0.4.9/library 245[132] /nix/store/1rvm1w9iv2c5n22p4drbjq8lr9wa2q2r-r-cowplot-1.0.0/library 246[133] /nix/store/rp8jhnasaw1vbv5ny5zx0mw30zgcp796-r-ggrepel-0.8.2/library 247[134] /nix/store/wb7y931mm8nsj7w9xin83bvbaq8wvi4d-r-corrplot-0.84/library 248[135] /nix/store/gdzcqivfvgdrsz247v5kmnnw1v6p9c1p-r-rpart.plot-3.0.8/library 249[136] /nix/store/6yqg37108r0v22476cm2kv0536wyilki-r-caret-6.0-86/library 250[137] /nix/store/6fjdgcwgisiqz451sg5fszxnn9z8vxg6-r-foreach-1.5.0/library 251[138] /nix/store/c3ph5i341gk7jdinrkkqf6y631xli424-r-iterators-1.0.12/library 252[139] /nix/store/sjm1rxshlpakpxbrynfhsjnnp1sjvc3r-r-ModelMetrics-1.2.2.2/library 253[140] /nix/store/vgk4m131d057xglmrrb9rijhzdr2qhhp-r-pROC-1.16.2/library 254[141] /nix/store/bv1kvy1wc2jx3v55rzn3cg2qjbv7r8zp-r-recipes-0.1.10/library 255[142] /nix/store/001h42q4za01gli7avjxhq7shpv73n9k-r-gower-0.2.1/library 256[143] /nix/store/ssffpl6ydffqyn9phscnccxnj71chnzg-r-ipred-0.9-9/library 257[144] /nix/store/baliqip8m6p0ylqhqcgqak29d8ghral1-r-prodlim-2019.11.13/library 258[145] /nix/store/j4n2wsv98asw83qiffg6a74dymk8r2hl-r-lava-1.6.7/library 259[146] /nix/store/hf5wq5kpsf6p9slglq5iav09s4by0y5i-r-numDeriv-2016.8-1.1/library 260[147] /nix/store/s58hm38078mx4gyqffvv09zn575xn648-r-SQUAREM-2020.2/library 261[148] /nix/store/g63ydzd53586pvr9kdgk8kf5szq5f2bc-r-timeDate-3043.102/library 262[149] /nix/store/0jkarmlf1kjv4g8a3svkc7jfarpp77ny-r-mlr3-0.2.0/library 263[150] /nix/store/g1m0n1w7by213v773iyn7vnxr25pkf56-r-checkmate-2.0.0/library 264[151] /nix/store/fc2ah8cz2sj6j2jk7zldvjmsjn1yakpn-r-lgr-0.3.4/library 265[152] /nix/store/0i2hs088j1s0a6i61124my6vnzq8l27m-r-mlbench-2.1-1/library 266[153] /nix/store/vzcs6k21pqrli3ispqnvj5qwkv14srf5-r-mlr3measures-0.1.3/library 267[154] /nix/store/h2yqqaia46bk3b1d1a7bq35zf09p1b1a-r-mlr3misc-0.2.0/library 268[155] /nix/store/c9mrkc928cmsvvnib50l0jb8lsz59nyk-r-paradox-0.2.0/library 269[156] /nix/store/vqpbdipi4p4advl2vxrn765mmgcrabvk-r-uuid-0.1-4/library 270[157] /nix/store/xpclynxnfq4h9218gk4y62nmgyyga6zl-r-mlr3viz-0.1.1/library 271[158] /nix/store/7w6pld5vir3p9bybay67kq0qwl0gnx17-r-mlr3learners-0.2.0/library 272[159] /nix/store/ca50rp6ha5s51qmhb1gjlj62r19xfzxs-r-mlr3pipelines-0.1.3/library 273[160] /nix/store/9hg0xap4pir64mhbgq8r8cgrfjn8aiz5-r-mlr3filters-0.2.0/library 274[161] /nix/store/jgqcmfix0xxm3y90m8wy3xkgmqf2b996-r-rstan-2.19.3/library 275[162] /nix/store/mvv1gjyrrpvf47fn7a8x722wdwrf5azk-r-inline-0.3.15/library 276[163] /nix/store/zmkw51x4w4d1v1awcws0xihj4hnxfr09-r-loo-2.2.0/library 277[164] /nix/store/30xxalfwzxl05bbfvj5sy8k3ysys6z5y-r-matrixStats-0.56.0/library 278[165] /nix/store/fhkww2l0izx87bjnf0pl9ydl1wprp0xv-r-StanHeaders-2.19.2/library 279[166] /nix/store/aflck5pzxa8ym5q1dxchx5hisfmfghkr-r-tidybayes-2.0.3/library 280[167] /nix/store/jhlbhiv4fg0wsbxwjz8igc4hcg79vw94-r-arrayhelpers-1.1-0/library 281[168] /nix/store/fv089zrnvicnavbi08hnzqpi9g1z4inj-r-svUnit-1.0.3/library 282[169] /nix/store/xci2rgjizx1fyb33818jx5s1bgn8v8k6-r-coda-0.19-3/library 283[170] /nix/store/dch9asd38yldz0sdn8nsgk9ivjrkbhva-r-HDInterval-0.2.0/library 284[171] /nix/store/rs8dri2m5cqdmpiw187rvl4yhjn0jg2v-r-e1071-1.7-3/library 285[172] /nix/store/qs1zyh3sbvccgnqjzas3br6pak399zgc-r-pvclust-2.2-0/library 286[173] /nix/store/sh3zxvdazp7rkjn1iczrag1h2358ifm1-r-forecast-8.12/library 287[174] /nix/store/h67kaxqr2ppdpyj77wg5hm684jypznji-r-fracdiff-1.5-1/library 288[175] /nix/store/fh0z465ligbpqyam5l1fwiijc7334kbk-r-lmtest-0.9-37/library 289[176] /nix/store/0lnsbwfg0axr80h137q52pa50cllbjpf-r-zoo-1.8-7/library 290[177] /nix/store/p7k4s3ivf83dp2kcxr1cr0wlc1rfk6jx-r-RcppArmadillo-0.9.860.2.0/library 291[178] /nix/store/ssnxv5x6zid2w11v8k5yvnyxis6n1qfk-r-tseries-0.10-47/library 292[179] /nix/store/zrbskjwaz0bzz4v76j044d771m24g6h8-r-quadprog-1.5-8/library 293[180] /nix/store/2x3w5sjalrfm6hf1dxd951j8y94nh765-r-quantmod-0.4.17/library 294[181] /nix/store/7g55xshf49s9379ijm1zi1qnh1vbsifq-r-TTR-0.23-6/library 295[182] /nix/store/6ilyzph46q6ijyanq4p7f0ccyni0d7j0-r-xts-0.12-0/library 296[183] /nix/store/17xhqghcnqha7pwbf98dxsq1729slqd5-r-urca-1.3-0/library 297[184] /nix/store/722lyn0k8y27pj1alik56r4vpjnncd9z-r-swdft-1.0.0/library 298[185] /nix/store/36n0zgy10fsqcq76n0qmdwjxrwh7pn9n-r-xgboost-1.0.0.2/library 299[186] /nix/store/ac0ar7lf75qx84xsdjv6j02rkdgnhybz-r-ranger-0.12.1/library 300[187] /nix/store/i1ighkq42x10dirqmzgbx2mhbnz1ynkb-r-DALEX-1.2.0/library 301[188] /nix/store/28fqnhsfng1bkphl0wvr7lg5y3p6va46-r-iBreakDown-1.2.0/library 302[189] /nix/store/dpym77x9qc2ksr4mwjm3pb9ar1kvwhdl-r-ingredients-1.2.0/library 303[190] /nix/store/sp4d281w6dpr31as0xdjqizdx8hhb01q-r-DALEXtra-0.2.1/library 304[191] /nix/store/ckhp9kpmjcs0wxb113pxn25c2wip2d0n-r-ggdendro-0.1-20/library 305[192] /nix/store/f3k7dxj1dsmqri2gn0svq4c9fvvl9g7q-r-glmnet-3.0-2/library 306[193] /nix/store/l6ccj6mwkqybjvh6dr8qzalygp0i7jyb-r-shape-1.4.4/library 307[194] /nix/store/418mqfwlafh6984xld8lzhl7rv29qw68-r-reticulate-1.15/library 308[195] /nix/store/qwh982mgxd2mzrgbjk14irqbasywa1jk-r-rappdirs-0.3.1/library 309[196] /nix/store/6sxs76abll23c6372h6nf101wi8fcr4c-r-FactoMineR-2.3/library 310[197] /nix/store/39d2va10ydgyzddwr07xwdx11fwk191i-r-ellipse-0.4.1/library 311[198] /nix/store/4lxym5nxdn8hb7l8a566n5vg9paqcfi2-r-flashClust-1.01-2/library 312[199] /nix/store/wp161zbjjs41fq4kn4k3m244c7b8l2l2-r-leaps-3.1/library 313[200] /nix/store/irghsaplrpb3hg3y7j831bbklf2cqs6d-r-scatterplot3d-0.3-41/library 314[201] /nix/store/09ahkf50g1q9isxanbdykqgcdrp8mxl1-r-factoextra-1.0.7/library 315[202] /nix/store/zi9bq7amsgc6w2x7fvd62g9qxz69vjfm-r-dendextend-1.13.4/library 316[203] /nix/store/wcywb7ydglzlxg57jf354x31nmy63923-r-viridis-0.5.1/library 317[204] /nix/store/pvnpg4vdvv93pmwrlgmy51ihrb68j55f-r-ggpubr-0.2.5/library 318[205] /nix/store/qpapsc4l9pylzfhc72ha9d82hcbac41z-r-ggsci-2.9/library 319[206] /nix/store/h0zg4x3bmkc82ggx8h4q595ffckcqgx5-r-ggsignif-0.6.0/library 320[207] /nix/store/vn5svgbf8vsgv8iy8fdzlj0izp279q15-r-polynom-1.4-0/library 321[208] /nix/store/mc1mlsjx5h3gc8nkl7jlpd4vg145nk1z-r-lindia-0.9/library 322[209] /nix/store/z1k4c8lhabp9niwfg1xylg58pf99ld9r-r-orgutils-0.4-1/library 323[210] /nix/store/ybj4538v74wx4f1l064m0qn589vyjmzg-r-textutils-0.2-0/library 324[211] /nix/store/hhm5j0wvzjc0bfd53170bw8w7mij2wnh-r-latex2exp-0.4.0/library 325[212] /nix/store/njlv5mkxgjyx3x8p984nr84dwa2v1iqp-r-kableExtra-1.1.0/library 326[213] /nix/store/lf2sb84ylh259m421ljbj731a4prjhsl-r-webshot-0.5.2/library 327[214] /nix/store/n6b8ap54b78h8l70kyx9nvayp44rnfzf-r-printr-0.1/library 328[215] /nix/store/02g1v6d3ly8zylpckigwk6w3l1mx2i9d-r-microbenchmark-1.4-7/library 329[216] /nix/store/ri6qm0fp8cyx2qnysxjv2wsk0nndl1x9-r-webchem-0.5.0/library 330[217] /nix/store/cg95rqc1gmaqxf5kxja3cz8m5w4vl76l-r-RCurl-1.98-1.2/library 331[218] /nix/store/qbpinv148778fzdz8372x8gp34hspvy1-r-bitops-1.0-6/library 332[219] /nix/store/1g0lbrx6si76k282sxr9cj0mgknrw0lx-r-devtools-2.3.0/library 333[220] /nix/store/hnvww0128czlx6w8aipjn0zs7nvmvak9-r-covr-3.5.0/library 334[221] /nix/store/p4nv59przmb14sxi49jwqarkv0l40jsp-r-rex-1.2.0/library 335[222] /nix/store/vnysmc3vkgkligwah1zh9l4sahr533a8-r-lazyeval-0.2.2/library 336[223] /nix/store/d638w33ahybsa3sqr52fafvxs2b7w9x3-r-DT-0.13/library 337[224] /nix/store/35nqc34wy2nhd9bl7lv6wriw0l3cghsw-r-crosstalk-1.1.0.1/library 338[225] /nix/store/03838i63x5irvgmpgwj67ah0wi56k9d7-r-htmlwidgets-1.5.1/library 339[226] /nix/store/l4640jxlsjzqhw63c18fziar5vc0xyhk-r-promises-1.1.0/library 340[227] /nix/store/rxrb8p3dxzsg10v7yqaq5pi3y3gk6nqh-r-later-1.0.0/library 341[228] /nix/store/giprr32bl6k18b9n4qjckpf102flarly-r-git2r-0.26.1/library 342[229] /nix/store/bbkpkf44b13ig1pkz7af32kw5dzp12vb-r-memoise-1.1.0/library 343[230] /nix/store/m31vzssnfzapsapl7f8v4m15003lcc8r-r-rcmdcheck-1.3.3/library 344[231] /nix/store/hbiylknhxsin9hp9zaa6dwc2c9ai1mqx-r-sessioninfo-1.1.1/library 345[232] /nix/store/8vwlbx3s345gjccrkiqa6h1bm9wq4s9q-r-xopen-1.0.0/library 346[233] /nix/store/mjnwnlv60cn56ap0rrzvrkqlh5qisszx-r-remotes-2.1.1/library 347[234] /nix/store/1rq4zyzqymml7cc11q89rl5g514ml9na-r-roxygen2-7.1.0/library 348[235] /nix/store/2658mrn1hpkq0fv629rvags91qg65pbn-r-brew-1.0-6/library 349[236] /nix/store/nvjalws9lzva4pd4nz1z2131xsb9b5p6-r-commonmark-1.7/library 350[237] /nix/store/qx900vivd9s2zjrxc6868s92ljfwj5dv-r-rversions-2.0.1/library 351[238] /nix/store/1drg446wilq5fjnxkglxnnv8pbp1hllg-r-usethis-1.6.0/library 352[239] /nix/store/p3f3wa41d304zbs5cwvw7vy4j17zd6nq-r-gh-1.1.0/library 353[240] /nix/store/769g7jh93da8w15ad0wsbn2aqziwwx56-r-ini-0.3.1/library 354[241] /nix/store/p7kifw1l6z2zg68a71s4sdbfj8gdmnv5-r-rematch2-2.1.1/library 355[242] /nix/store/6zhdqip9ld9vl6pvifqcf4gsqy2f5wix-r-rethinking/library 356[243] /nix/store/496p28klmflihdkc83c8p1cywg85mgk4-r-mvtnorm-1.1-0/library 357[244] /nix/store/xb1zn7ab4nka7h1vm678ginzfwg4w9wf-r-dagitty-0.2-2/library 358[245] /nix/store/3zj4dkjbdwgf3mdsl9nf9jkicpz1nwgc-r-V8-3.0.2/library 359[246] /nix/store/qiqsh62w69b5xgj2i4wjamibzxxji0mf-r-tidybayes.rethinking/library 360[247] /nix/store/4j6byy1klyk4hm2k6g3657682cf3wxcj-R-4.0.0/lib/R/library   Summer of 2020\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/sr2-ch5-ch6-ch7/","tags":["solutions","R","SR2"],"title":"SR2 :: Solutions for Chapters {5,6,7}"},{"categories":["programming"],"contents":" A short post on my current set-up for R with nixpkgs and emacs to auto-compile my system configuration.\n Background This is my third post on working with nixpkgs and R.\n Part I covered ways of working effectively with R and nixpkgs Part II dealt with composing dependent devtools packages in a per-package environment, with a focus on rethinking and tidybayes.rethinking  This final part is about automating the system-wide configuration using emacs. Specifically doom-emacs. Naturally, this is the most optimal way to work with nix packages as well.\nSystem Configuration After experimenting with a per-project layout, I decided to use the full system configuration instead of the per-project layout. So I simply set:\n1# $HOME/.config/nixpkgs/config.nix 2{ 3packageOverrides = super: 4let 5self = super.pkgs; 6rethinking = with self.rPackages; 7buildRPackage { 8name = \u0026#34;rethinking\u0026#34;; 9src = self.fetchFromGitHub { 10owner = \u0026#34;rmcelreath\u0026#34;; 11repo = \u0026#34;rethinking\u0026#34;; 12rev = \u0026#34;d0978c7f8b6329b94efa2014658d750ae12b1fa2\u0026#34;; 13sha256 = \u0026#34;1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0\u0026#34;; 14}; 15propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ]; 16}; 17tidybayes_rethinking = with self.rPackages; 18buildRPackage { 19name = \u0026#34;tidybayes.rethinking\u0026#34;; 20src = self.fetchFromGitHub { 21owner = \u0026#34;mjskay\u0026#34;; 22repo = \u0026#34;tidybayes.rethinking\u0026#34;; 23rev = \u0026#34;df903c88f4f4320795a47c616eef24a690b433a4\u0026#34;; 24sha256 = \u0026#34;1jl3189zdddmwm07z1mk58hcahirqrwx211ms0i1rzbx5y4zak0c\u0026#34;; 25}; 26propagatedBuildInputs = 27[ dplyr tibble rlang MASS tidybayes rethinking rstan ]; 28}; 29in { 30rEnv = super.rWrapper.override { 31packages = with self.rPackages; [ 32tidyverse 33devtools 34modelr 35purrr 36forcats 37#################### 38# Machine Learning # 39#################### 40# MLR3 41mlr3 42mlr3viz 43mlr3learners 44mlr3pipelines 45# Plotting tools 46ggplot2 47cowplot 48ggrepel 49RColorBrewer 50# Stan Stuff 51rstan 52tidybayes 53# Text Utilities 54orgutils 55latex2exp 56kableExtra 57knitr 58data_table 59printr 60# Devtools Stuff 61rethinking 62tidybayes_rethinking 63]; 64}; 65}; 66} If any of these look strange, refer to the earlier posts.\nAutomation Pains direnv, lorri and niv (the heroes of Part II) are not really useful for working with the system-wide configuration, but an elegant solution still exists, which leverages firestarter and after-save-hooks in emacs.\nFirestarter Firestarter is my favorite method of working with shell commands after saving things. My setup is simply:\n1; packages.el 2(package! firestarter) This is coupled with a simple configuration.\n1; config.el 2(use-package! firestarter 3:ensure t 4:init 5(firestarter-mode) 6:config 7(setq firestarter-default-type t) 8) The default type corresponds to demanding the shell outupt for the commands.\nNix-R Stuff To finalize this setup, we will need to modify our system configuration slightly. For brevity, we simply note the following local variables.\n1# $HOME/.config/nixpkgs/config.nix 2# Local Variables: 3# firestarter: \u0026#34;nix-env -f \u0026#39;\u0026lt;nixpkgs\u0026gt;\u0026#39; -iA rEnv\u0026#34; 4# firestarter-default-type: (quote failure) 5# End: The firestarter-default-type used here is to ensure that errors are displayed in a buffer.\nTo check what is being installed (if anything) simply run:\n1nix-env -f \u0026#34;\u0026lt;nixpkgs\u0026gt;\u0026#34; -iA rEnv --dry-run Conclusion This is my current setup. It works out better than most of my other attempts and seems to be an optimal approach. The packages are versioned, everything is automated, and I can reproduce changes across all my machines. Will stick with this.\n","permalink":"https://rgoswami.me/posts/emacs-nix-r/","tags":["tools","nix","workflow","R","emacs"],"title":"Emacs for Nix-R"},{"categories":["programming"],"contents":" This post describes how to set up a transparent automated setup for reproducible R workflows using nixpkgs, niv, and lorri. The explanatory example used throughout the post is one of setting up the rethinking package and running some examples from the excellent second edition of \u0026ldquo;Statistical Rethinking\u0026rdquo; by Richard McElreath.\n Background As detailed in an earlier post1, I had set up Nix to work with non-CRAN packages. If the rest of this section is unclear, please refer back to the earlier post.\nSetup For the remainder of the post, we will set up a basic project structure:\n1mkdir tryRnix/ Now we will create a shell.nix as2:\n1# shell.nix 2{ pkgs ? import \u0026lt;nixpkgs\u0026gt; { } }: 3with pkgs; 4let 5my-r-pkgs = rWrapper.override { 6packages = with rPackages; [ 7ggplot2 8tidyverse 9tidybayes 10tidybayes.rethinking 11(buildRPackage { 12name = \u0026#34;rethinking\u0026#34;; 13src = fetchFromGitHub { 14owner = \u0026#34;rmcelreath\u0026#34;; 15repo = \u0026#34;rethinking\u0026#34;; 16rev = \u0026#34;d0978c7f8b6329b94efa2014658d750ae12b1fa2\u0026#34;; 17sha256 = \u0026#34;1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0\u0026#34;; 18}; 19propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ]; 20}) 21]; 22}; 23in mkShell { 24buildInputs = with pkgs; [ git glibcLocales openssl which openssh curl wget ]; 25inputsFrom = [ my-r-pkgs ]; 26shellHook = \u0026#39;\u0026#39; 27mkdir -p \u0026#34;$(pwd)/_libs\u0026#34; 28export R_LIBS_USER=\u0026#34;$(pwd)/_libs\u0026#34; 29\u0026#39;\u0026#39;; 30GIT_SSL_CAINFO = \u0026#34;${cacert}/etc/ssl/certs/ca-bundle.crt\u0026#34;; 31LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux 32\u0026#34;${glibcLocales}/lib/locale/locale-archive\u0026#34;; 33} So we have:\n1tree tryRnix    tryRnix        └── shell.nix     0 directories, 1 file    Introspection At this point:\n I was able to install packages (system and R) arbitrarily I was able to use project specific folders Unlike npm, pipenv, poetry, conda and friends, my system was not bloated by downloading and setting up the same packages every-time I used them in different projects  However, though this is a major step up from being chained to RStudio and my system package manager, it is still perhaps not immediately obvious how this workflow is reproducible. Admittedly, I have defined my packages in a nice functional manner; but someone else might have a different upstream channel they are tracking, and thus will have different packages. Indeed the only packages which I could be sure of were the R packages I built from Github, since those were tied to a hash. Finally, the setup described for each project is pretty onerous, and it is not immediately clear how to leverage fantastic tools like direnv for working through this.\nTowards Reproducible Environments The astute reader will have noticed that I mentioned that the R packages were reproducible since they were tied to a hash, and might reasonable argue that the entire Nix ecosystem is about hashing in the first place. Once we realize that, the rest is relatively simple3.\nNiv and Pinning Niv essentially keeps track of the channel from which all the packages are installed. Setup is pretty minimal.\n1cd tryRnix/ 2nix-env -i niv 3niv init At this point, we have:\n1tree tryRnix    tryRnix        ├── nix     │  ├── sources.json    │  └── sources.nix    └── shell.nix           1 directory, 3 files    We will have to update our shell.nix to use the new sources.\n1let 2sources = import ./nix/sources.nix; 3pkgs = import sources.nixpkgs { }; 4stdenv = pkgs.stdenv; 5my-r-pkgs = pkgs.rWrapper.override { 6packages = with pkgs.rPackages; [ 7ggplot2 8tidyverse 9tidybayes 10]; 11}; 12in pkgs.mkShell { 13buildInputs = with pkgs;[ git glibcLocales openssl which openssh curl wget my-r-pkgs ]; 14shellHook = \u0026#39;\u0026#39; 15mkdir -p \u0026#34;$(pwd)/_libs\u0026#34; 16export R_LIBS_USER=\u0026#34;$(pwd)/_libs\u0026#34; 17\u0026#39;\u0026#39;; 18GIT_SSL_CAINFO = \u0026#34;${pkgs.cacert}/etc/ssl/certs/ca-bundle.crt\u0026#34;; 19LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux 20\u0026#34;${pkgs.glibcLocales}/lib/locale/locale-archive\u0026#34;; 21} We could inspect and edit these sources by hand, but it is much more convenient to simply use niv again when we need to update these.\n1cd tryRnix/ 2niv update nixpkgs -b nixpkgs-unstable At this stage we have a reproducible set of packages ready to use. However it is still pretty annoying to have to go through the trouble of writing nix-shell and also waiting while it rebuilds when we change things.\nLorri and Direnv In the past, I have made my admiration for direnv very clear (especially for python-poetry). However, though direnv does allow us to include arbitrary bash logic into our projects, it would be nice to have something which has some defaults for nix. Thankfully, the folks at TweagIO developed lorri to scratch that itch.\nThe basic setup is simple:\n1nix-env -i lorri 2cd tryRnix/ 3lorri init 1tree -a tryRnix/    tryRnix/        ├── .envrc     ├── nix     │  ├── sources.json    │  └── sources.nix    └── shell.nix           1 directory, 4 files    We can and should inspect the environment lorri wants us to load with direnv file:\n1cat tryRnix/.envrc 1$(lorri direnv) In and of itself that is not too descriptive, so we should run that on our own first.\n1EVALUATION_ROOT=\u0026#34;$HOME/.cache/lorri/gc_roots/407bd4df60fbda6e3a656c39f81c03c2/gc_root/shell_gc_root\u0026#34; 23watch_file \u0026#34;/run/user/1000/lorri/daemon.socket\u0026#34; 4watch_file \u0026#34;$EVALUATION_ROOT\u0026#34; 56#!/usr/bin/env bash 7# ^ shebang is unused as this file is sourced, but present for editor 8# integration. Note: Direnv guarantees it *will* be parsed using bash. 910function punt () { 11: 12} 1314# move \u0026#34;origPreHook\u0026#34; \u0026#34;preHook\u0026#34; \u0026#34;$@\u0026#34;;; 15move() { 16srcvarname=$1 # example: varname might contain the string \u0026#34;origPATH\u0026#34; 17# drop off the source variable name 18shift 1920destvarname=$1 # example: destvarname might contain the string \u0026#34;PATH\u0026#34; 21# drop off the destination variable name 22shift 2324# like: export origPATH=\u0026#34;...some-value...\u0026#34; 25export \u0026#34;${@?}\u0026#34;; 2627# set $original to the contents of the variable $srcvarname 28# refers to 29eval \u0026#34;$destvarname=\\\u0026#34;${!srcvarname}\\\u0026#34;\u0026#34; 3031# mark the destvarname as exported so direnv picks it up 32# (shellcheck: we do want to export the content of destvarname!) 33# shellcheck disable=SC2163 34export \u0026#34;$destvarname\u0026#34; 3536# remove the export from above, ie: export origPATH... 37unset \u0026#34;$srcvarname\u0026#34; 38} 3940function prepend() { 41varname=$1 # example: varname might contain the string \u0026#34;PATH\u0026#34; 4243# drop off the varname 44shift 4546separator=$1 # example: separator would usually be the string \u0026#34;:\u0026#34; 4748# drop off the separator argument, so the remaining arguments 49# are the arguments to export 50shift 5152# set $original to the contents of the the variable $varname 53# refers to 54original=\u0026#34;${!varname}\u0026#34; 5556# effectfully accept the new variable\u0026#39;s contents 57export \u0026#34;${@?}\u0026#34;; 5859# re-set $varname\u0026#39;s variable to the contents of varname\u0026#39;s 60# reference, plus the current (updated on the export) contents. 61# however, exclude the ${separator} unless ${original} starts 62# with a value 63eval \u0026#34;$varname=${!varname}${original:+${separator}${original}}\u0026#34; 64} 6566function append() { 67varname=$1 # example: varname might contain the string \u0026#34;PATH\u0026#34; 6869# drop off the varname 70shift 7172separator=$1 # example: separator would usually be the string \u0026#34;:\u0026#34; 73# drop off the separator argument, so the remaining arguments 74# are the arguments to export 75shift 767778# set $original to the contents of the the variable $varname 79# refers to 80original=\u0026#34;${!varname:-}\u0026#34; 8182# effectfully accept the new variable\u0026#39;s contents 83export \u0026#34;${@?}\u0026#34;; 8485# re-set $varname\u0026#39;s variable to the contents of varname\u0026#39;s 86# reference, plus the current (updated on the export) contents. 87# however, exclude the ${separator} unless ${original} starts 88# with a value 89eval \u0026#34;$varname=${original:+${original}${separator}}${!varname}\u0026#34; 90} 9192varmap() { 93if [ -f \u0026#34;$EVALUATION_ROOT/varmap-v1\u0026#34; ]; then 94# Capture the name of the variable being set 95IFS=\u0026#34;=\u0026#34; read -r -a cur_varname \u0026lt;\u0026lt;\u0026lt; \u0026#34;$1\u0026#34; 9697# With IFS=\u0026#39;\u0026#39; and the `read` delimiter being \u0026#39;\u0026#39;, we achieve 98# splitting on \\0 bytes while also preserving leading 99# whitespace: 100# 101# bash-3.2$ printf \u0026#39; \u0026lt;- leading space\\0bar\\0baz\\0\u0026#39; \\ 102# | (while IFS=\u0026#39;\u0026#39; read -d $\u0026#39;\\0\u0026#39; -r x; do echo \u0026#34;\u0026gt;$x\u0026lt;\u0026#34;; done) 103# \u0026gt; \u0026lt;- leading space\u0026lt; 104# \u0026gt;bar\u0026lt; 105# \u0026gt;baz\u0026lt;``` 106while IFS=\u0026#39;\u0026#39; read -r -d \u0026#39;\u0026#39; map_instruction \\ 107 \u0026amp;\u0026amp; IFS=\u0026#39;\u0026#39; read -r -d \u0026#39;\u0026#39; map_variable \\ 108 \u0026amp;\u0026amp; IFS=\u0026#39;\u0026#39; read -r -d \u0026#39;\u0026#39; map_separator; do 109unset IFS 110111if [ \u0026#34;$map_variable\u0026#34; == \u0026#34;${cur_varname[0]}\u0026#34; ]; then 112if [ \u0026#34;$map_instruction\u0026#34; == \u0026#34;append\u0026#34; ]; then 113append \u0026#34;$map_variable\u0026#34; \u0026#34;$map_separator\u0026#34; \u0026#34;$@\u0026#34; 114return 115fi 116fi 117done \u0026lt; \u0026#34;$EVALUATION_ROOT/varmap-v1\u0026#34; 118fi 119120121export \u0026#34;${@?}\u0026#34; 122} 123124function declare() { 125if [ \u0026#34;$1\u0026#34; == \u0026#34;-x\u0026#34; ]; then shift; fi 126127# Some variables require special handling. 128# 129# - punt: don\u0026#39;t set the variable at all 130# - prepend: take the new value, and put it before the current value. 131case \u0026#34;$1\u0026#34; in 132# vars from: https://github.com/NixOS/nix/blob/92d08c02c84be34ec0df56ed718526c382845d1a/src/nix-build/nix-build.cc#L100 133\u0026#34;HOME=\u0026#34;*) punt;; 134\u0026#34;USER=\u0026#34;*) punt;; 135\u0026#34;LOGNAME=\u0026#34;*) punt;; 136\u0026#34;DISPLAY=\u0026#34;*) punt;; 137\u0026#34;PATH=\u0026#34;*) prepend \u0026#34;PATH\u0026#34; \u0026#34;:\u0026#34; \u0026#34;$@\u0026#34;;; 138\u0026#34;TERM=\u0026#34;*) punt;; 139\u0026#34;IN_NIX_SHELL=\u0026#34;*) punt;; 140\u0026#34;TZ=\u0026#34;*) punt;; 141\u0026#34;PAGER=\u0026#34;*) punt;; 142\u0026#34;NIX_BUILD_SHELL=\u0026#34;*) punt;; 143\u0026#34;SHLVL=\u0026#34;*) punt;; 144145# vars from: https://github.com/NixOS/nix/blob/92d08c02c84be34ec0df56ed718526c382845d1a/src/nix-build/nix-build.cc#L385 146\u0026#34;TEMPDIR=\u0026#34;*) punt;; 147\u0026#34;TMPDIR=\u0026#34;*) punt;; 148\u0026#34;TEMP=\u0026#34;*) punt;; 149\u0026#34;TMP=\u0026#34;*) punt;; 150151# vars from: https://github.com/NixOS/nix/blob/92d08c02c84be34ec0df56ed718526c382845d1a/src/nix-build/nix-build.cc#L421 152\u0026#34;NIX_ENFORCE_PURITY=\u0026#34;*) punt;; 153154# vars from: https://www.gnu.org/software/bash/manual/html_node/Bash-Variables.html (last checked: 2019-09-26) 155# reported in https://github.com/target/lorri/issues/153 156\u0026#34;OLDPWD=\u0026#34;*) punt;; 157\u0026#34;PWD=\u0026#34;*) punt;; 158\u0026#34;SHELL=\u0026#34;*) punt;; 159160# https://github.com/target/lorri/issues/97 161\u0026#34;preHook=\u0026#34;*) punt;; 162\u0026#34;origPreHook=\u0026#34;*) move \u0026#34;origPreHook\u0026#34; \u0026#34;preHook\u0026#34; \u0026#34;$@\u0026#34;;; 163164*) varmap \u0026#34;$@\u0026#34; ;; 165esac 166} 167168export IN_NIX_SHELL=impure 169170if [ -f \u0026#34;$EVALUATION_ROOT/bash-export\u0026#34; ]; then 171# shellcheck disable=SC1090 172. \u0026#34;$EVALUATION_ROOT/bash-export\u0026#34; 173elif [ -f \u0026#34;$EVALUATION_ROOT\u0026#34; ]; then 174# shellcheck disable=SC1090 175. \u0026#34;$EVALUATION_ROOT\u0026#34; 176fi 177178unset declare 179180Jun 06 19:02:32.368 INFO lorri has not completed an evaluation for this project yet, expr: $HOME/Git/Github/WebDev/Mine/haozeke.github.io/content-org/tryRnix/shell.nix 181Jun 06 19:02:32.368 WARN `lorri direnv` should be executed by direnv from within an `.envrc` file, expr: $HOME/Git/Github/WebDev/Mine/haozeke.github.io/content-org/tryRnix/shell.nix Upon inspection, that seems to check out. So now we can enable this.\n1direnv allow Additionally, we will need to stick to using a pure environment as much as possible to prevent unexpected situations. So we set:\n1# .envrc 2eval \u0026#34;$(lorri direnv)\u0026#34; 3nix-shell --run bash --pure There\u0026rsquo;s still a catch though. We need to have lorri daemon running to make sure the packages are built automatically without us having to exit the shell and re-run things. We can turn to the documentation for this. Essentially, we need to have a user-level systemd socket file and service for lorri.\n1# ~/.config/systemd/user/lorri.socket 2[Unit] 3Description=Socket for Lorri Daemon 45[Socket] 6ListenStream=%t/lorri/daemon.socket 7RuntimeDirectory=lorri 89[Install] 10WantedBy=sockets.target 1# ~/.config/systemd/user/lorri.service 2[Unit] 3Description=Lorri Daemon 4Requires=lorri.socket 5After=lorri.socket 67[Service] 8ExecStart=%h/.nix-profile/bin/lorri daemon 9PrivateTmp=true 10ProtectSystem=strict 11ProtectHome=read-only 12Restart=on-failure With that we are finally ready to start working with our auto-managed, reproducible environments.\n1systemctl --user daemon-reload \u0026amp;\u0026amp; \\ 2systemctl --user enable --now lorri.socket Rethinking As promised, we will first test the setup to see that everything is working. Now is also a good time to try the tidybayes.rethinking package. In order to use it, we will need to define the rethinking package in a way so we can pass it to the buildInputs for tidybayes.rethinking. We will modify new shell.nix as follows:\n1# shell.nix 2let 3sources = import ./nix/sources.nix; 4pkgs = import sources.nixpkgs { }; 5stdenv = pkgs.stdenv; 6rethinking = with pkgs.rPackages; 7buildRPackage { 8name = \u0026#34;rethinking\u0026#34;; 9src = pkgs.fetchFromGitHub { 10owner = \u0026#34;rmcelreath\u0026#34;; 11repo = \u0026#34;rethinking\u0026#34;; 12rev = \u0026#34;d0978c7f8b6329b94efa2014658d750ae12b1fa2\u0026#34;; 13sha256 = \u0026#34;1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0\u0026#34;; 14}; 15propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ]; 16}; 17tidybayes_rethinking = with pkgs.rPackages; 18buildRPackage { 19name = \u0026#34;tidybayes.rethinking\u0026#34;; 20src = pkgs.fetchFromGitHub { 21owner = \u0026#34;mjskay\u0026#34;; 22repo = \u0026#34;tidybayes.rethinking\u0026#34;; 23rev = \u0026#34;df903c88f4f4320795a47c616eef24a690b433a4\u0026#34;; 24sha256 = \u0026#34;1jl3189zdddmwm07z1mk58hcahirqrwx211ms0i1rzbx5y4zak0c\u0026#34;; 25}; 26propagatedBuildInputs = 27[ dplyr tibble rlang MASS tidybayes rethinking rstan ]; 28}; 29rEnv = pkgs.rWrapper.override { 30packages = with pkgs.rPackages; [ 31ggplot2 32tidyverse 33tidybayes 34devtools 35modelr 36cowplot 37ggrepel 38RColorBrewer 39purrr 40forcats 41rstan 42rethinking 43tidybayes_rethinking 44]; 45}; 46in pkgs.mkShell { 47buildInputs = with pkgs; [ git glibcLocales which ]; 48inputsFrom = [ rEnv ]; 49shellHook = \u0026#39;\u0026#39; 50mkdir -p \u0026#34;$(pwd)/_libs\u0026#34; 51export R_LIBS_USER=\u0026#34;$(pwd)/_libs\u0026#34; 52\u0026#39;\u0026#39;; 53GIT_SSL_CAINFO = \u0026#34;${pkgs.cacert}/etc/ssl/certs/ca-bundle.crt\u0026#34;; 54LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux 55\u0026#34;${pkgs.glibcLocales}/lib/locale/locale-archive\u0026#34;; 56} The main thing to note here is that we need the output of the derivation we create here, i.e. we need to use inputsFrom and NOT buildInputs for rEnv.\nLet us try to get a nice graphic for the conclusion.\n1library(magrittr) 2library(dplyr) 3library(purrr) 4library(forcats) 5library(tidyr) 6library(modelr) 7library(tidybayes) 8library(tidybayes.rethinking) 9library(ggplot2) 10library(cowplot) 11library(rstan) 12library(rethinking) 13library(ggrepel) 14library(RColorBrewer) 1516theme_set(theme_tidybayes()) 17rstan_options(auto_write = TRUE) 18options(mc.cores = parallel::detectCores()) 192021set.seed(5) 22n = 10 23n_condition = 5 24ABC = 25tibble( 26condition = factor(rep(c(\u0026#34;A\u0026#34;,\u0026#34;B\u0026#34;,\u0026#34;C\u0026#34;,\u0026#34;D\u0026#34;,\u0026#34;E\u0026#34;), n)), 27response = rnorm(n * 5, c(0,1,2,1,-1), 0.5) 28) 2930mtcars_clean = mtcars %\u0026gt;% 31mutate(cyl = factor(cyl)) 3233m_cyl = ulam(alist( 34cyl ~ dordlogit(phi, cutpoint), 35phi \u0026lt;- b_mpg*mpg, 36b_mpg ~ student_t(3, 0, 10), 37cutpoint ~ student_t(3, 0, 10) 38), 39data = mtcars_clean, 40chains = 4, 41cores = parallel::detectCores(), 42iter = 2000 43) 4445cutpoints = m_cyl %\u0026gt;% 46recover_types(mtcars_clean) %\u0026gt;% 47spread_draws(cutpoint[cyl]) 4849# define the last cutpoint 50last_cutpoint = tibble( 51.draw = 1:max(cutpoints$.draw), 52cyl = \u0026#34;8\u0026#34;, 53cutpoint = Inf 54) 5556cutpoints = bind_rows(cutpoints, last_cutpoint) %\u0026gt;% 57# define the previous cutpoint (cutpoint_{j-1}) 58group_by(.draw) %\u0026gt;% 59arrange(cyl) %\u0026gt;% 60mutate(prev_cutpoint = lag(cutpoint, default = -Inf)) 6162fitted_cyl_probs = mtcars_clean %\u0026gt;% 63data_grid(mpg = seq_range(mpg, n = 101)) %\u0026gt;% 64add_fitted_draws(m_cyl) %\u0026gt;% 65inner_join(cutpoints, by = \u0026#34;.draw\u0026#34;) %\u0026gt;% 66mutate(`P(cyl | mpg)` = 67# this part is logit^-1(cutpoint_j - beta*x) - logit^-1(cutpoint_{j-1} - beta*x) 68plogis(cutpoint - .value) - plogis(prev_cutpoint - .value) 69) 707172data_plot = mtcars_clean %\u0026gt;% 73ggplot(aes(x = mpg, y = cyl, color = cyl)) + 74geom_point() + 75scale_color_brewer(palette = \u0026#34;Dark2\u0026#34;, name = \u0026#34;cyl\u0026#34;) 7677fit_plot = fitted_cyl_probs %\u0026gt;% 78ggplot(aes(x = mpg, y = `P(cyl | mpg)`, color = cyl)) + 79stat_lineribbon(aes(fill = cyl), alpha = 1/5) + 80scale_color_brewer(palette = \u0026#34;Dark2\u0026#34;) + 81scale_fill_brewer(palette = \u0026#34;Dark2\u0026#34;) 8283png(filename=\u0026#34;../images/rethinking.png\u0026#34;) 84plot_grid(ncol = 1, align = \u0026#34;v\u0026#34;, 85data_plot, 86fit_plot 87) 88dev.off Finally we will run this in our environment.\n1Rscript tesPlot.R  Conclusions This post was really more of an exploratory follow up to the previous post, and does not really work in isolation. Then again, at this point everything seems to have worked out well. R with Nix has finally become a truly viable combination for any and every analysis under the sun. Some parts of the workflow are still a bit janky, but will probably resolve themselves over time.\nUpdate: There is a final part detailing automated ways of reloading the system configuration\n  My motivations were laid out in the aforementioned post, and will not be repeated\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n For why these are the way they are see the this is written, see the aforementioned post\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Christine Dodrill has a great write up on using these tools as well\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/rethinking-r-nix/","tags":["tools","nix","workflow","R"],"title":"Statistical Rethinking and Nix"},{"categories":["programming"],"contents":" Setup details are described here, and the meta-post about these solutions is here.\n Materials The summmer course1 is based off of the second edition of Statistical Rethinking by Richard McElreath. This post covers the following exercise questions:\n Chapter 2  Easy {1,2,3,4} Medium {1,2,4}   Chapter 3  Easy {1,2,3,4,5} Medium {1,2,3,4,6}   Chapter 4  Easy {1,2,3,4,5} Medium {1,2,3,4,5,6,7}    Packages 1libsUsed\u0026lt;-c(\u0026#34;tidyverse\u0026#34;,\u0026#34;tidybayes\u0026#34;,\u0026#34;orgutils\u0026#34;, 2\u0026#34;rethinking\u0026#34;,\u0026#34;tidybayes.rethinking\u0026#34;, 3\u0026#34;ggplot2\u0026#34;,\u0026#34;kableExtra\u0026#34;,\u0026#34;dplyr\u0026#34;,\u0026#34;glue\u0026#34;, 4\u0026#34;latex2exp\u0026#34;,\u0026#34;data.table\u0026#34;,\u0026#34;printr\u0026#34;) 5invisible(lapply(libsUsed, library, character.only = TRUE)); We also set the following theme parameters for the plots.\n1theme_set(theme_grey(base_size=24)) Chapter II: The Golem of Prague Easy Questions (Ch2) 2E1 Which of the expressions below correspond to the statement: /the probability of rain on Monday?\n Pr(rain) Pr(rain|Monday) Pr(Monday|rain) Pr(rain, Monday)/Pr(Monday)  Solution We can read each of these sentences as follows:\n Pr(rain) Probability of rain Pr(rain|Monday) Probability of rain given that it is Monday, or probability that it rains on Monday Pr(Monday|rain) Probability of being a Monday given that it rains Pr(rain,Monday)/Pr(Monday) Compound statement  We further note that we can express the joint probability of 4. to be equivalent to the probability written in 2.\nHence the correct solutions are options 2 and 4.\n2E2 Which of the following statements corresponds to the expression: Pr(Monday|rain)?\n The probability of rain on Monday. The probability of rain, given that it is Monday. The probability that it is Monday, given that it is raining. The probability that it is Monday and that it is raining.  Solution Using the same logic as described previously we note that the sentences correspond to the following formulations:\n Pr(rain,Monday) The probability of rain on Monday. Pr(rain|Monday) The probability of rain, given that it is Monday. Pr(Monday|rain) The probability that it is Monday, given that it is raining. Pr(Monday,rain) The probability that it is Monday and that it is raining. Hence only option 3 is correct.\n  2E3 Which of the expressions below correspond to the statement: the probability that it is Monday, given that it is raining?\n Pr(Monday|rain) Pr(rain|Monday) Pr(rain|Monday)Pr(Monday) Pr(rain|Monday)Pr(Monday)/Pr(rain) Pr(Monday|rain)Pr(rain)/Pr(Monday)  Solution We will simplify these slightly.\nHence the correct solutions are options 1 and 4.\n2E4 The Bayesian statistician Bruno de Finetti (1906\u0026ndash;1985) began his 1973 book on probability theory with the declaration: \u0026ldquo;PROBABILITY DOES NOT EXIST.\u0026rdquo; The capitals appeared in the original, so I imagine de Finetti wanted us to shout this statement. What he meant is that probability is a device for describing uncertainty from the perspective of an observer with limited knowledge; it has no objective reality. Discuss the globe tossing example from the chapter, in light of this statement. What does it mean to say \u0026ldquo;the probability of water is 0.7\u0026rdquo;?\nSolution The de Finetti school of thought subscribed to the belief that all processes were deterministic and therefore did not have any inherent probabilitic interpretation. With this framework, uncertainty did not have any physical realization, and so the random effects of a process could be accounted for entirely in terms of aleatoric and epistemic uncertainity without needing to consider the fact that in some processes (not discovered then) like quantum mechanics, random effects are part of the physical system, and are not due to a lack of information. Under this assumption, the entirity of probability is simply an numerical artifact with which the lack of information about a process could be expressed. Thus the statement \u0026ldquo;the probability of water is 0.7\u0026rdquo; would mean that the observable is 0.7, i.e., it would express the partial knowledge of the observer, and not have any bearing on the (presumably fully deterministic) underlying process which is a (presumed) exact function of angular momentum, and other exact classical properties.\nQuestions of Medium Complexity (Ch2) 2M1 Recall the globe tossing model from the chapter. Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for \\(p\\).\n W, W, W W, W, W, L L, W, W, L, W, W, W  Solution 1nPoints\u0026lt;-50 2# Grid 3pGrid\u0026lt;-seq(0,1,length.out=nPoints) 4# Prior 5prior\u0026lt;-rep(1,nPoints) 6# Likelihood for each grid point 7likelihood\u0026lt;-pGrid %\u0026gt;% dbinom(3,3,prob=.) 8noStdPosterior\u0026lt;-likelihood*prior 9# Posterior 10posterior\u0026lt;- noStdPosterior / sum(noStdPosterior) Now we can visualize this.\n1tibble(pGrid,posterior) %\u0026gt;% ggplot(aes(x=pGrid,y=posterior))+ 2geom_line(size=3)+geom_point(size=5,color=\u0026#34;red\u0026#34;)+ 3labs( 4title=\u0026#34;Globe Tosses\u0026#34;, 5subtitle=\u0026#34;2M1.1)W W W\u0026#34;, 6y=\u0026#34;Posterior\u0026#34;, 7x=\u0026#34;Grid Approximation points\u0026#34; 8)+ 9scale_x_continuous(breaks = seq(0,1,length.out=10), 10labels = seq(0,1,length.out=10) %\u0026gt;% sprintf(\u0026#34;%.2f\u0026#34;,.), 11expand = c(0, 0))+ 12theme( 13plot.title.position = \u0026#34;plot\u0026#34;, 14)  For the remaining parts we will use a more abbreviated solution.\n1tibble(pGrid, 2li2=pGrid %\u0026gt;% dbinom(3,4,prob=.), 3prior) %\u0026gt;% mutate(post2unstd=li2*prior) %\u0026gt;% mutate(posterior=post2unstd/sum(post2unstd)) %\u0026gt;% ggplot(aes(x=pGrid,y=posterior))+ 4geom_line(size=3)+geom_point(size=5,color=\u0026#34;red\u0026#34;)+ 5labs( 6title=\u0026#34;Globe Tosses\u0026#34;, 7subtitle=\u0026#34;2M1.2) W W W L\u0026#34;, 8y=\u0026#34;Posterior\u0026#34;, 9x=\u0026#34;Grid Approximation points\u0026#34; 10)+ 11scale_x_continuous(breaks = seq(0,1,length.out=10), 12labels = seq(0,1,length.out=10) %\u0026gt;% sprintf(\u0026#34;%.2f\u0026#34;,.), 13expand = c(0, 0))+ 14theme( 15plot.title.position = \u0026#34;plot\u0026#34; 16)  For the final part, note that since the observations are independent, the ordering is irrelevant.\n1tibble(pGrid, 2li2=pGrid %\u0026gt;% dbinom(5,7,prob=.), 3prior) %\u0026gt;% mutate(post2unstd=li2*prior) %\u0026gt;% mutate(posterior=post2unstd/sum(post2unstd)) %\u0026gt;% ggplot(aes(x=pGrid,y=posterior))+ 4geom_line(size=3)+geom_point(size=5,color=\u0026#34;red\u0026#34;)+ 5labs( 6title=\u0026#34;Globe Tosses\u0026#34;, 7subtitle=\u0026#34;2M1.2) L W W L W W W\u0026#34;, 8y=\u0026#34;Posterior\u0026#34;, 9x=\u0026#34;Grid Approximation points\u0026#34; 10)+ 11scale_x_continuous(breaks = seq(0,1,length.out=10), 12labels = seq(0,1,length.out=10) %\u0026gt;% sprintf(\u0026#34;%.2f\u0026#34;,.), 13expand = c(0, 0))+ 14theme( 15plot.title.position = \u0026#34;plot\u0026#34; 16)  2M2 Now assume a prior for \\(p\\) that is equal to zero when \\(p \u0026lt; 0.5\\) and is a positive constant when \\(p\\geq 0.5\\). Again compute and plot the grid approximate posterior distribution for each of the sets of observations in the problem just above.\n W, W, W W, W, W, L L, W, W, L, W, W, W  Solution We proceed in much the same way as in the previous question. We also use the vectorized ifelse instead of explicitly using a for loop.\n1nPoints\u0026lt;-50 2## Grid 3pGrid\u0026lt;-seq(0,1,length.out=nPoints) 4## Prior 5prior\u0026lt;-ifelse(pGrid\u0026lt;0.5,0,1) 1tibble(pGrid, 2li2=pGrid %\u0026gt;% dbinom(3,3,prob=.), 3prior) %\u0026gt;% mutate(post2unstd=li2*prior) %\u0026gt;% mutate(posterior=post2unstd/sum(post2unstd)) %\u0026gt;% ggplot(aes(x=pGrid,y=posterior))+ 4geom_line(size=3)+geom_point(size=5,color=\u0026#34;red\u0026#34;)+ 5labs( 6title=\u0026#34;Globe Tosses with Prior information\u0026#34;, 7subtitle=\u0026#34;2M2.1) W W W\u0026#34;, 8y=\u0026#34;Posterior\u0026#34;, 9x=\u0026#34;Grid Approximation points\u0026#34; 10)+ 11scale_x_continuous(breaks = seq(0,1,length.out=10), 12labels = seq(0,1,length.out=10) %\u0026gt;% sprintf(\u0026#34;%.2f\u0026#34;,.), 13expand = c(0, 0))+ 14theme( 15plot.title.position = \u0026#34;plot\u0026#34; 16)  1tibble(pGrid, 2li2=pGrid %\u0026gt;% dbinom(3,4,prob=.), 3prior) %\u0026gt;% mutate(post2unstd=li2*prior) %\u0026gt;% mutate(posterior=post2unstd/sum(post2unstd)) %\u0026gt;% ggplot(aes(x=pGrid,y=posterior))+ 4geom_line(size=3)+geom_point(size=5,color=\u0026#34;red\u0026#34;)+ 5labs( 6title=\u0026#34;Globe Tosses with Prior information\u0026#34;, 7subtitle=\u0026#34;2M2.2) W W W L\u0026#34;, 8y=\u0026#34;Posterior\u0026#34;, 9x=\u0026#34;Grid Approximation points\u0026#34; 10)+ 11scale_x_continuous(breaks = seq(0,1,length.out=10), 12labels = seq(0,1,length.out=10) %\u0026gt;% sprintf(\u0026#34;%.2f\u0026#34;,.), 13expand = c(0, 0))+ 14theme( 15plot.title.position = \u0026#34;plot\u0026#34; 16)  1tibble(pGrid, 2li2=pGrid %\u0026gt;% dbinom(5,7,prob=.), 3prior) %\u0026gt;% mutate(post2unstd=li2*prior) %\u0026gt;% mutate(posterior=post2unstd/sum(post2unstd)) %\u0026gt;% ggplot(aes(x=pGrid,y=posterior))+ 4geom_line(size=3)+geom_point(size=5,color=\u0026#34;red\u0026#34;)+ 5labs( 6title=\u0026#34;Globe Tosses with Prior information\u0026#34;, 7subtitle=\u0026#34;2M2.3) L W W L W W W\u0026#34;, 8y=\u0026#34;Posterior\u0026#34;, 9x=\u0026#34;Grid Approximation points\u0026#34; 10)+ 11scale_x_continuous(breaks = seq(0,1,length.out=10), 12labels = seq(0,1,length.out=10) %\u0026gt;% sprintf(\u0026#34;%.2f\u0026#34;,.), 13expand = c(0, 0))+ 14theme( 15plot.title.position = \u0026#34;plot\u0026#34; 16)  2M4 Suppose you have a deck with only three cards. Each card has two sides, and each side is either black or white. One card has two black sides. The second card has one black and one white side. The third card has two white sides. Now suppose all three cards are placed in a bag and shuffled. Someone reaches into the bag and pulls out a card and places it flat on a table. A black side is shown facing up, but you don\u0026rsquo;t know the color of the side facing down. Show that the probability that the other side is also black is 2/3. Use the counting method (Section 2 of this chapter) to approach this problem. This means counting up the ways that each card could produce the observed data (a black side facing up on the table).\nSolution Let us begin by defining what is given to us.\n There are three cards  One has a black side and a white side (cBW) One is colored black on both sides (cBB) One is colored white on both sides (cWW)    Our total probability universe is defined by all possible states defined by the enumeration of possible states for each card and their combinations. In other words, it is a universe defined by color and the number of cards.\nThe question posed is essentially, given a single observation, that is that a random draw from our universe has produced a black side (note that we already know that there is one card so it satisfies the requirements of being a valid state of the universe we are considering), what is the probability of the other side being black as well?\nHence we can express this as O: cB? and we need P(B|cB?). We will enumerate possibilities of observing the black side in our universe.\n cBW This has \\(1\\) way of producing cB? cBB This has \\(2\\) ways of producing cB? cWW This has \\(0\\) ways of producing cB?     Card Ways to cB?     cBW 1   cBB 2   cWW 0    So we see that there are \\(3\\) ways to see a black side in a single draw, and two of these come from (cBB), thus the probability of seeing another black side is \\(\\frac{2}{3}\\).\nChapter III: Sampling the Imaginary Easy Questions (Ch3) These questions are associated with the following code snippet for the globe tossing example.\n1p_grid\u0026lt;-seq(from=0, to=1, length.out=1000) 2prior\u0026lt;-rep(1,1000) 3likelihood\u0026lt;-dbinom(6,size=9,prob=p_grid) 4posterior\u0026lt;-likelihood*prior 5posterior\u0026lt;-posterior/sum(posterior) 6set.seed(100) 7samples\u0026lt;-sample(p_grid,prob=posterior,size=1e4,replace=TRUE) 3E1 How much posterior probability lies below \\(p = 0.2\\)?\nSolution We can check the number of samples as follows:\n1ifelse(samples\u0026lt;0.2,1,0) %\u0026gt;% sum(.) 1[1] 4 Now we will simply divide by the number of samples.\n1ifelse(samples\u0026lt;0.2,1,0) %\u0026gt;% sum(.)/1e4 1[1] 4e-04 More practically, the percentage of the probability density below \\(p=0.2\\) is:\n1ifelse(samples\u0026lt;0.2,1,0) %\u0026gt;% sum(.)/1e4 *100 1[1] 0.04 3E2 How much posterior probability lies above \\(p = 0.8\\)?\nSolution 1ifelse(samples\u0026gt;0.8,1,0) %\u0026gt;% sum(.)/1e4 *100 1[1] 11.16 3E3 How much posterior probability lies between \\(p = 0.2\\) and \\(p = 0.8\\)?\nSolution 1ifelse(samples\u0026gt;0.2 \u0026amp; samples\u0026lt;0.8,1,0) %\u0026gt;% sum(.)/1e4 *100 1[1] 88.8 3E4 20% of the posterior probability lies below which value of \\(p\\)?\nSolution 1samples %\u0026gt;% quantile(0.2) 120% 20.5185185 3E5 20% of the posterior probability lies above which value of \\(p\\)?\nSolution 1samples %\u0026gt;% quantile(0.8) 180% 20.7557558 Questions of Medium Complexity (Ch3) 3M1 Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.\nSolution 1nPoints\u0026lt;-1000 2## Grid 3pGrid\u0026lt;-seq(0,1,length.out=nPoints) 4## Prior 5prior\u0026lt;-rep(1,nPoints) 1tibble(pGrid, 2li2=pGrid %\u0026gt;% dbinom(8,15,prob=.), 3prior) %\u0026gt;% mutate(post2unstd=li2*prior) %\u0026gt;% mutate(posterior=post2unstd/sum(post2unstd)) %\u0026gt;% ggplot(aes(x=pGrid,y=posterior))+ 4geom_line(size=3)+geom_point(size=5,color=\u0026#34;red\u0026#34;)+ 5labs( 6title=\u0026#34;Globe Tosses with a Flat Prior\u0026#34;, 7subtitle=\u0026#34;3M1) 8W in 15 tosses\u0026#34;, 8y=\u0026#34;Posterior\u0026#34;, 9x=\u0026#34;Grid Approximation points\u0026#34; 10)+ 11scale_x_continuous(breaks = seq(0,1,length.out=10), 12labels = seq(0,1,length.out=10) %\u0026gt;% sprintf(\u0026#34;%.2f\u0026#34;,.), 13expand = c(0, 0))+ 14theme( 15plot.title.position = \u0026#34;plot\u0026#34; 16)  3M2 Draw 10,000 samples from the grid approximation from above. Then use the samples to calculate the 90% HPDI for \\(p\\).\nSolution For this, we will create a tibble of the experiment and then sample from it.\n1exp3m2\u0026lt;-tibble(pGrid, 2li2=pGrid %\u0026gt;% dbinom(8,15,prob=.), 3prior) %\u0026gt;% mutate(post2unstd=li2*prior) %\u0026gt;% mutate(posterior=post2unstd/sum(post2unstd)) 45sample(pGrid,prob=exp3m2$posterior,size=1e4,replace=TRUE) %\u0026gt;% HPDI(prob=0.9) 12|0.9 0.9| 30.3293293 0.7167167 3M3 Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in \\(p\\). What is the probability of observing 8 water in 15 tosses.\nSolution As covered in the chapter, we will sample from the posterior distribution and use that to obtain experiment instances with rbinom. Finally we will then use these instances to count our way to the probability of observing the true data.\n1waterPred\u0026lt;-sample(pGrid,prob=exp3m2$posterior,size=1e4,replace=TRUE) %\u0026gt;% rbinom(1e4,size=15,prob=.) 2ifelse(waterPred==8,1,0) %\u0026gt;% sum(.)/1e4 1[1] 0.1447 This seems like a very poor model, but the spread of the realizations is probably a better measure.\n1waterPred %\u0026gt;% summary 1Min. 1st Qu. Median Mean 3rd Qu. Max. 20.000 6.000 8.000 7.936 10.000 15.000 This does seem to indicate that at any rate the realizations are close enough to 8 for the model to be viable. A visualization of this should also be checked.\n1waterPred %\u0026gt;% simplehist  3M4 Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses\nSolution We will reuse the posterior, and recalculate our samples (corresponding to new possible instances).\n1waterPred\u0026lt;-sample(pGrid,prob=exp3m2$posterior,size=1e4,replace=TRUE) %\u0026gt;% rbinom(1e4,size=9,prob=.) 1ifelse(waterPred==6,1,0) %\u0026gt;% sum(.)/1e4 2waterPred %\u0026gt;% summary 1[1] 0.1723 23Min. 1st Qu. Median Mean 3rd Qu. Max. 40.000 4.000 5.000 4.777 6.000 9.000 Though the numerical value is not very different from the previous solution, we note that the centrality measures are much worse.\n1waterPred %\u0026gt;% simplehist  3M6 Suppose you want to estimate the Earth\u0026rsquo;s proportion of water very precisely. Specifically, you want the 99% percentile interval of the posterior distribution of p to be only 0.05 wide. This means the distance between the upper and lower bound of the interval should be 0.05. How many times will you have to toss the globe to do this?\nSolution Since the no information of the model has been provided here, we will consider reason out an approach, before making an estimate.\nThe number globe tosses corresponds to the number of observations we require. A percentile interval this narrow will essentially require a large number of samples i.e. be in the large number limit, so the choice of prior should not matter much. The width of our interval should also be related to the number of grid points we use in our approximation.\nWe will first use the true amount of water on the planet (approximately 71 percent) to generate information for the number of throws.\nLet us generate observations.\n1nThrows\u0026lt;-10000 2nWater\u0026lt;-0.71*nThrows %\u0026gt;% round We will set up a simple model for this, with an indifferent prior along with a better prior.\n1nPoints\u0026lt;-1000 2## Grid 3pGrid\u0026lt;-seq(0,1,length.out=nPoints) 4## Prior 5prior\u0026lt;-rep(1,nPoints) 6betterPrior\u0026lt;-ifelse(pGrid\u0026lt;0.5,0,1) We will define a function for generating our results since we will need to perform this a few times.\n1genModel\u0026lt;- function(nThrows){ 2nWater\u0026lt;-0.71*nThrows %\u0026gt;% round 3tibble(pGrid, 4li2=pGrid %\u0026gt;% dbinom(nWater,nThrows,prob=.), 5prior,betterPrior) %\u0026gt;% 6mutate( 7postUnifNSD=li2*prior, 8postUnif=postUnifNSD/sum(postUnifNSD), 9smplUnif=sample(pGrid, 10prob=postUnif, 11size=nrow(.), 12replace=TRUE), 13predUnif=rbinom(nrow(.),size=9,prob=smplUnif), 14postBPNSD=li2*betterPrior, 15postBP=postBPNSD/sum(postBPNSD), 16smplBP=sample(pGrid, 17prob=postBP, 18size=nrow(.), 19replace=TRUE), 20predBP=rbinom(nrow(.),size=9,prob=smplBP) 21) 22} It would be nice to look at the different priors as well.\n1genModel2(1e4) %\u0026gt;% .$predBP %\u0026gt;% simplehist  1genModel(1e6) %\u0026gt;% .$predUnif %\u0026gt;% simplehist  As can be expected, with the large number of observations, the priors barely make a difference.\nWe note that what we are looking for is a credibility width of less than 0.05 at a probability of 0.99.\n1get99Width\u0026lt;- function(x,nx) { 2piInter\u0026lt;-PI(x$postUnif,prob=0.99) 3hpdiInter\u0026lt;-HPDI(x$postUnif,prob=0.99) 4tibble(wPI=piInter[2]-piInter[1],wHPDI=hpdiInter[2]-hpdiInter[1],nSamples=nx) 5} Now we are in a position to start testing samples.\n1base100\u0026lt;-genModel(1e2) %\u0026gt;% get99Width(1e2) 2t\u0026lt;-1000 3while(min(base100$wPI) \u0026amp; min(base100$wHPDI) \u0026gt; 0.005) { 4t\u0026lt;-t*10 5base100\u0026lt;-genModel(t) %\u0026gt;% get99Width(t) %\u0026gt;% bind_rows(base100) 6} 7base100 %\u0026gt;% toOrg 12| wPI | wHPDI | nSamples | 3|---------------------+---------------------+----------| 4| 0.0464554168997065 | 0.00121611594937107 | 1e+05 | 5| 0.0735809517149284 | 0.0511200696528017 | 10000 | 6| 0.00884431494702241 | 0.0088121623437198 | 100 | There is an inherent problem with this formulation, and that is that the model confidence is based on the observations which were drawn to emulate the true distribution of water on Earth. This means that a highly tight width, should be recognized to still be highly dependent on the observed data. Let us try to obtain the same intervals with a randomized observation setup instead.\nIn order to do this we simply modify the number of water observations.\n1genModel2\u0026lt;- function(nThrows){ 2tibble(pGrid, 3li2=pGrid %\u0026gt;% dbinom(sample(1:nThrows, 1, replace=TRUE),nThrows,prob=.), 4prior,betterPrior) %\u0026gt;% 5mutate( 6postUnifNSD=li2*prior, 7postUnif=postUnifNSD/sum(postUnifNSD), 8smplUnif=sample(pGrid, 9prob=postUnif, 10size=nrow(.), 11replace=TRUE), 12predUnif=rbinom(nrow(.),size=9,prob=smplUnif), 13postBPNSD=li2*betterPrior, 14postBP=postBPNSD/sum(postBPNSD), 15smplBP=sample(pGrid, 16prob=postBP, 17size=nrow(.), 18replace=TRUE), 19predBP=rbinom(nrow(.),size=9,prob=smplBP) 20) 21} We can now test these the same way.\n1base100\u0026lt;-genModel2(100) %\u0026gt;% get99Width(100) 2t\u0026lt;-100 3while(min(base100$wPI) \u0026amp; min(base100$wHPDI) \u0026gt; 0.05) { 4t\u0026lt;-t+100 5base100\u0026lt;-genModel2(t) %\u0026gt;% get99Width(t) %\u0026gt;% bind_rows(base100) 6} 7base100 %\u0026gt;% toOrg 12| wPI | wHPDI | nSamples | 3|---------------------+---------------------+----------| 4| 0.00853874459759265 | 0.00851020493831213 | 100 | Chapter IV: Geocentric Models Easy Questions (Ch4) 4E1 In the model definition below, which line is the likelihood?\nSolution The likelihood is defined by the first line, that is, \\(y_i \\sim\\mathrm{Normal}(\\mu, \\sigma)\\)\n4E2 In the model definition above, how many parameters are in the posterior distribution?\nSolution The model has two parameters for the posterior distribution, \\(\\mu\\) and \\(\\sigma\\).\n4E3 Using the model definition above, write down the appropriate form of Bayes\u0026rsquo; theorem that includes the proper likelihood and priors.\nSolution The appropriate form of Bayes\u0026rsquo; theorem in this case is:\n\\[ \\mathrm(Pr)(\\mu,\\sigma|y)=\\frac{\\mathrm{Normal}(y|\\mu,\\sigma)\\mathrm{Normal}(\\mu|0,10)\\mathrm{Exponential}(\\sigma|1)}{\\int\\int \\mathrm{Normal}(y|\\mu,\\sigma)\\mathrm{Normal}(\\mu|0,10)\\mathrm{Exponential}(\\sigma|1)d\\mu d\\sigma} \\]\n4E4 In the model definition below, which line is the linear model?\nSolution The second line is the linear model in the definition, that is: \\[\\mu_i = \\alpha + \\beta x_i\\]\n4E5 In the model definition just above, how many parameters are in the posterior distribution?\nSolution The model defined has three independent parameters for the posterior distribution, which are \\(\\alpha\\), \\(\\beta\\) and \\(\\sigma\\). Though \\(\\mu\\) is a parameter, it is defined in terms of \\(\\alpha\\), \\(\\beta\\) and \\(x\\) so will not be considered to be a parameter for the posterior.\nQuestions of Medium Complexity (Ch4) 4M1 For the model definition below, simulate observed \\(y\\) values from the prior (not the posterior).\nSolution Sampling from the prior involves averaging over the prior distributions of \\(\\mu\\) and \\(\\sigma\\).\n1muPrior\u0026lt;-rnorm(1e4,0,10) 2sigmaPrior\u0026lt;-rexp(1e4,1) 3hSim\u0026lt;-rnorm(1e4,muPrior,sigmaPrior) We can visualize this as well.\n1hSim %\u0026gt;% qplot(binwidth=0.8)  4M2 Translate the model just above into a quap formula.\nSolution 1ex4m2\u0026lt;-alist( 2y~dnorm(mu,sigma), 3mu~dnorm(0,10), 4sigma~dexp(1) 5) 4M3 Translate the quap model formula below into a mathematical model definition\nSolution The model defined can be expressed mathematically as:\n4M4 A sample of students is measured for height each year for 3y ears. After the third year,you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors.\nSolution Let us first declare a model.\nWhere the double subscript is meant to indicate that the data is obtained both per year and per student. That is, we have:\n \\(y_{ij}\\) is the height of each student each year \\(x_{j}\\) is the \u0026ldquo;reduced\u0026rdquo; year, that is the difference between a particular year and the average sample year  The prior distributions are not too complicated. Since the distribution of males and females in the student population is missing, as is information on the age distribution, we will set a conservative lower value of 120 centimeters, based on the understanding that the students have at-least 3 years in school, so they are growing, so we assume students grow around 8 centimeters every year.\nThe distribution for beta is a log-normal distribution to ensure that we do not sample negative values which might arise from sampling a Gaussian. This is due to the assumption that the students grow every year.\nFollowing the procedures of the chapter, we will visualize realizations from the prior.\n1nDraws\u0026lt;-50 2a\u0026lt;-rnorm(nDraws,100,8) 3b\u0026lt;-rlnorm(nDraws,0,2) 4sigmaPrior\u0026lt;-rexp(nDraws,1) We will express our yearly function:\n1genYr\u0026lt;- function(x,yr) {rnorm(1,a[x]+b[x]*(yr-1.5),sigmaPrior[x])} Now we will assume a class size of 35 for our priors.\n1testDat\u0026lt;-tibble(student=1:36) 2testDat$year1\u0026lt;-sapply(testDat$student,FUN=genYr,yr=1) 3testDat$year2\u0026lt;-sapply(testDat$student,FUN=genYr,yr=2) 4testDat$year3\u0026lt;-sapply(testDat$student,FUN=genYr,yr=3) We will now look at this.\n1testDat %\u0026gt;% pivot_longer(-student,names_to = \u0026#34;year\u0026#34;,values_to = \u0026#34;height\u0026#34;) %\u0026gt;% ggplot(aes(x=student,y=height,color=year))+ 2geom_line(size=4,alpha=0.4)+ 3geom_point(size=4,colour=\u0026#34;blue\u0026#34;)+ 4labs( 5title=\u0026#34;Student Height variation\u0026#34;, 6subtitle=\u0026#34;By year\u0026#34;, 7y=\u0026#34;Height\u0026#34;, 8x=\u0026#34;Student\u0026#34; 9)+ 10theme( 11plot.title.position = \u0026#34;plot\u0026#34;, 12)  1testDat %\u0026gt;% pivot_longer(-student,names_to = \u0026#34;year\u0026#34;,values_to = \u0026#34;height\u0026#34;) %\u0026gt;% ggplot(aes(x=year,y=height,color=student))+ 2geom_line(size=4,alpha=0.4)+ 3geom_point(size=4,colour=\u0026#34;blue\u0026#34;)+ 4labs( 5title=\u0026#34;Student Height variation\u0026#34;, 6subtitle=\u0026#34;By year\u0026#34;, 7y=\u0026#34;Height\u0026#34;, 8x=\u0026#34;Student\u0026#34; 9)+ 10theme( 11plot.title.position = \u0026#34;plot\u0026#34;, 12)  This seems to be a pretty reasonable model, all things considered.\n4M5 Now suppose I remind you that every student got taller each year. Does this information lead you to change your choice of priors? How?\nSolution Since we have incorporated a LogNormal term, we do not need to change our choice of prior. However, the parameters of our previous model did include some heights decreasing, so we will modify the LogNormal distribution on beta. Recall that we would like to see around 7 centimeters of growth, so our mean should be exp(1+2/2) which is around 7.\nSimulating draws as before.\n1nDraws\u0026lt;-50 2a\u0026lt;-rnorm(nDraws,100,8) 3b\u0026lt;-rlnorm(nDraws,1,2) 4sigmaPrior\u0026lt;-rexp(nDraws,1) 5genYr\u0026lt;- function(x,yr) {rnorm(1,a[x]+b[x]*(yr-1.5),sigmaPrior[x])} 6testDat\u0026lt;-tibble(student=1:36) 7testDat$year1\u0026lt;-sapply(testDat$student,FUN=genYr,yr=1) 8testDat$year2\u0026lt;-sapply(testDat$student,FUN=genYr,yr=2) 9testDat$year3\u0026lt;-sapply(testDat$student,FUN=genYr,yr=3) 1testDat %\u0026gt;% pivot_longer(-student,names_to = \u0026#34;year\u0026#34;,values_to = \u0026#34;height\u0026#34;) %\u0026gt;% ggplot(aes(x=student,y=height,color=year))+ 2geom_line(size=4,alpha=0.4)+ 3geom_point(size=4,colour=\u0026#34;blue\u0026#34;)+ 4labs( 5title=\u0026#34;Student Height variation\u0026#34;, 6subtitle=\u0026#34;By year with better priors\u0026#34;, 7y=\u0026#34;Height\u0026#34;, 8x=\u0026#34;Student\u0026#34; 9)+ 10theme( 11plot.title.position = \u0026#34;plot\u0026#34;, 12)  4M6 Now suppose I tell you that the variance among heights for students of the same age is never more than 64cm. How does this lead you to revise your priors?\nSolution This information will change the variance term in our model. We will incorporate this into our model by using a uniform distribution for sigma. The new model is then:\nSimulating draws as before.\n1nDraws\u0026lt;-50 2a\u0026lt;-rnorm(nDraws,100,8) 3b\u0026lt;-rlnorm(nDraws,1,2) 4sigmaPrior\u0026lt;-runif(nDraws,0,8) 5genYr\u0026lt;- function(x,yr) {rnorm(1,a[x]+b[x]*(yr-1.5),sigmaPrior[x])} 6testDat\u0026lt;-tibble(student=1:36) 7testDat$year1\u0026lt;-sapply(testDat$student,FUN=genYr,yr=1) 8testDat$year2\u0026lt;-sapply(testDat$student,FUN=genYr,yr=2) 9testDat$year3\u0026lt;-sapply(testDat$student,FUN=genYr,yr=3) We will now look at this.\n1testDat %\u0026gt;% pivot_longer(-student,names_to = \u0026#34;year\u0026#34;,values_to = \u0026#34;height\u0026#34;) %\u0026gt;% ggplot(aes(x=year,y=height,color=student))+ 2geom_line(size=4,alpha=0.4)+ 3geom_point(size=4,colour=\u0026#34;blue\u0026#34;)+ 4labs( 5title=\u0026#34;Student Height variation\u0026#34;, 6subtitle=\u0026#34;By year with additional information\u0026#34;, 7y=\u0026#34;Height\u0026#34;, 8x=\u0026#34;Student\u0026#34; 9)+ 10theme( 11plot.title.position = \u0026#34;plot\u0026#34;, 12)  1testDat %\u0026gt;% pivot_longer(-student,names_to = \u0026#34;year\u0026#34;,values_to = \u0026#34;height\u0026#34;) %\u0026gt;% mutate(year=year %\u0026gt;% as.factor %\u0026gt;% as.numeric) %\u0026gt;% filter(student==3) %\u0026gt;% ggplot(aes(x=year,y=height,color=student))+ 2geom_line(size=4,alpha=0.4)+ 3geom_point(size=4,colour=\u0026#34;blue\u0026#34;)+ 4labs( 5title=\u0026#34;Student Height variation with additional information\u0026#34;, 6subtitle=\u0026#34;By year\u0026#34;, 7y=\u0026#34;Height\u0026#34;, 8x=\u0026#34;year\u0026#34; 9)+ 10theme( 11plot.title.position = \u0026#34;plot\u0026#34;, 12) This is now much more reasonable.\n4M7 Refit model m4.3 from the chapter, but omit the mean weight xbar this time. Compare the new model\u0026rsquo;s posterior to that of the original model. In particular, look at the covariance among the parameters. What is different? Then compare the posterior predictions of both models.\nSolution Let us re-create the original data model first.\n1data(Howell1) 2howDat\u0026lt;-Howell1 3howDat\u0026lt;-howDat %\u0026gt;% filter(age\u0026gt;=18) Recall that the original model was given by:\n1xbar\u0026lt;-mean(howDat$weight) 2m43\u0026lt;-quap( 3alist( 4height ~ dnorm(mu,sigma), 5mu\u0026lt;-a+b*(weight-xbar), 6a ~ dnorm(178,20), 7b ~ dlnorm(0,1), 8sigma ~ dunif(0,50) 9),data=howDat 10) 11m43 %\u0026gt;% precis %\u0026gt;% toOrg    row.names mean sd 5.5% 94.5%     a 154.601366268055 0.270307670586024 154.169362403256 155.033370132855   b 0.90328084058895 0.041923632631821 0.83627877851613 0.970282902661771   sigma 5.07188106954248 0.191154797986941 4.76637878273641 5.37738335634854    Now we can incorporate the new information we have.\n1m43b\u0026lt;-quap( 2alist( 3height ~ dnorm(mu,sigma), 4mu\u0026lt;-a+b*weight, 5a ~ dnorm(178,20), 6b ~ dlnorm(0,1), 7sigma ~ dunif(0,50) 8),data=howDat 9) 10m43b %\u0026gt;% precis %\u0026gt;% toOrg    row.names mean sd 5.5% 94.5%     a 114.515009160638 1.89397291870405 111.488074634765 117.54194368651   b 0.891112682994587 0.0416750085549524 0.824507970215838 0.957717395773337   sigma 5.06263423083996 0.190299406826185 4.75849902431897 5.36676943736095    Thus we note that the slope is the same, while the intercept changes. Now we need to check the covariances. We will convert from the variable covariance scale to the correlation matrix.\n1m43b %\u0026gt;% vcov %\u0026gt;% cov2cor %\u0026gt;% round(.,2) 1a b sigma 2a 1.00 -0.99 0.02 3b -0.99 1.00 -0.02 4sigma 0.02 -0.02 1.00 1m43 %\u0026gt;% vcov %\u0026gt;% cov2cor %\u0026gt;% round(.,2) 1a b sigma 2a 1 0 0 3b 0 1 0 4sigma 0 0 1 The contrast between the two is very clear from the correlation matrices, the new model has an almost perfect negatively correlated intercept and slope.\n1m43 %\u0026gt;% pairs  1m43b %\u0026gt;% pairs  This indicates that the scaling of the variables leads to a completely different model in terms of the parameter relationships, in-spite of the fact that the models have almost the same posterior predictions.\n  Summer of 2020\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/sr2-ch2-ch3-ch4/","tags":["solutions","R","SR2"],"title":"SR2 :: Solutions for Chapters {2,3,4}"},{"categories":["programming"],"contents":" This post discusses briefly, the nix-shell environment for reproducible programming. In particular, there is an emphasis on extensions for installing and working with packages not in CRAN, i.e. packages off Github which are normally installed with devtools.\n Background The entire nix ecosystem is fantastic, and is the main packaging system used by d-SEAMS as well. Recently I began working through the excellent second edition of \u0026ldquo;Statistical Rethinking\u0026rdquo; by Richard McElreath1.\nUnfortunately, the rethinking package which is a major component of the book itself depends on the V8 engine for some reason. The reigning AUR2 package (V8-r) broke with a fun error message I couldn\u0026rsquo;t be bothered to deal with. Ominously, the rest of the logs prominently featured Warning: Running gclient on Python 3.. Given that older python versions have been permanently retired, this seemed like a bad thing to deal with3. In any case, having weaned off non-nix dependency tools for python and friends, it seemed strange to not do the same for R.\nThe standard installation for the package entails obtaining rstan (which is trivial with nixpkgs) and then using:\n1install.packages(c(\u0026#34;coda\u0026#34;,\u0026#34;mvtnorm\u0026#34;,\u0026#34;devtools\u0026#34;,\u0026#34;loo\u0026#34;,\u0026#34;dagitty\u0026#34;)) 2library(devtools) 3devtools::install_github(\u0026#34;rmcelreath/rethinking\u0026#34;) We will break this down and work through this installation in Nix space.\nNix and R The standard approach to setting up a project shell.nix is simply by using the mkshell function. There are some common aspects to this workflow, with more language specific details documented here. A simple first version might be:\n1let 2pkgs = import \u0026lt;nixpkgs\u0026gt; { }; 3in pkgs.mkShell { 4buildInputs = with pkgs; [ 5zsh 6R 7rPackages.ggplot 8rPackages.data_table 9]; 10shellHook = \u0026#39;\u0026#39; 11echo \u0026#34;hello\u0026#34; 12\u0026#39;\u0026#39;; 13LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux 14\u0026#34;${glibcLocales}/lib/locale/locale-archive\u0026#34;; 15} Where we note that we can install CRAN packages as easily as regular packages (like R), except for the fact that they are kept in a pkgs.rPackages environment, as opposed to pkgs. This is actually a common convention most languages with central repos. The most interesting thing to note is that, similar to the convention for nix-python setups, packages with a dot in the name will be converted to having an underscore, i.e. data.table -\u0026gt; data_table.\nHowever, for the rethinking package, and many others, there is no current CRAN package, and so the rPackages approach fails.\nThe LOCALE_ARCHIVE needs to be set for Linux machines, and is required for working with other packages.\nNix-R and Devtools To work with non-CRAN packages, we need to modify our package setup a little. We will also simplify our file to split the pkgs and the r-pkgs.\nNaive Approach The naive approach works by using the shellHook to set R_LIBS_USER to save user packages per-project.\n1{ pkgs ? import \u0026lt;nixpkgs\u0026gt; { } }: 2with pkgs; 3let 4my-r-pkgs = rWrapper.override { 5packages = with rPackages; [ 6ggplot2 7knitr 8rstan 9tidyverse 10V8 11dagitty 12coda 13mvtnorm 14shape 15Rcpp 16tidybayes 17]; 18}; 19in mkShell { 20buildInputs = = with pkgs;[ git glibcLocales openssl openssh curl wget ]; 21inputsFrom = [ my-r-pkgs ]; 22shellHook = \u0026#39;\u0026#39; 23mkdir -p \u0026#34;$(pwd)/_libs\u0026#34; 24export R_LIBS_USER=\u0026#34;$(pwd)/_libs\u0026#34; 25\u0026#39;\u0026#39;; 26GIT_SSL_CAINFO = \u0026#34;${cacert}/etc/ssl/certs/ca-bundle.crt\u0026#34;; 27LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux 28\u0026#34;${glibcLocales}/lib/locale/locale-archive\u0026#34;; 29} Note that here we will also need to set the GIT_SSL_CAINFO to prevent some errors during the build process4.\nNative Approach The native approach essentially leverages the nix method for building R packages. This is the most reproducible of the lot, and also has the useful property of storing the files in the nix-store so re-using packages across different projects will not store, build or download the package again. The values required can be calculated from nix-prefetch-git as follows:\n1nix-env -i nix-prefetch-git 2nix-prefetch-git https://github.com/rmcelreath/rethinking.git The crux of this approach is the following snippet5:\n1(buildRPackage { 2name = \u0026#34;rethinking\u0026#34;; 3src = fetchFromGitHub { 4owner = \u0026#34;rmcelreath\u0026#34;; 5repo = \u0026#34;rethinking\u0026#34;; 6rev = \u0026#34;d0978c7f8b6329b94efa2014658d750ae12b1fa2\u0026#34;; 7sha256 = \u0026#34;1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0\u0026#34;; 8}; 9propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ]; 10}) Project Shell This formulation for some strange reason does not work from the shell or environment by default, but does work with nix-shell --run bash --pure.\n1{ pkgs ? import \u0026lt;nixpkgs\u0026gt; { } }: 2with pkgs; 3let 4my-r-pkgs = rWrapper.override { 5packages = with rPackages; [ 6ggplot2 7knitr 8rstan 9tidyverse 10V8 11dagitty 12coda 13mvtnorm 14shape 15Rcpp 16tidybayes 17(buildRPackage { 18name = \u0026#34;rethinking\u0026#34;; 19src = fetchFromGitHub { 20owner = \u0026#34;rmcelreath\u0026#34;; 21repo = \u0026#34;rethinking\u0026#34;; 22rev = \u0026#34;d0978c7f8b6329b94efa2014658d750ae12b1fa2\u0026#34;; 23sha256 = \u0026#34;1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0\u0026#34;; 24}; 25propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ]; 26}) 27]; 28}; 29in mkShell { 30buildInputs = with pkgs; [ git glibcLocales openssl which openssh curl wget my-r-pkgs ]; 31shellHook = \u0026#39;\u0026#39; 32mkdir -p \u0026#34;$(pwd)/_libs\u0026#34; 33export R_LIBS_USER=\u0026#34;$(pwd)/_libs\u0026#34; 34echo ${my-r-pkgs}/bin/R 35\u0026#39;\u0026#39;; 36GIT_SSL_CAINFO = \u0026#34;${cacert}/etc/ssl/certs/ca-bundle.crt\u0026#34;; 37LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux 38\u0026#34;${glibcLocales}/lib/locale/locale-archive\u0026#34;; 39} The reason behind this is simply that rWrapper forms an extra package which has lower precedence than the user profile R, which is documented in more detail here on the NixOS wiki.\nUser Profile This is a more general approach which defines the environment for R with all the relevant libraries and is described in the nixpkgs manual. The following code should be placed in $HOME/.config/nixpkgs/config.nix:\n1{ 2packageOverrides = super: 3let self = super.pkgs; 4in { 5rEnv = super.rWrapper.override { 6packages = with self.rPackages; [ 7ggplot2 8knitr 9tidyverse 10tidybayes 11(buildRPackage { 12name = \u0026#34;rethinking\u0026#34;; 13src = self.fetchFromGitHub { 14owner = \u0026#34;rmcelreath\u0026#34;; 15repo = \u0026#34;rethinking\u0026#34;; 16rev = \u0026#34;d0978c7f8b6329b94efa2014658d750ae12b1fa2\u0026#34;; 17sha256 = \u0026#34;1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0\u0026#34;; 18}; 19propagatedBuildInputs = 20[ coda MASS mvtnorm loo shape rstan dagitty ]; 21}) 22]; 23}; 24}; 25} This snippet allows us to use our R as follows:\n1# Install things 2nix-env -f \u0026#34;\u0026lt;nixpkgs\u0026gt;\u0026#34; -iA rEnv 3# Fix locale 4export LOCALE_ARCHIVE=\u0026#34;$(nix-build --no-out-link \u0026#34;\u0026lt;nixpkgs\u0026gt;\u0026#34; -A glibcLocales)/lib/locale/locale-archive\u0026#34; 5# Profit 6R Note that in this method, on Linux systems, the locale problem has to be fixed with the explicit export. This means that this should be used mostly with project level environments, instead of populating the global shell RC files.\nUpdate: There is another post with methods to reload this configuration automatically\nConclusions Of the methods described, the most useful method for working with packages not hosted on CRAN is through the user-profile, while the shell.nix method is useful in conjunction, for managing various projects. So the ideal approach is then to use the user profile for installing anything which normally uses devtools and then use shell.nix for the rest.\nNote that if the Project Shell is used with a User Profile as described in the next section, all packages defined there can be dropped and then the project shell does not need to execute R by default. The simplified shell.nix is then simply:\n1{ pkgs ? import \u0026lt;nixpkgs\u0026gt; { } }: 2with pkgs; 3let 4my-r-pkgs = rWrapper.override { 5packages = with rPackages; [ 6ggplot2 7]; 8}; 9in mkShell { 10buildInputs = with pkgs;[ git glibcLocales openssl which openssh curl wget my-r-pkgs ]; 11inputsFrom = [ my-r-pkgs ]; 12shellHook = \u0026#39;\u0026#39; 13mkdir -p \u0026#34;$(pwd)/_libs\u0026#34; 14export R_LIBS_USER=\u0026#34;$(pwd)/_libs\u0026#34; 15\u0026#39;\u0026#39;; 16GIT_SSL_CAINFO = \u0026#34;${cacert}/etc/ssl/certs/ca-bundle.crt\u0026#34;; 17LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux 18\u0026#34;${glibcLocales}/lib/locale/locale-archive\u0026#34;; 19} The entire workflow for rethinking is continued here.\n  As part of a summer course at the University of Iceland relating to their successful COVID-19 model\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The Arch User Repository is the port of first call for most ArchLinux users\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Though, like any good AUR user, I did post a bug report\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This approach is also discussed here\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n As discussed on this issue, this stackoverflow question and also seen here\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/nix-r-devtools/","tags":["tools","nix","workflow","R"],"title":"Nix with R and devtools"},{"categories":["notes"],"contents":"Background I recently had the opportunity to take part in an AMA (ask me anything) session for the CS106A students on Machine Learning for the Physical Sciences. This is a post about the technical issues, and also includes a video if you read through.\nZoom and LosslessCut Zoom recordings are one of the nicer ways to deal with switching windows and screen sharing, especially after fixing the dark screen glitch. However, though LosslessCut works really well to get cut-points, exporting and merging the file into one caused a bunch of glitches.\nEnter Handbrake To not beat around the bush, the solution was to simply encode the Zoom recording with Handbrake before using LosslessCut1. Since the conversion takes a while, it is also neat to note that you can directly export the cut points made with LosslessCut on the original video, then import them onto the newly encoded file.\nConclusions I am not really sure how this will turn out, but it is a useful thing to keep in mind. The introductory video turned out to be:\n{{\u0026lt; youtube aOuqgyHHOK4 \u0026gt;}}\n  For me, the Vimeo Youtube HQ 1080p60 preset worked out well\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/losslesscut-zoom-ama/","tags":["teaching","cs106a","tools"],"title":"LosslessCut, Zoom and an AMA for CS106A"},{"categories":["notes"],"contents":"Background I have been leading the fantastic section 881 as a virtual section leader for the Stanford CS106A: Code in Place initiative for the past four weeks. I have also spent a lot of time on Zoom, sharing my screen. Fun fact. My screen shares look like this:\n Figure 1: Zoom screen share with weird overlay\n  This post is about hunting down what caused this amazing zoom glitch1 and how I finally fixed it.\nTiling Windows and Compositors For reasons best left to another post, I use the fabulous i3 window manager, with colemak keybindings described here. Recall that, from Wikipedia:\n A compositing window manager is a window manager that provides applications with an off-screen buffer for each window. The window manager composites the window buffers into an image representing the screen and writes the result into the display memory.\n For reasons I can no longer recall, compton has been a traditional aspect of my workflow. As per my last update back in April last year; my configuration is here.\nCompton to Picom Some time ago (actually many months ago), compton itself transitioned over to picom, but remained largely compatible with my old configuration2. To be clear, the transition was largely painless, with ample warnings in the terminal showing up; along with very reasonable fallbacks. The key aspect of my compton.conf which caused the shadowing was:\n1shadow = true; 2shadow-radius = 5; 3shadow-offset-x = -5; 4shadow-offset-y = -5; 5shadow-opacity = 0.5; The corrective measure was simply to set shadow-opacity to nothing; that is:\n1shadow-opacity = 0.0; The rest of the configuration is here; and contains a lot more, mostly pertaining to opacity and other pretty effects3.\nConclusion Finally we have achieved the goal of having normal screen sharing capabilities; as seen below:\n Figure 2: Just in time to see an excellent pun\n  The struggle was real, though the cause was trivial, and really highlights the need to always know your system packages. In this case, no doubt my students would have preferred not having to suffer through the darkness of my screen4. This has been a rather trivial post, but one to keep in mind none-the-less.\nComments The older commenting system was implemented with utteranc.es as seen below.\n   To be clear, none of the windows were the glitch. The issue was the darkened overlay\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n As always, the ArchLinux Wiki is a great place for more information\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The rest of my Dotfiles, managed by the excellent dotgit are also worth a look\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Though it might have also served as a metaphor for darkness\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/compton-zoom-shadow/","tags":["workflow","tools"],"title":"Compton to Picom and Zoom Glitches"},{"categories":["programming"],"contents":"Background One of the main reasons to use orgmode is definitely to get a better note taking workflow. Closely related to blogging or writing, the ideal note workflow is one which lets you keep a bunch of throwaway ideas and also somehow have access to them in a coherent manner. This will be a long post, and it is a work-in-progress, so, keep that in mind. Since this is mainly me1 work-shopping my technique, the philosophy will come in a later post probably. This workflow is documented more sparsely in my config file here, in the noteYoda section2. Some parts of this post also include mini video clips for clarity3.\nThe entire workflow will end up being something like this4:\n{{\u0026lt; youtube UWB6ZABRVq0 \u0026gt;}}\nConcept While working through ideas, it actually was more useful to describe the workflow I want, and then implement it, instead of relying on the canned approaches of each package. So the basics of the ideology are listed below.\nReference Management Reference management is one of the main reasons to consider a plain-text setup, and mine is no different. The options most commonly seen are:\n Mendeley This is a great option, and the most mobile friendly of the bunch. Sadly, the price tiers aren\u0026rsquo;t very friendly so I have to give it a hard pass. Jabref This is fun, but really more of a per-project management system, but it works well for that. The fact that it is Java based was a major issue for me. Zotero This is what I personally use and recommend. More on that in a later post.  Notes The idea is to be able to create notes for all kinds of content. Specifically, papers or books, along with webpages. This then requires a separate system for each which is described by:\n Search Engine The search engine is key, both in terms of accessibility and scalability. It is assumed that there will be many notes, and that they will have a wide variety of content. The search interface must then simply allow us to narrow down our candidates in a meaningful manner. Contextual Representation This aspect of the workflow deals with representations, which should transcend the usage of tags or categories. In particular, it would be nice to be able to visualize the flow of ideas, each represented by a note. Backlinks In particular, by backlinks at this point we are referring to the ability to link to a pdf or a website with a unique key such that notes can be added or removed at will. Storage Not actually part of the workflow in the same way, since it will be handled at the system level, it is worth nothing, that in this workflow Zotero is used to export a master bib file and keeps it updated, while the notes themselves are version controlled5.  The concepts above will be handled by the following packages.\n   Concept Package Note     Search deft Has a great interface   Context org-roam Allows the export of graphiz mindmaps   Backlinks org-roam, org-ref, org-noter Covers websites, bibliographies, and pdfs respectively    A key component in this workflow is actually facilitated by the fabulous org-roam-bibtex or ORB. The basic idea is to ensure meaningful templates which interpolate smoothly with org-roam, org-ref, helm-bibtex, and org-capture.\nBasic Variables Given the packages we will be using, some variable settings are in order, namely:\n1(setq 2org_notes (concat (getenv \u0026#34;HOME\u0026#34;) \u0026#34;/Git/Gitlab/Mine/Notes/\u0026#34;) 3zot_bib (concat (getenv \u0026#34;HOME\u0026#34;) \u0026#34;/GDrive/zotLib.bib\u0026#34;) 4org-directory org_notes 5deft-directory org_notes 6org-roam-directory org_notes 7) Search For the search setup, the doom-emacs deft setup, by adding +deft in my init.el, worked out of the box for me. For those who do not use doom6, the following should suffice:\n1(use-package deft 2:commands deft 3:init 4(setq deft-default-extension \u0026#34;org\u0026#34; 5;; de-couples filename and note title: 6deft-use-filename-as-title nil 7deft-use-filter-string-for-filename t 8;; disable auto-save 9deft-auto-save-interval -1.0 10;; converts the filter string into a readable file-name using kebab-case: 11deft-file-naming-rules 12\u0026#39;((noslash . \u0026#34;-\u0026#34;) 13(nospace . \u0026#34;-\u0026#34;) 14(case-fn . downcase))) 15:config 16(add-to-list \u0026#39;deft-extensions \u0026#34;tex\u0026#34;) 17) For more about the doom-emacs defaults, check the Github repo. The other aspect of interacting with the notes is via the org-roam interface and will be covered below.\nBibliography Since I will be using org-ref, it makes no sense to load or work with the +biblio module at the moment. Thus this section is actually doom agnostic. The basic tools of bibliographic management from the emacs end are the venerable helm-bibtex (repo here) and org-ref (repo here). In order to make this guide complete, I will also describe the Zotero settings I have.\nZotero Without getting too deep into the weeds here, the basic requirements are:\n Zotero The better bibtex extension  The idea is to then have one top level .bib file in some handy location which you will set up to sync automatically. To make life easier, there is a tiny recording of the next steps.\n{{\u0026lt; youtube iDRpIo7mcKE \u0026gt;}}\nHelm-Bibtex This venerable package is really good at interfacing with a variety of externally formatted bibliographic managers.\n1(setq 2bibtex-completion-notes-path \u0026#34;/home/haozeke/Git/Gitlab/Mine/Notes/\u0026#34; 3bibtex-completion-bibliography \u0026#34;/home/haozeke/GDrive/zotLib.bib\u0026#34; 4bibtex-completion-pdf-field \u0026#34;file\u0026#34; 5bibtex-completion-notes-template-multiple-files 6(concat 7\u0026#34;#+TITLE: ${title}\\n\u0026#34; 8\u0026#34;#+ROAM_KEY: cite:${=key=}\\n\u0026#34; 9\u0026#34;* TODO Notes\\n\u0026#34; 10\u0026#34;:PROPERTIES:\\n\u0026#34; 11\u0026#34;:Custom_ID: ${=key=}\\n\u0026#34; 12\u0026#34;:NOTER_DOCUMENT: %(orb-process-file-field \\\u0026#34;${=key=}\\\u0026#34;)\\n\u0026#34; 13\u0026#34;:AUTHOR: ${author-abbrev}\\n\u0026#34; 14\u0026#34;:JOURNAL: ${journaltitle}\\n\u0026#34; 15\u0026#34;:DATE: ${date}\\n\u0026#34; 16\u0026#34;:YEAR: ${year}\\n\u0026#34; 17\u0026#34;:DOI: ${doi}\\n\u0026#34; 18\u0026#34;:URL: ${url}\\n\u0026#34; 19\u0026#34;:END:\\n\\n\u0026#34; 20) 21) doom-emacs users like me might want to wrap the above in a nice after! org-ref expression, but it doesn\u0026rsquo;t really matter.\nExplanation To break-down aspects of the configuration snippet above:\n The template includes the orb-process-file-field function to allow selecting the pdf to be used with org-noter The file field is specified to work with the .bib file generated by Zotero helm-bibtex allows for any of the keys in a .bib file to be used in a template, and an overly expressive one is more useful The ROAM_KEY is defined to ensure that cite backlinks work correctly with org-roam As I prefer to have one notes file per pdf, I have only configured the bibtex-completion-notes-template-multiple-files variable  Org-Ref As discussed above, this just makes citations much more meaningful in orgmode.\n1(use-package org-ref 2:config 3(setq 4org-ref-completion-library \u0026#39;org-ref-ivy-cite 5org-ref-get-pdf-filename-function \u0026#39;org-ref-get-pdf-filename-helm-bibtex 6org-ref-default-bibliography (list \u0026#34;/home/haozeke/GDrive/zotLib.bib\u0026#34;) 7org-ref-bibliography-notes \u0026#34;/home/haozeke/Git/Gitlab/Mine/Notes/bibnotes.org\u0026#34; 8org-ref-note-title-format \u0026#34;* TODO %y - %t\\n :PROPERTIES:\\n :Custom_ID: %k\\n :NOTER_DOCUMENT: %F\\n :ROAM_KEY: cite:%k\\n :AUTHOR: %9a\\n :JOURNAL: %j\\n :YEAR: %y\\n :VOLUME: %v\\n :PAGES: %p\\n :DOI: %D\\n :URL: %U\\n :END:\\n\\n\u0026#34; 9org-ref-notes-directory \u0026#34;/home/haozeke/Git/Gitlab/Mine/Notes/\u0026#34; 10org-ref-notes-function \u0026#39;orb-edit-notes 11)) An essential aspect of this configuration is just that most of heavy lifting in terms of the notes are palmed off to helm-bibtex.\nExplanation To break-down aspects of the configuration snippet above:\n The org-ref-get-pdf-filename-function simply uses the helm-bibtex settings to find the pdf The default bibliography and notes directory are set to the same location as all the org-roam files, to encourage a flat hierarchy The org-ref-notes-function simply ensures that, like the helm-bibtex settings, I expect one file per pdf, and that I would like to use my org-roam template instead of the org-ref or helm-bibtex one  Note that for some reason, the format specifiers for org-ref are not the keys in .bib but are instead, the following7:\n1In the format, the following percent escapes will be expanded. 2%l The BibTeX label of the citation. 3%a List of author names, see also `reftex-cite-punctuation\u0026#39;. 4%2a Like %a, but abbreviate more than 2 authors like Jones et al. 5%A First author name only. 6%e Works like %a, but on list of editor names. (%2e and %E work as well) 7It is also possible to access all other BibTeX database fields: 8%b booktitle %c chapter %d edition %h howpublished 9%i institution %j journal %k key %m month 10%n number %o organization %p pages %P first page 11%r address %s school %u publisher %t title 12%v volume %y year 13%B booktitle, abbreviated %T title, abbreviated 14%U url 15%D doi 16%S series %N note 17%f pdf filename 18%F absolute pdf filename 19Usually, only %l is needed. The other stuff is mainly for the echo area 20display, and for (setq reftex-comment-citations t). 21%\u0026lt; as a special operator kills punctuation and space around it after the 22string has been formatted. 23A pair of square brackets indicates an optional argument, and RefTeX 24will prompt for the values of these arguments. Indexing Notes This part of the workflow builds on the concepts best known as the Zettelkasten method. More details about the philosophy behind org-roam is here.\nOrg-Roam The first part of this interface is essentially just the doom-emacs configuration, adapted for those who don\u0026rsquo;t believe in the dark side below.\n1(use-package org-roam 2:hook (org-load . org-roam-mode) 3:commands (org-roam-buffer-toggle-display 4org-roam-find-file 5org-roam-graph 6org-roam-insert 7org-roam-switch-to-buffer 8org-roam-dailies-date 9org-roam-dailies-today 10org-roam-dailies-tomorrow 11org-roam-dailies-yesterday) 12:preface 13;; Set this to nil so we can later detect whether the user has set a custom 14;; directory for it, and default to `org-directory\u0026#39; if they haven\u0026#39;t. 15(defvar org-roam-directory nil) 16:init 17:config 18(setq org-roam-directory (expand-file-name (or org-roam-directory \u0026#34;roam\u0026#34;) 19org-directory) 20org-roam-verbose nil ; https://youtu.be/fn4jIlFwuLU 21org-roam-buffer-no-delete-other-windows t ; make org-roam buffer sticky 22org-roam-completion-system \u0026#39;default 23) 2425;; Normally, the org-roam buffer doesn\u0026#39;t open until you explicitly call 26;; `org-roam\u0026#39;. If `+org-roam-open-buffer-on-find-file\u0026#39; is non-nil, the 27;; org-roam buffer will be opened for you when you use `org-roam-find-file\u0026#39; 28;; (but not `find-file\u0026#39;, to limit the scope of this behavior). 29(add-hook \u0026#39;find-file-hook 30(defun +org-roam-open-buffer-maybe-h () 31(and +org-roam-open-buffer-on-find-file 32(memq \u0026#39;org-roam-buffer--update-maybe post-command-hook) 33(not (window-parameter nil \u0026#39;window-side)) ; don\u0026#39;t proc for popups 34(not (eq \u0026#39;visible (org-roam-buffer--visibility))) 35(with-current-buffer (window-buffer) 36(org-roam-buffer--get-create))))) 3738;; Hide the mode line in the org-roam buffer, since it serves no purpose. This 39;; makes it easier to distinguish among other org buffers. 40(add-hook \u0026#39;org-roam-buffer-prepare-hook #\u0026#39;hide-mode-line-mode)) 414243;; Since the org module lazy loads org-protocol (waits until an org URL is 44;; detected), we can safely chain `org-roam-protocol\u0026#39; to it. 45(use-package org-roam-protocol 46:after org-protocol) 474849(use-package company-org-roam 50:after org-roam 51:config 52(set-company-backend! \u0026#39;org-mode \u0026#39;(company-org-roam company-yasnippet company-dabbrev))) Once again, for more details, check the Github repo.\nOrg-Roam-Bibtex The configuration required is:\n1(use-package org-roam-bibtex 2:after (org-roam) 3:hook (org-roam-mode . org-roam-bibtex-mode) 4:config 5(setq org-roam-bibtex-preformat-keywords 6\u0026#39;(\u0026#34;=key=\u0026#34; \u0026#34;title\u0026#34; \u0026#34;url\u0026#34; \u0026#34;file\u0026#34; \u0026#34;author-or-editor\u0026#34; \u0026#34;keywords\u0026#34;)) 7(setq orb-templates 8\u0026#39;((\u0026#34;r\u0026#34; \u0026#34;ref\u0026#34; plain (function org-roam-capture--get-point) 9\u0026#34;\u0026#34; 10:file-name \u0026#34;${slug}\u0026#34; 11:head \u0026#34;#+TITLE: ${=key=}: ${title}\\n#+ROAM_KEY: ${ref} 1213- tags :: 14- keywords :: ${keywords} 1516\\n* ${title}\\n :PROPERTIES:\\n :Custom_ID: ${=key=}\\n :URL: ${url}\\n :AUTHOR: ${author-or-editor}\\n :NOTER_DOCUMENT: %(orb-process-file-field \\\u0026#34;${=key=}\\\u0026#34;)\\n :NOTER_PAGE: \\n :END:\\n\\n\u0026#34; 1718:unnarrowed t)))) Where most of the configuration is essentially the template again. Like helm-bibtex, ORB allows taking arbitrary keys from the .bib file.\nOrg Noter The final aspect of a pdf workflow is simply ensuring that every pdf is associated with notes. The philosophy of org-noter is best described here. Only minor tweaks should be required to get this working with interleave as well.\n1(use-package org-noter 2:after (:any org pdf-view) 3:config 4(setq 5;; The WM can handle splits 6org-noter-notes-window-location \u0026#39;other-frame 7;; Please stop opening frames 8org-noter-always-create-frame nil 9;; I want to see the whole file 10org-noter-hide-other nil 11;; Everything is relative to the main notes file 12org-noter-notes-search-path (list org_notes) 13) 14) Evidently, from my configuration, it appears that I decided to use org-noter over the more commonly described interleave because it has better support for working with multiple documents linked to one file.\nOrg-Protocol I will only cover the bare minimum relating to the use of org-capture here, because eventually I intend to handle a lot more cases with orca. Note that this part of the workflow has more to do with using org-roam with websites than pdf files.\nTemplates This might get complicated but I am only trying to get the bare minimum for org-protocol right now.\n1;; Actually start using templates 2(after! org-capture 3;; Firefox and Chrome 4(add-to-list \u0026#39;org-capture-templates 5\u0026#39;(\u0026#34;P\u0026#34; \u0026#34;Protocol\u0026#34; entry ; key, name, type 6(file+headline +org-capture-notes-file \u0026#34;Inbox\u0026#34;) ; target 7\u0026#34;* %^{Title}\\nSource: %u, %c\\n #+BEGIN_QUOTE\\n%i\\n#+END_QUOTE\\n\\n\\n%?\u0026#34; 8:prepend t ; properties 9:kill-buffer t)) 10(add-to-list \u0026#39;org-capture-templates 11\u0026#39;(\u0026#34;L\u0026#34; \u0026#34;Protocol Link\u0026#34; entry 12(file+headline +org-capture-notes-file \u0026#34;Inbox\u0026#34;) 13\u0026#34;* %? [[%:link][%(transform-square-brackets-to-round-ones \\\u0026#34;%:description\\\u0026#34;)]]\\n\u0026#34; 14:prepend t 15:kill-buffer t)) 16) Conclusions At this point, many might argue that since by the end, only one template is called, defining the rest were pointless. They would be right, however, this is just how my configuration evolved. Feel free to cannibalize this for your personal benefit. Eventually I plan to expand this into something with org-journal as well, but not right now.\nComments The older commenting system was implemented with utteranc.es as seen below.\n   Rohit Goswami that is, from the landing page; obviously\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This is a reference to my fantastic pet, named Yoda\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Recorded with SimpleScreenRecorder, cut with LosslessCut, uploaded to YouTube, and embedded with a Hugo shortcode\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The video uses org-ref-notes-function-many-files as the org-ref-notes-function so the template looks a little different\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n For some strange reason a lot of online posts suggested Dropbox for syncing notes, which makes no sense to me, it is always better to have version control and ignore rules\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Therefore clearly proving that the cookies of the dark side have no power in the holy text editor war\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Where these are from the org-ref documentation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/org-note-workflow/","tags":["tools","emacs","workflow","orgmode"],"title":"An Orgmode Note Workflow"},{"categories":["programming"],"contents":" A more actionable follow up to my personal recollections relating to my switch to Colemak.\n Background I have, in the past written about how I made the switch to Colemak. However, until recently, I was still trying to mimic the VIM keybindings from QWERTY. This is a post where I discuss the changes I made to ensure that I never have to stretch my fingers in odd ways again. The main idea is expressed well by vim-colemak.\n1Colemak layout: | QWERTY layout: 2`12345 67890-= Move around: | (instead of) `12345 67890-= 3qwfpg jluy;[]\\  e | k qwert yuiop[]\\ 4 arstd HNEIo\u0026#39; h i | h l asdfg HJKL;\u0026#39; 5zxcvb km,./ n | j zxcvb nm,./ Sudoers It is important to note that the sudo command does not automatically pick up on your keyboard layout. It is best to set this explicitly. Use visudo and un-comment Defaults env_keep += \u0026quot;LANG LANGUAGE LINGUAS LC_* _XKB_CHARSET\u0026quot;, or:\n1su 2echo \u0026#39;Defaults env_keep += \u0026#34;LANG LANGUAGE LINGUAS LC_* _XKB_CHARSET\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/sudoers Emacs Though I have mentioned publicly, that I was using the regular QWERTY motion keys, I realized I had actually started to use the mouse more often, simply because it was a pain to navigate. Thankfully, emacs has evil-colemak-basics, which is fabulous. For reference, these make it really easy for QWERTY users to make the switch if they\u0026rsquo;re previously used to VIM bindings.\n   Colemak Qwerty Action States At Qwerty position? Remarks     h, n, e, i h, j, k, l navigate mnvo yes    k, K n, N search next/previous mnvo yes    u, U i, I insert _nv_ yes    l u undo _nv_ yes    N J join lines _nv_ yes    E K lookup mnv_ yes    u i inner text object keymap ___o yes    f, F e, E jump to end of word mnvo yes with t-f-j rotation   t, T f, f jump to character mnvo yes with t-f-j rotation   j, J t, T jump until character mnvo no with t-f-j rotation   j, J e, E jump to end of word mnvo no without t-f-j rotation    Where the table above is from the fantastic readme.\nI still had some issues, mostly relating to searching in buffers, so I ended using swiper-isearch more which is a bonus too.\nVisual Lines Since I tend to keep visual-line-mode all the time, it makes sense to actually swap working with lines and visual lines. To work this through this needs evil-better-visual-line.\n1(use-package! evil-better-visual-line 2:after evil-colemak-basics 3:config 4(evil-better-visual-line-on) 5(map! :map evil-colemak-basics-keymap 6(:nvm \u0026#34;n\u0026#34; \u0026#39;evil-better-visual-line-next-line 7:nvm \u0026#34;e\u0026#34; \u0026#39;evil-better-visual-line-previous-line 8:nvm \u0026#34;g n\u0026#34; \u0026#39;evil-next-line 9:nvm \u0026#34;g e\u0026#34; \u0026#39;evil-previous-line)) 10) Pdf-Tools For my doom-emacs configuration, I also set the following map:\n1(after! pdf-view 2(add-hook! \u0026#39;pdf-view-mode-hook (evil-colemak-basics-mode -1)) 3(map! 4:map pdf-view-mode-map 5:n \u0026#34;g g\u0026#34; #\u0026#39;pdf-view-first-page 6:n \u0026#34;G\u0026#34; #\u0026#39;pdf-view-last-page 7:n \u0026#34;N\u0026#34; #\u0026#39;pdf-view-next-page-command 8:n \u0026#34;E\u0026#34; #\u0026#39;pdf-view-previous-page-command 9:n \u0026#34;e\u0026#34; #\u0026#39;evil-collection-pdf-view-previous-line-or-previous-page 10:n \u0026#34;n\u0026#34; #\u0026#39;evil-collection-pdf-view-next-line-or-next-page 11) Where the most important thing is the hook which removes the evil-colemak-basics binding. Since it is a single mode and hook, after-hook! is the same as after-hook1.\nWindow Management Somehow these are not part of the evil-colemak defaults.\n1(after! evil 2(map! :map evil-window-map 3(:leader 4(:prefix (\u0026#34;w\u0026#34; . \u0026#34;Select Window\u0026#34;) 5:n :desc \u0026#34;Left\u0026#34; \u0026#34;h\u0026#34; \u0026#39;evil-window-left 6:n :desc \u0026#34;Up\u0026#34; \u0026#34;e\u0026#34; \u0026#39;evil-window-up 7:n :desc \u0026#34;Down\u0026#34; \u0026#34;n\u0026#34; \u0026#39;evil-window-down 8:n :desc \u0026#34;Right\u0026#34; \u0026#34;i\u0026#34; \u0026#39;evil-window-right 9)) 10)) Search Harmonizing with Vimium.\n1(after! evil (map! :map evil-motion-state-map 2(:n :desc \u0026#34;Previous match\u0026#34; \u0026#34;K\u0026#34; \u0026#39;evil-ex-search-previous 3:n :desc \u0026#34;Next match\u0026#34; \u0026#34;k\u0026#34; \u0026#39;evil-ex-search-next 4:n :desc \u0026#34;Forward search\u0026#34; \u0026#34;/\u0026#34; \u0026#39;evil-search-forward 5) 6)) Page Movement Though this is more of a personal preference, I find it more natural to bind N and E to page-wise movement instead of join lines and lookup, since I almost never use those commands, and the movement keys echo what I expect elsewhere.\n1(after! evil 2(map! :map evil-colemak-basics-keymap 3:nv \u0026#34;N\u0026#34; \u0026#39;evil-scroll-page-up 4:nv \u0026#34;E\u0026#34; \u0026#39;evil-scroll-page-down) 5) Evil Org Annoyingly, evil-org-mode had a map which kept overriding all my other settings. Thankfully it has a helper variable to set movement. I also do not need this anyway, at-least not by default.\n1(after! org 2(remove-hook \u0026#39;org-mode-hook \u0026#39;evil-org-mode) 3(setq evil-org-movement-bindings 4\u0026#39;((up . \u0026#34;e\u0026#34;) (down . \u0026#34;n\u0026#34;) 5(left . \u0026#34;h\u0026#34;) (right . \u0026#34;i\u0026#34;)) 6) 7) Vimium I use the excellent vimium to make Chrome be a little less annoying. Luckily the Wiki seems to have a reasonable suggestion for colemak. The basic idea is to migrate the underlying keys directly to ensure very few manual changes are required.\n1mapkey n j 2mapkey N J 3mapkey e k 4mapkey E K 5mapkey i l 6mapkey I L 7mapkey k n 8mapkey K N 9mapkey l i 10mapkey L I 11mapkey j e 12mapkey J E Tridactyl I still use the fantastic tridactyl for Firefox when I can. However, the bindings are slightly more involved, since there is no equivalent for the mapkey which Vimium has.\n1\u0026#34; Rebinds for colemak 2\u0026#34; hjkl --\u0026gt; hnei 3bind h scrollpx -50 4bind n scrollline 10 5bind e scrollline -10 6bind i scrollpx 50 7\u0026#34; HJKL --\u0026gt; HNEI 8bind H back 9bind N tabprev 10bind E tabnext 11bind I forward Vim For a lot of terminal edits, vim is still my editor of choice, and vim-colemak works without any trouble in my configuration.\nZsh To ensure uniform bindings, I used to use bindkey -v but will need some minor changes to that set up. I based this part of my configuration off the bindings of bunnyfly.\n1bindkey -v 2# Colemak. 3bindkey -M vicmd \u0026#34;h\u0026#34; backward-char 4bindkey -M vicmd \u0026#34;n\u0026#34; down-line-or-history 5bindkey -M vicmd \u0026#34;e\u0026#34; up-line-or-history 6bindkey -M vicmd \u0026#34;i\u0026#34; forward-char 7bindkey -M vicmd \u0026#34;s\u0026#34; vi-insert 8bindkey -M vicmd \u0026#34;S\u0026#34; vi-insert-bol 9bindkey -M vicmd \u0026#34;k\u0026#34; vi-repeat-search 10bindkey -M vicmd \u0026#34;K\u0026#34; vi-rev-repeat-search 11bindkey -M vicmd \u0026#34;l\u0026#34; beginning-of-line 12bindkey -M vicmd \u0026#34;L\u0026#34; end-of-line 13bindkey -M vicmd \u0026#34;j\u0026#34; vi-forward-word-end 14bindkey -M vicmd \u0026#34;J\u0026#34; vi-forward-blank-word-end 1516# Sane Undo, Redo, Backspace, Delete. 17bindkey -M vicmd \u0026#34;u\u0026#34; undo 18bindkey -M vicmd \u0026#34;U\u0026#34; redo 19bindkey -M vicmd \u0026#34;^?\u0026#34; backward-delete-char 20bindkey -M vicmd \u0026#34;^[[3~\u0026#34; delete-char 2122# Keep ctrl+r searching 23bindkey -M viins \u0026#39;^R\u0026#39; history-incremental-pattern-search-forward 24bindkey -M viins \u0026#39;^r\u0026#39; history-incremental-pattern-search-backward Zathura There is no better pdf viewer than zathura, and it also works for djvu and friends. As a plus point, it normally has very reasonable vim bindings, and an excellent configuration system, so we will leverage that. The best part is that we can just add to it using include zathuraColemak or whatever so as to be minimally invasive.\n1map h scroll left2map n scroll down3map e scroll up4map i scroll right56map N scroll half-down7map E scroll half-up89map k search forward10map K search backward1112# For TOC navigation13map [index] o toggle_index1415# hjkl → hnei16map [index] n navigate_index down17map [index] e navigate_index up18map [index] h navigate_index collapse19map [index] i navigate_index expand2021map [index] H navigate_index collapse-all22map [index] I navigate_index expand-allZathura is a complicated beast, however, and my full configuration contains a lot more information.\ni3 I have some bindings set up in terms of $left $right $up and $down, so it was simple to re-bind them.\n1set $left h2set $down n3set $up e4set $right iMailMate Sadly, one of the email clients I do use regularly of late is MailMate. It supports a rather rich set of keybindings placed, e.g. with \u0026quot;~/Library/Application\\ Support/MailMate/Resources/KeyBindings/\u0026quot; configured as follows:\n1{ 2\u0026#34;c\u0026#34;\t= \u0026#34;newMessage:\u0026#34;; 3\u0026#34;/\u0026#34;\t= \u0026#34;searchAllMessages:\u0026#34;; 4\u0026#34;n\u0026#34;\t= \u0026#34;nextMessage:\u0026#34;; 5\u0026#34;e\u0026#34;\t= \u0026#34;previousMessage:\u0026#34;; 6\u0026#34;h\u0026#34; = \u0026#34;collapseThread:\u0026#34;; 7\u0026#34;i\u0026#34; = \u0026#34;expandThread:\u0026#34;; 8\u0026#34;H\u0026#34; = \u0026#34;rootOfThread:\u0026#34;; 9\u0026#34;I\u0026#34; = \u0026#34;lastOfThread:\u0026#34;; 10\u0026#34;N\u0026#34; = \u0026#34;nextThread:\u0026#34;; 11\u0026#34;E\u0026#34; = \u0026#34;previousThread:\u0026#34;; 12\u0026#34;o\u0026#34;\t= \u0026#34;openMessages:\u0026#34;; 13\u0026#34;x\u0026#34; = ( \u0026#34;deleteMessage:\u0026#34;, \u0026#34;nextMessage:\u0026#34; ); // Defaults to going to the previous message 14 \u0026#34;a\u0026#34;\t= \u0026#34;archive:\u0026#34;; 15\u0026#34;s\u0026#34;\t= \u0026#34;toggleFlag:\u0026#34;; 16\u0026#34;!\u0026#34;\t= \u0026#34;moveToJunk:\u0026#34;; 17\u0026#34;r\u0026#34;\t= \u0026#34;reply:\u0026#34;; 18\u0026#34;R\u0026#34;\t= \u0026#34;replyAll:\u0026#34;; 19\u0026#34;f\u0026#34;\t= \u0026#34;forwardMessage:\u0026#34;; 20\u0026#34;^s\u0026#34;\t= \u0026#34;saveDocument:\u0026#34;; 21\u0026#34;u\u0026#34; = \u0026#34;toggleReadState:\u0026#34;; 22} Conclusions That seems to be it for now. If I think of more programs I use regularly which allow VIM bindings, or keybindings in general, I\u0026rsquo;ll probably just update this post. My full dotfiles are present here, and now include a colemak target.\n  The hook fix was suggested by the fantastic hlissner on the super friendly doom Discord server.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/colemak-dots-refactor/","tags":["workflow"],"title":"Refactoring Dotfiles For Colemak"},{"categories":["programming"],"contents":"Background One of the best things about writing in orgmode is that we can embed and execute arbitrary code snippets. However, not all languages have an exporter, for obvious reasons. Somewhat surprisingly, there is no way to call pandoc on embedded snippets, which feels like a waste, especially when a whole bunch of documentation formats can be converted to orgmode with it.\nConsider the following beautifully highlighted snippet of an rst (ReStructured Text) list table.\n1.. list-table:: Title2 :widths: 25 25 503 :header-rows: 14 5* - Heading row 1, column 16 - Heading row 1, column 27 - Heading row 1, column 38 * - Row 1, column 19 -10 - Row 1, column 311 * - Row 2, column 112 - Row 2, column 213 - Row 2, column 3Trying to run this will generate the sort of obvious error:\n1org-babel-execute-src-block: No org-babel-execute function for rst! Writing an Exporter For this post, I will be focusing on rst, but this can be defined for any of the pandoc back-ends. The approach was inspired by ob-markdown.\n1(defun org-babel-execute:rst (body params) 2\u0026#34;Execute a block of rst code with org-babel. 3This function is called by `org-babel-execute-src-block\u0026#39;.\u0026#34; 4(let* ((result-params (split-string (or (cdr (assoc :results params)) \u0026#34;\u0026#34;))) 5(in-file (org-babel-temp-file \u0026#34;rst-\u0026#34;)) 6(cmdline (cdr (assoc :cmdline params))) 7(to (cdr (assoc :to params))) 8(template (cdr (assoc :template params))) 9(cmd (concat \u0026#34;pandoc\u0026#34; 10\u0026#34; -t org\u0026#34; 11\u0026#34; -i \u0026#34; (org-babel-process-file-name in-file) 12\u0026#34; -f rst \u0026#34; 13\u0026#34; \u0026#34; cmdline))) 14(with-temp-file in-file (insert body)) 15(message cmd) 16(shell-command-to-string cmd))) ;; Send to results 1718(defun org-babel-prep-session:rst (session params) 19\u0026#34;Return an error because rst does not support sessions.\u0026#34; 20(error \u0026#34;rst does not support sessions\u0026#34;)) Trying it out With that done, it is pretty trivial to re-run the above example.\n1.. list-table:: Title2 :widths: 25 25 503 :header-rows: 14 5* - Heading row 1, column 16 - Heading row 1, column 27 - Heading row 1, column 38 * - Row 1, column 19 -10 - Row 1, column 311 * - Row 2, column 112 - Row 2, column 213 - Row 2, column 3   Heading row 1, column 1 Heading row 1, column 2 Heading row 1, column 3     Row 1, column 1  Row 1, column 3   Row 2, column 1 Row 2, column 2          Note that we have used rst :exports both :results raw as the header argument.\nConclusions Will probably follow this up with an actual package, which should handle the entire spectrum of pandoc back-ends.\n","permalink":"https://rgoswami.me/posts/org-pandoc-babel/","tags":["tools","emacs","workflow","orgmode"],"title":"Pandoc to Orgmode with Babel"},{"categories":["programming"],"contents":"Background I have been wanting to find a workflow which allows me to bypass writing a lot of TeX by hand for a while now. To that end I looked into using a computer algebra system (CAS). Naturally, my first choice was the FOSS Maxima (also because it uses Lisp under the hood). However, for all the reasons listed here, relating to its accuracy, which have not been fixed even though the post was over 5 years ago, I ended up having to go with the closed source Mathematica.\nPackages Support for Mathematica in modern orgmode is mainly through the use of ob-mathematica, which is the official org-babel extension (from contrib) for working with Mathematica. However, ob-mathematica relies on the now-defunct mma package for font-locking, which is less than ideal. Thankfully, there exists the excellent wolfram-mode package which happens to be in MELPA as well. Finally, since the default return type of a mathematica block is an input-string meant to be used in another mathematica block, which is not useful when we work with org-babel, we will use the excellent mash.pl utility from here, as suggested by the ob-mathematica package to sanitize our output and set a unifying path.\nSo to recap, use your favorite manager to get:\n ob-mathematica (in contrib) wolfram-mode (MELPA) mash.pl (from here)1  After obtaining the packages, the configuration is then simply2:\n1;; Load mathematica from contrib 2(org-babel-do-load-languages \u0026#39;org-babel-load-languages 3(append org-babel-load-languages 4\u0026#39;((mathematica . t)) 5)) 6;; Sanitize output and deal with paths 7(setq org-babel-mathematica-command \u0026#34;~/.local/bin/mash\u0026#34;) 8;; Font-locking 9(add-to-list \u0026#39;org-src-lang-modes \u0026#39;(\u0026#34;mathematica\u0026#34; . wolfram)) 10;; For wolfram-mode 11(setq mathematica-command-line \u0026#34;~/.local/bin/mash\u0026#34;) Results LaTeX Now we are in a position to simply evaluate content with font-locking. We will test our set up with an example lifted from the ob-mathematica source-code.\n\nTable 1: A table     1 4     2 4   3 6   4 8   7 0    1(1+Transpose@x)//TeXFormWhere our header-line (with #+begin_src) is:\n1mathematica :var x=example-table :results latex Sanity Checks We can also test the example from the blog post earlier to test basic mathematical sanity.\n1Limit[Log[b-a+Ieta],eta-\u0026gt;0,Direction-\u0026gt;-1,Assumptions-\u0026gt;{a\u0026gt;0,b\u0026gt;0,a\u0026gt;b}]2TeXForm[Limit[Log[b-a+Ieta],eta-\u0026gt;0,Direction-\u0026gt;1,Assumptions-\u0026gt;{a\u0026gt;0,b\u0026gt;0,a\u0026gt;b}]]\\((I*Pi + Log[a - b])*\\log (a-b)-i \\pi\\)\nInline Math Note that we can now also write fractions, integrals and other cumbersome TeX objects a lot faster with this syntax, like \\(\\frac{x^3}{3}\\). Where we are using the following snippet:\n1src_mathematica[:exports none :results raw]{Integrate[x^2,x] // TeXForm} Plots For plots, the standard orgmode rules apply, that is, we have to export to a file and return the name through our code snippet. Consider:\n1p=Plot[Sin[x],{x,0,6Pi},Frame-\u0026gt;True];2Export[\u0026#34;images/sine.png\u0026#34;,p];3Print[\u0026#34;images/sine.png\u0026#34;] Figure 1: An exported Mathematica image\n  Where we have used mathematica :results file as our header line.\nComments The older commenting system was implemented with utteranc.es as seen below.\n   As noted in the comments, it is nicer to rename mash.pl to mash\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n For reference, my whole config is here\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/org-mathematica/","tags":["tools","emacs","workflow","orgmode"],"title":"Using Mathematica with Orgmode"},{"categories":["notes"],"contents":"Background As I mentioned earlier, I\u0026rsquo;m leading a section for Stanford CS106A: Code in Place. This post relates to the notes and thoughts garnered during the small group training session1.\nReflections Demographics Redacted. Did not use breakout meetings due to privacy issues.\nEngagement and Participation  Some people were more active (skewed responses) Some of the more rudimentary questions might have been suppressed  Highlighted Moments  Covering multiple perspectives Different mental models  Challenges and Transformations  Technical debt was an issue Lack of engagement Went on for too long  For me in particular:\n It took over two hours, and though most people stayed on, not everyone was engaged.\n Scenarios These are to be dealt with as per the guidelines here. Since different groups covered different scenarios, not all of these have answers here.\nEnsuring Engagement  You have some students who didn\u0026rsquo;t participate at all in the section. What do you do?\n Effective Communication  What might not be effective about the policy, “Students should just tell me if I say something that offends them”?\n Sharing Experiences  You just finished your section and are staying behind to answer questions from your students. A couple students asked what it’s like studying/working in an engineering/tech field.\nWhat things might you want to keep in mind when answering their questions?\n Time Management  Section went way over time due to lots of questions being asked by students. What are some time management strategies you can use moving forward?\n Homework Assists  A sectionee posts in your Ed group, “I am a little bit frustrated because I don\u0026rsquo;t really know where to start on the first assignment. A little hint would be very helpful.” How do you respond?\n Debugging  A sectionee shows you the following buggy code for printing all the elements in a list:\nmy_lst = [\u0026lsquo;apple\u0026rsquo;, \u0026lsquo;banana\u0026rsquo;, \u0026lsquo;carrot\u0026rsquo;] i = 0 while len(my_lst) \u0026gt; 0: print(my_lst[i]) i = i + 1\nThey explain that the code works (it prints all the elements in the right order) but then throws a weird error: “IndexError: list index out of range.” How would you help them find their bug?\n Quitting  You have a student who is already discouraged by how difficult the first assignment is and has told you they don’t feel cut out for CS. What do you say to them?\n  Provide encouragement Give examples of hardship faced Be positive and make sure they don’t feel worse, even if they do follow through and quit “You’re not the first” Takes a lot of time. Doesn’t happen overnight Ask them why they don’t feel cut out and try to solve that problem  Looking up issues  Why might it be problematic to say something like, “It’s easy to download X or look up the answer to Y”? Why might those statements not be true?\n  Difficulty in backgrounds (language barriers) They might not be able to understand stackoverflow.com until they learn more CS They might not know where to look online (lack of domain expertise) Dependencies (for downloads) Makes them feel bad if they don’t end up finding it easy    This post was created on the day of training, 21-04-20, but will be posted later\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/scp-smallgrp-trainig/","tags":["teaching","cs106a"],"title":"CS106A Small Group Training"},{"categories":["notes"],"contents":"Background As I mentioned earlier, I\u0026rsquo;m leading a section for Stanford CS106A: Code in Place. I did also mention I\u0026rsquo;d try to keep a set of short notes on the process. I finally had my first section meeting!\nPreparation I went through the following:\n Sent out a welcome message Detailed the workflow Set up a HackMD instance Set up some slides in beamer1  However, after that, I was still concerned since I didn\u0026rsquo;t get much of a response on the ice-breakers for EdStem. Thankfully, everyone showed up.\nTeaching  I had a fabulous session, and we went through a variety of concepts. Didn\u0026rsquo;t spend much time on icebreakers, but did get a feel for where the students stand on the functional vs imperative programming paradigms Possibly because of working through two different approaches, the 40 minute long session went on for two hours and fifteen minutes. Some students had more of a background than the others, thankfully computational thinking is not normally taught very well  Conclusion  The notes are visible here, and the session was recorded here2 It was fun, and I hope the students enjoyed it as much as I did. I will probably expand this in terms of the concepts covered, to give the students more of an overview of what was covered    Even though most of the session was supposed to be live, it was still helpful to show I was interested enough to set up slides\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n As always, advice is much appreciated (and moderated)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/scp-smallgrp-meet1/","tags":["teaching","cs106a"],"title":"CS106A Section Meeting I"},{"categories":["notes"],"contents":"Background As I mentioned in my last post, I\u0026rsquo;m leading a section for Stanford CS106A: Code in Place. I did also mention I\u0026rsquo;d try to keep a set of short notes on the process. So there1.\nThe Training Given the overwhelming number of students, and section leaders, the small groups are for fostering a community of teachers.\n Consider allowing for daisy chaining during introductions Discussions are the primary take-away Only the instructor should be coding during the session  Core components  Clarity Content Atmosphere Section management Correctness  Sectional Details  Check in at the start Notice the space Check in regularly Avoid negative phrases Establish norms and the general culture  Zoom Norms  Have people introduce themselves Mute people when they aren\u0026rsquo;t talking Raise hands Try to use icebreakers which respect privacy  Materials Here\u0026rsquo;s some of the stuff which, being as it was open-sourced, I suppose is OK to put here2.\n Section Leader Training Section Leaders\u0026rsquo; Guide to Virtual Sections Some Zoom Icebreakers    As you may know, the official playlist is here\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n If you know otherwise, let me know in the comments\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/scp-smallgrp/","tags":["teaching","cs106a"],"title":"Small Section On-boarding"},{"categories":["notes"],"contents":"Background A few weeks ago, I ended up recording a video for the Stanford CS106A: Code in Place initiative (which can be found here). I heard back a while ago, and am now to lead a section for the course!\nI\u0026rsquo;ll probably be making a series of short posts as this process continues.\nOn-Boarding This was very reminiscent of the Carpentries instructor training, which makes sense, given how well thought out that experience was.\nWe started out with a pre-presentation where people were able to just spitball and connect, which is pretty neat.\nOne of the interesting parts of this, was the idea of interactive recorded lectures, where the professors will be watching lectures with the students. The entire slide deck is here.\nThe other great idea for this kind of long course was the idea of having a Tea room and a Teachers lounge where people can just tune in to chat.\nCaveats A couple of things which keep cropping up for online teaching in general are the following:\n Zoom does not have persistent chats, so an auxiliary tool like an Etherpad is great  ","permalink":"https://rgoswami.me/posts/scp-onboarding/","tags":["ideas","teaching","cs106a"],"title":"On-boarding for Code in Place"},{"categories":["notes"],"contents":"Background Like a lot of my tech based rants, this was brought on by a recent Hacker News post. I won\u0026rsquo;t go into why the product listed there is a hollow faux FOSS rip-off. I won\u0026rsquo;t discuss how the \u0026lsquo;free\u0026rsquo; analytics option, like many others are just hobby projects taking pot shots at other projects. Or how insanely overpriced most alternatives are.\nI will however discuss why and how I transitioned to using the awesome Goat Counter.\nGoogle Analytics I would like to point out that it is OK to start out with Google Analytics. It is easy, and free, and scales well. There are reasons not to, but it is a good starting point.\nPros  Google Analytics is free, truly free The metrics are very detailed It is easy to set up  Cons  Privacy concerns Blocked by people Easy to obsess over metrics  Goat Counter As with most Hacker News posts, the article itself was nothing compared to the excellent comment thread. It was there that I came across people praising Goat Counter.\nPros  Is open sourced (here on Github) Super lightweight Anonymous statistics Easy to share  Cons  Has an upper limit on free accounts (10k a month) I am not very fond of Go  Conclusions I might eventually go back to GA, if I go over the 10k page view limit. Then again, I might not. It might be more like, I only care about the first 10k people who make it to my site.\nUPDATE: This site has since shifted to Clicky, for reasons outlined here\n","permalink":"https://rgoswami.me/posts/goat-google/","tags":["tools","rationale","workflow","webdev"],"title":"Analytics: Google to Goat"},{"categories":["notes"],"contents":" Explain why using bagging for prediction trees generally improves predictions over regular prediction trees.\n Introduction Bagging (or Bootstrap Aggregation) is one of the most commonly used ensemble method for improving the prediction of trees. We will broadly follow a historical development trend to understand the process. That is, we will begin by considering the Bootstrap method. This in turn requires knowledge of the Jacknife method, which is understandable from a simple bias variance perspective. Finally we will close out the discussion by considering the utility and trade-offs of the Bagging technique, and will draw attention to the fact that the Bagging method was contrasted to another popular ensemble method, namely the Random Forest method, in the previous section.\nBefore delving into the mathematics, recall that the approach taken by bagging is given as per Cichosz (2015) to be:\n create base models with bootstrap samples of the training set combine models by unweighted voting (for classification) or by averaging (for regression)  The reason for covering the Jacknife method is to develop an intuition relating to the sampling of data described in the following table:\n   Data-set Size per sample Estimator     Reduces Jacknife   Remains the same Bootstrap   Increases data-augmentation    Bias Variance Trade-offs We will recall, for this discussion, the bias variance trade off which is the basis of our model accuracy estimates (for regression) as per the formulation of James et al. (2013).\n\\begin{equation} E(y₀-\\hat{f}(x₀))²=\\mathrm{Var}(\\hat{f}(x₀))+[\\mathrm{Bias(\\hat{f(x₀)})}]²+\\mathrm{Var}(ε) \\end{equation}\nWhere:\n \\(E(y_{0}-\\hat{f}(x_{0}))²\\) is the expected test MSE, or the average test MSE if \\(f\\) is estimated with a large number of training sets and tested at each \\(x₀\\) The variance is the amount by which our approximation \\(\\hat{f}\\) will change if estimated by a different training set, or the flexibility error The bias is the (reducible) approximation error, caused by not fitting to the training set exactly \\(\\mathrm{Var}(ε)\\) is the irreducible error  We will also keep in mind, going forward the following requirements of a good estimator:\n Low variance AND low bias Typically, the variance increases while the bias decreases as we use more flexible methods (i.e. methods which fit the training set better1)  Also for the rest of this section, we will need to recall from Hastie, Tibshirani, and Friedman (2009), that the bias is given by:\n\\begin{equation} [E(\\hat{f_{k}}(x₀)-f(x₀)]² \\end{equation}\nWhere the expectation averages over the randomness in the training data.\nTo keep things in perspective, recall from Hastie, Tibshirani, and Friedman (2009):\n Figure 1: Test and training error as a function of model complexity\n  Jacknife Estimates We will model our discussion on the work of Efron (1982). Note that:\n The \\(\\hat{θ}\\) symbol is an estimate of the true quantity \\(θ\\) This is defined by the estimate being \\(\\hat{θ}=θ(\\hat{F})\\) \\(\\hat{F}\\) is the empirical probability distribution, defined by mass \\(1/n\\) at \\(xᵢ ∀ i∈I\\), i is from 1 to n  The points above establishes our bias to be given by \\(E_Fθ(\\hat{F})-θ(F)\\) such that \\(E_F\\) is the expectation under x₁⋯xₙ~F.\nTo derive the Jacknife estimate \\((\\tilde{θ})\\) we will simply sequentially delete points xᵢ (changing \\(\\hat{F}\\)), and recompute our estimate \\(\\hat{θ}\\), which then simplifies to:\n\\begin{equation} \\tilde{θ}\\equiv n\\hat{θ}-(\\frac{n-1}{n})∑_{i=1}ⁿ\\hat{θ} \\end{equation}\nIn essence, the Jacknife estimate is obtained by making repeated estimates on increasingly smaller data-sets. This intuition lets us imagine a method which actually makes estimates on larger data-sets (which is the motivation for data augmentation) or, perhaps not so intuitively, on estimates on data-sets of the same size.\nBootstrap Estimates Continuing with the same notation, we will note that the bootstrap is obtained by draw random data-sets with replacement from the training data, where each sample is the same size as the original; as noted by Hastie, Tibshirani, and Friedman (2009).\nWe will consider the bootstrap estimate for the standard deviation of the \\(\\hat{θ}\\) operator, which is denoted by \\(σ(F,n,\\hat{\\theta})=σ(F)\\)\nThe bootstrap is simple the standard deviation at the approximate F, i.e., at \\(F=\\hat{F}\\):\n\\begin{equation} \\hat{\\mathrm{SD}}=\\sigma(\\hat{F}) \\end{equation}\nSince we generally have no closed form analytical form for \\(σ(F)\\) we must use a Monte Carlo algorithm:\n Fit a non parametric maximum likelihood estimate (MLE) of F, i.e. \\(\\hat{F}\\) Draw a sample from \\(\\hat{F}\\) and calculate the estimate of \\(\\hat{θ}\\) on that sample, say, \\(\\hat{θ}^*\\) Repeat 2 to get multiple (say B) replications of \\(\\hat{θ}^*\\)  Now we know that as \\(B→∞\\) then our estimate would match \\(σ(\\hat{F})\\) perfectly, however, since that itself is an estimate of the value we are actually interested in, in practice there is no real point using a very high B value.\nNote that in actual practice we simply use the given training data with repetition and do not actually use an MLE of the approximate true distribution to generate samples. This causes the bootstrap estimate to be unreasonably good, since there is always significant overlap between the training and test samples during the model fit. This is why cross validation demands non-overlapping data partitions.\nConnecting Estimates The somewhat surprising result can be proved when \\(\\hat{θ}=θ(\\hat{F}\\) is a quadratic functional, namely:\n\\begin{equation}\\hat{\\mathrm{Bias}}_{boot}=\\frac{n-1}{n} \\hat{\\mathrm{Bias}}_{jack}\\end{equation}\nIn practice however, we will simply recall that the Jacknife tends to overestimate, and the Bootstrap tends to underestimation.\nBagging Bagging, is motivated by using the bootstrap methodology to improve the estimate or prediction directly, instead of using it as a method to asses the accuracy of an estimate. It is a representative of the so-called parallel ensemble methods where the base learners are generated in parallel. As such, the motivation is to reduce the error by exploiting the independence of base learners (true for mathematically exact bootstrap samples, but not really true in practice).\nMathematically the formulation of Hastie, Tibshirani, and Friedman (2009) establishes a connection between the Bayesian understanding of the bootstrap mean as a posterior average, however, here we will use a more heuristic approach.\nWe have noted above that the bagging process simply involves looking at different samples in differing orders. This has some stark repercussions for tree-based methods, since the trees are grown with a greedy approach.\n Bootstrap samples may cause different trees to be produced This causes a reduction in the variance, especially when not too many samples are considered Averaging, reduces variance while leaving bias unchanged  Practically, these separate trees being averaged allows for varying importance values of the variables to be calculated.\nIn particular, following Hastie, Tibshirani, and Friedman (2009), it is possible to see that the MSE tends to decrease by bagging.\n\\begin{align} E_P[Y-\\hat{f}^*(x)]² \u0026amp; = \u0026amp; E_P[Y-f*{ag}(x)+f^*_{ag}(x)-\\hat{f}^*(x)]² \\\\ \u0026amp; = \u0026amp; E_P[Y-f^*_{ag}(x)]²+E_P[\\hat{f}^*(x)-f^*_{ag}(x)]² ≥ E_P[Y-f^*_{ag}(x)]² \\end{align}\nWhere:\n The training observations are independently drawn from a distribution \\(P\\) \\(f_{ag}(x)=E_P\\hat{f}^*(x)\\) is the ideal aggregate estimator  For the formulation above, we assume that \\(f_{ag}\\) is a true bagging estimate, which draws samples from the actual population. The upper bound is obtained from the variance of the \\(\\hat{f}^*(x)\\) around the mean, \\(f_{ag}\\)\nPractically, we should note the following:\n The regression trees are deep The greedy algorithm growing the trees cause them to be unstable (sensitive to changes in input data) Each tree has a high variance, and low bias Averaging these trees reduces the variance  Missing from the discussion above is how exactly the training and test sets are used in a bagging algorithm, as well as an estimate for the error for each base learner. This has been reported in the code above as the OOB error, or out of bag error. We have, as noted by Zhou (2012) and Breiman (1996) the following considerations.\n Given \\(m\\) training samples, the probability that the iᵗʰ sample is selected 0,1,2\u0026hellip; times is approximately Poisson distributed with \\(λ=1\\) The probability of the iᵗʰ example will occur at least once is then \\(1-(1/e)≈0.632\\) This means for each base learner, there are around \\(36.8\\) % original training samples which have not been used in its training process  The goodness can thus be estimated using these OOB error, which is simply an estimate of the error of the base tree on the OOB samples.\nAs a final note, random forests are conceptually easily understood by combining bagging with subspace sampling, which is why in most cases and packages, we used bagging as a special case of random forests, i.e. when no subspace sampling is performed, random forests algorithms perform bagging.\nReferences Breiman, Leo. 1996. \u0026ldquo;Bagging Predictors.\u0026rdquo; Machine Learning 24 (2): 123\u0026ndash;40. https://doi.org/10.1023/A:1018054314350.\n Cichosz, Pawel. 2015. Data Mining Algorithms: Explained Using R. Chichester, West Sussex ; Malden, MA: John Wiley \u0026amp; Sons Inc.\n Efron, Bradley. 1982. The Jackknife, the Bootstrap, and Other Resampling Plans. CBMS-NSF Regional Conference Series in Applied Mathematics 38. Philadelphia, Pa: Society for Industrial and Applied Mathematics.\n Hastie, Trevor, Robert Tibshirani, and J. H. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. Springer Series in Statistics. New York, NY: Springer.\n James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Vol. 103. Springer Texts in Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-1-4614-7138-7.\n Zhou, Zhi-Hua. 2012. Ensemble Methods: Foundations and Algorithms. 0th ed. Chapman and Hall/CRC. https://doi.org/10.1201/b12207.\n    This is mostly true for reasonably smooth true functions\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/trees-and-bags/","tags":["theory","statistics","math"],"title":"Trees and Bags"},{"categories":["programming"],"contents":"Background  I recently read this post written by the now deceased Prof. David MacKay 1 It should be read widely, however, given that it is distributed as a ps.gz which is then a .ps file, and thus probably inaccessible to many of the people who should read it, I decided to rework it for native online consumption (there is also a pdf) THIS IS NOT MY CONTENT 2 Now, enjoy the post  Everyone Should Get an A Imagine a University – call it Camwick – where all students arrive with straight A grades. They are successful, enthusiastic, and curious. By the time they leave, only one third still receive straight As. The other two thirds get lower grades, do not enjoy their studies, and are not fun to teach. Is Camwick University a success? Camwick could point to its excellent teaching assessment scores and argue that it is ‘adding value’: students emerge knowing more. Future employers love the University’s policy of assigning grades – the University ranks its students, saving companies the bother of assessing job applicants themselves. But should a University be a sorting service? Isn’t something wrong with an institution that takes in mainly A-quality input and turns out less than half A-quality output? If a University fails to turn out as much A-quality enthusiasts as come in, is it in fact a place of intellectual destruction, throwing away the potential of the majority of its students? What are the roots of this destruction?\nExams I would recommend that Camwick consider abolishing traditional exams. In the current system, Camwick teaches Anna, Bob, and Charlie, who are all smart, then examines them; Anna comes \u0026rsquo;top\u0026rsquo;, Bob \u0026lsquo;second\u0026rsquo; and Charlie \u0026rsquo;third\u0026rsquo;. Perhaps Charlie, given a little more time, would have figured out the material, but he wasn\u0026rsquo;t quite ready when the exam arrived - perhaps because other courses consumed his attention.\nBob\u0026rsquo;s response to his \u0026lsquo;failure\u0026rsquo; is to adopt strategies of tlittle educational value: he parrot learns, he crams, and he asks lecturers to tell him what\u0026rsquo;s going to be on the exam. The exams become the focus of attention, even though the purpose of Bob\u0026rsquo;s going to the University was learning.\nCharlie\u0026rsquo;s response is to give up on doing \u0026lsquo;well\u0026rsquo;, and coast through University, no longer understanding everything. He loses self-worth and resents the University for making him feel bad.\nSome courses at Camwick assign grades using continuous assessment instead of exams. But continuous assessment has the same effect as exams on Bob and Charlie. So course grades based on continuous assessment should be abolished at the same time as exams.\n Figure 1: Everyone can get an A, regardless of learning rate, if their education is not halted by exams. Traditional system on the left, with an educational system on the right.\n  If Camwick had no exams, the focus of attention would have to be elsewhere. How about education, for example? Students could spend their time at Camwick exploring subjects that interest them, and attending classes that offer something they want to know about, free from the stress and misdirection of the exam system. Lecturers would at all times be friends rather than adversaries. [When I was an undergraduate at Cambridge, I asked a physis lecturer to clarify topic \\(N\\), which I felt had not been covered clearly. His response: \u0026ldquo;That\u0026rsquo;s what I love about \\(N\\): some students get it, some don\u0026rsquo;t - so we get beautiful bell shaped curves in the exam.\u0026rdquo;]\nOf course the extreme suggestion of abolishing all exams will not go down well: \u0026ldquo;What about standards?\u0026rdquo; \u0026ldquo;How can we get funding if we do not\u0026rdquo; \u0026ldquo;How do we award degrees that people will respect?\u0026rdquo; Traditionalists might say that students appreciate exams for the targets and feedback. Well, there\u0026rsquo;s nothing to stop us giving students targets or feedback. We can provide events just like exams, if students want them - self-administered tests, for example, would allow students to check how well they have assimilated all the material in a course. Other systems of targets and feedback that students enjoy include project work, problem-based learning, and portfolio-based assessment.\nAs a compromise, let\u0026rsquo;s modify our proposal a little: Camwick should become a place where the only achievable grade is an A. I\u0026rsquo;m not recommending that we simply give everyone an A. It\u0026rsquo;s a crime to let standards slip. When I say everyone should get an A, I mean that everyone should be allowed to get to an A.\nThink back to Alice, Bob, and Charlie. Alice grasped most of the material in the course and achieved an A. Given a little more time and little less stress, Bob and Charlie could probably have grasped it all too, and become equally strong masters of the material. What good does it do Bob and Charlie to record the fact that they were a little slower than alice? Wouldn\u0026rsquo;t it have been better, educationally, to give Bob and Charlie a little more time and help, so that they achieved the same A standard?\nDoes a bus-driver-training school rank its graduating drivers? No, it ensures that all attain the standard required of a bus-driver. Would you like to be treated by a C-grade doctor? No, everyone wants an A-grade doctor! So doctors and drivers are (I hope!) trained and trained and not let out until they are A-grade in standard. Why should other professions be treated differently?\nFigure 1a shows the command of the material of each student as a function of time in the traditional system. A traditional exam interrupts the learning process, and Bob and Charlie are recorded as having achieved a lower standard. Figure 1b shows the same students in an exam-free system, assuming they learn at the same rate as in the old system. Each student takes a different time to achieve full command of the course material. Every student has the satisfaction of achieving full command of the material.\n Figure 2: Everyone can get an A, regardless of learning rate, if their education is not halted by exams. Traditional system on the left, with an educational system on the right.\n  The difference between the two systems is also striking if we assume that students start the course at different levels of ability. In Figure 2, albert comes from a privileged background and already knows half the course material when he arrives. Brenda and Catharine arrive at a lower educational level. Brenda and Catharine are actually faster learners than Albert, but, as Figure 2a shows, the traditional exam system rewards Albert with the A grade (\u0026lsquo;congratulations, you started first!\u0026rsquo;), and brands Brenda and Catharine failures. In the \u0026lsquo;Only A-grades\u0026rsquo; system, everyone attains an A-grade in due course; and Albert isn\u0026rsquo;t actually first to finish.\nThe information about \u0026lsquo;who finished when\u0026rsquo; could in principle be retained in order to provide some sort of student-ranking service to employers, but I would strongly urge the destruction of all such records. Only the achieving of an A grade should be recorded, nothing else. Why?\n Because being ranked creates stress. Because students who are competing with each other for ranks may be reluctant to help each other learn. In contrast, in the \u0026lsquo;Only A-grades\u0026rsquo; system, the top students lose nothing if they help their peers; indeed, they may gain in several ways: peer-teaching strengthens the students\u0026rsquo; grasp on material, and often speeds up the whole class. Evidence that a student is a quick learner may well make itself evident in her transcript without rankings being made: Alice, covering material quickly, will have time to take extra courses. So in one year she\u0026rsquo;ll accumulate a slightly fatter sheaf of A-grade qualifications. What value are rankings? If future employers want students to be formally evaluated, they can pay for an evaluation service. Why ruin a great institution? The very best students might like grades too, as they enjoy being congratulated. But the \u0026lsquo;only A-grades\u0026rsquo; system will congratualte them too.  These ideas are not new, nor are they unprecedented. In many German Universities, first- and second-year courses have no grades, no obligatory coursework, and no obligatory exams. End-of-course exams are provided only as a service to students, to help them find out if they have indeed grasped the material and are ready progress to the next stage.\nIn practice, how should we organize courses so that everyone reaches 100% mastery? For Bob and Charlie\u0026rsquo;s benefit, the average pace probably has to be reduced. Figure 3 shows one way of organizing the material in stages, so that a class is kept together. Whenever Alice has completed the material in a stage, she can spend time on other interests, or can help other members of the class.\n Figure 3: Possible course plan. This scheme assumes that the students have rates of progress ranging from A (fastest) to C (slowest). Every two weeks, a consolidation period is inserted to ensure that C has assimilated all the learning objectives. Alice can use the consolidation period to pursue others interests or act as a peer-teacher.\n  Camwick staff who say \u0026ldquo;we can\u0026rsquo;t possibly cover a full degree course if we reduce the pace!\u0026rdquo; should bear in mind that, had Bob and charlie gone to a less prestigious University, they probably would have got first-class degrees. How can this paradox - going slowing and arriving at almost the same time - be explained? I suspect an important factor is this: struggling students get ever slower if we pile on new material before they have assimilated the old. For example, 2ⁿᵈ-year Lagrangian dynamics is difficult to absorb if one hasn\u0026rsquo;t grasped 1ˢᵗ-year Newtonian dynamics. So the steady linear progress assumed in Figures 1 to 3 is a poor model of Carlie. The more Charlie is left behind, the slower he learns. This means that the true difference in pace between Alice and Charlie need not be very big. If Charlie gets lost and left behind, we are wasting everyone\u0026rsquo;s time by having him sit in classes where new material is presented. A stitch in time saves nine (Figure 4).\n Figure 4: A stitch in time saves nine. Curve C shows Charlie\u0026rsquo;s progress in a course taught at the pace that is ideal for Alice. The more Charlie is left behind, the slower he learns. By the end of the course, there is a big gap between A and C. Curve C′ shows Charlie\u0026rsquo;s progress in a course taught at the pace that is ideal for him. Just a small decrease in class pace allows the big gap between Alice and Charlie to be eliminated.\n  Teaching methods must be modified to ensure that everyone in the class benefits. I advocate interactive teaching: students are asked questions and encouraged to ask questions and to be active participants in their own learning. It\u0026rsquo;s not enough to ask a question and let one person in the class (Alice!) answer it. The whole class must have the chance to think, puzzle and discuss; the teacher must ascertain the level of understanding of the whole class. In large classes, I find Mazur\u0026rsquo;s voting method works well: a lecture is centered on two or three carefully chosen questions with multiple-choice answers. Students discuss a question with their neighbors, then all vote. The vote informs the lecturer whether previous material has been understood. Diversity of votes can seed a useful discussion.\nTo conclude, here are a few further advantages of the educational approach advocated here:\n Happy, curious, and self-motivated students are fun to teach. At present, British students have little choice of university teaching and assessment style: all universities give out grades. Shouldn\u0026rsquo;t we offer them a choice? Some students would like the chance to go to a place with high standards where only A-grades are awarded. If some universities adopt student-centered educational policies and stop ranking students, perhaps these attitudes will spread to schools, with consequent benefits to pupils, and in due course, to universities. Dumbed-down A levels could be replaced by educational programmes that ensure that everyone attains their maximum potential and feels happy about it. Happy graduates who get A grades are likely to become grateful alumni donors.    Also known for the fabulous free book called Information Theory, Inference, and Learning Algorithms\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n If you have a good reason why this should not be distributed here in this manner, please contact me and I will do the needful\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/mackay-all-a/","tags":["academics","teaching","evaluation","ideas"],"title":"Everyone Should Get an A - David MacKay"},{"categories":["notes"],"contents":"Background  I have had a lot of discussions regarding the teaching of git This is mostly as a part of the SoftwareCarpentries, or in view of my involvement with univ.ai, or simply in every public space I am associated with Without getting into my views, I just wanted to keep this resource in mind  The site  Learning git is a highly contentious thing People seem to be fond of GUI tools, especially since on non *nix systems, it seems that there is a lot of debate surrounding obtaining the git utility in the first place  One of the best ways of understanding (without installing stuff) the mental models required for working with git is this site\n Figure 1: A screenshot of the site\n    However, as is clear, this is not exactly a replacement for a good old command-line.\n  It does make for a good resource for teaching with slides, or for generating other static visualizations, where live coding is not an option\n  ","permalink":"https://rgoswami.me/posts/d3git/","tags":["tools","rationale","workflow","ideas"],"title":"D3 for Git"},{"categories":["notes"],"contents":"Background Sometime this year, I realized that I no longer have access to a lot of my older communication. This included, a lot of resources I enjoyed and shared with the people who were around me at that point in time. To counter this, I have decided to opt for shorter posts, even if they don\u0026rsquo;t always include the same level of detail I would prefer to provide.\nAlternatives  I have an automated system based around IFTTT combined with Twitter, Diigo, and even Pocket However, that doesn\u0026rsquo;t really tell me much, and trawling through a massive glut of data is often pointless as well There\u0026rsquo;s always Twitter, but I don\u0026rsquo;t really care to hear the views of others when I want to revisit my own ideas  Conclusions  I will be making shorter posts here, like the random one on octobox  ","permalink":"https://rgoswami.me/posts/shortpost/","tags":["tools","rationale","workflow","ideas"],"title":"Shorter Posts"},{"categories":["programming"],"contents":"Background My dotfiles turned 4 years old a few months ago (since 9th Jan 2017) and remains one of my most frequently updated projects for obvious reasons. Going through the changes reminds me of a whole of posts I never got around to writing.\nAnyway, recently I gained access to another HPC cluster, with a standard configuration (bash, old CentOS) and decided to track my provisioning steps. This is really a very streamlined experience by now, since I\u0026rsquo;ve used the same setup across scores of machines. This is actually also a generic intro to configuring user setups on HPC (high performance cluster) machines, if one is inclined to read it in that manner. To that end, sections of this post involve restrictions relating to user privileges which aren\u0026rsquo;t normally part of most Dotfile setups.\nAside  Dotfiles define most people who maintain them No two sets are ever exactly alike They fall somewhere between winging it for each machine and using something like Chef or Ansible Tracking dotfiles is really close to having a sort of out-of-context journal  Before I settled on using the fabulous dotgit, I considered several alternatives, most notably GNU stow.\nPreliminaries It is important to note the environment into which I had to get my setup.\nSSH Setup  The very first thing to do is to use a new ssh-key   1export myKey=\u0026#34;someName\u0026#34; 2ssh-keygen -f $HOME/.ssh/$myKey 3# I normally don\u0026#39;t set a password 4ssh-add $HOME/.ssh/$myKey 5ssh-copy-id $myHPC 6# myHPC being an IP address I more often than not tend to back this up with a cutesy alias, also because I do not always get my username of choice on these machines. So in $HOME/.ssh/config I use:\n1Host myHPC 2Hostname 127.0.0.1 3User somethingIgot 4IdentityFile ~/.ssh/myKey Harvesting Information  I normally use neofetch on new machines   1mkdir -p $HOME/Git/Github 2cd $HOME/Git/Github 3git clone https://github.com/dylanaraps/neofetch.git 4cd neofetch 5./neofetch  Figure 1: Neofetch Output\n  Where the top has been tastefully truncated. Just for context, the latest bash as of this writing is v5.0.16 so, that\u0026rsquo;s not too bad, given that neofetch works for bash ≥ 3.2\nTODO Circumventing User Restrictions with Nix  A post in and of itself would be required to explain why and how users are normally restricted from activities in cluster nodes Here, we leverage the nix-package management system to circumvent these User installation of nix is sadly non-trivial, so this might be of some use1  Testing nix-user-chroot  We will first check namespace support   1# Errored out 2unshare --user --pid echo YES 3# Worked! 4zgrep CONFIG_USER_NS /boot/config-$(uname -r) 5# CONFIG_USER_NS=y Thankfully we have support for namespaces, so we can continue with nix-user-chroot.\n Since we definitely do not have rustup or rustc on the HPC, we will use a prebuilt binary of nix-user-chroot   1cd $HOME \u0026amp;\u0026amp; wget -O nix-user-chroot https://github.com/nix-community/nix-user-chroot/releases/download/1.0.2/nix-user-chroot-bin-1.0.2-x86_64-unknown-linux-musl  Similar to the wiki example, we will use $HOME/.nix   1cd ~/ 2chmod +x nix-user-chroot 3mkdir -m 0755 ~/.nix 4./nix-user-chroot ~/.nix bash -c \u0026#39;curl https://nixos.org/nix/install | sh\u0026#39;  Only, this doesn\u0026rsquo;t work  Turns out that since unshare is too old, nix-user-chroot won\u0026rsquo;t work either.\nUsing PRoot PRoot is pretty neat in general, they even have a nice website describing it.\n Set a folder up for local installations (this is normally done by my Dotfiles, but we might as well have one here too)   1mkdir -p $HOME/.local/bin 2export PATH=$PATH:$HOME/.local/bin  Get a binary from the GitLab artifacts   1cd $HOME 2mkdir tmp 3cd tmp 4wget -O artifacts.zip https://gitlab.com/proot/proot/-/jobs/452350181/artifacts/download 5unzip artifacts.zip 6mv dist/proot $HOME/.local/bin  Bind and install nix   1mkdir ~/.nix 2export PROOT_NO_SECCOMP=1 3proot -b ~/.nix:/nix 4export PROOT_NO_SECCOMP=1 5curl https://nixos.org/nix/install | sh If you\u0026rsquo;re very unlucky, like I was, you may be greeted by a lovely little error message along the lines of:\n1/nix/store/ddmmzn4ggz1f66lwxjy64n89864yj9w9-nix-2.3.3/bin/nix-store: /opt/ohpc/pub/compiler/gcc/5.4.0/lib64/libstdc++.so.6: version `GLIBCXX_3.4.22\u0026#39; not found (required by /nix/store/c0b76xh2za9r9r4b0g3iv4x2lkw1zzcn-aws-sdk-cpp-1.7.90/lib/libaws-cpp-sdk-core.so) Which basically is as bad as it sounds. At this stage, we need a newer compiler to even get nix up and running, but can\u0026rsquo;t without getting an OS update. This chicken and egg situation calls for the drastic measure of leveraging brew first2.\n1sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Linuxbrew/install/master/install.sh)\u0026#34; Note that nothing in this section suggests the best way is not to lobby your sys-admin to install nix system-wide in multi-user mode.\nGiving Up with Linuxbrew  Somewhere around this point, linuxbrew is a good idea More on this later  Shell Stuff zsh is my shell of choice, and is what my Dotfiles expect and work best with.\n I did end up making a quick change to update the dotfiles with a target which includes a snippet to transition to zsh from the default bash shell  Dotfiles The actual installation steps basically tracks the readme instructions.\n1git clone https://github.com/kobus-v-schoor/dotgit.git 2mkdir -p ~/.bin 3cp -r dotgit/old/bin/dotgit* ~/.bin 4cat dotgit/old/bin/bash_completion \u0026gt;\u0026gt; ~/.bash_completion 5rm -rf dotgit 6# echo \u0026#39;export PATH=\u0026#34;$PATH:$HOME/.bin\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc 7echo \u0026#39;export PATH=\u0026#34;$PATH:$HOME/.bin\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc   Much of this section is directly adapted from the NixOS wiki\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This used to be called linuxbrew, but the new site makes it clear that it\u0026rsquo;s all one brew now.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/prov-dots/","tags":["workflow","projects","hpc"],"title":"Provisioning Dotfiles on an HPC"},{"categories":["personal"],"contents":" Thoughts on and rationale behind leaving QWERTY and touch typing in general. Followed by this post on refactoring my Dotfiles.\n Background I just realized that it has been over two years since I switched from QWERTY to Colemak but somehow never managed to write about it. It was a major change in my life, and it took forever to get acclimatized to. I do not think I\u0026rsquo;ll ever again be in a position to make such a change in my life again, but it was definitely worth it.\nTouch Typing My interest in touch typing in I decided to digitize my notes for posterity, during the last two years of my undergraduate studies back in Harcourt Butler Technical Institute (HBTI) Kanpur, India. in one of my many instances of yak shaving, I realized I could probably consume and annotate a lot more content by typing faster. Given that at that stage I was already a fast talker, it seemed like a natural extension. There was probably an element of nostalgia involved as well. That and the end of a bachelors involves the thesis, which generally involves a lot of typing.\nThere were (and are) some fantastic resources for learning to touch type nowadays, I personally used:\n Typing.com This is short, but a pretty good basic setup. The numbering and special characters are a bit much to take in at the level of practice you get by completing all the previous exercises, but eventually they make for a good workout. TypingClub This is what I ended up working my way through. It is comprehensive, beautiful, and fun.  Also, later, I ended up using keybr a lot, simply because typing gibberish is a good way of practicing, and it is independent of the keyboard layout.\nJust to foreshadow things, the enemy facing me at this point was the layout itself1.\n Alternate layouts Having finally broken into the giddy regimes of 150+ wpm, I was ecstatic, and decided to start working my way through some longer reports. However, I quickly realized I was unable to type for more than a couple of minutes without getting terribly cramped. Once it got to the point of having to visit a physiotherapist, I had to call it quits. At that stage, relearning the entire touch typing corpus, given that I already was used to QWERTY, seemed pretty bleak.\nIt took forever, and I ended up applying my choices to my phone keyboard as well, which presumably helped me in terms of increasing familiarity, had the unintended effect of making me seem distant to people I was close to, since my verbose texts suddenly devolved to painful one-liners.\nThe alternative layouts I tried were:\n DVORAK At the time, TypingClub only supported QWERTY and DVORAK, so it was pretty natural for me to try it out. There are also some very nice comics about it. I remember that it was pretty neat, with a good even distribution, until I tried coding. The placement of the semicolons make it impossible to use while programming. I would still say it makes for a comfortable layout, as long as special characters are not required.    CarpalX I experimented with the entire carpalx family, but I was unable to get used to it. I liked QFMLWY best. I do recommend reading the training methodology, especially if anyone is interested in numerical optimization in general. More importantly, though it was relatively easy to set up on my devices and operating systems, the fact that it wasn\u0026rsquo;t natively supported meant a lot of grief whenever I inevitably had to use a public computer.    Colemak Eventually I decided to go with Colemak, especially since it is widely available. Nothing is easier than setxkbmap us -variant colemak -option grp:alt_shift_toggle on public machines and it\u0026rsquo;s easy on Windows as well. Colemak seems like a good compromise. I personally have not been able to reach the same speeds I managed with QWERTY, even after a year, but then again, I can be a lot more consistent, and it hurts less. Nowadays, Colemak has made its way onto most typing sites as well, including TypingClub   What about VIM?  DVORAK makes it impossible, so do most other layouts, but there are some tutorials purporting to help use vim movement with DVORAK Colemak isn\u0026rsquo;t any better, but the fact of the matter is that once you know VIM on QWERTY, and have separately internalized colemak or something else, hitting keys is just hitting keys  All that said, I still occasionally simply remap HJKL (QWERTY movement) to HNEI (Colemak analog) when it is feasible. update: I actually ended up refactoring my entire Dotfiles to use more Colemak native bindings, as described in this post.\nConclusion Changing layouts was a real struggle. Watching my WPM drop back to lower than hunt and peck styles was pretty humiliating, especially since the reports kept coming in, and more than once I switched to QWERTY. However, since then, I have managed to stay on course. I guess if I think about it, it boils down to a few scattered thoughts:\n Typing is kinda like running a marathon, knowing how it is done and doing it are two different things Tell everyone, so people can listen to you lament your reduced speed and not hate you for replying slowly Practice everyday, because, well, it works out in the long run, even when you plateau Alternate shifts! That\u0026rsquo;s really something which should show up more in tutorials, especially for listicles, not changing the shifts will really hurt Try and get a mechanical keyboard (like the Anne Pro 2 or the Coolermaster Masterkeys), they\u0026rsquo;re fun and easy to change layouts on  Comments The older commenting system was implemented with utteranc.es as seen below.\n   The images are from here, where there\u0026rsquo;s also an effort based metric used to score keyboard layouts.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/colemak-switch/","tags":["workflow","explanations"],"title":"Switching to Colemak"},{"categories":["personal"],"contents":"Background For a while I was worried about writing about a TV show here. I thought it might be frivolous, or worse, might outweigh the other kinds of articles I would like to write. However, like most things, that which is ignored just grows, so it is easier to just write and forget about it.\nThe Show Much has been said about how Bojack Horseman is one of the best shows ever, and they\u0026rsquo;re all correct. For that matter I won\u0026rsquo;t be going into the details of how every episode ties together a tapestry of lives in a meaningful way, or any of that. The show was amazingly poignant. The characters felt real. Which actually leads me to the real issue.\nThe End The end of Bojack was good. It was the way it was meant to be. For a slice-of-life show, it is a natural conclusion. It isn\u0026rsquo;t necessary that any catharsis occurs or that the characters change or become better or all that jazz. It isn\u0026rsquo;t about giving the viewers closure. It is simply about a window onto the lives of (fictional) characters being shut. To that end, I disliked attempts to bring closure in the show itself.\nOne of the main reasons why I felt strongly enough to write this, is simply because when I looked around, the prevailing opinion was that the main character should have been killed off, for his sins. This strikes me as a very flippant attitude to take. It reeks of people trying to make the show a cautionary tale, which is frankly speaking a weird approach to take towards any fictional story. The idea that the character should be redeemed also seemed equally weak, for much the same reasons.\nThe fact that the characters are hypocrites, and that none of them are as good or bad as they make themselves out to be is one of the best parts of the show.\nConclusion That\u0026rsquo;s actually all I have to say about this. I thought of adding relevant memes or listing episodes or name dropping sites, but this isn\u0026rsquo;t buzzfeed. The show is incredible, and there are far better ways of proving that. Bust out your favorite search engine + streaming content provider / digital piracy eye-patch and give it a whirl. The only thing I\u0026rsquo;d suggest is watching everything in order, it\u0026rsquo;s just that kind of show.\n","permalink":"https://rgoswami.me/posts/bojack-horseman/","tags":["thoughts","random","review","TV"],"title":"Bojack Horseman"},{"categories":["programming"],"contents":"Chapter VII - Moving Beyond Linearity All the questions are as per the ISL seventh printing of the First edition1.\nCommon 1libsUsed\u0026lt;-c(\u0026#34;dplyr\u0026#34;,\u0026#34;ggplot2\u0026#34;,\u0026#34;tidyverse\u0026#34;, 2\u0026#34;ISLR\u0026#34;,\u0026#34;caret\u0026#34;,\u0026#34;MASS\u0026#34;, \u0026#34;gridExtra\u0026#34;, 3\u0026#34;pls\u0026#34;,\u0026#34;latex2exp\u0026#34;,\u0026#34;data.table\u0026#34;) 4invisible(lapply(libsUsed, library, character.only = TRUE)) Question 7.6 - Page 299 In this exercise, you will further analyze the Wage data set considered throughout this chapter.\n(a) Perform polynomial regression to predict wage using age. Use cross-validation to select the optimal degree d for the polynomial. What degree was chosen, and how does this compare to the results of hypothesis testing using ANOVA? Make a plot of the resulting polynomial fit to the data.\n(b) Fit a step function to predict wage using age, and perform cross-validation to choose the optimal number of cuts. Make a plot of the fit obtained. In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.\nAnswer Lets get the data.\n1set.seed(1984) 2wageDat\u0026lt;-ISLR::Wage 3wageDat %\u0026gt;% str %\u0026gt;% print 1## \u0026#39;data.frame\u0026#39;: 3000 obs. of 11 variables: 2## $ year : int 2006 2004 2003 2003 2005 2008 2009 2008 2006 2004 ... 3## $ age : int 18 24 45 43 50 54 44 30 41 52 ... 4## $ maritl : Factor w/ 5 levels \u0026#34;1. Never Married\u0026#34;,..: 1 1 2 2 4 2 2 1 1 2 ... 5## $ race : Factor w/ 4 levels \u0026#34;1. White\u0026#34;,\u0026#34;2. Black\u0026#34;,..: 1 1 1 3 1 1 4 3 2 1 ... 6## $ education : Factor w/ 5 levels \u0026#34;1. \u0026lt; HS Grad\u0026#34;,..: 1 4 3 4 2 4 3 3 3 2 ... 7## $ region : Factor w/ 9 levels \u0026#34;1. New England\u0026#34;,..: 2 2 2 2 2 2 2 2 2 2 ... 8## $ jobclass : Factor w/ 2 levels \u0026#34;1. Industrial\u0026#34;,..: 1 2 1 2 2 2 1 2 2 2 ... 9## $ health : Factor w/ 2 levels \u0026#34;1. \u0026lt;=Good\u0026#34;,\u0026#34;2. \u0026gt;=Very Good\u0026#34;: 1 2 1 2 1 2 2 1 2 2 ... 10## $ health_ins: Factor w/ 2 levels \u0026#34;1. Yes\u0026#34;,\u0026#34;2. No\u0026#34;: 2 2 1 1 1 1 1 1 1 1 ... 11## $ logwage : num 4.32 4.26 4.88 5.04 4.32 ... 12## $ wage : num 75 70.5 131 154.7 75 ... 13## NULL 1wageDat %\u0026gt;% summary %\u0026gt;% print 1## year age maritl race 2## Min. :2003 Min. :18.00 1. Never Married: 648 1. White:2480 3## 1st Qu.:2004 1st Qu.:33.75 2. Married :2074 2. Black: 293 4## Median :2006 Median :42.00 3. Widowed : 19 3. Asian: 190 5## Mean :2006 Mean :42.41 4. Divorced : 204 4. Other: 37 6## 3rd Qu.:2008 3rd Qu.:51.00 5. Separated : 55 7## Max. :2009 Max. :80.00 8## 9## education region jobclass 10## 1. \u0026lt; HS Grad :268 2. Middle Atlantic :3000 1. Industrial :1544 11## 2. HS Grad :971 1. New England : 0 2. Information:1456 12## 3. Some College :650 3. East North Central: 0 13## 4. College Grad :685 4. West North Central: 0 14## 5. Advanced Degree:426 5. South Atlantic : 0 15## 6. East South Central: 0 16## (Other) : 0 17## health health_ins logwage wage 18## 1. \u0026lt;=Good : 858 1. Yes:2083 Min. :3.000 Min. : 20.09 19## 2. \u0026gt;=Very Good:2142 2. No : 917 1st Qu.:4.447 1st Qu.: 85.38 20## Median :4.653 Median :104.92 21## Mean :4.654 Mean :111.70 22## 3rd Qu.:4.857 3rd Qu.:128.68 23## Max. :5.763 Max. :318.34 24## 1wageDat %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) %\u0026gt;% print 1## year age maritl race education region jobclass 2## 7 61 5 4 5 1 2 3## health health_ins logwage wage 4## 2 2 508 508 1library(boot) 1## 2## Attaching package: \u0026#39;boot\u0026#39; 1## The following object is masked from \u0026#39;package:lattice\u0026#39;: 2## 3## melanoma a) Polynomial regression 1all.deltas = rep(NA, 10) 2for (i in 1:10) { 3glm.fit = glm(wage~poly(age, i), data=Wage) 4all.deltas[i] = cv.glm(Wage, glm.fit, K=10)$delta[2] 5} 6plot(1:10, all.deltas, xlab=\u0026#34;Degree\u0026#34;, ylab=\u0026#34;CV error\u0026#34;, type=\u0026#34;l\u0026#34;, pch=20, lwd=2, ylim=c(1590, 1700)) 7min.point = min(all.deltas) 8sd.points = sd(all.deltas) 9abline(h=min.point + 0.2 * sd.points, col=\u0026#34;red\u0026#34;, lty=\u0026#34;dashed\u0026#34;) 10abline(h=min.point - 0.2 * sd.points, col=\u0026#34;red\u0026#34;, lty=\u0026#34;dashed\u0026#34;) 11legend(\u0026#34;topright\u0026#34;, \u0026#34;0.2-standard deviation lines\u0026#34;, lty=\u0026#34;dashed\u0026#34;, col=\u0026#34;red\u0026#34;)  1# ANOVA 2fits=list() 3for (i in 1:10) { 4fits[[i]]=glm(wage~poly(age,i),data=wageDat) 5} 6anova(fits[[1]],fits[[2]],fits[[3]],fits[[4]],fits[[5]], 7fits[[6]],fits[[7]],fits[[8]],fits[[9]],fits[[10]]) 1## Analysis of Deviance Table 2## 3## Model 1: wage ~ poly(age, i) 4## Model 2: wage ~ poly(age, i) 5## Model 3: wage ~ poly(age, i) 6## Model 4: wage ~ poly(age, i) 7## Model 5: wage ~ poly(age, i) 8## Model 6: wage ~ poly(age, i) 9## Model 7: wage ~ poly(age, i) 10## Model 8: wage ~ poly(age, i) 11## Model 9: wage ~ poly(age, i) 12## Model 10: wage ~ poly(age, i) 13## Resid. Df Resid. Dev Df Deviance 14## 1 2998 5022216 15## 2 2997 4793430 1 228786 16## 3 2996 4777674 1 15756 17## 4 2995 4771604 1 6070 18## 5 2994 4770322 1 1283 19## 6 2993 4766389 1 3932 20## 7 2992 4763834 1 2555 21## 8 2991 4763707 1 127 22## 9 2990 4756703 1 7004 23## 10 2989 4756701 1 3  The 4th degree looks the best at the moment   1# 3rd or 4th degrees look best based on ANOVA test 2# let\u0026#39;s go with 4th degree fit 3plot(wage~age, data=wageDat, col=\u0026#34;darkgrey\u0026#34;) 4agelims = range(wageDat$age) 5age.grid = seq(from=agelims[1], to=agelims[2]) 6lm.fit = lm(wage~poly(age, 4), data=wageDat) 7lm.pred = predict(lm.fit, data.frame(age=age.grid)) 8lines(age.grid, lm.pred, col=\u0026#34;blue\u0026#34;, lwd=2)  b) Step function and cross-validation 1# cross-validation 2cv.error \u0026lt;- rep(0,9) 3for (i in 2:10) { 4wageDat$age.cut \u0026lt;- cut(wageDat$age,i) 5glm.fit \u0026lt;- glm(wage~age.cut, data=wageDat) 6cv.error[i-1] \u0026lt;- cv.glm(wageDat, glm.fit, K=10)$delta[1] # [1]:std, [2]:bias-corrected 7} 8cv.error 1## [1] 1732.337 1682.978 1636.736 1635.600 1624.174 1610.688 1604.081 1612.005 2## [9] 1607.022 1cv.error 1## [1] 1732.337 1682.978 1636.736 1635.600 1624.174 1610.688 1604.081 1612.005 2## [9] 1607.022 1plot(2:10, cv.error, type=\u0026#34;b\u0026#34;)  1cut.fit \u0026lt;- glm(wage~cut(age,8), data=wageDat) 2preds \u0026lt;- predict(cut.fit, newdata=list(age=age.grid), se=TRUE) 3se.bands \u0026lt;- preds$fit + cbind(2*preds$se.fit, -2*preds$se.fit) 4plot(wageDat$age, wageDat$wage, xlim=agelims, cex=0.5, col=\u0026#34;darkgrey\u0026#34;) 5title(\u0026#34;Fit with 8 Age Bands\u0026#34;) 6lines(age.grid, preds$fit, lwd=2, col=\u0026#34;blue\u0026#34;) 7matlines(age.grid, se.bands, lwd=1, col=\u0026#34;blue\u0026#34;, lty=3)  Question 7.8 - Page 299 Fit some of the non-linear models investigated in this chapter to the Auto data set. Is there evidence for non-linear relationships in this data set? Create some informative plots to justify your answer.\nAnswer 1autoDat\u0026lt;-ISLR::Auto 1autoDat %\u0026gt;% pivot_longer(-c(mpg,name),names_to=\u0026#34;Params\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=mpg,y=Value)) + 2geom_point() + 3facet_wrap(~ Params, scales = \u0026#34;free_y\u0026#34;)  Very clearly there is a lot of non-linearity in the mpg data, especially for acceleration, weight, displacement, horsepower.\n1rss = rep(NA, 10) 2fits = list() 3for (d in 1:10) { 4fits[[d]] = lm(mpg ~ poly(displacement, d), data = autoDat) 5rss[d] = deviance(fits[[d]]) 6} 7rss %\u0026gt;% print 1## [1] 8378.822 7412.263 7392.322 7391.722 7380.838 7270.746 7089.716 6917.401 2## [9] 6737.801 6610.190 1anova(fits[[1]],fits[[2]],fits[[3]],fits[[4]],fits[[5]], 2fits[[6]],fits[[7]],fits[[8]],fits[[9]],fits[[10]]) 1## Analysis of Variance Table 2## 3## Model 1: mpg ~ poly(displacement, d) 4## Model 2: mpg ~ poly(displacement, d) 5## Model 3: mpg ~ poly(displacement, d) 6## Model 4: mpg ~ poly(displacement, d) 7## Model 5: mpg ~ poly(displacement, d) 8## Model 6: mpg ~ poly(displacement, d) 9## Model 7: mpg ~ poly(displacement, d) 10## Model 8: mpg ~ poly(displacement, d) 11## Model 9: mpg ~ poly(displacement, d) 12## Model 10: mpg ~ poly(displacement, d) 13## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) 14## 1 390 8378.8 15## 2 389 7412.3 1 966.56 55.7108 5.756e-13 *** 16## 3 388 7392.3 1 19.94 1.1494 0.284364 17## 4 387 7391.7 1 0.60 0.0346 0.852549 18## 5 386 7380.8 1 10.88 0.6273 0.428823 19## 6 385 7270.7 1 110.09 6.3455 0.012177 * 20## 7 384 7089.7 1 181.03 10.4343 0.001344 ** 21## 8 383 6917.4 1 172.31 9.9319 0.001753 ** 22## 9 382 6737.8 1 179.60 10.3518 0.001404 ** 23## 10 381 6610.2 1 127.61 7.3553 0.006990 ** 24## --- 25## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Confirming our visual indications, we see that the second degree models work well.\n1library(glmnet) 1## Loading required package: Matrix 1## 2## Attaching package: \u0026#39;Matrix\u0026#39; 1## The following objects are masked from \u0026#39;package:tidyr\u0026#39;: 2## 3## expand, pack, unpack 1## Loaded glmnet 3.0-2 1library(boot) 1cv.errs = rep(NA, 15) 2for (d in 1:15) { 3fit = glm(mpg ~ poly(displacement, d), data = Auto) 4cv.errs[d] = cv.glm(Auto, fit, K = 15)$delta[2] 5} 6which.min(cv.errs) 1## [1] 10 Strangely, we seem to have ended up with a ten variable model here.\n1# Step functions 2cv.errs = rep(NA, 10) 3for (c in 2:10) { 4Auto$dis.cut = cut(Auto$displacement, c) 5fit = glm(mpg ~ dis.cut, data = Auto) 6cv.errs[c] = cv.glm(Auto, fit, K = 10)$delta[2] 7} 8which.min(cv.errs) %\u0026gt;% print 1## [1] 9 1library(splines) 2cv.errs = rep(NA, 10) 3for (df in 3:10) { 4fit = glm(mpg ~ ns(displacement, df = df), data = Auto) 5cv.errs[df] = cv.glm(Auto, fit, K = 10)$delta[2] 6} 7which.min(cv.errs) %\u0026gt;% print 1## [1] 10 1library(gam) 1## Loading required package: foreach 1## 2## Attaching package: \u0026#39;foreach\u0026#39; 1## The following objects are masked from \u0026#39;package:purrr\u0026#39;: 2## 3## accumulate, when 1## Loaded gam 1.16.1 1# GAMs 2fit = gam(mpg ~ s(displacement, 4) + s(horsepower, 4), data = Auto) 1## Warning in model.matrix.default(mt, mf, contrasts): non-list contrasts argument 2## ignored 1summary(fit) 1## 2## Call: gam(formula = mpg ~ s(displacement, 4) + s(horsepower, 4), data = Auto) 3## Deviance Residuals: 4## Min 1Q Median 3Q Max 5## -11.2982 -2.1592 -0.4394 2.1247 17.0946 6## 7## (Dispersion Parameter for gaussian family taken to be 15.3543) 8## 9## Null Deviance: 23818.99 on 391 degrees of freedom 10## Residual Deviance: 5880.697 on 382.9999 degrees of freedom 11## AIC: 2194.05 12## 13## Number of Local Scoring Iterations: 2 14## 15## Anova for Parametric Effects 16## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) 17## s(displacement, 4) 1 15254.9 15254.9 993.524 \u0026lt; 2e-16 *** 18## s(horsepower, 4) 1 1038.4 1038.4 67.632 3.1e-15 *** 19## Residuals 383 5880.7 15.4 20## --- 21## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 22## 23## Anova for Nonparametric Effects 24## Npar Df Npar F Pr(F) 25## (Intercept) 26## s(displacement, 4) 3 13.613 1.863e-08 *** 27## s(horsepower, 4) 3 15.606 1.349e-09 *** 28## --- 29## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Question 7.9 - Pages 299-300 This question uses the variables dis (the weighted mean of distances to five Boston employment centers) and nox (nitrogen oxides concentration in parts per 10 million) from the Boston data. We will treat dis as the predictor and nox as the response.\n(a) Use the poly() function to fit a cubic polynomial regression to predict nox using dis. Report the regression output, and plot the resulting data and polynomial fits.\n(b) Plot the polynomial fits for a range of different polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares.\n(c) Perform cross-validation or another approach to select the optimal degree for the polynomial, and explain your results.\n(d) Use the bs() function to fit a regression spline to predict nox using dis. Report the output for the fit using four degrees of freedom. How did you choose the knots? Plot the resulting fit.\n(e) Now fit a regression spline for a range of degrees of freedom, and plot the resulting fits and report the resulting RSS. Describe the results obtained.\n(f) Perform cross-validation or another approach in order to select the best degrees of freedom for a regression spline on this data. Describe your results.\nAnswer 1boston\u0026lt;-MASS::Boston 2boston %\u0026gt;% str %\u0026gt;% print 1## \u0026#39;data.frame\u0026#39;: 506 obs. of 14 variables: 2## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... 3## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... 4## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... 5## $ chas : int 0 0 0 0 0 0 0 0 0 0 ... 6## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... 7## $ rm : num 6.58 6.42 7.18 7 7.15 ... 8## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... 9## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... 10## $ rad : int 1 2 2 3 3 3 5 5 5 5 ... 11## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... 12## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... 13## $ black : num 397 397 393 395 397 ... 14## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... 15## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... 16## NULL 1boston %\u0026gt;% summary %\u0026gt;% print 1## crim zn indus chas 2## Min. : 0.00632 Min. : 0.00 Min. : 0.46 Min. :0.00000 3## 1st Qu.: 0.08204 1st Qu.: 0.00 1st Qu.: 5.19 1st Qu.:0.00000 4## Median : 0.25651 Median : 0.00 Median : 9.69 Median :0.00000 5## Mean : 3.61352 Mean : 11.36 Mean :11.14 Mean :0.06917 6## 3rd Qu.: 3.67708 3rd Qu.: 12.50 3rd Qu.:18.10 3rd Qu.:0.00000 7## Max. :88.97620 Max. :100.00 Max. :27.74 Max. :1.00000 8## nox rm age dis 9## Min. :0.3850 Min. :3.561 Min. : 2.90 Min. : 1.130 10## 1st Qu.:0.4490 1st Qu.:5.886 1st Qu.: 45.02 1st Qu.: 2.100 11## Median :0.5380 Median :6.208 Median : 77.50 Median : 3.207 12## Mean :0.5547 Mean :6.285 Mean : 68.57 Mean : 3.795 13## 3rd Qu.:0.6240 3rd Qu.:6.623 3rd Qu.: 94.08 3rd Qu.: 5.188 14## Max. :0.8710 Max. :8.780 Max. :100.00 Max. :12.127 15## rad tax ptratio black 16## Min. : 1.000 Min. :187.0 Min. :12.60 Min. : 0.32 17## 1st Qu.: 4.000 1st Qu.:279.0 1st Qu.:17.40 1st Qu.:375.38 18## Median : 5.000 Median :330.0 Median :19.05 Median :391.44 19## Mean : 9.549 Mean :408.2 Mean :18.46 Mean :356.67 20## 3rd Qu.:24.000 3rd Qu.:666.0 3rd Qu.:20.20 3rd Qu.:396.23 21## Max. :24.000 Max. :711.0 Max. :22.00 Max. :396.90 22## lstat medv 23## Min. : 1.73 Min. : 5.00 24## 1st Qu.: 6.95 1st Qu.:17.02 25## Median :11.36 Median :21.20 26## Mean :12.65 Mean :22.53 27## 3rd Qu.:16.95 3rd Qu.:25.00 28## Max. :37.97 Max. :50.00 1boston %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) %\u0026gt;% print 1## crim zn indus chas nox rm age dis rad tax 2## 504 26 76 2 81 446 356 412 9 66 3## ptratio black lstat medv 4## 46 357 455 229 a) Polynomial 1fit.03 \u0026lt;- lm(nox~poly(dis,3), data=boston) 2dislims \u0026lt;- range(boston$dis) 3dis.grid \u0026lt;- seq(dislims[1], dislims[2], 0.1) 4preds \u0026lt;- predict(fit.03, newdata=list(dis=dis.grid), se=TRUE) 5se.bands \u0026lt;- preds$fit + cbind(2*preds$se.fit, -2*preds$se.fit) 6par(mfrow=c(1,1), mar=c(4.5,4.5,1,1), oma=c(0,0,4,0)) 7plot(boston$dis, boston$nox, xlim=dislims, cex=0.5, col=\u0026#34;darkgrey\u0026#34;) 8title(\u0026#34;Degree 3 Polynomial Fit\u0026#34;) 9lines(dis.grid, preds$fit, lwd=2, col=\u0026#34;blue\u0026#34;) 10matlines(dis.grid, se.bands, lwd=1, col=\u0026#34;blue\u0026#34;, lty=3)  1summary(fit.03) 1## 2## Call: 3## lm(formula = nox ~ poly(dis, 3), data = boston) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -0.121130 -0.040619 -0.009738 0.023385 0.194904 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 0.554695 0.002759 201.021 \u0026lt; 2e-16 *** 12## poly(dis, 3)1 -2.003096 0.062071 -32.271 \u0026lt; 2e-16 *** 13## poly(dis, 3)2 0.856330 0.062071 13.796 \u0026lt; 2e-16 *** 14## poly(dis, 3)3 -0.318049 0.062071 -5.124 4.27e-07 *** 15## --- 16## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 17## 18## Residual standard error: 0.06207 on 502 degrees of freedom 19## Multiple R-squared: 0.7148, Adjusted R-squared: 0.7131 20## F-statistic: 419.3 on 3 and 502 DF, p-value: \u0026lt; 2.2e-16 b) Multiple Polynomials 1rss.error \u0026lt;- rep(0,10) 2for (i in 1:10) { 3lm.fit \u0026lt;- lm(nox~poly(dis,i), data=boston) 4rss.error[i] \u0026lt;- sum(lm.fit$residuals^2) 5} 6rss.error 1## [1] 2.768563 2.035262 1.934107 1.932981 1.915290 1.878257 1.849484 1.835630 2## [9] 1.833331 1.832171 1plot(rss.error, type=\u0026#34;b\u0026#34;)  c) Cross validation and polynomial selection 1require(boot) 2set.seed(1) 3cv.error \u0026lt;- rep(0,10) 4for (i in 1:10) { 5glm.fit \u0026lt;- glm(nox~poly(dis,i), data=boston) 6cv.error[i] \u0026lt;- cv.glm(boston, glm.fit, K=10)$delta[1] # [1]:std, [2]:bias-corrected 7} 8cv.error 1## [1] 0.005558263 0.004085706 0.003876521 0.003863342 0.004237452 0.005686862 2## [7] 0.010278897 0.006810868 0.033308607 0.004075599 1plot(cv.error, type=\u0026#34;b\u0026#34;)   I feel like the second degree fit would be the most reasonable, though the fourth degree seems to be doing well.  d) Regression spline 1fit.sp \u0026lt;- lm(nox~bs(dis, df=4), data=boston) 2pred \u0026lt;- predict(fit.sp, newdata=list(dis=dis.grid), se=T) 3plot(boston$dis, boston$nox, col=\u0026#34;gray\u0026#34;) 4lines(dis.grid, pred$fit, lwd=2) 5lines(dis.grid, pred$fit+2*pred$se, lty=\u0026#34;dashed\u0026#34;) 6lines(dis.grid, pred$fit-2*pred$se, lty=\u0026#34;dashed\u0026#34;)  1# set df to select knots at uniform quantiles of `dis` 2attr(bs(boston$dis,df=4),\u0026#34;knots\u0026#34;) # only 1 knot at 50th percentile 1## 50% 2## 3.20745 e) Range of regression splines 1rss.error \u0026lt;- rep(0,7) 2for (i in 4:10) { 3fit.sp \u0026lt;- lm(nox~bs(dis, df=i), data=boston) 4rss.error[i-3] \u0026lt;- sum(fit.sp$residuals^2) 5} 6rss.error 1## [1] 1.922775 1.840173 1.833966 1.829884 1.816995 1.825653 1.792535 1plot(4:10, rss.error, type=\u0026#34;b\u0026#34;)   As the model gains more degrees of freedom, it tends to over fit to the training data better  f) Cross validation for best spline 1cv.error \u0026lt;- rep(0,7) 2for (i in 4:10) { 3glm.fit \u0026lt;- glm(nox~bs(dis, df=i), data=boston) 4cv.error[i-3] \u0026lt;- cv.glm(boston, glm.fit, K=10)$delta[1] 5} 1## Warning in bs(dis, degree = 3L, knots = c(`50%` = 3.1523), Boundary.knots = 2## c(1.1296, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned 3## bases 45## Warning in bs(dis, degree = 3L, knots = c(`50%` = 3.1523), Boundary.knots = 6## c(1.1296, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned 7## bases 1## Warning in bs(dis, degree = 3L, knots = c(`50%` = 3.2157), Boundary.knots = 2## c(1.137, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`50%` = 3.2157), Boundary.knots = 5## c(1.137, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`33.33333%` = 2.35953333333333, : some 2## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`33.33333%` = 2.35953333333333, : some 5## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`33.33333%` = 2.38403333333333, : some 2## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`33.33333%` = 2.38403333333333, : some 5## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`25%` = 2.07945, `50%` = 3.1323, : 2## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`25%` = 2.07945, `50%` = 3.1323, : 5## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`25%` = 2.1103, `50%` = 3.2797, : some 2## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`25%` = 2.1103, `50%` = 3.2797, : some 5## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`20%` = 1.9682, `40%` = 2.7147, : some 2## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`20%` = 1.9682, `40%` = 2.7147, : some 5## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`20%` = 1.95434, `40%` = 2.59666, : 2## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`20%` = 1.95434, `40%` = 2.59666, : 5## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`16.66667%` = 1.82203333333333, : some 2## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`16.66667%` = 1.82203333333333, : some 5## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`16.66667%` = 1.8226, `33.33333%` = 2## 2.3817, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`16.66667%` = 1.8226, `33.33333%` = 5## 2.3817, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`14.28571%` = 1.7936, `28.57143%` 2## = 2.16972857142857, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill- 3## conditioned bases 45## Warning in bs(dis, degree = 3L, knots = c(`14.28571%` = 1.7936, `28.57143%` 6## = 2.16972857142857, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill- 7## conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`12.5%` = 1.754625, `25%` = 2.10215, : 2## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`12.5%` = 1.754625, `25%` = 2.10215, : 5## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`12.5%` = 1.751575, `25%` = 2.08755, : 2## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`12.5%` = 1.751575, `25%` = 2.08755, : 5## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1cv.error 1## [1] 0.003898810 0.003694675 0.003732665 0.003766202 0.003716389 0.003723126 2## [7] 0.003727358 1plot(4:10, cv.error, type=\u0026#34;b\u0026#34;)   A fifth degree polynomial is clearly indicated  Question 10 - Page 300 This question relates to the College data set.\n(a) Split the data into a training set and a test set. Using out-of-state tuition as the response and the other variables as the predictors, perform forward stepwise selection on the training set in order to identify a satisfactory model that uses just a subset of the predictors.\n(b) Fit a GAM on the training data, using out-of-state tuition as the response and the features selected in the previous step as the predictors. Plot the results, and explain your ﬁndings.\n(c) Evaluate the model obtained on the test set, and explain the results obtained.\n(d) For which variables, if any, is there evidence of a non-linear relationship with the response?\nAnswer 1colDat\u0026lt;-ISLR::College 2colDat %\u0026gt;% str %\u0026gt;% print 1## \u0026#39;data.frame\u0026#39;: 777 obs. of 18 variables: 2## $ Private : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 2 2 2 2 2 2 2 2 ... 3## $ Apps : num 1660 2186 1428 417 193 ... 4## $ Accept : num 1232 1924 1097 349 146 ... 5## $ Enroll : num 721 512 336 137 55 158 103 489 227 172 ... 6## $ Top10perc : num 23 16 22 60 16 38 17 37 30 21 ... 7## $ Top25perc : num 52 29 50 89 44 62 45 68 63 44 ... 8## $ F.Undergrad: num 2885 2683 1036 510 249 ... 9## $ P.Undergrad: num 537 1227 99 63 869 ... 10## $ Outstate : num 7440 12280 11250 12960 7560 ... 11## $ Room.Board : num 3300 6450 3750 5450 4120 ... 12## $ Books : num 450 750 400 450 800 500 500 450 300 660 ... 13## $ Personal : num 2200 1500 1165 875 1500 ... 14## $ PhD : num 70 29 53 92 76 67 90 89 79 40 ... 15## $ Terminal : num 78 30 66 97 72 73 93 100 84 41 ... 16## $ S.F.Ratio : num 18.1 12.2 12.9 7.7 11.9 9.4 11.5 13.7 11.3 11.5 ... 17## $ perc.alumni: num 12 16 30 37 2 11 26 37 23 15 ... 18## $ Expend : num 7041 10527 8735 19016 10922 ... 19## $ Grad.Rate : num 60 56 54 59 15 55 63 73 80 52 ... 20## NULL 1colDat %\u0026gt;% summary %\u0026gt;% print 1## Private Apps Accept Enroll Top10perc 2## No :212 Min. : 81 Min. : 72 Min. : 35 Min. : 1.00 3## Yes:565 1st Qu.: 776 1st Qu.: 604 1st Qu.: 242 1st Qu.:15.00 4## Median : 1558 Median : 1110 Median : 434 Median :23.00 5## Mean : 3002 Mean : 2019 Mean : 780 Mean :27.56 6## 3rd Qu.: 3624 3rd Qu.: 2424 3rd Qu.: 902 3rd Qu.:35.00 7## Max. :48094 Max. :26330 Max. :6392 Max. :96.00 8## Top25perc F.Undergrad P.Undergrad Outstate 9## Min. : 9.0 Min. : 139 Min. : 1.0 Min. : 2340 10## 1st Qu.: 41.0 1st Qu.: 992 1st Qu.: 95.0 1st Qu.: 7320 11## Median : 54.0 Median : 1707 Median : 353.0 Median : 9990 12## Mean : 55.8 Mean : 3700 Mean : 855.3 Mean :10441 13## 3rd Qu.: 69.0 3rd Qu.: 4005 3rd Qu.: 967.0 3rd Qu.:12925 14## Max. :100.0 Max. :31643 Max. :21836.0 Max. :21700 15## Room.Board Books Personal PhD 16## Min. :1780 Min. : 96.0 Min. : 250 Min. : 8.00 17## 1st Qu.:3597 1st Qu.: 470.0 1st Qu.: 850 1st Qu.: 62.00 18## Median :4200 Median : 500.0 Median :1200 Median : 75.00 19## Mean :4358 Mean : 549.4 Mean :1341 Mean : 72.66 20## 3rd Qu.:5050 3rd Qu.: 600.0 3rd Qu.:1700 3rd Qu.: 85.00 21## Max. :8124 Max. :2340.0 Max. :6800 Max. :103.00 22## Terminal S.F.Ratio perc.alumni Expend 23## Min. : 24.0 Min. : 2.50 Min. : 0.00 Min. : 3186 24## 1st Qu.: 71.0 1st Qu.:11.50 1st Qu.:13.00 1st Qu.: 6751 25## Median : 82.0 Median :13.60 Median :21.00 Median : 8377 26## Mean : 79.7 Mean :14.09 Mean :22.74 Mean : 9660 27## 3rd Qu.: 92.0 3rd Qu.:16.50 3rd Qu.:31.00 3rd Qu.:10830 28## Max. :100.0 Max. :39.80 Max. :64.00 Max. :56233 29## Grad.Rate 30## Min. : 10.00 31## 1st Qu.: 53.00 32## Median : 65.00 33## Mean : 65.46 34## 3rd Qu.: 78.00 35## Max. :118.00 1colDat %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) %\u0026gt;% print 1## Private Apps Accept Enroll Top10perc Top25perc 2## 2 711 693 581 82 89 3## F.Undergrad P.Undergrad Outstate Room.Board Books Personal 4## 714 566 640 553 122 294 5## PhD Terminal S.F.Ratio perc.alumni Expend Grad.Rate 6## 78 65 173 61 744 81 1plotLEAP=function(leapObj){ 2par(mfrow = c(2,2)) 3bar2=which.max(leapObj$adjr2) 4bbic=which.min(leapObj$bic) 5bcp=which.min(leapObj$cp) 6plot(leapObj$rss,xlab=\u0026#34;Number of variables\u0026#34;,ylab=\u0026#34;RSS\u0026#34;,type=\u0026#34;b\u0026#34;) 7plot(leapObj$adjr2,xlab=\u0026#34;Number of variables\u0026#34;,ylab=TeX(\u0026#34;Adjusted R^2\u0026#34;),type=\u0026#34;b\u0026#34;) 8points(bar2,leapObj$adjr2[bar2],col=\u0026#34;green\u0026#34;,cex=2,pch=20) 9plot(leapObj$bic,xlab=\u0026#34;Number of variables\u0026#34;,ylab=TeX(\u0026#34;BIC\u0026#34;),type=\u0026#34;b\u0026#34;) 10points(bbic,leapObj$bic[bbic],col=\u0026#34;blue\u0026#34;,cex=2,pch=20) 11plot(leapObj$cp,xlab=\u0026#34;Number of variables\u0026#34;,ylab=TeX(\u0026#34;C_p\u0026#34;),type=\u0026#34;b\u0026#34;) 12points(bcp,leapObj$cp[bcp],col=\u0026#34;red\u0026#34;,cex=2,pch=20) 13} a) Train test 1train_ind = sample(colDat %\u0026gt;% nrow,100) 2test_ind = setdiff(seq_len(colDat %\u0026gt;% nrow), train_ind) Best subset selection 1train_set\u0026lt;-colDat[train_ind,] 2test_set\u0026lt;-colDat[-train_ind,] 1library(leaps) 1modelFit\u0026lt;-regsubsets(Outstate~.,data=colDat,nvmax=20) 2modelFit %\u0026gt;% summary %\u0026gt;% print 1## Subset selection object 2## Call: regsubsets.formula(Outstate ~ ., data = colDat, nvmax = 20) 3## 17 Variables (and intercept) 4## Forced in Forced out 5## PrivateYes FALSE FALSE 6## Apps FALSE FALSE 7## Accept FALSE FALSE 8## Enroll FALSE FALSE 9## Top10perc FALSE FALSE 10## Top25perc FALSE FALSE 11## F.Undergrad FALSE FALSE 12## P.Undergrad FALSE FALSE 13## Room.Board FALSE FALSE 14## Books FALSE FALSE 15## Personal FALSE FALSE 16## PhD FALSE FALSE 17## Terminal FALSE FALSE 18## S.F.Ratio FALSE FALSE 19## perc.alumni FALSE FALSE 20## Expend FALSE FALSE 21## Grad.Rate FALSE FALSE 22## 1 subsets of each size up to 17 23## Selection Algorithm: exhaustive 24## PrivateYes Apps Accept Enroll Top10perc Top25perc F.Undergrad 25## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 26## 2 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 27## 3 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 28## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 29## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 30## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 31## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 32## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 33## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 34## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 35## 11 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 36## 12 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 37## 13 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 38## 14 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 39## 15 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 40## 16 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 41## 17 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 42## P.Undergrad Room.Board Books Personal PhD Terminal S.F.Ratio 43## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 44## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 45## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 46## 4 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 47## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 48## 6 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 49## 7 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 50## 8 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 51## 9 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 52## 10 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 53## 11 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 54## 12 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 55## 13 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 56## 14 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 57## 15 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 58## 16 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 59## 17 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 60## perc.alumni Expend Grad.Rate 61## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 62## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 63## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 64## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 65## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 66## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 67## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 68## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 69## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 70## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 71## 11 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 72## 12 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 73## 13 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 74## 14 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 75## 15 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 76## 16 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 77## 17 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; We might want to take a look at these.\n1par(mfrow=c(2,2)) 2plot(modelFit) 3plot(modelFit,scale=\u0026#39;Cp\u0026#39;) 4plot(modelFit,scale=\u0026#39;r2\u0026#39;) 5plot(modelFit,scale=\u0026#39;adjr2\u0026#39;)  1plotLEAP(modelFit %\u0026gt;% summary)   So we like 14 variables, namely   1coefficients(modelFit,id=14) %\u0026gt;% print 1## (Intercept) PrivateYes Apps Accept Enroll 2## -1.817040e+03 2.256946e+03 -2.999022e-01 8.023519e-01 -5.372545e-01 3## Top10perc F.Undergrad Room.Board Personal PhD 4## 2.365529e+01 -9.569936e-02 8.741819e-01 -2.478418e-01 1.269506e+01 5## Terminal S.F.Ratio perc.alumni Expend Grad.Rate 6## 2.297296e+01 -4.700560e+01 4.195006e+01 2.003912e-01 2.383197e+01  But five seems like a better bet.   1coefficients(modelFit,id=5) 1## (Intercept) PrivateYes Room.Board PhD perc.alumni 2## -2864.6325619 2936.7416766 1.0677573 40.5334088 61.3147684 3## Expend 4## 0.2253945 b) GAM 1library(gam) 1fit = gam(Outstate ~ Private+s(Apps,3)+Accept+Enroll+ 2Top10perc+F.Undergrad+Room.Board+ 3Personal+PhD+Terminal+S.F.Ratio+ 4perc.alumni+Expend+Grad.Rate 5, data = colDat) 1## Warning in model.matrix.default(mt, mf, contrasts): non-list contrasts argument 2## ignored 1fit2 = gam(Outstate ~ Private+s(Room.Board,2)+s(PhD,3)+s(perc.alumni)+Expend 2, data = colDat) 1## Warning in model.matrix.default(mt, mf, contrasts): non-list contrasts argument 2## ignored 1par(mfrow=c(2,2)) 2plot(fit,se=TRUE) 1par(mfrow=c(2,2)) 2plot(fit2,se=TRUE)  c) Evaluate 1pred \u0026lt;- predict(fit, test_set) 2mse.error \u0026lt;- mean((test_set$Outstate - pred)^2) 3mse.error %\u0026gt;% print 1## [1] 3691891 1gam.tss = mean((test_set$Outstate - mean(test_set$Outstate))^2) 2test.rss = 1 - mse.error/gam.tss 3test.rss %\u0026gt;% print 1## [1] 0.7731239 1pred2 \u0026lt;- predict(fit2, test_set) 2mse.error2 \u0026lt;- mean((test_set$Outstate - pred2)^2) 3mse.error2 %\u0026gt;% print 1## [1] 4121902 1gam.tss2 = mean((test_set$Outstate - mean(test_set$Outstate))^2) 2test.rss2 = 1 - mse.error2/gam.tss2 3test.rss2 %\u0026gt;% print 1## [1] 0.7466987 This is pretty good model, all told.\nd) Summary 1summary(fit) %\u0026gt;% print 1## 2## Call: gam(formula = Outstate ~ Private + s(Apps, 3) + Accept + Enroll + 3## Top10perc + F.Undergrad + Room.Board + Personal + PhD + Terminal + 4## S.F.Ratio + perc.alumni + Expend + Grad.Rate, data = colDat) 5## Deviance Residuals: 6## Min 1Q Median 3Q Max 7## -6641.083 -1262.806 -5.698 1270.911 9965.901 8## 9## (Dispersion Parameter for gaussian family taken to be 3749048) 10## 11## Null Deviance: 12559297426 on 776 degrees of freedom 12## Residual Deviance: 2849276343 on 760 degrees of freedom 13## AIC: 13985.3 14## 15## Number of Local Scoring Iterations: 2 16## 17## Anova for Parametric Effects 18## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) 19## Private 1 4034912907 4034912907 1076.250 \u0026lt; 2.2e-16 *** 20## s(Apps, 3) 1 1344548030 1344548030 358.637 \u0026lt; 2.2e-16 *** 21## Accept 1 90544274 90544274 24.151 1.091e-06 *** 22## Enroll 1 144471570 144471570 38.535 8.838e-10 *** 23## Top10perc 1 1802244831 1802244831 480.721 \u0026lt; 2.2e-16 *** 24## F.Undergrad 1 45230645 45230645 12.065 0.0005430 *** 25## Room.Board 1 1110285773 1110285773 296.151 \u0026lt; 2.2e-16 *** 26## Personal 1 47886988 47886988 12.773 0.0003738 *** 27## PhD 1 220249039 220249039 58.748 5.476e-14 *** 28## Terminal 1 66366007 66366007 17.702 2.892e-05 *** 29## S.F.Ratio 1 190811028 190811028 50.896 2.274e-12 *** 30## perc.alumni 1 225293653 225293653 60.094 2.904e-14 *** 31## Expend 1 258162295 258162295 68.861 4.805e-16 *** 32## Grad.Rate 1 57947219 57947219 15.457 9.214e-05 *** 33## Residuals 760 2849276343 3749048 34## --- 35## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 36## 37## Anova for Nonparametric Effects 38## Npar Df Npar F Pr(F) 39## (Intercept) 40## Private 41## s(Apps, 3) 2 8.571 0.0002085 *** 42## Accept 43## Enroll 44## Top10perc 45## F.Undergrad 46## Room.Board 47## Personal 48## PhD 49## Terminal 50## S.F.Ratio 51## perc.alumni 52## Expend 53## Grad.Rate 54## --- 55## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 1summary(fit2) %\u0026gt;% print 1## 2## Call: gam(formula = Outstate ~ Private + s(Room.Board, 2) + s(PhD, 3## 3) + s(perc.alumni) + Expend, data = colDat) 4## Deviance Residuals: 5## Min 1Q Median 3Q Max 6## -8676.030 -1345.678 -8.409 1265.524 9590.459 7## 8## (Dispersion Parameter for gaussian family taken to be 4175193) 9## 10## Null Deviance: 12559297426 on 776 degrees of freedom 11## Residual Deviance: 3194023899 on 765.0002 degrees of freedom 12## AIC: 14064.05 13## 14## Number of Local Scoring Iterations: 2 15## 16## Anova for Parametric Effects 17## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) 18## Private 1 3751107814 3751107814 898.43 \u0026lt; 2.2e-16 *** 19## s(Room.Board, 2) 1 2913770756 2913770756 697.88 \u0026lt; 2.2e-16 *** 20## s(PhD, 3) 1 1149711330 1149711330 275.37 \u0026lt; 2.2e-16 *** 21## s(perc.alumni) 1 556759894 556759894 133.35 \u0026lt; 2.2e-16 *** 22## Expend 1 554812125 554812125 132.88 \u0026lt; 2.2e-16 *** 23## Residuals 765 3194023899 4175193 24## --- 25## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 26## 27## Anova for Nonparametric Effects 28## Npar Df Npar F Pr(F) 29## (Intercept) 30## Private 31## s(Room.Board, 2) 1 4.9853 0.0258517 * 32## s(PhD, 3) 2 9.1614 0.0001171 *** 33## s(perc.alumni) 3 0.8726 0.4548496 34## Expend 35## --- 36## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1   James, G., Witten, D., Hastie, T., \u0026amp; Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Berlin, Germany: Springer Science \u0026amp; Business Media.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/islr-ch7/","tags":["solutions","R","ISLR"],"title":"ISLR :: Moving Beyond Linearity"},{"categories":["programming"],"contents":"Chapter VI - Linear Model Selection and Regularization All the questions are as per the ISL seventh printing of the First edition1.\nCommon Instead of using the standard functions, we will leverage the mlr3 package2.\n1#install.packages(\u0026#34;mlr3\u0026#34;,\u0026#34;data.table\u0026#34;,\u0026#34;mlr3viz\u0026#34;,\u0026#34;mlr3learners\u0026#34;) Actually for R version 3.6.2, the steps to get it working were a bit more involved.\nLoad ISLR and other libraries.\n1libsUsed\u0026lt;-c(\u0026#34;dplyr\u0026#34;,\u0026#34;ggplot2\u0026#34;,\u0026#34;tidyverse\u0026#34;, 2\u0026#34;ISLR\u0026#34;,\u0026#34;caret\u0026#34;,\u0026#34;MASS\u0026#34;, \u0026#34;gridExtra\u0026#34;, 3\u0026#34;pls\u0026#34;,\u0026#34;latex2exp\u0026#34;,\u0026#34;data.table\u0026#34;) 4invisible(lapply(libsUsed, library, character.only = TRUE)) Question 6.8 - Page 262 In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.\n(a) Use the =rnorm()=function to generate a predictor \\(X\\) of length \\(n = 100\\), as well as a noise vector \\(\\eta\\) of length \\(n = 100\\).\n(b) Generate a response vector \\(Y\\) of length \\(n = 100\\) according to the model \\[Y = \\beta_0 + \\beta_1X + \\beta2X^2 + \\beta_3X^3 + \\eta\\], where \\(\\beta_{0}\\) , \\(\\beta_{1}\\), \\(\\beta_{2}\\), and \\(\\beta_{3}\\) are constants of your choice.\n(c) Use the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors \\(X\\), \\(X^{2}\\), \u0026hellip;, \\(X^{10}\\). What is the best model obtained according to \\(C_p\\) , BIC, and adjusted \\(R^2\\) ? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both \\(X\\) and \\(Y\\).\n(d) Repeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)?\n(e) Now fit a lasso model to the simulated data, again using \\(X\\), \\(X^{2}\\), \u0026hellip;, \\(X^{10}\\) as predictors. Use cross-validation to select the optimal value of \\(\\lambda\\). Create plots of the cross-validation error as a function of \\(\\lambda\\). Report the resulting coefficient estimates, and discuss the results obtained.\n(f) Now generate a response vector Y according to the model \\[Y = \\beta_{0} + \\beta_{7}X^{7} + \\eta,\\] and perform best subset selection and the lasso. Discuss the results obtained.\nAnswer a) Generate model 1set.seed(1984) 2x\u0026lt;-rnorm(100) 3noise\u0026lt;-rnorm(100) b) Response vector 1beta=c(43,5,3,6) 2y\u0026lt;-beta[1] + beta[2]*x + beta[3]*x^2 + beta[4]*x^3 + noise 3qplot(x,y)  c) Subset selection Since the question requires it, we will be using the leaps libraries.\n1library(leaps) 2df\u0026lt;-data.frame(y=y,x=x) 3sets=regsubsets(y~poly(x,10,raw=T),data=df,nvmax=10) 4sets %\u0026gt;% summary 1## Subset selection object 2## Call: regsubsets.formula(y ~ poly(x, 10, raw = T), data = df, nvmax = 10) 3## 10 Variables (and intercept) 4## Forced in Forced out 5## poly(x, 10, raw = T)1 FALSE FALSE 6## poly(x, 10, raw = T)2 FALSE FALSE 7## poly(x, 10, raw = T)3 FALSE FALSE 8## poly(x, 10, raw = T)4 FALSE FALSE 9## poly(x, 10, raw = T)5 FALSE FALSE 10## poly(x, 10, raw = T)6 FALSE FALSE 11## poly(x, 10, raw = T)7 FALSE FALSE 12## poly(x, 10, raw = T)8 FALSE FALSE 13## poly(x, 10, raw = T)9 FALSE FALSE 14## poly(x, 10, raw = T)10 FALSE FALSE 15## 1 subsets of each size up to 10 16## Selection Algorithm: exhaustive 17## poly(x, 10, raw = T)1 poly(x, 10, raw = T)2 poly(x, 10, raw = T)3 18## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 19## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 20## 3 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 21## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 22## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 23## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 24## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 25## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 26## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 27## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 28## poly(x, 10, raw = T)4 poly(x, 10, raw = T)5 poly(x, 10, raw = T)6 29## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 30## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 31## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 32## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 33## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 34## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 35## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 36## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 37## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 38## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 39## poly(x, 10, raw = T)7 poly(x, 10, raw = T)8 poly(x, 10, raw = T)9 40## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 41## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 42## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 43## 4 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 44## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 45## 6 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 46## 7 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 47## 8 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 48## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 49## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 50## poly(x, 10, raw = T)10 51## 1 ( 1 ) \u0026#34; \u0026#34; 52## 2 ( 1 ) \u0026#34; \u0026#34; 53## 3 ( 1 ) \u0026#34; \u0026#34; 54## 4 ( 1 ) \u0026#34; \u0026#34; 55## 5 ( 1 ) \u0026#34;*\u0026#34; 56## 6 ( 1 ) \u0026#34;*\u0026#34; 57## 7 ( 1 ) \u0026#34;*\u0026#34; 58## 8 ( 1 ) \u0026#34;*\u0026#34; 59## 9 ( 1 ) \u0026#34;*\u0026#34; 60## 10 ( 1 ) \u0026#34;*\u0026#34; We also want the best parameters.\n1summarySet\u0026lt;-summary(sets) 2which.min(summarySet$cp) %\u0026gt;% print 1## [1] 3 1which.min(summarySet$bic) %\u0026gt;% print 1## [1] 3 1which.max(summarySet$adjr2) %\u0026gt;% print 1## [1] 7 We might want to see this as a plot.\n1plot(summarySet$cp, xlab = \u0026#34;Subset Size\u0026#34;, ylab = \u0026#34;Cp\u0026#34;, pch = 20, type = \u0026#34;l\u0026#34;) 2points(3,summarySet$cp[3],pch=4,col=\u0026#39;red\u0026#39;,lwd=7)  1plot(summarySet$bic, xlab = \u0026#34;Subset Size\u0026#34;, ylab = \u0026#34;BIC\u0026#34;, pch = 20, type = \u0026#34;l\u0026#34;) 2points(3,summarySet$bic[3],pch=4,col=\u0026#39;red\u0026#39;,lwd=7)  1plot(summarySet$adjr2, xlab = \u0026#34;Subset Size\u0026#34;, ylab = \u0026#34;Adjusted R2\u0026#34;, pch = 20, type = \u0026#34;l\u0026#34;) 2points(3,summarySet$adjr2[3],pch=4,col=\u0026#39;red\u0026#39;,lwd=7)  Lets check the coefficients.\n1coefficients(sets,id=3) %\u0026gt;% print 1## (Intercept) poly(x, 10, raw = T)1 poly(x, 10, raw = T)2 2## 42.895657 5.108094 3.034408 3## poly(x, 10, raw = T)3 4## 5.989367 1beta %\u0026gt;% print 1## [1] 43 5 3 6 We see that we actually have a pretty good set of coefficients.\nd) Forward and backward stepwise models 1modelX\u0026lt;-poly(x,10,raw=T) 2forwardFit\u0026lt;-regsubsets(y~modelX,data=df,nvmax=10,method=\u0026#34;forward\u0026#34;) 3forwardFit %\u0026gt;% summary %\u0026gt;% print 1## Subset selection object 2## Call: regsubsets.formula(y ~ modelX, data = df, nvmax = 10, method = \u0026#34;forward\u0026#34;) 3## 10 Variables (and intercept) 4## Forced in Forced out 5## modelX1 FALSE FALSE 6## modelX2 FALSE FALSE 7## modelX3 FALSE FALSE 8## modelX4 FALSE FALSE 9## modelX5 FALSE FALSE 10## modelX6 FALSE FALSE 11## modelX7 FALSE FALSE 12## modelX8 FALSE FALSE 13## modelX9 FALSE FALSE 14## modelX10 FALSE FALSE 15## 1 subsets of each size up to 10 16## Selection Algorithm: forward 17## modelX1 modelX2 modelX3 modelX4 modelX5 modelX6 modelX7 modelX8 18## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 19## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 20## 3 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 21## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 22## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 23## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 24## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 25## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 26## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 27## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 28## modelX9 modelX10 29## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 30## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 31## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 32## 4 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 33## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 34## 6 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 35## 7 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 36## 8 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 37## 9 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 38## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; We might want to take a look at these.\n1par(mfrow=c(2,2)) 2plot(forwardFit) 3plot(forwardFit,scale=\u0026#39;Cp\u0026#39;) 4plot(forwardFit,scale=\u0026#39;r2\u0026#39;) 5plot(forwardFit,scale=\u0026#39;adjr2\u0026#39;)  I find these not as fun to look at, so we will do better.\n1plotLEAP=function(leapObj){ 2par(mfrow = c(2,2)) 3bar2=which.max(leapObj$adjr2) 4bbic=which.min(leapObj$bic) 5bcp=which.min(leapObj$cp) 6plot(leapObj$rss,xlab=\u0026#34;Number of variables\u0026#34;,ylab=\u0026#34;RSS\u0026#34;,type=\u0026#34;b\u0026#34;) 7plot(leapObj$adjr2,xlab=\u0026#34;Number of variables\u0026#34;,ylab=TeX(\u0026#34;Adjusted R^2\u0026#34;),type=\u0026#34;b\u0026#34;) 8points(bar2,leapObj$adjr2[bar2],col=\u0026#34;green\u0026#34;,cex=2,pch=20) 9plot(leapObj$bic,xlab=\u0026#34;Number of variables\u0026#34;,ylab=TeX(\u0026#34;BIC\u0026#34;),type=\u0026#34;b\u0026#34;) 10points(bbic,leapObj$bic[bbic],col=\u0026#34;blue\u0026#34;,cex=2,pch=20) 11plot(leapObj$cp,xlab=\u0026#34;Number of variables\u0026#34;,ylab=TeX(\u0026#34;C_p\u0026#34;),type=\u0026#34;b\u0026#34;) 12points(bcp,leapObj$cp[bcp],col=\u0026#34;red\u0026#34;,cex=2,pch=20) 13} 1plotLEAP(forwardFit %\u0026gt;% summary)  Lets check the backward selection as well.\n1modelX\u0026lt;-poly(x,10,raw=T) 2backwardFit\u0026lt;-regsubsets(y~modelX,data=df,nvmax=10,method=\u0026#34;backward\u0026#34;) 3backwardFit %\u0026gt;% summary %\u0026gt;% print 1## Subset selection object 2## Call: regsubsets.formula(y ~ modelX, data = df, nvmax = 10, method = \u0026#34;backward\u0026#34;) 3## 10 Variables (and intercept) 4## Forced in Forced out 5## modelX1 FALSE FALSE 6## modelX2 FALSE FALSE 7## modelX3 FALSE FALSE 8## modelX4 FALSE FALSE 9## modelX5 FALSE FALSE 10## modelX6 FALSE FALSE 11## modelX7 FALSE FALSE 12## modelX8 FALSE FALSE 13## modelX9 FALSE FALSE 14## modelX10 FALSE FALSE 15## 1 subsets of each size up to 10 16## Selection Algorithm: backward 17## modelX1 modelX2 modelX3 modelX4 modelX5 modelX6 modelX7 modelX8 18## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 19## 2 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 20## 3 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 21## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 22## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 23## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 24## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 25## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 26## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 27## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 28## modelX9 modelX10 29## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 30## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 31## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 32## 4 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 33## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 34## 6 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 35## 7 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 36## 8 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 37## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 38## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; We might want to take a look at these.\n1par(mfrow=c(2,2)) 2plot(backwardFit) 3plot(backwardFit,scale=\u0026#39;Cp\u0026#39;) 4plot(backwardFit,scale=\u0026#39;r2\u0026#39;) 5plot(backwardFit,scale=\u0026#39;adjr2\u0026#39;)  1plotLEAP(backwardFit %\u0026gt;% summary)  In spite of some slight variations, overall all methods converge to the same best set of parameters, that of the third model.\ne) LASSO and Cross Validation For this, instead of using glmnet directly, we will use caret.\n1df\u0026lt;-df %\u0026gt;% mutate(x2=x^2,x3=x^3, 2x4=x^4,x5=x^5, 3x6=x^6,x7=x^7, 4x8=x^8,x9=x^9, 5x10=x^10) 1lambda\u0026lt;-10^seq(-3, 3, length = 100) 2lassoCaret= train(y~.,data=df,method=\u0026#34;glmnet\u0026#34;,tuneGrid=expand.grid(alpha=1,lambda=lambda)) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1lassoCaret %\u0026gt;% print 1## glmnet 2## 3## 100 samples 4## 10 predictor 5## 6## No pre-processing 7## Resampling: Bootstrapped (25 reps) 8## Summary of sample sizes: 100, 100, 100, 100, 100, 100, ... 9## Resampling results across tuning parameters: 10## 11## lambda RMSE Rsquared MAE 12## 1.000000e-03 1.009696 0.9965632 0.8051425 13## 1.149757e-03 1.009696 0.9965632 0.8051425 14## 1.321941e-03 1.009696 0.9965632 0.8051425 15## 1.519911e-03 1.009696 0.9965632 0.8051425 16## 1.747528e-03 1.009696 0.9965632 0.8051425 17## 2.009233e-03 1.009696 0.9965632 0.8051425 18## 2.310130e-03 1.009696 0.9965632 0.8051425 19## 2.656088e-03 1.009696 0.9965632 0.8051425 20## 3.053856e-03 1.009696 0.9965632 0.8051425 21## 3.511192e-03 1.009696 0.9965632 0.8051425 22## 4.037017e-03 1.009696 0.9965632 0.8051425 23## 4.641589e-03 1.009696 0.9965632 0.8051425 24## 5.336699e-03 1.009696 0.9965632 0.8051425 25## 6.135907e-03 1.009696 0.9965632 0.8051425 26## 7.054802e-03 1.009696 0.9965632 0.8051425 27## 8.111308e-03 1.009696 0.9965632 0.8051425 28## 9.326033e-03 1.009696 0.9965632 0.8051425 29## 1.072267e-02 1.009696 0.9965632 0.8051425 30## 1.232847e-02 1.009696 0.9965632 0.8051425 31## 1.417474e-02 1.009696 0.9965632 0.8051425 32## 1.629751e-02 1.009696 0.9965632 0.8051425 33## 1.873817e-02 1.009696 0.9965632 0.8051425 34## 2.154435e-02 1.009696 0.9965632 0.8051425 35## 2.477076e-02 1.009696 0.9965632 0.8051425 36## 2.848036e-02 1.009696 0.9965632 0.8051425 37## 3.274549e-02 1.009696 0.9965632 0.8051425 38## 3.764936e-02 1.009696 0.9965632 0.8051425 39## 4.328761e-02 1.009696 0.9965632 0.8051425 40## 4.977024e-02 1.009696 0.9965632 0.8051425 41## 5.722368e-02 1.009696 0.9965632 0.8051425 42## 6.579332e-02 1.009696 0.9965632 0.8051425 43## 7.564633e-02 1.009637 0.9965632 0.8050666 44## 8.697490e-02 1.009216 0.9965637 0.8047862 45## 1.000000e-01 1.008901 0.9965636 0.8046468 46## 1.149757e-01 1.009470 0.9965616 0.8054790 47## 1.321941e-01 1.011206 0.9965561 0.8074253 48## 1.519911e-01 1.014475 0.9965476 0.8104930 49## 1.747528e-01 1.019202 0.9965383 0.8147296 50## 2.009233e-01 1.025943 0.9965259 0.8203974 51## 2.310130e-01 1.035374 0.9965094 0.8284187 52## 2.656088e-01 1.048294 0.9964878 0.8393282 53## 3.053856e-01 1.065717 0.9964592 0.8530952 54## 3.511192e-01 1.088903 0.9964215 0.8701072 55## 4.037017e-01 1.119433 0.9963715 0.8918217 56## 4.641589e-01 1.158919 0.9963053 0.9193677 57## 5.336699e-01 1.209841 0.9962136 0.9532842 58## 6.135907e-01 1.275467 0.9960778 0.9957151 59## 7.054802e-01 1.357247 0.9958966 1.0471169 60## 8.111308e-01 1.457886 0.9956561 1.1087362 61## 9.326033e-01 1.580743 0.9953362 1.1818188 62## 1.072267e+00 1.729330 0.9949070 1.2696235 63## 1.232847e+00 1.907599 0.9943306 1.3758463 64## 1.417474e+00 2.120178 0.9935518 1.5059031 65## 1.629751e+00 2.369642 0.9924954 1.6673393 66## 1.873817e+00 2.662906 0.9910539 1.8621728 67## 2.154435e+00 3.007271 0.9890638 2.0978907 68## 2.477076e+00 3.409377 0.9863097 2.3788439 69## 2.848036e+00 3.864727 0.9825900 2.7053428 70## 3.274549e+00 4.350785 0.9778541 3.0659309 71## 3.764936e+00 4.847045 0.9724311 3.4403210 72## 4.328761e+00 5.369017 0.9668240 3.8351441 73## 4.977024e+00 5.919492 0.9626812 4.2512694 74## 5.722368e+00 6.562134 0.9580843 4.7389049 75## 6.579332e+00 7.307112 0.9534537 5.2945905 76## 7.564633e+00 8.132296 0.9500300 5.8774541 77## 8.697490e+00 9.067321 0.9486589 6.4760997 78## 1.000000e+01 10.167822 0.9483195 7.1226569 79## 1.149757e+01 11.473284 0.9482975 7.8556639 80## 1.321941e+01 13.002703 0.9482975 8.6990451 81## 1.519911e+01 14.727852 0.9454119 9.6414650 82## 1.747528e+01 16.325210 0.9426796 10.5303097 83## 2.009233e+01 17.740599 0.9357286 11.3560865 84## 2.310130e+01 18.585795 0.9227167 11.8799668 85## 2.656088e+01 18.939596 0.9080584 12.1336575 86## 3.053856e+01 19.123568 0.9109065 12.2733471 87## 3.511192e+01 19.197966 NaN 12.3308613 88## 4.037017e+01 19.197966 NaN 12.3308613 89## 4.641589e+01 19.197966 NaN 12.3308613 90## 5.336699e+01 19.197966 NaN 12.3308613 91## 6.135907e+01 19.197966 NaN 12.3308613 92## 7.054802e+01 19.197966 NaN 12.3308613 93## 8.111308e+01 19.197966 NaN 12.3308613 94## 9.326033e+01 19.197966 NaN 12.3308613 95## 1.072267e+02 19.197966 NaN 12.3308613 96## 1.232847e+02 19.197966 NaN 12.3308613 97## 1.417474e+02 19.197966 NaN 12.3308613 98## 1.629751e+02 19.197966 NaN 12.3308613 99## 1.873817e+02 19.197966 NaN 12.3308613 100## 2.154435e+02 19.197966 NaN 12.3308613 101## 2.477076e+02 19.197966 NaN 12.3308613 102## 2.848036e+02 19.197966 NaN 12.3308613 103## 3.274549e+02 19.197966 NaN 12.3308613 104## 3.764936e+02 19.197966 NaN 12.3308613 105## 4.328761e+02 19.197966 NaN 12.3308613 106## 4.977024e+02 19.197966 NaN 12.3308613 107## 5.722368e+02 19.197966 NaN 12.3308613 108## 6.579332e+02 19.197966 NaN 12.3308613 109## 7.564633e+02 19.197966 NaN 12.3308613 110## 8.697490e+02 19.197966 NaN 12.3308613 111## 1.000000e+03 19.197966 NaN 12.3308613 112## 113## Tuning parameter \u0026#39;alpha\u0026#39; was held constant at a value of 1 114## RMSE was used to select the optimal model using the smallest value. 115## The final values used for the model were alpha = 1 and lambda = 0.1. 1lassoCaret %\u0026gt;% ggplot  1lassoCaret %\u0026gt;% varImp %\u0026gt;% ggplot  1library(glmnet) 1## Loading required package: Matrix 1## 2## Attaching package: \u0026#39;Matrix\u0026#39; 1## The following objects are masked from \u0026#39;package:tidyr\u0026#39;: 2## 3## expand, pack, unpack 1## Loaded glmnet 3.0-2 1library(boot) 1## 2## Attaching package: \u0026#39;boot\u0026#39; 1## The following object is masked from \u0026#39;package:lattice\u0026#39;: 2## 3## melanoma 1lasso.mod \u0026lt;- cv.glmnet(as.matrix(df[-1]), y, alpha=1) 2lambda \u0026lt;- lasso.mod$lambda.min 3plot(lasso.mod)  1predict(lasso.mod, s=lambda, type=\u0026#34;coefficients\u0026#34;) 1## 11 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) 42.975240 4## x 5.005023 5## x2 2.947540 6## x3 5.989105 7## x4 . 8## x5 . 9## x6 . 10## x7 . 11## x8 . 12## x9 . 13## x10 . Clearly, the only important variables are \\(x\\), \\(x^2\\) and \\(x^3\\).\nf) New model Our new model requires a newly expanded set of betas as well.\n1y2\u0026lt;-beta[1]+23*x^7+noise 1modelX\u0026lt;-poly(x,10,raw=T) 2newDF\u0026lt;-data.frame(x=as.matrix(modelX),y=y2) 3newSub\u0026lt;-regsubsets(y2~.,data=newDF,nvmax=10) 4newSub %\u0026gt;% summary 1## Subset selection object 2## Call: regsubsets.formula(y2 ~ ., data = newDF, nvmax = 10) 3## 11 Variables (and intercept) 4## Forced in Forced out 5## x.1 FALSE FALSE 6## x.2 FALSE FALSE 7## x.3 FALSE FALSE 8## x.4 FALSE FALSE 9## x.5 FALSE FALSE 10## x.6 FALSE FALSE 11## x.7 FALSE FALSE 12## x.8 FALSE FALSE 13## x.9 FALSE FALSE 14## x.10 FALSE FALSE 15## y FALSE FALSE 16## 1 subsets of each size up to 10 17## Selection Algorithm: exhaustive 18## x.1 x.2 x.3 x.4 x.5 x.6 x.7 x.8 x.9 x.10 y 19## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 20## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 21## 3 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 22## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 23## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 24## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 25## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 26## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 27## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 28## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 1plotLEAP(newSub %\u0026gt;% summary)  Or in its more native look,\n1par(mfrow=c(2,2)) 2plot(newSub) 3plot(newSub,scale=\u0026#39;Cp\u0026#39;) 4plot(newSub,scale=\u0026#39;r2\u0026#39;) 5plot(newSub,scale=\u0026#39;adjr2\u0026#39;)  1library(glmnet) 2library(boot) 3lasso.mod2 \u0026lt;- cv.glmnet(as.matrix(newDF[-1]), y, alpha=1) 4lambda2 \u0026lt;- lasso.mod2$lambda.min 5plot(lasso.mod2)  1predict(lasso.mod2, s=lambda, type=\u0026#34;coefficients\u0026#34;) 1## 11 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) 42.67982691 4## x.2 3.22521396 5## x.3 8.56699146 6## x.4 . 7## x.5 -0.10229572 8## x.6 . 9## x.7 -0.03184905 10## x.8 . 11## x.9 . 12## x.10 . 13## y . 1lambda\u0026lt;-10^seq(-3, 3, length = 100) 2lassocaret2= train(y~.,data=newDF,method=\u0026#34;glmnet\u0026#34;,tuneGrid=expand.grid(alpha=1,lambda=lambda)) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1lassocaret2 %\u0026gt;% print 1## glmnet 2## 3## 100 samples 4## 10 predictor 5## 6## No pre-processing 7## Resampling: Bootstrapped (25 reps) 8## Summary of sample sizes: 100, 100, 100, 100, 100, 100, ... 9## Resampling results across tuning parameters: 10## 11## lambda RMSE Rsquared MAE 12## 1.000000e-03 40.03231 0.9999955 14.48774 13## 1.149757e-03 40.03231 0.9999955 14.48774 14## 1.321941e-03 40.03231 0.9999955 14.48774 15## 1.519911e-03 40.03231 0.9999955 14.48774 16## 1.747528e-03 40.03231 0.9999955 14.48774 17## 2.009233e-03 40.03231 0.9999955 14.48774 18## 2.310130e-03 40.03231 0.9999955 14.48774 19## 2.656088e-03 40.03231 0.9999955 14.48774 20## 3.053856e-03 40.03231 0.9999955 14.48774 21## 3.511192e-03 40.03231 0.9999955 14.48774 22## 4.037017e-03 40.03231 0.9999955 14.48774 23## 4.641589e-03 40.03231 0.9999955 14.48774 24## 5.336699e-03 40.03231 0.9999955 14.48774 25## 6.135907e-03 40.03231 0.9999955 14.48774 26## 7.054802e-03 40.03231 0.9999955 14.48774 27## 8.111308e-03 40.03231 0.9999955 14.48774 28## 9.326033e-03 40.03231 0.9999955 14.48774 29## 1.072267e-02 40.03231 0.9999955 14.48774 30## 1.232847e-02 40.03231 0.9999955 14.48774 31## 1.417474e-02 40.03231 0.9999955 14.48774 32## 1.629751e-02 40.03231 0.9999955 14.48774 33## 1.873817e-02 40.03231 0.9999955 14.48774 34## 2.154435e-02 40.03231 0.9999955 14.48774 35## 2.477076e-02 40.03231 0.9999955 14.48774 36## 2.848036e-02 40.03231 0.9999955 14.48774 37## 3.274549e-02 40.03231 0.9999955 14.48774 38## 3.764936e-02 40.03231 0.9999955 14.48774 39## 4.328761e-02 40.03231 0.9999955 14.48774 40## 4.977024e-02 40.03231 0.9999955 14.48774 41## 5.722368e-02 40.03231 0.9999955 14.48774 42## 6.579332e-02 40.03231 0.9999955 14.48774 43## 7.564633e-02 40.03231 0.9999955 14.48774 44## 8.697490e-02 40.03231 0.9999955 14.48774 45## 1.000000e-01 40.03231 0.9999955 14.48774 46## 1.149757e-01 40.03231 0.9999955 14.48774 47## 1.321941e-01 40.03231 0.9999955 14.48774 48## 1.519911e-01 40.03231 0.9999955 14.48774 49## 1.747528e-01 40.03231 0.9999955 14.48774 50## 2.009233e-01 40.03231 0.9999955 14.48774 51## 2.310130e-01 40.03231 0.9999955 14.48774 52## 2.656088e-01 40.03231 0.9999955 14.48774 53## 3.053856e-01 40.03231 0.9999955 14.48774 54## 3.511192e-01 40.03231 0.9999955 14.48774 55## 4.037017e-01 40.03231 0.9999955 14.48774 56## 4.641589e-01 40.03231 0.9999955 14.48774 57## 5.336699e-01 40.03231 0.9999955 14.48774 58## 6.135907e-01 40.03231 0.9999955 14.48774 59## 7.054802e-01 40.03231 0.9999955 14.48774 60## 8.111308e-01 40.03231 0.9999955 14.48774 61## 9.326033e-01 40.03231 0.9999955 14.48774 62## 1.072267e+00 40.03231 0.9999955 14.48774 63## 1.232847e+00 40.03231 0.9999955 14.48774 64## 1.417474e+00 40.03231 0.9999955 14.48774 65## 1.629751e+00 40.03231 0.9999955 14.48774 66## 1.873817e+00 40.03231 0.9999955 14.48774 67## 2.154435e+00 40.03231 0.9999955 14.48774 68## 2.477076e+00 40.03231 0.9999955 14.48774 69## 2.848036e+00 40.03231 0.9999955 14.48774 70## 3.274549e+00 40.03231 0.9999955 14.48774 71## 3.764936e+00 40.03231 0.9999955 14.48774 72## 4.328761e+00 40.03231 0.9999955 14.48774 73## 4.977024e+00 40.03231 0.9999955 14.48774 74## 5.722368e+00 40.03231 0.9999955 14.48774 75## 6.579332e+00 40.03231 0.9999955 14.48774 76## 7.564633e+00 40.43005 0.9999955 14.59881 77## 8.697490e+00 41.25214 0.9999955 14.81913 78## 1.000000e+01 42.30446 0.9999955 15.09937 79## 1.149757e+01 43.59429 0.9999955 15.44307 80## 1.321941e+01 45.43633 0.9999955 15.93255 81## 1.519911e+01 47.55425 0.9999955 16.49605 82## 1.747528e+01 49.98935 0.9999955 17.14447 83## 2.009233e+01 52.90533 0.9999955 17.91650 84## 2.310130e+01 57.57589 0.9999955 19.10125 85## 2.656088e+01 63.25484 0.9999955 20.53147 86## 3.053856e+01 70.51580 0.9999955 22.36400 87## 3.511192e+01 78.93391 0.9999955 24.49105 88## 4.037017e+01 88.61274 0.9999955 26.93830 89## 4.641589e+01 99.97831 0.9999955 29.83601 90## 5.336699e+01 113.48225 0.9999955 33.39320 91## 6.135907e+01 129.17536 0.9999955 37.58303 92## 7.054802e+01 147.76452 0.9999957 42.74333 93## 8.111308e+01 169.60027 0.9999961 48.98043 94## 9.326033e+01 194.94266 0.9999965 56.29001 95## 1.072267e+02 224.07631 0.9999969 64.70026 96## 1.232847e+02 257.56092 0.9999971 74.36989 97## 1.417474e+02 296.13382 0.9999971 85.51504 98## 1.629751e+02 340.49129 0.9999971 98.33212 99## 1.873817e+02 391.49185 0.9999971 113.06864 100## 2.154435e+02 450.13031 0.9999971 130.01206 101## 2.477076e+02 509.28329 0.9999970 147.15405 102## 2.848036e+02 564.17558 0.9999969 163.34475 103## 3.274549e+02 618.84080 0.9999969 179.85589 104## 3.764936e+02 681.69265 0.9999969 198.83969 105## 4.328761e+02 741.14452 0.9999967 217.28049 106## 4.977024e+02 807.25385 0.9999967 237.88938 107## 5.722368e+02 883.26360 0.9999967 261.58461 108## 6.579332e+02 970.65640 0.9999967 288.82836 109## 7.564633e+02 1037.84801 0.9999960 312.54099 110## 8.697490e+02 1088.92551 0.9999960 334.04769 111## 1.000000e+03 1131.46176 0.9999955 354.62317 112## 113## Tuning parameter \u0026#39;alpha\u0026#39; was held constant at a value of 1 114## RMSE was used to select the optimal model using the smallest value. 115## The final values used for the model were alpha = 1 and lambda = 6.579332. 1lassocaret2 %\u0026gt;% ggplot  1lassocaret2 %\u0026gt;% varImp %\u0026gt;% ggplot  Clearly, the LASSO model has correctly reduced the model down to the correct single variable form, though best subset seems to suggest using more predictors, their coefficients are low enough to recognize that they are noise.\nQuestion 6.9 - Page 263 In this exercise, we will predict the number of applications received using the other variables in the College data set.\n(a) Split the data set into a training set and a test set.\n(b) Fit a linear model using least squares on the training set, and report the test error obtained.\n(c) Fit a ridge regression model on the training set, with \\(\\lambda\\) chosen by cross-validation. Report the test error obtained.\n(d) Fit a lasso model on the training set, with \\(\\lambda\\) chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.\n(e) Fit a PCR model on the training set, with \\(M\\) chosen by crossvalidation. Report the test error obtained, along with the value of \\(M\\) selected by cross-validation.\n(f) Fit a PLS model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation.\n(g) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?\nAnswer We will use the caret package, since at the moment, mlr3 does not have learners for PCR and PLS.\n1colDat\u0026lt;-ISLR::College 2colDat %\u0026gt;% summary %\u0026gt;% print 1## Private Apps Accept Enroll Top10perc 2## No :212 Min. : 81 Min. : 72 Min. : 35 Min. : 1.00 3## Yes:565 1st Qu.: 776 1st Qu.: 604 1st Qu.: 242 1st Qu.:15.00 4## Median : 1558 Median : 1110 Median : 434 Median :23.00 5## Mean : 3002 Mean : 2019 Mean : 780 Mean :27.56 6## 3rd Qu.: 3624 3rd Qu.: 2424 3rd Qu.: 902 3rd Qu.:35.00 7## Max. :48094 Max. :26330 Max. :6392 Max. :96.00 8## Top25perc F.Undergrad P.Undergrad Outstate 9## Min. : 9.0 Min. : 139 Min. : 1.0 Min. : 2340 10## 1st Qu.: 41.0 1st Qu.: 992 1st Qu.: 95.0 1st Qu.: 7320 11## Median : 54.0 Median : 1707 Median : 353.0 Median : 9990 12## Mean : 55.8 Mean : 3700 Mean : 855.3 Mean :10441 13## 3rd Qu.: 69.0 3rd Qu.: 4005 3rd Qu.: 967.0 3rd Qu.:12925 14## Max. :100.0 Max. :31643 Max. :21836.0 Max. :21700 15## Room.Board Books Personal PhD 16## Min. :1780 Min. : 96.0 Min. : 250 Min. : 8.00 17## 1st Qu.:3597 1st Qu.: 470.0 1st Qu.: 850 1st Qu.: 62.00 18## Median :4200 Median : 500.0 Median :1200 Median : 75.00 19## Mean :4358 Mean : 549.4 Mean :1341 Mean : 72.66 20## 3rd Qu.:5050 3rd Qu.: 600.0 3rd Qu.:1700 3rd Qu.: 85.00 21## Max. :8124 Max. :2340.0 Max. :6800 Max. :103.00 22## Terminal S.F.Ratio perc.alumni Expend 23## Min. : 24.0 Min. : 2.50 Min. : 0.00 Min. : 3186 24## 1st Qu.: 71.0 1st Qu.:11.50 1st Qu.:13.00 1st Qu.: 6751 25## Median : 82.0 Median :13.60 Median :21.00 Median : 8377 26## Mean : 79.7 Mean :14.09 Mean :22.74 Mean : 9660 27## 3rd Qu.: 92.0 3rd Qu.:16.50 3rd Qu.:31.00 3rd Qu.:10830 28## Max. :100.0 Max. :39.80 Max. :64.00 Max. :56233 29## Grad.Rate 30## Min. : 10.00 31## 1st Qu.: 53.00 32## Median : 65.00 33## Mean : 65.46 34## 3rd Qu.: 78.00 35## Max. :118.00 1colDat %\u0026gt;% str %\u0026gt;% print 1## \u0026#39;data.frame\u0026#39;: 777 obs. of 18 variables: 2## $ Private : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 2 2 2 2 2 2 2 2 ... 3## $ Apps : num 1660 2186 1428 417 193 ... 4## $ Accept : num 1232 1924 1097 349 146 ... 5## $ Enroll : num 721 512 336 137 55 158 103 489 227 172 ... 6## $ Top10perc : num 23 16 22 60 16 38 17 37 30 21 ... 7## $ Top25perc : num 52 29 50 89 44 62 45 68 63 44 ... 8## $ F.Undergrad: num 2885 2683 1036 510 249 ... 9## $ P.Undergrad: num 537 1227 99 63 869 ... 10## $ Outstate : num 7440 12280 11250 12960 7560 ... 11## $ Room.Board : num 3300 6450 3750 5450 4120 ... 12## $ Books : num 450 750 400 450 800 500 500 450 300 660 ... 13## $ Personal : num 2200 1500 1165 875 1500 ... 14## $ PhD : num 70 29 53 92 76 67 90 89 79 40 ... 15## $ Terminal : num 78 30 66 97 72 73 93 100 84 41 ... 16## $ S.F.Ratio : num 18.1 12.2 12.9 7.7 11.9 9.4 11.5 13.7 11.3 11.5 ... 17## $ perc.alumni: num 12 16 30 37 2 11 26 37 23 15 ... 18## $ Expend : num 7041 10527 8735 19016 10922 ... 19## $ Grad.Rate : num 60 56 54 59 15 55 63 73 80 52 ... 20## NULL 1colDat %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) %\u0026gt;% print 1## Private Apps Accept Enroll Top10perc Top25perc 2## 2 711 693 581 82 89 3## F.Undergrad P.Undergrad Outstate Room.Board Books Personal 4## 714 566 640 553 122 294 5## PhD Terminal S.F.Ratio perc.alumni Expend Grad.Rate 6## 78 65 173 61 744 81 Clearly, there are no psuedo-factors which might have been converted at this stage.\na) Train-Test split 1train_ind\u0026lt;-createDataPartition(colDat$Apps,p=0.8,times=1,list=FALSE) 2train_set\u0026lt;-colDat[train_ind,] 3test_set\u0026lt;-colDat[-train_ind,] b) Linear least squares 1linCol\u0026lt;-train(Apps~.,data=train_set,method=\u0026#34;lm\u0026#34;) 2linCol %\u0026gt;% summary 1## 2## Call: 3## lm(formula = .outcome ~ ., data = dat) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -5145.6 -414.8 -20.3 340.5 7526.8 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) -2.918e+02 4.506e+02 -0.648 0.517486 12## PrivateYes -5.351e+02 1.532e+02 -3.494 0.000511 *** 13## Accept 1.617e+00 4.258e-02 37.983 \u0026lt; 2e-16 *** 14## Enroll -1.012e+00 1.959e-01 -5.165 3.26e-07 *** 15## Top10perc 5.379e+01 6.221e+00 8.647 \u0026lt; 2e-16 *** 16## Top25perc -1.632e+01 5.046e+00 -3.235 0.001282 ** 17## F.Undergrad 6.836e-02 3.457e-02 1.978 0.048410 * 18## P.Undergrad 7.929e-02 3.367e-02 2.355 0.018854 * 19## Outstate -7.303e-02 2.098e-02 -3.481 0.000536 *** 20## Room.Board 1.695e-01 5.367e-02 3.159 0.001663 ** 21## Books 9.998e-02 2.578e-01 0.388 0.698328 22## Personal -3.145e-03 6.880e-02 -0.046 0.963553 23## PhD -8.926e+00 5.041e+00 -1.771 0.077112 . 24## Terminal -2.298e+00 5.608e+00 -0.410 0.682152 25## S.F.Ratio 6.038e+00 1.420e+01 0.425 0.670757 26## perc.alumni -5.085e-01 4.560e+00 -0.112 0.911249 27## Expend 4.668e-02 1.332e-02 3.505 0.000490 *** 28## Grad.Rate 9.042e+00 3.379e+00 2.676 0.007653 ** 29## --- 30## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 31## 32## Residual standard error: 1042 on 606 degrees of freedom 33## Multiple R-squared: 0.9332, Adjusted R-squared: 0.9313 34## F-statistic: 497.7 on 17 and 606 DF, p-value: \u0026lt; 2.2e-16 1linPred\u0026lt;-predict(linCol,test_set) 2linPred %\u0026gt;% postResample(obs = test_set$Apps) 1## RMSE Rsquared MAE 2## 1071.6360025 0.9017032 625.7827996 Do note that the metrics are calculated in a manner to ensure no negative values are obtained.\nc) Ridge regression with CV for λ 1L2Grid \u0026lt;- expand.grid(alpha=0, 2lambda=10^seq(from=-3,to=30,length=100)) 1ridgCol\u0026lt;-train(Apps~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L2Grid) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1ridgCol %\u0026gt;% summary %\u0026gt;% print 1## Length Class Mode 2## a0 100 -none- numeric 3## beta 1700 dgCMatrix S4 4## df 100 -none- numeric 5## dim 2 -none- numeric 6## lambda 100 -none- numeric 7## dev.ratio 100 -none- numeric 8## nulldev 1 -none- numeric 9## npasses 1 -none- numeric 10## jerr 1 -none- numeric 11## offset 1 -none- logical 12## call 5 -none- call 13## nobs 1 -none- numeric 14## lambdaOpt 1 -none- numeric 15## xNames 17 -none- character 16## problemType 1 -none- character 17## tuneValue 2 data.frame list 18## obsLevels 1 -none- logical 19## param 0 -none- list 1coef(ridgCol$finalModel, ridgCol$bestTune$lambda) %\u0026gt;% print 1## 18 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) -1.407775e+03 4## PrivateYes -5.854245e+02 5## Accept 1.042778e+00 6## Enroll 3.511219e-01 7## Top10perc 2.780211e+01 8## Top25perc 2.883536e-02 9## F.Undergrad 6.825141e-02 10## P.Undergrad 5.281320e-02 11## Outstate -2.011504e-02 12## Room.Board 2.155224e-01 13## Books 1.517585e-01 14## Personal -3.711406e-02 15## PhD -4.453155e+00 16## Terminal -3.783231e+00 17## S.F.Ratio 6.897360e+00 18## perc.alumni -9.301831e+00 19## Expend 5.601144e-02 20## Grad.Rate 1.259989e+01 1ggplot(ridgCol)  1ridgPred\u0026lt;-predict(ridgCol,test_set) 2ridgPred %\u0026gt;% postResample(obs = test_set$Apps) 1## RMSE Rsquared MAE 2## 1047.7545250 0.9051726 644.4535063 d) LASSO with CV for λ 1L1Grid \u0026lt;- expand.grid(alpha=1, # for lasso 2lambda=10^seq(from=-3,to=30,length=100)) 1lassoCol\u0026lt;-train(Apps~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L1Grid) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1lassoCol %\u0026gt;% summary %\u0026gt;% print 1## Length Class Mode 2## a0 81 -none- numeric 3## beta 1377 dgCMatrix S4 4## df 81 -none- numeric 5## dim 2 -none- numeric 6## lambda 81 -none- numeric 7## dev.ratio 81 -none- numeric 8## nulldev 1 -none- numeric 9## npasses 1 -none- numeric 10## jerr 1 -none- numeric 11## offset 1 -none- logical 12## call 5 -none- call 13## nobs 1 -none- numeric 14## lambdaOpt 1 -none- numeric 15## xNames 17 -none- character 16## problemType 1 -none- character 17## tuneValue 2 data.frame list 18## obsLevels 1 -none- logical 19## param 0 -none- list 1coef(lassoCol$finalModel, lassoCol$bestTune$lambda) %\u0026gt;% print 1## 18 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) -325.51554340 4## PrivateYes -532.28956305 5## Accept 1.60370798 6## Enroll -0.90158328 7## Top10perc 51.96610325 8## Top25perc -14.87886847 9## F.Undergrad 0.05352324 10## P.Undergrad 0.07832395 11## Outstate -0.07047302 12## Room.Board 0.16783269 13## Books 0.08836704 14## Personal . 15## PhD -8.67634519 16## Terminal -2.18494018 17## S.F.Ratio 5.25050018 18## perc.alumni -0.67848535 19## Expend 0.04597728 20## Grad.Rate 8.67569015 1ggplot(lassoCol)  1lassoPred\u0026lt;-predict(lassoCol,test_set) 2lassoPred %\u0026gt;% postResample(obs = test_set$Apps) 1## RMSE Rsquared MAE 2## 1068.9834769 0.9021268 622.7029418 e) PCR with CV for M 1mGrid \u0026lt;- expand.grid(ncomp=seq(from=1,to=20,length=10)) 1pcrCol\u0026lt;-train(Apps~.,data=train_set,method=\u0026#34;pcr\u0026#34;,tuneGrid = mGrid) 2pcrCol %\u0026gt;% summary %\u0026gt;% print 1## Data: X dimension: 624 17 2## Y dimension: 624 1 3## Fit method: svdpc 4## Number of components considered: 17 5## TRAINING: % variance explained 6## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps 7## X 48.2314 87.24 95.02 97.26 98.63 99.43 99.91 8## .outcome 0.2419 76.54 77.88 80.19 91.27 91.34 91.34 9## 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps 14 comps 10## X 99.96 100.00 100.00 100.00 100.00 100.00 100.00 11## .outcome 91.65 91.66 92.26 92.65 92.66 92.67 92.76 12## 15 comps 16 comps 17 comps 13## X 100.00 100.00 100.00 14## .outcome 93.17 93.18 93.32 15## NULL 1ggplot(pcrCol)  1pcrPred\u0026lt;-predict(pcrCol,test_set) 2pcrPred %\u0026gt;% postResample(obs = test_set$Apps) 1## RMSE Rsquared MAE 2## 1071.6360025 0.9017032 625.7827996 f) PLS with CV for M 1plsCol\u0026lt;-train(Apps~.,data=train_set,method=\u0026#34;pls\u0026#34;,tuneGrid = mGrid) 2plsCol %\u0026gt;% summary %\u0026gt;% print 1## Data: X dimension: 624 17 2## Y dimension: 624 1 3## Fit method: oscorespls 4## Number of components considered: 17 5## TRAINING: % variance explained 6## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps 7## X 39.02 56.4 91.83 96.61 98.62 99.22 99.49 8## .outcome 78.04 84.1 86.88 91.09 91.38 91.49 91.66 9## 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps 14 comps 10## X 99.96 99.99 100.00 100.00 100.00 100.00 100.00 11## .outcome 91.68 91.85 92.64 92.87 93.16 93.18 93.18 12## 15 comps 16 comps 17 comps 13## X 100.00 100.00 100.00 14## .outcome 93.18 93.19 93.32 15## NULL 1ggplot(plsCol)  1plsPred\u0026lt;-predict(plsCol,test_set) 2plsPred %\u0026gt;% postResample(obs = test_set$Apps) 1## RMSE Rsquared MAE 2## 1071.6360039 0.9017032 625.7827987 g) Comments and Comparison 1models \u0026lt;- list(ridge = ridgCol, lasso = lassoCol, pcr = pcrCol, pls=plsCol,linear=linCol) 2resamples(models) %\u0026gt;% summary 1## 2## Call: 3## summary.resamples(object = .) 4## 5## Models: ridge, lasso, pcr, pls, linear 6## Number of resamples: 25 7## 8## MAE 9## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s 10## ridge 536.9612 600.5398 623.2005 649.6713 707.4014 793.4972 0 11## lasso 573.8563 616.3883 671.9453 655.8858 691.7620 732.2155 0 12## pcr 576.1427 618.8694 650.0360 662.9040 714.8491 767.5535 0 13## pls 553.3999 607.9757 637.1985 638.6619 668.5120 735.4479 0 14## linear 556.5553 619.2395 654.1478 659.4635 686.7747 792.4912 0 15## 16## RMSE 17## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s 18## ridge 882.2646 920.5934 1000.519 1168.603 1163.377 1939.541 0 19## lasso 801.9415 990.0724 1168.234 1184.329 1302.221 1584.712 0 20## pcr 828.1370 942.2678 1131.207 1144.071 1284.178 1544.078 0 21## pls 786.7989 1038.3265 1167.764 1157.026 1274.041 1461.434 0 22## linear 798.3771 1063.3690 1134.291 1135.977 1215.115 1403.576 0 23## 24## Rsquared 25## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s 26## ridge 0.8735756 0.8962010 0.9185736 0.9136429 0.9306819 0.9474913 0 27## lasso 0.8851991 0.9132766 0.9217660 0.9191638 0.9284838 0.9398772 0 28## pcr 0.8658504 0.9080179 0.9235117 0.9146884 0.9281892 0.9471991 0 29## pls 0.8881249 0.9080786 0.9183968 0.9173632 0.9258994 0.9420894 0 30## linear 0.8840049 0.8986452 0.9222319 0.9160913 0.9296275 0.9492198 0 1resamples(models) %\u0026gt;% bwplot(scales=\u0026#34;free\u0026#34;)   Given the tighter spread of PLS, it seems more reliable than PCR Ridge is just poor in every way OLS does well, but it also has a worrying outlier LASSO appears to be doing alright as well We also have kept track of the performance on the test_set  We might want to see the variable significance values as well.\n1lgp\u0026lt;-linCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;OLS Variable Importance\u0026#34;) 2rgp\u0026lt;-ridgCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Ridge Variable Importance\u0026#34;) 3lsgp\u0026lt;-lassoCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Lasso Variable Importance\u0026#34;) 4pcgp\u0026lt;-pcrCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;PCR Variable Importance\u0026#34;) 5plgp\u0026lt;-plsCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;PLS Variable Importance\u0026#34;) 6grid.arrange(lgp,rgp,lsgp,pcgp,plgp,ncol=3)  Question 6.10 - Pages 263-264 We have seen that as the number of features used in a model increases, the training error will necessarily decrease, but the test error may not. We will now explore this in a simulated data set.\n(a) Generate a data set with \\(p = 20\\) features, \\(n = 1,000\\) observations, and an associated quantitative response vector generated according to the model \\[Y = X\\beta + \\eta,\\] where \\(\\beta\\) has some elements that are exactly equal to zero.\n(b) Split your data set into a training set containing \\(100\\) observations and a test set containing \\(900\\) observations.\n(c) Perform best subset selection on the training set, and plot the training set MSE associated with the best model of each size.\n(d) Plot the test set MSE associated with the best model of each size.\n(e) For which model size does the test set MSE take on its minimum value? Comment on your results. If it takes on its minimum value for a model containing only an intercept or a model containing all of the features, then play around with the way that you are generating the data in (a) until you come up with a scenario in which the test set MSE is minimized for an intermediate model size.\n(f) How does the model at which the test set MSE is minimized compare to the true model used to generate the data? Comment on the coefficient values.\n(g) Create a plot displaying \\(\\sqrt{\\Sum_{j=1}^{p}(\\beta_{j}-\\hat{\\beta}_{j}^{r})^{2}}\\) for a range of values of \\(r\\), where \\(\\hat{\\beta}_{j}^{r}\\) is the $j$th coefficient estimate for the best model containing \\(r\\) coefficients. Comment on what you observe. How does this compare to the test MSE plot from (d)?\nAnswer Model creation 1p=20 2n=1000 3noise\u0026lt;-rnorm(n) 4xmat\u0026lt;-matrix(rnorm(n*p),nrow=n,ncol=p) 5beta\u0026lt;-sample(-10:34,20) 6beta[sample(1:20,4)]=0 7myY\u0026lt;-xmat %*% beta + noise 8modelDat\u0026lt;-data.frame(x=as.matrix(xmat),y=myY)  As always we will want to take a peak   1modelDat %\u0026gt;% str %\u0026gt;% print 1## \u0026#39;data.frame\u0026#39;: 1000 obs. of 21 variables: 2## $ x.1 : num -0.406 -1.375 0.858 -0.231 -0.601 ... 3## $ x.2 : num -0.129 -0.218 -0.17 0.573 -0.513 ... 4## $ x.3 : num 0.127 -0.224 1.014 0.896 0.159 ... 5## $ x.4 : num 0.499 -0.151 -0.488 -0.959 2.187 ... 6## $ x.5 : num -0.235 -0.345 -0.773 -0.346 0.773 ... 7## $ x.6 : num 0.26 -0.429 -1.183 -1.159 0.959 ... 8## $ x.7 : num 0.567 1.647 0.149 -0.593 -0.902 ... 9## $ x.8 : num -0.092 0.8391 -1.4835 0.0229 -0.1353 ... 10## $ x.9 : num -0.998 -1.043 -0.563 -0.377 0.324 ... 11## $ x.10: num -0.4401 -0.195 -0.5139 -0.0156 -0.9543 ... 12## $ x.11: num -0.147 0.829 0.165 0.101 -0.105 ... 13## $ x.12: num -0.0118 1.02 1.0794 1.3184 -2.2844 ... 14## $ x.13: num -1.683 0.487 -1.142 -0.744 -0.175 ... 15## $ x.14: num 0.228 -1.031 -2.798 -0.646 0.56 ... 16## $ x.15: num -0.718 0.508 0.637 -0.556 0.585 ... 17## $ x.16: num -1.6378 0.581 -0.9939 0.0537 -0.5854 ... 18## $ x.17: num 1.758 -0.616 1.377 -0.876 -1.174 ... 19## $ x.18: num -1.438 0.373 1.364 0.399 0.949 ... 20## $ x.19: num -0.715 -0.731 1.142 0.149 0.916 ... 21## $ x.20: num 2.774 -2.024 1.316 0.138 0.187 ... 22## $ y : num 77.5 -82.8 -38.9 -79.7 64.9 ... 23## NULL 1modelDat %\u0026gt;% summary %\u0026gt;% print 1## x.1 x.2 x.3 x.4 2## Min. :-2.79766 Min. :-3.13281 Min. :-2.71232 Min. :-4.29604 3## 1st Qu.:-0.60516 1st Qu.:-0.66759 1st Qu.:-0.60561 1st Qu.:-0.66598 4## Median : 0.04323 Median : 0.03681 Median : 0.06556 Median : 0.06589 5## Mean : 0.06879 Mean : 0.01004 Mean : 0.06443 Mean : 0.02244 6## 3rd Qu.: 0.74049 3rd Qu.: 0.68234 3rd Qu.: 0.70521 3rd Qu.: 0.71174 7## Max. : 3.50354 Max. : 3.47268 Max. : 3.02817 Max. : 3.27326 8## x.5 x.6 x.7 x.8 9## Min. :-3.228376 Min. :-4.24014 Min. :-2.98577 Min. :-3.27770 10## 1st Qu.:-0.698220 1st Qu.:-0.69448 1st Qu.:-0.59092 1st Qu.:-0.52939 11## Median :-0.058778 Median :-0.01141 Median : 0.01732 Median : 0.05703 12## Mean : 0.000126 Mean :-0.05158 Mean : 0.04767 Mean : 0.08231 13## 3rd Qu.: 0.663570 3rd Qu.: 0.64217 3rd Qu.: 0.67438 3rd Qu.: 0.72849 14## Max. : 3.036307 Max. : 3.27572 Max. : 2.72163 Max. : 3.33409 15## x.9 x.10 x.11 x.12 16## Min. :-3.08957 Min. :-3.21268 Min. :-3.00572 Min. :-3.72016 17## 1st Qu.:-0.65456 1st Qu.:-0.69401 1st Qu.:-0.68226 1st Qu.:-0.63043 18## Median :-0.04242 Median :-0.03069 Median :-0.04777 Median : 0.07079 19## Mean : 0.02049 Mean :-0.02400 Mean :-0.03729 Mean : 0.03769 20## 3rd Qu.: 0.71209 3rd Qu.: 0.61540 3rd Qu.: 0.64873 3rd Qu.: 0.67155 21## Max. : 3.23110 Max. : 2.76059 Max. : 2.87306 Max. : 3.48569 22## x.13 x.14 x.15 x.16 23## Min. :-3.20126 Min. :-3.55432 Min. :-2.857575 Min. :-3.5383 24## 1st Qu.:-0.68535 1st Qu.:-0.66752 1st Qu.:-0.658708 1st Qu.:-0.7813 25## Median :-0.01329 Median :-0.03302 Median : 0.020581 Median :-0.0740 26## Mean : 0.01094 Mean : 0.02113 Mean : 0.007976 Mean :-0.0883 27## 3rd Qu.: 0.64877 3rd Qu.: 0.74919 3rd Qu.: 0.670464 3rd Qu.: 0.5568 28## Max. : 2.78973 Max. : 3.47923 Max. : 2.891527 Max. : 3.0938 29## x.17 x.18 x.19 x.20 30## Min. :-3.28570 Min. :-4.06416 Min. :-3.0443 Min. :-4.06307 31## 1st Qu.:-0.72302 1st Qu.:-0.72507 1st Qu.:-0.6684 1st Qu.:-0.70518 32## Median :-0.02439 Median :-0.04941 Median :-0.0610 Median :-0.07697 33## Mean :-0.01459 Mean :-0.03164 Mean :-0.0414 Mean :-0.05302 34## 3rd Qu.: 0.62692 3rd Qu.: 0.68115 3rd Qu.: 0.6381 3rd Qu.: 0.58597 35## Max. : 2.86446 Max. : 3.32958 Max. : 3.1722 Max. : 3.01358 36## y 37## Min. :-199.268 38## 1st Qu.: -54.758 39## Median : -1.607 40## Mean : -1.710 41## 3rd Qu.: 49.367 42## Max. : 278.244 b) Train Test Split 1train_ind = sample(modelDat %\u0026gt;% nrow,100) 2test_ind = setdiff(seq_len(modelDat %\u0026gt;% nrow), train_set) Best subset selection 1train_set\u0026lt;-modelDat[train_ind,] 2test_set\u0026lt;-modelDat[-train_ind,] 1linCol\u0026lt;-train(y~.,data=train_set,method=\u0026#34;lm\u0026#34;) 2linCol %\u0026gt;% summary 1## 2## Call: 3## lm(formula = .outcome ~ ., data = dat) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -2.12474 -0.53970 -0.00944 0.42398 2.21086 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) -0.06052 0.09604 -0.630 0.530 12## x.1 -0.02265 0.09198 -0.246 0.806 13## x.2 28.91650 0.09879 292.719 \u0026lt;2e-16 *** 14## x.3 14.16532 0.09343 151.610 \u0026lt;2e-16 *** 15## x.4 28.16256 0.09828 286.564 \u0026lt;2e-16 *** 16## x.5 0.13742 0.09658 1.423 0.159 17## x.6 27.01497 0.08540 316.341 \u0026lt;2e-16 *** 18## x.7 31.15917 0.09003 346.092 \u0026lt;2e-16 *** 19## x.8 -9.66308 0.11095 -87.094 \u0026lt;2e-16 *** 20## x.9 0.11641 0.10768 1.081 0.283 21## x.10 19.06687 0.09662 197.344 \u0026lt;2e-16 *** 22## x.11 -9.09956 0.08627 -105.472 \u0026lt;2e-16 *** 23## x.12 -8.01933 0.10198 -78.633 \u0026lt;2e-16 *** 24## x.13 4.26852 0.09888 43.170 \u0026lt;2e-16 *** 25## x.14 20.22366 0.09853 205.247 \u0026lt;2e-16 *** 26## x.15 -0.16607 0.10466 -1.587 0.117 27## x.16 7.95594 0.11250 70.721 \u0026lt;2e-16 *** 28## x.17 10.89851 0.11157 97.684 \u0026lt;2e-16 *** 29## x.18 -1.09760 0.09391 -11.688 \u0026lt;2e-16 *** 30## x.19 22.05197 0.08697 253.553 \u0026lt;2e-16 *** 31## x.20 20.88796 0.09274 225.221 \u0026lt;2e-16 *** 32## --- 33## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 34## 35## Residual standard error: 0.8583 on 79 degrees of freedom 36## Multiple R-squared: 0.9999, Adjusted R-squared: 0.9999 37## F-statistic: 4.71e+04 on 20 and 79 DF, p-value: \u0026lt; 2.2e-16 1linPred\u0026lt;-predict(linCol,test_set) 2linPred %\u0026gt;% postResample(obs = test_set$y) 1## RMSE Rsquared MAE 2## 1.2151815 0.9997265 0.9638378 1L2Grid \u0026lt;- expand.grid(alpha=0, 2lambda=10^seq(from=-3,to=30,length=100)) 1ridgCol\u0026lt;-train(y~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L2Grid) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1ridgCol %\u0026gt;% summary %\u0026gt;% print 1## Length Class Mode 2## a0 100 -none- numeric 3## beta 2000 dgCMatrix S4 4## df 100 -none- numeric 5## dim 2 -none- numeric 6## lambda 100 -none- numeric 7## dev.ratio 100 -none- numeric 8## nulldev 1 -none- numeric 9## npasses 1 -none- numeric 10## jerr 1 -none- numeric 11## offset 1 -none- logical 12## call 5 -none- call 13## nobs 1 -none- numeric 14## lambdaOpt 1 -none- numeric 15## xNames 20 -none- character 16## problemType 1 -none- character 17## tuneValue 2 data.frame list 18## obsLevels 1 -none- logical 19## param 0 -none- list 1coef(ridgCol$finalModel, ridgCol$bestTune$lambda) %\u0026gt;% print 1## 21 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) 0.03898376 4## x.1 -0.12140945 5## x.2 27.63771674 6## x.3 13.46853844 7## x.4 26.54402352 8## x.5 -0.13838118 9## x.6 25.87706885 10## x.7 29.90687677 11## x.8 -9.42088971 12## x.9 -0.08983349 13## x.10 17.45444598 14## x.11 -8.33991071 15## x.12 -7.23653865 16## x.13 3.35145521 17## x.14 19.42178898 18## x.15 -0.02794731 19## x.16 7.63951382 20## x.17 11.08083907 21## x.18 -1.36872894 22## x.19 20.90257005 23## x.20 20.07494414 1ggplot(ridgCol)  1ridgPred\u0026lt;-predict(ridgCol,test_set) 2ridgPred %\u0026gt;% postResample(obs = test_set$y) 1## RMSE Rsquared MAE 2## 3.7554417 0.9994231 3.0184859 1L1Grid \u0026lt;- expand.grid(alpha=1, # for lasso 2lambda=10^seq(from=-3,to=30,length=100)) 1lassoCol\u0026lt;-train(y~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L1Grid) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1lassoCol %\u0026gt;% summary %\u0026gt;% print 1## Length Class Mode 2## a0 47 -none- numeric 3## beta 940 dgCMatrix S4 4## df 47 -none- numeric 5## dim 2 -none- numeric 6## lambda 47 -none- numeric 7## dev.ratio 47 -none- numeric 8## nulldev 1 -none- numeric 9## npasses 1 -none- numeric 10## jerr 1 -none- numeric 11## offset 1 -none- logical 12## call 5 -none- call 13## nobs 1 -none- numeric 14## lambdaOpt 1 -none- numeric 15## xNames 20 -none- character 16## problemType 1 -none- character 17## tuneValue 2 data.frame list 18## obsLevels 1 -none- logical 19## param 0 -none- list 1coef(lassoCol$finalModel, lassoCol$bestTune$lambda) %\u0026gt;% print 1## 21 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) 0.1158884 4## x.1 . 5## x.2 28.5897869 6## x.3 13.3637110 7## x.4 27.2558797 8## x.5 . 9## x.6 26.6625588 10## x.7 30.6841774 11## x.8 -9.1388677 12## x.9 . 13## x.10 17.9220939 14## x.11 -8.2461257 15## x.12 -7.0603651 16## x.13 3.2052101 17## x.14 19.7219890 18## x.15 . 19## x.16 7.2082509 20## x.17 10.4137411 21## x.18 -0.6693664 22## x.19 21.5357460 23## x.20 20.5226071 1ggplot(lassoCol)  1lassoPred\u0026lt;-predict(lassoCol,test_set) 2lassoPred %\u0026gt;% postResample(obs = test_set$y) 1## RMSE Rsquared MAE 2## 2.7289452 0.9992454 2.2029482 1mGrid \u0026lt;- expand.grid(ncomp=seq(from=1,to=20,length=10)) 1pcrCol\u0026lt;-train(y~.,data=train_set,method=\u0026#34;pcr\u0026#34;,tuneGrid = mGrid) 2pcrCol %\u0026gt;% summary %\u0026gt;% print 1## Data: X dimension: 100 20 2## Y dimension: 100 1 3## Fit method: svdpc 4## Number of components considered: 20 5## TRAINING: % variance explained 6## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps 7## X 10.040 18.46 26.62 34.56 41.87 48.54 54.56 8## .outcome 8.425 34.90 41.09 43.12 45.06 48.09 66.44 9## 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps 14 comps 10## X 60.33 65.37 70.01 74.46 78.72 82.32 85.67 11## .outcome 85.76 88.81 89.93 91.66 91.91 92.04 92.08 12## 15 comps 16 comps 17 comps 18 comps 19 comps 20 comps 13## X 88.85 91.94 94.59 96.73 98.51 100.00 14## .outcome 92.15 94.96 99.51 99.57 99.76 99.99 15## NULL 1ggplot(pcrCol)  1pcrPred\u0026lt;-predict(pcrCol,test_set) 2pcrPred %\u0026gt;% postResample(obs = test_set$y) 1## RMSE Rsquared MAE 2## 1.2151815 0.9997265 0.9638378 1plsCol\u0026lt;-train(y~.,data=train_set,method=\u0026#34;pls\u0026#34;,tuneGrid = mGrid) 2plsCol %\u0026gt;% summary %\u0026gt;% print 1## Data: X dimension: 100 20 2## Y dimension: 100 1 3## Fit method: oscorespls 4## Number of components considered: 20 5## TRAINING: % variance explained 6## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps 7## X 7.762 14.79 21.01 26.89 31.55 36.13 41.12 8## .outcome 92.765 98.81 99.75 99.96 99.98 99.99 99.99 9## 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps 14 comps 10## X 46.35 51.21 56.12 60.34 65.63 71.57 76.16 11## .outcome 99.99 99.99 99.99 99.99 99.99 99.99 99.99 12## 15 comps 16 comps 17 comps 18 comps 19 comps 20 comps 13## X 80.72 84.69 88.98 92.69 96.71 100.00 14## .outcome 99.99 99.99 99.99 99.99 99.99 99.99 15## NULL 1ggplot(plsCol)  1plsPred\u0026lt;-predict(plsCol,test_set) 2plsPred %\u0026gt;% postResample(obs = test_set$y) 1## RMSE Rsquared MAE 2## 1.2151815 0.9997265 0.9638378 d) Test MSE for best models  All the models have the same R² but Ridge does the worst followed by LASSO  For the rest of the question, we will consider the OLS model.\n1modelFit\u0026lt;-regsubsets(y~.,data=modelDat,nvmax=20) 2modelFit %\u0026gt;% summary %\u0026gt;% print 1## Subset selection object 2## Call: regsubsets.formula(y ~ ., data = modelDat, nvmax = 20) 3## 20 Variables (and intercept) 4## Forced in Forced out 5## x.1 FALSE FALSE 6## x.2 FALSE FALSE 7## x.3 FALSE FALSE 8## x.4 FALSE FALSE 9## x.5 FALSE FALSE 10## x.6 FALSE FALSE 11## x.7 FALSE FALSE 12## x.8 FALSE FALSE 13## x.9 FALSE FALSE 14## x.10 FALSE FALSE 15## x.11 FALSE FALSE 16## x.12 FALSE FALSE 17## x.13 FALSE FALSE 18## x.14 FALSE FALSE 19## x.15 FALSE FALSE 20## x.16 FALSE FALSE 21## x.17 FALSE FALSE 22## x.18 FALSE FALSE 23## x.19 FALSE FALSE 24## x.20 FALSE FALSE 25## 1 subsets of each size up to 20 26## Selection Algorithm: exhaustive 27## x.1 x.2 x.3 x.4 x.5 x.6 x.7 x.8 x.9 x.10 x.11 x.12 x.13 x.14 x.15 28## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 29## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 30## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 31## 4 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 32## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 33## 6 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 34## 7 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 35## 8 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 36## 9 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 37## 10 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 38## 11 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 39## 12 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 40## 13 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 41## 14 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 42## 15 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 43## 16 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 44## 17 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 45## 18 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 46## 19 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 47## 20 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 48## x.16 x.17 x.18 x.19 x.20 49## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 50## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 51## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 52## 4 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 53## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 54## 6 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 55## 7 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 56## 8 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 57## 9 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 58## 10 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 59## 11 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 60## 12 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 61## 13 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 62## 14 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 63## 15 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 64## 16 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 65## 17 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 66## 18 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 67## 19 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 68## 20 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; We might want to take a look at these.\n1par(mfrow=c(2,2)) 2plot(modelFit) 3plot(modelFit,scale=\u0026#39;Cp\u0026#39;) 4plot(modelFit,scale=\u0026#39;r2\u0026#39;) 5plot(modelFit,scale=\u0026#39;adjr2\u0026#39;)  1plotLEAP(modelFit %\u0026gt;% summary)  It would appear that 16 variables would be a good bet. We note that the lasso model did void out 4 parameters, namely x₁,x₃,x₁₃ and x₁₇.\nLets take a quick look at the various model variable significance values.\n1lgp\u0026lt;-linCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;OLS Variable Importance\u0026#34;) 2rgp\u0026lt;-ridgCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Ridge Variable Importance\u0026#34;) 3lsgp\u0026lt;-lassoCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Lasso Variable Importance\u0026#34;) 4pcgp\u0026lt;-pcrCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;PCR Variable Importance\u0026#34;) 5plgp\u0026lt;-plsCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;PLS Variable Importance\u0026#34;) 6grid.arrange(lgp,rgp,lsgp,pcgp,plgp,ncol=3,bottom=\u0026#34;Effective Importance, scaled\u0026#34;)  e) Model size The test set numeric minimum RMSE is a tie between OLS and PCR, and this was achieved for the (effective) 16 variable OLS model, as well as the 18 variable PCR model.\nf) Best model We will consider the OLS and PCR models and its parameters.\n1linCol$finalModel %\u0026gt;% print 1## 2## Call: 3## lm(formula = .outcome ~ ., data = dat) 4## 5## Coefficients: 6## (Intercept) x.1 x.2 x.3 x.4 x.5 7## -0.06052 -0.02265 28.91650 14.16532 28.16256 0.13742 8## x.6 x.7 x.8 x.9 x.10 x.11 9## 27.01497 31.15917 -9.66308 0.11641 19.06687 -9.09956 10## x.12 x.13 x.14 x.15 x.16 x.17 11## -8.01933 4.26852 20.22366 -0.16607 7.95594 10.89851 12## x.18 x.19 x.20 13## -1.09760 22.05197 20.88796 1pcrCol$bestTune %\u0026gt;% print 1## ncomp 2## 10 20 Now to compare this to the original.\n1beta %\u0026gt;% print 1## [1] 0 29 14 28 0 27 31 -10 0 19 -9 -8 4 20 0 8 11 -1 22 2## [20] 21 1t=data.frame(linCol$finalModel$coefficients[-1]) %\u0026gt;% rename(\u0026#34;Model_Coeffs\u0026#34;=1) %\u0026gt;% add_column(beta) %\u0026gt;% rename(\u0026#34;Original_Coeffs\u0026#34;=2) 2print(t) 1## Model_Coeffs Original_Coeffs 2## x.1 -0.02265289 0 3## x.2 28.91649699 29 4## x.3 14.16532050 14 5## x.4 28.16255937 28 6## x.5 0.13741621 0 7## x.6 27.01497459 27 8## x.7 31.15917172 31 9## x.8 -9.66308362 -10 10## x.9 0.11641282 0 11## x.10 19.06687041 19 12## x.11 -9.09955826 -9 13## x.12 -8.01932598 -8 14## x.13 4.26852334 4 15## x.14 20.22366153 20 16## x.15 -0.16606531 0 17## x.16 7.95593559 8 18## x.17 10.89851353 11 19## x.18 -1.09759687 -1 20## x.19 22.05196537 22 21## x.20 20.88795623 21 We see that the coefficients are pretty similar.\ng) Plotting differences 1val.errors = rep(NaN, p) 2a = rep(NaN, p) 3b = rep(NaN, p) 4x_cols = colnames(xmat, do.NULL = FALSE, prefix = \u0026#34;x.\u0026#34;) 5for (i in 1:p) { 6coefi = coef(modelFit, id = i) 7a[i] = length(coefi) - 1 ## Not counting the intercept 8b[i] = sqrt(sum((beta[x_cols %in% names(coefi)] - coefi[names(coefi) %in% x_cols])^2) + 9sum(beta[!(x_cols %in% names(coefi))])^2) ## Handling the intercept 10} 11plot(x = a, y = b, xlab = \u0026#34;Number of Coefficients\u0026#34;, ylab = \u0026#34;Relative Error\u0026#34;)  Question 6.11 - Page 264 We will now try to predict per capita crime rate in the Boston data set.\n(a) Try out some of the regression methods explored in this chapter, such as best subset selection, the lasso, ridge regression, and PCR. Present and discuss results for the approaches that you consider.\n(b) Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, crossvalidation, or some other reasonable alternative, as opposed to using training error.\n(c) Does your chosen model involve all of the features in the data set? Why or why not?\nAnswer 1boston\u0026lt;-MASS::Boston  Summarize   1boston %\u0026gt;% str %\u0026gt;% print 1## \u0026#39;data.frame\u0026#39;: 506 obs. of 14 variables: 2## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... 3## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... 4## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... 5## $ chas : int 0 0 0 0 0 0 0 0 0 0 ... 6## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... 7## $ rm : num 6.58 6.42 7.18 7 7.15 ... 8## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... 9## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... 10## $ rad : int 1 2 2 3 3 3 5 5 5 5 ... 11## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... 12## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... 13## $ black : num 397 397 393 395 397 ... 14## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... 15## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... 16## NULL 1boston %\u0026gt;% summary %\u0026gt;% print 1## crim zn indus chas 2## Min. : 0.00632 Min. : 0.00 Min. : 0.46 Min. :0.00000 3## 1st Qu.: 0.08204 1st Qu.: 0.00 1st Qu.: 5.19 1st Qu.:0.00000 4## Median : 0.25651 Median : 0.00 Median : 9.69 Median :0.00000 5## Mean : 3.61352 Mean : 11.36 Mean :11.14 Mean :0.06917 6## 3rd Qu.: 3.67708 3rd Qu.: 12.50 3rd Qu.:18.10 3rd Qu.:0.00000 7## Max. :88.97620 Max. :100.00 Max. :27.74 Max. :1.00000 8## nox rm age dis 9## Min. :0.3850 Min. :3.561 Min. : 2.90 Min. : 1.130 10## 1st Qu.:0.4490 1st Qu.:5.886 1st Qu.: 45.02 1st Qu.: 2.100 11## Median :0.5380 Median :6.208 Median : 77.50 Median : 3.207 12## Mean :0.5547 Mean :6.285 Mean : 68.57 Mean : 3.795 13## 3rd Qu.:0.6240 3rd Qu.:6.623 3rd Qu.: 94.08 3rd Qu.: 5.188 14## Max. :0.8710 Max. :8.780 Max. :100.00 Max. :12.127 15## rad tax ptratio black 16## Min. : 1.000 Min. :187.0 Min. :12.60 Min. : 0.32 17## 1st Qu.: 4.000 1st Qu.:279.0 1st Qu.:17.40 1st Qu.:375.38 18## Median : 5.000 Median :330.0 Median :19.05 Median :391.44 19## Mean : 9.549 Mean :408.2 Mean :18.46 Mean :356.67 20## 3rd Qu.:24.000 3rd Qu.:666.0 3rd Qu.:20.20 3rd Qu.:396.23 21## Max. :24.000 Max. :711.0 Max. :22.00 Max. :396.90 22## lstat medv 23## Min. : 1.73 Min. : 5.00 24## 1st Qu.: 6.95 1st Qu.:17.02 25## Median :11.36 Median :21.20 26## Mean :12.65 Mean :22.53 27## 3rd Qu.:16.95 3rd Qu.:25.00 28## Max. :37.97 Max. :50.00 1boston %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) %\u0026gt;% print 1## crim zn indus chas nox rm age dis rad tax 2## 504 26 76 2 81 446 356 412 9 66 3## ptratio black lstat medv 4## 46 357 455 229 a) Test regression models 1train_ind = sample(boston %\u0026gt;% nrow,100) 2test_ind = setdiff(seq_len(boston %\u0026gt;% nrow), train_set) 1train_set\u0026lt;-boston[train_ind,] 2test_set\u0026lt;-boston[-train_ind,] 1linCol\u0026lt;-train(crim~.,data=train_set,method=\u0026#34;lm\u0026#34;) 2linCol %\u0026gt;% summary 1## 2## Call: 3## lm(formula = .outcome ~ ., data = dat) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -6.2431 -1.0344 -0.0563 0.8187 8.1318 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 0.9339246 7.6508393 0.122 0.9031 12## zn 0.0046819 0.0157375 0.297 0.7668 13## indus 0.0276209 0.0875254 0.316 0.7531 14## chas -1.1602278 1.2386869 -0.937 0.3516 15## nox -7.5024503 5.0207818 -1.494 0.1388 16## rm 1.1240874 0.7462340 1.506 0.1356 17## age 0.0020182 0.0137404 0.147 0.8836 18## dis -0.3934753 0.2454365 -1.603 0.1126 19## rad 0.4540613 0.0791580 5.736 1.41e-07 *** 20## tax 0.0008469 0.0052593 0.161 0.8724 21## ptratio -0.2978204 0.1637629 -1.819 0.0725 . 22## black 0.0030642 0.0045281 0.677 0.5004 23## lstat 0.1322779 0.0626485 2.111 0.0376 * 24## medv -0.0841382 0.0590700 -1.424 0.1580 25## --- 26## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 27## 28## Residual standard error: 2.362 on 86 degrees of freedom 29## Multiple R-squared: 0.775, Adjusted R-squared: 0.741 30## F-statistic: 22.78 on 13 and 86 DF, p-value: \u0026lt; 2.2e-16 1linPred\u0026lt;-predict(linCol,test_set) 2linPred %\u0026gt;% postResample(obs = test_set$crim) 1## RMSE Rsquared MAE 2## 7.3794735 0.4056002 2.6774969 1L2Grid \u0026lt;- expand.grid(alpha=0, 2lambda=10^seq(from=-3,to=30,length=100)) 1ridgCol\u0026lt;-train(crim~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L2Grid) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1ridgCol %\u0026gt;% summary %\u0026gt;% print 1## Length Class Mode 2## a0 100 -none- numeric 3## beta 1300 dgCMatrix S4 4## df 100 -none- numeric 5## dim 2 -none- numeric 6## lambda 100 -none- numeric 7## dev.ratio 100 -none- numeric 8## nulldev 1 -none- numeric 9## npasses 1 -none- numeric 10## jerr 1 -none- numeric 11## offset 1 -none- logical 12## call 5 -none- call 13## nobs 1 -none- numeric 14## lambdaOpt 1 -none- numeric 15## xNames 13 -none- character 16## problemType 1 -none- character 17## tuneValue 2 data.frame list 18## obsLevels 1 -none- logical 19## param 0 -none- list 1coef(ridgCol$finalModel, ridgCol$bestTune$lambda) %\u0026gt;% print 1## 14 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) -3.881166065 4## zn 0.002597790 5## indus -0.005103517 6## chas -0.674764337 7## nox -0.053645732 8## rm 0.600064844 9## age 0.001153570 10## dis -0.179295384 11## rad 0.267082956 12## tax 0.006447932 13## ptratio -0.075885753 14## black -0.001650403 15## lstat 0.086462700 16## medv -0.027519270 1ggplot(ridgCol)  1ridgPred\u0026lt;-predict(ridgCol,test_set) 2ridgPred %\u0026gt;% postResample(obs = test_set$crim) 1## RMSE Rsquared MAE 2## 7.5065916 0.4017056 2.4777547 1L1Grid \u0026lt;- expand.grid(alpha=1, # for lasso 2lambda=10^seq(from=-3,to=30,length=100)) 1lassoCol\u0026lt;-train(crim~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L1Grid) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1lassoCol %\u0026gt;% summary %\u0026gt;% print 1## Length Class Mode 2## a0 78 -none- numeric 3## beta 1014 dgCMatrix S4 4## df 78 -none- numeric 5## dim 2 -none- numeric 6## lambda 78 -none- numeric 7## dev.ratio 78 -none- numeric 8## nulldev 1 -none- numeric 9## npasses 1 -none- numeric 10## jerr 1 -none- numeric 11## offset 1 -none- logical 12## call 5 -none- call 13## nobs 1 -none- numeric 14## lambdaOpt 1 -none- numeric 15## xNames 13 -none- character 16## problemType 1 -none- character 17## tuneValue 2 data.frame list 18## obsLevels 1 -none- logical 19## param 0 -none- list 1coef(lassoCol$finalModel, lassoCol$bestTune$lambda) %\u0026gt;% print 1## 14 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) -2.024006430 4## zn . 5## indus . 6## chas . 7## nox . 8## rm . 9## age . 10## dis -0.008506188 11## rad 0.386379255 12## tax 0.001779579 13## ptratio . 14## black . 15## lstat 0.040788606 16## medv . 1ggplot(lassoCol)  1lassoPred\u0026lt;-predict(lassoCol,test_set) 2lassoPred %\u0026gt;% postResample(obs = test_set$crim) 1## RMSE Rsquared MAE 2## 7.5868293 0.3859121 2.4892258 1mGrid \u0026lt;- expand.grid(ncomp=seq(from=1,to=20,length=10))  All the models have the same R² but Ridge does the worst followed by LASSO  For the rest of the question, we will consider the OLS model.\n1modelFit\u0026lt;-regsubsets(crim~.,data=boston,nvmax=20) 2modelFit %\u0026gt;% summary %\u0026gt;% print 1## Subset selection object 2## Call: regsubsets.formula(crim ~ ., data = boston, nvmax = 20) 3## 13 Variables (and intercept) 4## Forced in Forced out 5## zn FALSE FALSE 6## indus FALSE FALSE 7## chas FALSE FALSE 8## nox FALSE FALSE 9## rm FALSE FALSE 10## age FALSE FALSE 11## dis FALSE FALSE 12## rad FALSE FALSE 13## tax FALSE FALSE 14## ptratio FALSE FALSE 15## black FALSE FALSE 16## lstat FALSE FALSE 17## medv FALSE FALSE 18## 1 subsets of each size up to 13 19## Selection Algorithm: exhaustive 20## zn indus chas nox rm age dis rad tax ptratio black lstat medv 21## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 22## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 23## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 24## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 25## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 26## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 27## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 28## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 29## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 30## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 31## 11 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 32## 12 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 33## 13 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; We might want to take a look at these.\n1par(mfrow=c(2,2)) 2plot(modelFit) 3plot(modelFit,scale=\u0026#39;Cp\u0026#39;) 4plot(modelFit,scale=\u0026#39;r2\u0026#39;) 5plot(modelFit,scale=\u0026#39;adjr2\u0026#39;)  1plotLEAP(modelFit %\u0026gt;% summary)  It would appear that 16 variables would be a good bet. We note that the lasso model did void out 4 parameters, namely x₁,x₃,x₁₃ and x₁₇.\nLets take a quick look at the various model variable significance values.\n1lgp\u0026lt;-linCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;OLS Variable Importance\u0026#34;) 2rgp\u0026lt;-ridgCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Ridge Variable Importance\u0026#34;) 3lsgp\u0026lt;-lassoCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Lasso Variable Importance\u0026#34;) 4grid.arrange(lgp,rgp,lsgp,ncol=2,bottom=\u0026#34;Effective Importance, scaled\u0026#34;)  b) Propose a model   Given the data and plots, I would probably end up using the Ridge regression model\n  Clearly, LASSO is not working very well since it seems to have taken mainly 3 variables, one of which is largely categorical (9 levels)\n  c) Model properties 1boston %\u0026gt;% str %\u0026gt;% print 1## \u0026#39;data.frame\u0026#39;: 506 obs. of 14 variables: 2## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... 3## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... 4## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... 5## $ chas : int 0 0 0 0 0 0 0 0 0 0 ... 6## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... 7## $ rm : num 6.58 6.42 7.18 7 7.15 ... 8## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... 9## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... 10## $ rad : int 1 2 2 3 3 3 5 5 5 5 ... 11## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... 12## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... 13## $ black : num 397 397 393 395 397 ... 14## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... 15## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... 16## NULL 1boston %\u0026gt;% summary %\u0026gt;% print 1## crim zn indus chas 2## Min. : 0.00632 Min. : 0.00 Min. : 0.46 Min. :0.00000 3## 1st Qu.: 0.08204 1st Qu.: 0.00 1st Qu.: 5.19 1st Qu.:0.00000 4## Median : 0.25651 Median : 0.00 Median : 9.69 Median :0.00000 5## Mean : 3.61352 Mean : 11.36 Mean :11.14 Mean :0.06917 6## 3rd Qu.: 3.67708 3rd Qu.: 12.50 3rd Qu.:18.10 3rd Qu.:0.00000 7## Max. :88.97620 Max. :100.00 Max. :27.74 Max. :1.00000 8## nox rm age dis 9## Min. :0.3850 Min. :3.561 Min. : 2.90 Min. : 1.130 10## 1st Qu.:0.4490 1st Qu.:5.886 1st Qu.: 45.02 1st Qu.: 2.100 11## Median :0.5380 Median :6.208 Median : 77.50 Median : 3.207 12## Mean :0.5547 Mean :6.285 Mean : 68.57 Mean : 3.795 13## 3rd Qu.:0.6240 3rd Qu.:6.623 3rd Qu.: 94.08 3rd Qu.: 5.188 14## Max. :0.8710 Max. :8.780 Max. :100.00 Max. :12.127 15## rad tax ptratio black 16## Min. : 1.000 Min. :187.0 Min. :12.60 Min. : 0.32 17## 1st Qu.: 4.000 1st Qu.:279.0 1st Qu.:17.40 1st Qu.:375.38 18## Median : 5.000 Median :330.0 Median :19.05 Median :391.44 19## Mean : 9.549 Mean :408.2 Mean :18.46 Mean :356.67 20## 3rd Qu.:24.000 3rd Qu.:666.0 3rd Qu.:20.20 3rd Qu.:396.23 21## Max. :24.000 Max. :711.0 Max. :22.00 Max. :396.90 22## lstat medv 23## Min. : 1.73 Min. : 5.00 24## 1st Qu.: 6.95 1st Qu.:17.02 25## Median :11.36 Median :21.20 26## Mean :12.65 Mean :22.53 27## 3rd Qu.:16.95 3rd Qu.:25.00 28## Max. :37.97 Max. :50.00 1boston %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) %\u0026gt;% print 1## crim zn indus chas nox rm age dis rad tax 2## 504 26 76 2 81 446 356 412 9 66 3## ptratio black lstat medv 4## 46 357 455 229  A good idea would be removing rad and chas from the regression   1boston\u0026lt;-boston %\u0026gt;% subset(select=-c(rad,chas)) 1train_ind = sample(boston %\u0026gt;% nrow,100) 2test_ind = setdiff(seq_len(boston %\u0026gt;% nrow), train_set) 1train_set\u0026lt;-boston[train_ind,] 2test_set\u0026lt;-boston[-train_ind,] 1L2Grid \u0026lt;- expand.grid(alpha=0, 2lambda=10^seq(from=-3,to=30,length=100)) 1ridgCol\u0026lt;-train(crim~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L2Grid) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1ridgCol %\u0026gt;% summary %\u0026gt;% print 1## Length Class Mode 2## a0 100 -none- numeric 3## beta 1100 dgCMatrix S4 4## df 100 -none- numeric 5## dim 2 -none- numeric 6## lambda 100 -none- numeric 7## dev.ratio 100 -none- numeric 8## nulldev 1 -none- numeric 9## npasses 1 -none- numeric 10## jerr 1 -none- numeric 11## offset 1 -none- logical 12## call 5 -none- call 13## nobs 1 -none- numeric 14## lambdaOpt 1 -none- numeric 15## xNames 11 -none- character 16## problemType 1 -none- character 17## tuneValue 2 data.frame list 18## obsLevels 1 -none- logical 19## param 0 -none- list 1coef(ridgCol$finalModel, ridgCol$bestTune$lambda) %\u0026gt;% print 1## 12 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) 0.23015398 4## zn 0.01595378 5## indus -0.01006683 6## nox 3.42104450 7## rm -0.05904911 8## age 0.01419585 9## dis -0.16724715 10## tax 0.01021790 11## ptratio 0.05741508 12## black -0.01414707 13## lstat 0.15519923 14## medv -0.04483670 1ggplot(ridgCol)  1ridgPred\u0026lt;-predict(ridgCol,test_set) 2ridgPred %\u0026gt;% postResample(obs = test_set$crim) 1## RMSE Rsquared MAE 2## 6.9365371 0.3619974 2.8570012 1L1Grid \u0026lt;- expand.grid(alpha=1, # for lasso 2lambda=10^seq(from=-3,to=30,length=100)) 1lassoCol\u0026lt;-train(crim~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L1Grid) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1lassoCol %\u0026gt;% summary %\u0026gt;% print 1## Length Class Mode 2## a0 74 -none- numeric 3## beta 814 dgCMatrix S4 4## df 74 -none- numeric 5## dim 2 -none- numeric 6## lambda 74 -none- numeric 7## dev.ratio 74 -none- numeric 8## nulldev 1 -none- numeric 9## npasses 1 -none- numeric 10## jerr 1 -none- numeric 11## offset 1 -none- logical 12## call 5 -none- call 13## nobs 1 -none- numeric 14## lambdaOpt 1 -none- numeric 15## xNames 11 -none- character 16## problemType 1 -none- character 17## tuneValue 2 data.frame list 18## obsLevels 1 -none- logical 19## param 0 -none- list 1coef(lassoCol$finalModel, lassoCol$bestTune$lambda) %\u0026gt;% print 1## 12 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) 0.86635621 4## zn . 5## indus . 6## nox . 7## rm . 8## age . 9## dis . 10## tax 0.01402174 11## ptratio . 12## black -0.01454496 13## lstat 0.15290845 14## medv . 1ggplot(lassoCol)  1lassoPred\u0026lt;-predict(lassoCol,test_set) 2lassoPred %\u0026gt;% postResample(obs = test_set$crim) 1## RMSE Rsquared MAE 2## 6.9630616 0.3693364 2.6767742 1rgp\u0026lt;-ridgCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Ridge Variable Importance\u0026#34;) 2lsgp\u0026lt;-lassoCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Lasso Variable Importance\u0026#34;) 3grid.arrange(rgp,lsgp,ncol=2,bottom=\u0026#34;Effective Importance, scaled\u0026#34;)  None of these models are actually any good apparently, given that we have an R² of 0.3619974 for the L2 regularization and 0.3693364 for the L1.\n  James, G., Witten, D., Hastie, T., \u0026amp; Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Berlin, Germany: Springer Science \u0026amp; Business Media.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Lang et al., (2019). mlr3: A modern object-oriented machine learning framework in R. Journal of Open Source Software, 4(44), 1903, https://doi.org/10.21105/joss.01903\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/islr-ch6/","tags":["solutions","R","ISLR"],"title":"ISLR :: Linear Model Selection and Regularization"},{"categories":["programming"],"contents":"Chapter V - Resampling Methods All the questions are as per the ISL seventh printing of the First edition1.\nCommon Instead of using the standard functions, we will leverage the mlr3 package2.\n1#install.packages(\u0026#34;mlr3\u0026#34;,\u0026#34;data.table\u0026#34;,\u0026#34;mlr3viz\u0026#34;,\u0026#34;mlr3learners\u0026#34;) Actually for R version 3.6.2, the steps to get it working were a bit more involved.\n1install.packages(\u0026#34;remotes\u0026#34;,\u0026#34;data.table\u0026#34;, 2\u0026#34;GGally\u0026#34;,\u0026#34;precerec\u0026#34;) # For plots 1library(remotes) 2remotes::install_github(\u0026#34;mlr-org/mlr3\u0026#34;) 3remotes::install_github(\u0026#34;mlr-org/mlr3viz\u0026#34;) 4remotes::install_github(\u0026#34;mlr-org/mlr3learners\u0026#34;) Load ISLR and other libraries.\n1libsUsed\u0026lt;-c(\u0026#34;dplyr\u0026#34;,\u0026#34;ggplot2\u0026#34;,\u0026#34;tidyverse\u0026#34;, 2\u0026#34;ISLR\u0026#34;,\u0026#34;caret\u0026#34;,\u0026#34;MASS\u0026#34;, 3\u0026#34;pROC\u0026#34;,\u0026#34;mlr3\u0026#34;,\u0026#34;data.table\u0026#34;, 4\u0026#34;mlr3viz\u0026#34;,\u0026#34;mlr3learners\u0026#34;) 5invisible(lapply(libsUsed, library, character.only = TRUE)) Question 5.5 - Page 198 In Chapter 4, we used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.\n(a) Fit a logistic regression model that uses income and balance to predict default.\n(b) Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:\n  Split the sample set into a training set and a validation set.\n  Fit a multiple logistic regression model using only the training observations.\n  Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than \\(0.5\\).\n  Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.\n  (c) Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained.\n(d) Now consider a logistic regression model that predicts the prob- ability of default using income , balance , and a dummy variable for student. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate.\nAnswer We will need our data.\n1defDat\u0026lt;-ISLR::Default  Very quick peek   1defDat %\u0026gt;% summary 1## default student balance income 2## No :9667 No :7056 Min. : 0.0 Min. : 772 3## Yes: 333 Yes:2944 1st Qu.: 481.7 1st Qu.:21340 4## Median : 823.6 Median :34553 5## Mean : 835.4 Mean :33517 6## 3rd Qu.:1166.3 3rd Qu.:43808 7## Max. :2654.3 Max. :73554 1defDat %\u0026gt;% str 1## \u0026#39;data.frame\u0026#39;: 10000 obs. of 4 variables: 2## $ default: Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 1 1 1 1 1 1 1 1 1 1 ... 3## $ student: Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 1 2 1 1 1 2 1 2 1 1 ... 4## $ balance: num 730 817 1074 529 786 ... 5## $ income : num 44362 12106 31767 35704 38463 ... a) Logistic Model with mlr3 Following the new approach which leverages R6 features leads us to define a classification task first. As far as I can tell, the data needs to be filtered to contain only the things we need to predict with, in this case we are required to use only income and balance so we will do so.\n1set.seed(1984) 2redDat\u0026lt;-defDat %\u0026gt;% subset(select=c(income,balance,default)) 3tskLogiFull=TaskClassif$new(id=\u0026#34;credit\u0026#34;,backend=redDat,target=\u0026#34;default\u0026#34;) 4print(tskLogiFull) 1## \u0026lt;TaskClassif:credit\u0026gt; (10000 x 3) 2## * Target: default 3## * Properties: twoclass 4## * Features (2): 5## - dbl (2): balance, income This can be visualized neatly as well.\n1autoplot(tskLogiFull)  Figure 1: MLR3 Visualizations\n  We have a pretty imbalanced data-set.\n1autoplot(tskLogiFull,type=\u0026#34;pairs\u0026#34;) 1## Registered S3 method overwritten by \u0026#39;GGally\u0026#39;: 2## method from 3## +.gg ggplot2 1## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 2## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  Figure 2: Paired mlr3 data\n  We can use any of the learners implemented, so it is a good idea to take a quick peek at them all.\n1as.data.table(mlr_learners) 1## key feature_types 2## 1: classif.debug logical,integer,numeric,character,factor,ordered 3## 2: classif.featureless logical,integer,numeric,character,factor,ordered 4## 3: classif.glmnet logical,integer,numeric 5## 4: classif.kknn logical,integer,numeric,factor,ordered 6## 5: classif.lda logical,integer,numeric,factor,ordered 7## 6: classif.log_reg logical,integer,numeric,character,factor,ordered 8## 7: classif.naive_bayes logical,integer,numeric,factor 9## 8: classif.qda logical,integer,numeric,factor,ordered 10## 9: classif.ranger logical,integer,numeric,character,factor,ordered 11## 10: classif.rpart logical,integer,numeric,factor,ordered 12## 11: classif.svm logical,integer,numeric 13## 12: classif.xgboost logical,integer,numeric 14## 13: regr.featureless logical,integer,numeric,character,factor,ordered 15## 14: regr.glmnet logical,integer,numeric 16## 15: regr.kknn logical,integer,numeric,factor,ordered 17## 16: regr.km logical,integer,numeric 18## 17: regr.lm logical,integer,numeric,factor 19## 18: regr.ranger logical,integer,numeric,character,factor,ordered 20## 19: regr.rpart logical,integer,numeric,factor,ordered 21## 20: regr.svm logical,integer,numeric 22## 21: regr.xgboost logical,integer,numeric 23## key feature_types 24## packages 25## 1: 26## 2: 27## 3: glmnet 28## 4: kknn 29## 5: MASS 30## 6: stats 31## 7: e1071 32## 8: MASS 33## 9: ranger 34## 10: rpart 35## 11: e1071 36## 12: xgboost 37## 13: stats 38## 14: glmnet 39## 15: kknn 40## 16: DiceKriging 41## 17: stats 42## 18: ranger 43## 19: rpart 44## 20: e1071 45## 21: xgboost 46## packages 47## properties 48## 1: missings,multiclass,twoclass 49## 2: importance,missings,multiclass,selected_features,twoclass 50## 3: multiclass,twoclass,weights 51## 4: multiclass,twoclass 52## 5: multiclass,twoclass,weights 53## 6: twoclass,weights 54## 7: multiclass,twoclass 55## 8: multiclass,twoclass,weights 56## 9: importance,multiclass,oob_error,twoclass,weights 57## 10: importance,missings,multiclass,selected_features,twoclass,weights 58## 11: multiclass,twoclass 59## 12: importance,missings,multiclass,twoclass,weights 60## 13: importance,missings,selected_features 61## 14: weights 62## 15: 63## 16: 64## 17: weights 65## 18: importance,oob_error,weights 66## 19: importance,missings,selected_features,weights 67## 20: 68## 21: importance,missings,weights 69## properties 70## predict_types 71## 1: response,prob 72## 2: response,prob 73## 3: response,prob 74## 4: response,prob 75## 5: response,prob 76## 6: response,prob 77## 7: response,prob 78## 8: response,prob 79## 9: response,prob 80## 10: response,prob 81## 11: response,prob 82## 12: response,prob 83## 13: response,se 84## 14: response 85## 15: response 86## 16: response,se 87## 17: response,se 88## 18: response,se 89## 19: response 90## 20: response 91## 21: response 92## predict_types We can now pick the logistic one. Note that this essentially proxies our requests down to the stats package.\n1learner = mlr_learners$get(\u0026#34;classif.log_reg\u0026#34;) Now we can final solve the question, which is to simply use the model on all our data and return the accuracy metrics.\n1trainFullCred=learner$train(tskLogiFull) 2print(learner$predict(tskLogiFull)$confusion) 1## truth 2## response No Yes 3## No 9629 225 4## Yes 38 108 1measure = msr(\u0026#34;classif.acc\u0026#34;) 2print(learner$predict(tskLogiFull)$score(measure)) 1## classif.acc 2## 0.9737 Note that this style of working with objects does not really utilize the familiar %\u0026gt;% interface.\nThe caret package still has neater default metrics so we will use that as well.\n1confusionMatrix(learner$predict(tskLogiFull)$response,defDat$default) 1## Confusion Matrix and Statistics 2## 3## Reference 4## Prediction No Yes 5## No 9629 225 6## Yes 38 108 7## 8## Accuracy : 0.9737 9## 95% CI : (0.9704, 0.9767) 10## No Information Rate : 0.9667 11## P-Value [Acc \u0026gt; NIR] : 3.067e-05 12## 13## Kappa : 0.4396 14## 15## Mcnemar\u0026#39;s Test P-Value : \u0026lt; 2.2e-16 16## 17## Sensitivity : 0.9961 18## Specificity : 0.3243 19## Pos Pred Value : 0.9772 20## Neg Pred Value : 0.7397 21## Prevalence : 0.9667 22## Detection Rate : 0.9629 23## Detection Prevalence : 0.9854 24## Balanced Accuracy : 0.6602 25## 26## \u0026#39;Positive\u0026#39; Class : No 27## 1autoplot(learner$predict(tskLogiFull))  Figure 3: Autoplot results\n  We can get some other plots as well, but we need our probabilities to be returned.\n1# For ROC curves 2lrnprob = lrn(\u0026#34;classif.log_reg\u0026#34;,predict_type=\u0026#34;prob\u0026#34;) 3lrnprob$train(tskLogiFull) 4autoplot(lrnprob$predict(tskLogiFull),type=\u0026#34;roc\u0026#34;)  Figure 4: ROC curve\n  b) Validation Sets with mlr3 Though the question seems to require a manual validation set generation and thresholding, we can simply use the defaults.\n1train_set = sample(tskLogiFull$nrow, 0.8 * tskLogiFull$nrow) 2test_set = setdiff(seq_len(tskLogiFull$nrow), train_set) 3learner$train(tskLogiFull,row_ids=train_set) 4confusionMatrix(learner$predict(tskLogiFull, row_ids=test_set)$response,defDat[-train_set,]$default) 1## Confusion Matrix and Statistics 2## 3## Reference 4## Prediction No Yes 5## No 1921 47 6## Yes 9 23 7## 8## Accuracy : 0.972 9## 95% CI : (0.9638, 0.9788) 10## No Information Rate : 0.965 11## P-Value [Acc \u0026gt; NIR] : 0.04663 12## 13## Kappa : 0.4387 14## 15## Mcnemar\u0026#39;s Test P-Value : 7.641e-07 16## 17## Sensitivity : 0.9953 18## Specificity : 0.3286 19## Pos Pred Value : 0.9761 20## Neg Pred Value : 0.7188 21## Prevalence : 0.9650 22## Detection Rate : 0.9605 23## Detection Prevalence : 0.9840 24## Balanced Accuracy : 0.6620 25## 26## \u0026#39;Positive\u0026#39; Class : No 27## For a reasonable comparison, we will demonstrate a standard approach as well. In this instance we will not use caret to ensure that our class distribution in the train and test sets are not sampled to remain the same.\n1trainNoCaret\u0026lt;-sample(nrow(defDat), size = floor(.8*nrow(defDat)), replace = F) 2glm.fit=glm(default~income+balance,data=defDat,family=binomial,subset=trainNoCaret) 3glm.probs\u0026lt;-predict(glm.fit,defDat[-trainNoCaret,],type=\u0026#34;response\u0026#34;) 4glm.preds\u0026lt;-ifelse(glm.probs \u0026lt; 0.5, \u0026#34;No\u0026#34;, \u0026#34;Yes\u0026#34;) 5confusionMatrix(glm.preds %\u0026gt;% factor,defDat[-trainNoCaret,]$default) 1## Confusion Matrix and Statistics 2## 3## Reference 4## Prediction No Yes 5## No 1930 46 6## Yes 6 18 7## 8## Accuracy : 0.974 9## 95% CI : (0.966, 0.9805) 10## No Information Rate : 0.968 11## P-Value [Acc \u0026gt; NIR] : 0.06859 12## 13## Kappa : 0.3986 14## 15## Mcnemar\u0026#39;s Test P-Value : 6.362e-08 16## 17## Sensitivity : 0.9969 18## Specificity : 0.2812 19## Pos Pred Value : 0.9767 20## Neg Pred Value : 0.7500 21## Prevalence : 0.9680 22## Detection Rate : 0.9650 23## Detection Prevalence : 0.9880 24## Balanced Accuracy : 0.6391 25## 26## \u0026#39;Positive\u0026#39; Class : No 27## Since the two approaches use different samples there is a little variation, but we can see that the accuracy is essentially the same.\nc) 3-fold cross validation As per the question, we can repeat the block above three times, or extract it into a function which takes a seed value and run that three times. Either way, here we will present the mlr3 approach to cross validation and resampling.\n1rr = resample(tskLogiFull, lrnprob, rsmp(\u0026#34;cv\u0026#34;, folds = 3)) 1## INFO [22:12:30.025] Applying learner \u0026#39;classif.log_reg\u0026#39; on task \u0026#39;credit\u0026#39; (iter 1/3) 2## INFO [22:12:30.212] Applying learner \u0026#39;classif.log_reg\u0026#39; on task \u0026#39;credit\u0026#39; (iter 2/3) 3## INFO [22:12:30.360] Applying learner \u0026#39;classif.log_reg\u0026#39; on task \u0026#39;credit\u0026#39; (iter 3/3) 1autoplot(rr,type=\u0026#34;roc\u0026#34;)  Figure 5: Resampled ROC curve\n  We might want the average as well.\n1rr$aggregate(msr(\u0026#34;classif.ce\u0026#34;)) %\u0026gt;% print 1## classif.ce 2## 0.02630035 Adding Student as a dummy variable We will stick to the mlr3 approach because it is faster.\n1redDat2\u0026lt;-defDat %\u0026gt;% mutate(student=as.numeric(defDat$student)) 2tskLogi2=TaskClassif$new(id=\u0026#34;credit\u0026#34;,backend=redDat2,target=\u0026#34;default\u0026#34;) 3print(tskLogi2) 1## \u0026lt;TaskClassif:credit\u0026gt; (10000 x 4) 2## * Target: default 3## * Properties: twoclass 4## * Features (3): 5## - dbl (3): balance, income, student 1autoplot(tskLogi2,type=\u0026#34;pairs\u0026#34;) 1## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 2## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 3## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  Figure 6: Logistic regression pairs data\n  This gives us a visual indicator and premonition that we might not be getting incredible results with our new variable in the mix, but we should still work it through.\n1confusionMatrix(lrnprob$predict(tskLogi2)$response,defDat$default) 1## Confusion Matrix and Statistics 2## 3## Reference 4## Prediction No Yes 5## No 9629 225 6## Yes 38 108 7## 8## Accuracy : 0.9737 9## 95% CI : (0.9704, 0.9767) 10## No Information Rate : 0.9667 11## P-Value [Acc \u0026gt; NIR] : 3.067e-05 12## 13## Kappa : 0.4396 14## 15## Mcnemar\u0026#39;s Test P-Value : \u0026lt; 2.2e-16 16## 17## Sensitivity : 0.9961 18## Specificity : 0.3243 19## Pos Pred Value : 0.9772 20## Neg Pred Value : 0.7397 21## Prevalence : 0.9667 22## Detection Rate : 0.9629 23## Detection Prevalence : 0.9854 24## Balanced Accuracy : 0.6602 25## 26## \u0026#39;Positive\u0026#39; Class : No 27## 1autoplot(lrnprob$predict(tskLogi2))  Figure 7: Autoplot figure\n  1lrnprob$train(tskLogi2) 2autoplot(lrnprob$predict(tskLogi2),type=\u0026#34;roc\u0026#34;)  Figure 8: ROC plot\n  Although we have slightly better accuracy with the new variable, it needs to be compared to determine if it is worth further investigation.\nWith a three-fold validation approach,\n1library(\u0026#34;gridExtra\u0026#34;) 1## 2## Attaching package: \u0026#39;gridExtra\u0026#39; 1## The following object is masked from \u0026#39;package:dplyr\u0026#39;: 2## 3## combine 1rr2 = resample(tskLogi2, lrnprob, rsmp(\u0026#34;cv\u0026#34;, folds = 3)) 1## INFO [22:12:39.670] Applying learner \u0026#39;classif.log_reg\u0026#39; on task \u0026#39;credit\u0026#39; (iter 1/3) 2## INFO [22:12:39.731] Applying learner \u0026#39;classif.log_reg\u0026#39; on task \u0026#39;credit\u0026#39; (iter 2/3) 3## INFO [22:12:39.780] Applying learner \u0026#39;classif.log_reg\u0026#39; on task \u0026#39;credit\u0026#39; (iter 3/3) 1wS\u0026lt;-autoplot(rr2) 2nS\u0026lt;-autoplot(rr) 3grid.arrange(wS,nS,ncol=2,bottom=\u0026#34;With student (left) and without (right)\u0026#34;)  Figure 9: Plot of accuracy\n  Given the results, it is fair to say that adding the student data is useful in general.\nQuestion 5.6 - Page 199 We continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set. In particular, we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways: (1) using the bootstrap, and (2) using the standard formula for computing the standard errors in the glm() function. Do not forget to set a random seed before beginning your analysis.\n(a) Using the summary() and glm() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors.\n(b) Write a function, boot.fn() , that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.\n(c) Use the boot() function together with your boot.fn() function to estimate the standard errors of the logistic regression coefficients for income and balance.\n(d) Comment on the estimated standard errors obtained using the glm() function and using your bootstrap function.\nAnswer This question is slightly more specific to the packages in the book so we will use them.\na) Fit summary 1glm.fit %\u0026gt;% summary 1## 2## Call: 3## glm(formula = default ~ income + balance, family = binomial, 4## data = defDat, subset = trainNoCaret) 5## 6## Deviance Residuals: 7## Min 1Q Median 3Q Max 8## -2.1943 -0.1488 -0.0588 -0.0217 3.7058 9## 10## Coefficients: 11## Estimate Std. Error z value Pr(\u0026gt;|z|) 12## (Intercept) -1.150e+01 4.814e-01 -23.885 \u0026lt; 2e-16 *** 13## income 2.288e-05 5.553e-06 4.121 3.78e-05 *** 14## balance 5.593e-03 2.509e-04 22.295 \u0026lt; 2e-16 *** 15## --- 16## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 17## 18## (Dispersion parameter for binomial family taken to be 1) 19## 20## Null deviance: 2354.0 on 7999 degrees of freedom 21## Residual deviance: 1283.6 on 7997 degrees of freedom 22## AIC: 1289.6 23## 24## Number of Fisher Scoring iterations: 8 b) Function 1boot.fn=function(data,subs){return(coef(glm(default~income+balance,data=data, family=binomial,subset=subs)))} 1boot.fn(defDat,train_set) %\u0026gt;% print 1## (Intercept) income balance 2## -1.136824e+01 1.846153e-05 5.576468e-03 1glm(default~income+balance,data=defDat,family=binomial,subset=train_set) %\u0026gt;% summary 1## 2## Call: 3## glm(formula = default ~ income + balance, family = binomial, 4## data = defDat, subset = train_set) 5## 6## Deviance Residuals: 7## Min 1Q Median 3Q Max 8## -2.4280 -0.1465 -0.0582 -0.0218 3.7115 9## 10## Coefficients: 11## Estimate Std. Error z value Pr(\u0026gt;|z|) 12## (Intercept) -1.137e+01 4.813e-01 -23.618 \u0026lt; 2e-16 *** 13## income 1.846e-05 5.553e-06 3.324 0.000886 *** 14## balance 5.576e-03 2.529e-04 22.046 \u0026lt; 2e-16 *** 15## --- 16## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 17## 18## (Dispersion parameter for binomial family taken to be 1) 19## 20## Null deviance: 2313.6 on 7999 degrees of freedom 21## Residual deviance: 1266.4 on 7997 degrees of freedom 22## AIC: 1272.4 23## 24## Number of Fisher Scoring iterations: 8 We see that the statistics obtained from both are the same.\nc) Bootstrap The old fashioned way. R is the resample rate, boot.fn is the statistic used.\n1library(boot) 1## 2## Attaching package: \u0026#39;boot\u0026#39; 1## The following object is masked from \u0026#39;package:lattice\u0026#39;: 2## 3## melanoma 1boot(defDat,boot.fn,R=184) %\u0026gt;% print 1## 2## ORDINARY NONPARAMETRIC BOOTSTRAP 3## 4## 5## Call: 6## boot(data = defDat, statistic = boot.fn, R = 184) 7## 8## 9## Bootstrap Statistics : 10## original bias std. error 11## t1* -1.154047e+01 -1.407368e-02 4.073453e-01 12## t2* 2.080898e-05 -6.386634e-08 4.720109e-06 13## t3* 5.647103e-03 1.350950e-05 2.111547e-04 d) Comparison  Clearly, there is not much difference in the standard error estimates  Var | Bootstrap | Summary |\n| :\u0026mdash;\u0026mdash;\u0026mdash;: | \u0026mdash;\u0026mdash;\u0026mdash; |\nIntercept | 4.428026e-01 | 4.883e-01 |\nincome | 2.797011e-06 | 5.548e-06 |\nbalance | 2.423002e-04 | 2.591e-04 |\nQuestion 5.8 - Page 200 We will now perform cross-validation on a simulated data set. (a) Generate a simulated data set as follows:\n1\u0026gt; set . seed (1) 2\u0026gt; y = rnorm (100) 3\u0026gt; x = rnorm (100) 4\u0026gt; y =x -2\\* x ^2+ rnorm (100) In this data set, what is n and what is p? Write out the model used to generate the data in equation form.\n(b) Create a scatterplot of \\(X\\) against \\(Y\\). Comment on what you find.\n(c) Set a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares:\n  \\(Y=\\beta_0+\\beta_1X+\\eta\\)\n  \\(Y=\\beta_0+\\beta_1X+\\beta_2X^2+\\eta\\)\n  \\(Y=\\beta_0+\\beta_1X+\\beta_2X^2+\\beta_{3}X^{3}+\\eta\\)\n  \\(Y=\\beta_0+\\beta_1X+\\beta_2X^2+\\beta_{3}X^{3}+\\beta_{4}X^{4}+\\eta\\)\n  Note you may find it helpful to use the data.frame() function to create a single data set containing both \\(X\\) and \\(Y\\).\n(d) Repeat (c) using another random seed, and report your results. Are your results the same as what you got in (c)? Why?\n(e) Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer.\n(f) Comment on the statistical significance of the coefficient esti- mates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?\nAnswer a) Modeling data 1set.seed(1) 2y \u0026lt;- rnorm(100) 3x \u0026lt;- rnorm(100) 4y \u0026lt;- x - 2*x^2 + rnorm(100) Clearly:\n Our equation is \\(y=x-2x^{2}+\\epsilon\\) where \\(epsilon\\) is normally distributed from 100 samples We have \\(n=100\\) observations \\(p=2\\) where \\(p\\) is the number of features  b) Visual inspection 1qplot(x,y)  Figure 10: Model data plot\n  We observe that the data is quadratic, as we also know from the generating function, which was a quadratic equation plus normally distributed noise.\nc) Least squares fits Not very important, but here we use the caret form.\n1pow=function(x,y){return(x^y)} 2dfDat \u0026lt;- data.frame(y,x,x2=pow(x,2),x3=pow(x,3),x4=pow(x,4)) We might have also just used poly(x,n) to skip making the data frame.\nWe will set our resampling method as follows:\n1fitControl\u0026lt;-trainControl(method=\u0026#34;LOOCV\u0026#34;) 1train(y~x,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 1 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 2.427134 0.05389864 1.878566 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE 1train(y~x+x2,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 2 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 1.042399 0.8032414 0.8029942 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE 1train(y~x+x2+x3,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 3 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 1.050041 0.8003517 0.8073024 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE 1train(y~x+x2+x3+x4,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 4 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 1.055828 0.7982111 0.8150296 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE d) Seeding effects 1set.seed(1995) 1train(y~x,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 1 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 2.427134 0.05389864 1.878566 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE 1train(y~x+x2,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 2 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 1.042399 0.8032414 0.8029942 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE 1train(y~x+x2+x3,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 3 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 1.050041 0.8003517 0.8073024 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE 1train(y~x+x2+x3+x4,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 4 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 1.055828 0.7982111 0.8150296 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE We note that there is no change on varying the seed because LOOCV is exhaustive and uses n folds for each observation.\ne) Analysis 1train(y~x,data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 1 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 2.427134 0.05389864 1.878566 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE 1train(y~poly(x,2),data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 1 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 1.042399 0.8032414 0.8029942 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE 1train(y~poly(x,3),data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 1 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 1.050041 0.8003517 0.8073024 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE 1train(y~poly(x,4),data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 1 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 1.055828 0.7982111 0.8150296 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE Clearly the quadratic polynomial has the lowest error, which makes sense given how the data was generated.\nf) Statistical significance 1train(y~x,data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% summary %\u0026gt;% print 1## 2## Call: 3## lm(formula = .outcome ~ ., data = dat) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -7.3469 -0.9275 0.8028 1.5608 4.3974 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) -1.8185 0.2364 -7.692 1.14e-11 *** 12## x 0.2430 0.2479 0.981 0.329 13## --- 14## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 15## 16## Residual standard error: 2.362 on 98 degrees of freedom 17## Multiple R-squared: 0.009717, Adjusted R-squared: -0.0003881 18## F-statistic: 0.9616 on 1 and 98 DF, p-value: 0.3292 1train(y~poly(x,2),data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% summary %\u0026gt;% print 1## 2## Call: 3## lm(formula = .outcome ~ ., data = dat) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -2.89884 -0.53765 0.04135 0.61490 2.73607 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) -1.8277 0.1032 -17.704 \u0026lt;2e-16 *** 12## `poly(x, 2)1` 2.3164 1.0324 2.244 0.0271 * 13## `poly(x, 2)2` -21.0586 1.0324 -20.399 \u0026lt;2e-16 *** 14## --- 15## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 16## 17## Residual standard error: 1.032 on 97 degrees of freedom 18## Multiple R-squared: 0.8128, Adjusted R-squared: 0.8089 19## F-statistic: 210.6 on 2 and 97 DF, p-value: \u0026lt; 2.2e-16 1train(y~poly(x,3),data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% summary %\u0026gt;% print 1## 2## Call: 3## lm(formula = .outcome ~ ., data = dat) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -2.87250 -0.53881 0.02862 0.59383 2.74350 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) -1.8277 0.1037 -17.621 \u0026lt;2e-16 *** 12## `poly(x, 3)1` 2.3164 1.0372 2.233 0.0279 * 13## `poly(x, 3)2` -21.0586 1.0372 -20.302 \u0026lt;2e-16 *** 14## `poly(x, 3)3` -0.3048 1.0372 -0.294 0.7695 15## --- 16## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 17## 18## Residual standard error: 1.037 on 96 degrees of freedom 19## Multiple R-squared: 0.813, Adjusted R-squared: 0.8071 20## F-statistic: 139.1 on 3 and 96 DF, p-value: \u0026lt; 2.2e-16 1train(y~poly(x,4),data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% summary %\u0026gt;% print 1## 2## Call: 3## lm(formula = .outcome ~ ., data = dat) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -2.8914 -0.5244 0.0749 0.5932 2.7796 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) -1.8277 0.1041 -17.549 \u0026lt;2e-16 *** 12## `poly(x, 4)1` 2.3164 1.0415 2.224 0.0285 * 13## `poly(x, 4)2` -21.0586 1.0415 -20.220 \u0026lt;2e-16 *** 14## `poly(x, 4)3` -0.3048 1.0415 -0.293 0.7704 15## `poly(x, 4)4` -0.4926 1.0415 -0.473 0.6373 16## --- 17## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 18## 19## Residual standard error: 1.041 on 95 degrees of freedom 20## Multiple R-squared: 0.8134, Adjusted R-squared: 0.8055 21## F-statistic: 103.5 on 4 and 95 DF, p-value: \u0026lt; 2.2e-16  Clearly, the second order terms are the most significant, as expected  Question 5.9 - Page 201 We will now consider the Boston housing data set, from the MASS library.\n(a) Based on this data set, provide an estimate for the population mean of medv. Call this estimate \\(\\hat{\\mu}\\).\n(b) Provide an estimate of the standard error of \\(\\hat{\\mu}\\). Interpret this result. Hint: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations.\n(c) Now estimate the standard error of \\(\\hat{\\mu}\\) using the bootstrap. How does this compare to your answer from (b)?\n(d) Based on your bootstrap estimate from (c), provide a 95 % confidence interval for the mean of medv. Compare it to the results obtained using t.test(Boston\\$medv). Hint: You can approximate a 95 % confidence interval using the formula \\([\\hat{\\mu} − 2SE(\\hat{\\mu}), \\hat{\\mu} + 2SE(\\hat{\\mu})]\\).\n(e) Based on this data set, provide an estimate, \\(\\hat{\\mu_{med}}\\), for the median value of medv in the population.\n(f) We now would like to estimate the standard error of \\(\\hat{\\mu}\\) med. Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your findings.\n(g) Based on this data set, provide an estimate for the tenth percentile of medv in Boston suburbs. Call this quantity \\(\\hat{\\mu_{0.1}}\\). (You can use the quantile() function.)\n(h) Use the bootstrap to estimate the standard error of \\(\\hat{\\mu_{0.1}}\\). Comment on your findings.\nAnswer 1boston\u0026lt;-MASS::Boston  Reminder   1boston %\u0026gt;% summary %\u0026gt;% print 1## crim zn indus chas 2## Min. : 0.00632 Min. : 0.00 Min. : 0.46 Min. :0.00000 3## 1st Qu.: 0.08204 1st Qu.: 0.00 1st Qu.: 5.19 1st Qu.:0.00000 4## Median : 0.25651 Median : 0.00 Median : 9.69 Median :0.00000 5## Mean : 3.61352 Mean : 11.36 Mean :11.14 Mean :0.06917 6## 3rd Qu.: 3.67708 3rd Qu.: 12.50 3rd Qu.:18.10 3rd Qu.:0.00000 7## Max. :88.97620 Max. :100.00 Max. :27.74 Max. :1.00000 8## nox rm age dis 9## Min. :0.3850 Min. :3.561 Min. : 2.90 Min. : 1.130 10## 1st Qu.:0.4490 1st Qu.:5.886 1st Qu.: 45.02 1st Qu.: 2.100 11## Median :0.5380 Median :6.208 Median : 77.50 Median : 3.207 12## Mean :0.5547 Mean :6.285 Mean : 68.57 Mean : 3.795 13## 3rd Qu.:0.6240 3rd Qu.:6.623 3rd Qu.: 94.08 3rd Qu.: 5.188 14## Max. :0.8710 Max. :8.780 Max. :100.00 Max. :12.127 15## rad tax ptratio black 16## Min. : 1.000 Min. :187.0 Min. :12.60 Min. : 0.32 17## 1st Qu.: 4.000 1st Qu.:279.0 1st Qu.:17.40 1st Qu.:375.38 18## Median : 5.000 Median :330.0 Median :19.05 Median :391.44 19## Mean : 9.549 Mean :408.2 Mean :18.46 Mean :356.67 20## 3rd Qu.:24.000 3rd Qu.:666.0 3rd Qu.:20.20 3rd Qu.:396.23 21## Max. :24.000 Max. :711.0 Max. :22.00 Max. :396.90 22## lstat medv 23## Min. : 1.73 Min. : 5.00 24## 1st Qu.: 6.95 1st Qu.:17.02 25## Median :11.36 Median :21.20 26## Mean :12.65 Mean :22.53 27## 3rd Qu.:16.95 3rd Qu.:25.00 28## Max. :37.97 Max. :50.00 1boston %\u0026gt;% str %\u0026gt;% print 1## \u0026#39;data.frame\u0026#39;: 506 obs. of 14 variables: 2## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... 3## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... 4## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... 5## $ chas : int 0 0 0 0 0 0 0 0 0 0 ... 6## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... 7## $ rm : num 6.58 6.42 7.18 7 7.15 ... 8## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... 9## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... 10## $ rad : int 1 2 2 3 3 3 5 5 5 5 ... 11## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... 12## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... 13## $ black : num 397 397 393 395 397 ... 14## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... 15## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... 16## NULL a) Mean 1muhat=boston$medv %\u0026gt;% mean() 2print(muhat) 1## [1] 22.53281 b) Standard error Recall that \\(SE=\\frac{SD}{\\sqrt{N_{obs}}}\\)\n1boston$medv %\u0026gt;% sd/(nrow(boston)^0.5) %\u0026gt;% print 1## [1] 22.49444 1## [1] 0.4088611 c) Bootstrap estimate 1library(boot) 2myMean\u0026lt;-function(frame,ind){return(mean(frame[ind]))} 1boot(boston$medv,myMean,R=184) %\u0026gt;% print 1## 2## ORDINARY NONPARAMETRIC BOOTSTRAP 3## 4## 5## Call: 6## boot(data = boston$medv, statistic = myMean, R = 184) 7## 8## 9## Bootstrap Statistics : 10## original bias std. error 11## t1* 22.53281 0.03451839 0.409621 We see that the bootstrapped error over 184 samples is 0.4341499 while without it we had 0.4088611 which is similar enough.\nd) Confidence intervals with bootstrap and t.test 1boston$medv %\u0026gt;% t.test %\u0026gt;% print 1## 2## One Sample t-test 3## 4## data: . 5## t = 55.111, df = 505, p-value \u0026lt; 2.2e-16 6## alternative hypothesis: true mean is not equal to 0 7## 95 percent confidence interval: 8## 21.72953 23.33608 9## sample estimates: 10## mean of x 11## 22.53281 We can approximate this with what we already have\n1bRes=boot(boston$medv,myMean,R=184) 2seBoot\u0026lt;-bRes$t %\u0026gt;% var %\u0026gt;% sqrt 3xlow=muhat-2*(seBoot) 4xhigh=muhat+2*(seBoot) 5c(xlow,xhigh) %\u0026gt;% print 1## [1] 21.72675 23.33887 Our intervals are also pretty close to each other.\ne) Median 1boston$medv %\u0026gt;% sort %\u0026gt;% median %\u0026gt;% print 1## [1] 21.2 f) Median standard error We can reuse the logic of the myMean function defined previously.\n1myMedian=function(data,ind){return(median(data[ind]))} 1boston$medv %\u0026gt;% boot(myMedian,R=1500) %\u0026gt;% print 1## 2## ORDINARY NONPARAMETRIC BOOTSTRAP 3## 4## 5## Call: 6## boot(data = ., statistic = myMedian, R = 1500) 7## 8## 9## Bootstrap Statistics : 10## original bias std. error 11## t1* 21.2 -0.03773333 0.387315 We see that the standard error is 0.3767072.\ng) Tenth percentile 1mu0one\u0026lt;-boston$medv %\u0026gt;% quantile(c(0.1)) 2print(mu0one) 1## 10% 2## 12.75 h) Bootstrap Once again.\n1myQuant=function(data,ind){return(quantile(data[ind],0.1))} 1boston$medv %\u0026gt;% boot(myQuant,R=500) %\u0026gt;% print 1## 2## ORDINARY NONPARAMETRIC BOOTSTRAP 3## 4## 5## Call: 6## boot(data = ., statistic = myQuant, R = 500) 7## 8## 9## Bootstrap Statistics : 10## original bias std. error 11## t1* 12.75 -0.0095 0.4951415 The standard error is 0.5024526\n  James, G., Witten, D., Hastie, T., \u0026amp; Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Berlin, Germany: Springer Science \u0026amp; Business Media.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Lang et al., (2019). mlr3: A modern object-oriented machine learning framework in R. Journal of Open Source Software, 4(44), 1903, https://doi.org/10.21105/joss.01903\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/islr-ch5/","tags":["solutions","R","ISLR"],"title":"ISLR :: Resampling Methods"},{"categories":["programming"],"contents":"Chapter IV - Classification All the questions are as per the ISL seventh printing of the First edition 1.\nCommon Stuff Here I\u0026rsquo;ll load things I will be using throughout, mostly libraries.\n1libsUsed\u0026lt;-c(\u0026#34;dplyr\u0026#34;,\u0026#34;ggplot2\u0026#34;,\u0026#34;tidyverse\u0026#34;,\u0026#34;ISLR\u0026#34;,\u0026#34;caret\u0026#34;) 2invisible(lapply(libsUsed, library, character.only = TRUE)) Question 4.10 - Page 171 This question should be answered using the Weekly data set, which is part of the ISLR package. This data is similar in nature to the Smarket data from this chapter\u0026rsquo;s lab, except that it contains 1, 089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.\n(a) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?\n(b) Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?\n(c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.\n(d) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).\n(e) Repeat (d) using LDA.\n(f) Repeat (d) using QDA.\n(g) Repeat (d) using KNN with \\(K = 1\\).\n(h) Which of these methods appears to provide the best results on this data?\n(i) Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier.\nAnswer We will need the data in a variable for ease of use.\n1weeklyDat\u0026lt;-ISLR::Weekly a) Summary Statistics Text Most of this segment relies heavily on usage of dplyr and especially the %\u0026gt;% or pipe operator for readability. The use of the skimr package2 might added more descriptive statistics, but is not covered here.\nBasic Summaries 1weeklyDat %\u0026gt;% str 1# \u0026#39;data.frame\u0026#39;: 1089 obs. of 9 variables: 2# $ Year : num 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 ... 3# $ Lag1 : num 0.816 -0.27 -2.576 3.514 0.712 ... 4# $ Lag2 : num 1.572 0.816 -0.27 -2.576 3.514 ... 5# $ Lag3 : num -3.936 1.572 0.816 -0.27 -2.576 ... 6# $ Lag4 : num -0.229 -3.936 1.572 0.816 -0.27 ... 7# $ Lag5 : num -3.484 -0.229 -3.936 1.572 0.816 ... 8# $ Volume : num 0.155 0.149 0.16 0.162 0.154 ... 9# $ Today : num -0.27 -2.576 3.514 0.712 1.178 ... 10# $ Direction: Factor w/ 2 levels \u0026#34;Down\u0026#34;,\u0026#34;Up\u0026#34;: 1 1 2 2 2 1 2 2 2 1 ... We see that there is only one Factor, which makes sense.\n1weeklyDat %\u0026gt;% summary 1# Year Lag1 Lag2 Lag3 2# Min. :1990 Min. :-18.1950 Min. :-18.1950 Min. :-18.1950 3# 1st Qu.:1995 1st Qu.: -1.1540 1st Qu.: -1.1540 1st Qu.: -1.1580 4# Median :2000 Median : 0.2410 Median : 0.2410 Median : 0.2410 5# Mean :2000 Mean : 0.1506 Mean : 0.1511 Mean : 0.1472 6# 3rd Qu.:2005 3rd Qu.: 1.4050 3rd Qu.: 1.4090 3rd Qu.: 1.4090 7# Max. :2010 Max. : 12.0260 Max. : 12.0260 Max. : 12.0260 8# Lag4 Lag5 Volume Today 9# Min. :-18.1950 Min. :-18.1950 Min. :0.08747 Min. :-18.1950 10# 1st Qu.: -1.1580 1st Qu.: -1.1660 1st Qu.:0.33202 1st Qu.: -1.1540 11# Median : 0.2380 Median : 0.2340 Median :1.00268 Median : 0.2410 12# Mean : 0.1458 Mean : 0.1399 Mean :1.57462 Mean : 0.1499 13# 3rd Qu.: 1.4090 3rd Qu.: 1.4050 3rd Qu.:2.05373 3rd Qu.: 1.4050 14# Max. : 12.0260 Max. : 12.0260 Max. :9.32821 Max. : 12.0260 15# Direction 16# Down:484 17# Up :605 18# 19# 20# 21# Unique Values We might also want to know how many unique values are there in each column.\n1weeklyDat %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) 1# Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today 2# 21 1004 1005 1005 1005 1005 1089 1003 3# Direction 4# 2 We note that year has disproportionately lower values, something to keep in mind while constructing models later.\nRange The range of each variable might be useful as well, but we have to ignore the factor.\n1weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% sapply(range) 1# Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today 2# [1,] 1990 -18.195 -18.195 -18.195 -18.195 -18.195 0.087465 -18.195 3# [2,] 2010 12.026 12.026 12.026 12.026 12.026 9.328214 12.026 The most interesting thing about this is probably that the Lag variables all have the same range, also something to be kept in mind while applying transformations to the variable (if at all).\nMean and Std. Dev By now we might have a pretty good idea of how this will look, but it is still worth seeing.\n1weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% sapply(mean) 1# Year Lag1 Lag2 Lag3 Lag4 Lag5 2# 2000.0486685 0.1505849 0.1510790 0.1472048 0.1458182 0.1398926 3# Volume Today 4# 1.5746176 0.1498990 As expected, the Lag values have almost the same mean, what is a bit interesting though, is that the Today variable has roughly the same mean as the Lag variables.\n1weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% sapply(sd) 1# Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today 2# 6.033182 2.357013 2.357254 2.360502 2.360279 2.361285 1.686636 2.356927 This is largely redundant in terms of new information.\nCorrelations 1weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% cor 1# Year Lag1 Lag2 Lag3 Lag4 2# Year 1.00000000 -0.032289274 -0.03339001 -0.03000649 -0.031127923 3# Lag1 -0.03228927 1.000000000 -0.07485305 0.05863568 -0.071273876 4# Lag2 -0.03339001 -0.074853051 1.00000000 -0.07572091 0.058381535 5# Lag3 -0.03000649 0.058635682 -0.07572091 1.00000000 -0.075395865 6# Lag4 -0.03112792 -0.071273876 0.05838153 -0.07539587 1.000000000 7# Lag5 -0.03051910 -0.008183096 -0.07249948 0.06065717 -0.075675027 8# Volume 0.84194162 -0.064951313 -0.08551314 -0.06928771 -0.061074617 9# Today -0.03245989 -0.075031842 0.05916672 -0.07124364 -0.007825873 10# Lag5 Volume Today 11# Year -0.030519101 0.84194162 -0.032459894 12# Lag1 -0.008183096 -0.06495131 -0.075031842 13# Lag2 -0.072499482 -0.08551314 0.059166717 14# Lag3 0.060657175 -0.06928771 -0.071243639 15# Lag4 -0.075675027 -0.06107462 -0.007825873 16# Lag5 1.000000000 -0.05851741 0.011012698 17# Volume -0.058517414 1.00000000 -0.033077783 18# Today 0.011012698 -0.03307778 1.000000000 Useful though this is, it is kind of difficult to work with, in this form, so we might as well programmatic-ally remove strongly correlated data instead.\n1# Uses caret 2corrCols=weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% cor %\u0026gt;% findCorrelation(cutoff=0.8) 3reducedDat\u0026lt;-weeklyDat[-c(corrCols)] 4reducedDat %\u0026gt;% summary 1# Year Lag1 Lag2 Lag3 2# Min. :1990 Min. :-18.1950 Min. :-18.1950 Min. :-18.1950 3# 1st Qu.:1995 1st Qu.: -1.1540 1st Qu.: -1.1540 1st Qu.: -1.1580 4# Median :2000 Median : 0.2410 Median : 0.2410 Median : 0.2410 5# Mean :2000 Mean : 0.1506 Mean : 0.1511 Mean : 0.1472 6# 3rd Qu.:2005 3rd Qu.: 1.4050 3rd Qu.: 1.4090 3rd Qu.: 1.4090 7# Max. :2010 Max. : 12.0260 Max. : 12.0260 Max. : 12.0260 8# Lag4 Lag5 Today Direction 9# Min. :-18.1950 Min. :-18.1950 Min. :-18.1950 Down:484 10# 1st Qu.: -1.1580 1st Qu.: -1.1660 1st Qu.: -1.1540 Up :605 11# Median : 0.2380 Median : 0.2340 Median : 0.2410 12# Mean : 0.1458 Mean : 0.1399 Mean : 0.1499 13# 3rd Qu.: 1.4090 3rd Qu.: 1.4050 3rd Qu.: 1.4050 14# Max. : 12.0260 Max. : 12.0260 Max. : 12.0260 We can see that the Volume variable has been dropped, since it evidently is strongly correlated with Year. This may or may not be a useful insight, but it is good to keep in mind.\nVisualization We will be using the ggplot2 library throughout for this segment.\nLets start with some scatter plots in a one v/s all scheme, similar to the methodology described here.\n1weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% gather(-Year,key=\u0026#34;Variable\u0026#34;, value=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=Value,y=Year)) + 2geom_point() + 3facet_wrap(~Variable) + 4coord_flip()  Figure 1: One v/s all for Direction\n  That didn\u0026rsquo;t really tell us much which we didn\u0026rsquo;t already get from the cor() function, but we can go the whole hog and do this for every variable since we don\u0026rsquo;t have that many in the first place..\n1weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% pairs  Figure 2: Pairs\n  This is not especially useful, and it is doubtful if more scatter-plots will help at all, so lets move on to box plots.\n1weeklyDat %\u0026gt;% pivot_longer(-c(Direction,Volume,Today,Year),names_to=\u0026#34;Lag\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=Direction,y=Value,fill=Lag)) + 2geom_boxplot()  Figure 3: Box plots for Direction\n  1weeklyDat %\u0026gt;% pivot_longer(-c(Direction,Volume,Today,Year),names_to=\u0026#34;Lag\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=Today,y=Value,fill=Lag)) + 2geom_boxplot()  Figure 4: More box plots\n  1weeklyDat %\u0026gt;% pivot_longer(-c(Direction,Volume,Today,Year),names_to=\u0026#34;Lag\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=Lag,y=Value,fill=Direction)) + 2geom_boxplot()  Figure 5: Lag v/s all\n  This does summarize our text analysis quite well. Importantly, it tells us that the Today value is largely unrelated to the \\(4\\) Lag variables.\nA really good-looking box-plot is easy to get with the caret library:\n1weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% featurePlot( 2y = weeklyDat$Direction, 3plot = \u0026#34;box\u0026#34;, 4# Pass in options to bwplot() 5scales = list(y = list(relation=\u0026#34;free\u0026#34;), 6x = list(rot = 90)), 7auto.key = list(columns = 2))  Figure 6: Plots with caret\n  We might want to visualize our correlation matrix as well.\n1library(reshape2) 1# 2# Attaching package: \u0026#39;reshape2\u0026#39; 1# The following object is masked from \u0026#39;package:tidyr\u0026#39;: 2# 3# smiths 1weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% cor %\u0026gt;% melt %\u0026gt;% ggplot(aes(x=Var1,y=Var2,fill=value)) + 2geom_tile()  Figure 7: Heatmap of the correlation matrix\n  b) Logistic Regression - Predictor Significance Lets start with the native glm function.\n1glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=weeklyDat, family=binomial) 2summary(glm.fit) 1# 2# Call: 3# glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + 4# Volume, family = binomial, data = weeklyDat) 5# 6# Deviance Residuals: 7# Min 1Q Median 3Q Max 8# -1.6949 -1.2565 0.9913 1.0849 1.4579 9# 10# Coefficients: 11# Estimate Std. Error z value Pr(\u0026gt;|z|) 12# (Intercept) 0.26686 0.08593 3.106 0.0019 ** 13# Lag1 -0.04127 0.02641 -1.563 0.1181 14# Lag2 0.05844 0.02686 2.175 0.0296 * 15# Lag3 -0.01606 0.02666 -0.602 0.5469 16# Lag4 -0.02779 0.02646 -1.050 0.2937 17# Lag5 -0.01447 0.02638 -0.549 0.5833 18# Volume -0.02274 0.03690 -0.616 0.5377 19# --- 20# Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 21# 22# (Dispersion parameter for binomial family taken to be 1) 23# 24# Null deviance: 1496.2 on 1088 degrees of freedom 25# Residual deviance: 1486.4 on 1082 degrees of freedom 26# AIC: 1500.4 27# 28# Number of Fisher Scoring iterations: 4 Evidently, only the Lag2 value is of statistical significance.\nIt is always of importance to figure out what numeric values R will assign to our factors, and it is best not to guess.\n1contrasts(weeklyDat$Direction) 1# Up 2# Down 0 3# Up 1 c) Confusion Matrix and Metrics Essentially:\n Predict the response Create an output length vector Apply thresholding to obtain labels   1glm.probs = predict(glm.fit, type = \u0026#34;response\u0026#34;) 2glm.pred = rep(\u0026#34;Up\u0026#34;,length(glm.probs)) 3glm.pred[glm.probs\u0026lt;0.5]=\u0026#34;Down\u0026#34; 4glm.pred=factor(glm.pred) 5confusionMatrix(glm.pred,weeklyDat$Direction) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction Down Up 5# Down 54 48 6# Up 430 557 7# 8# Accuracy : 0.5611 9# 95% CI : (0.531, 0.5908) 10# No Information Rate : 0.5556 11# P-Value [Acc \u0026gt; NIR] : 0.369 12# 13# Kappa : 0.035 14# 15# Mcnemar\u0026#39;s Test P-Value : \u0026lt;2e-16 16# 17# Sensitivity : 0.11157 18# Specificity : 0.92066 19# Pos Pred Value : 0.52941 20# Neg Pred Value : 0.56434 21# Prevalence : 0.44444 22# Detection Rate : 0.04959 23# Detection Prevalence : 0.09366 24# Balanced Accuracy : 0.51612 25# 26# \u0026#39;Positive\u0026#39; Class : Down 27#  We have used the confusionMatrix function from caret (documented here) instead of displaying the results with table and then calculating precision, recall and the rest by hand.  d) Train Test Splits Although we could have used the indices and passed it to glm as the subset attribute, it is cleaner to just make subsets instead.\n1weeklyVal\u0026lt;-weeklyDat %\u0026gt;% filter(Year\u0026gt;=2009) 2weeklyTrain\u0026lt;-weeklyDat %\u0026gt;% filter(Year\u0026lt;2009) Now we can train a model on our training data.\n1glm.fit=glm(Direction~Lag2,data=weeklyTrain,family=binomial) 2summary(glm.fit) 1# 2# Call: 3# glm(formula = Direction ~ Lag2, family = binomial, data = weeklyTrain) 4# 5# Deviance Residuals: 6# Min 1Q Median 3Q Max 7# -1.536 -1.264 1.021 1.091 1.368 8# 9# Coefficients: 10# Estimate Std. Error z value Pr(\u0026gt;|z|) 11# (Intercept) 0.20326 0.06428 3.162 0.00157 ** 12# Lag2 0.05810 0.02870 2.024 0.04298 * 13# --- 14# Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 15# 16# (Dispersion parameter for binomial family taken to be 1) 17# 18# Null deviance: 1354.7 on 984 degrees of freedom 19# Residual deviance: 1350.5 on 983 degrees of freedom 20# AIC: 1354.5 21# 22# Number of Fisher Scoring iterations: 4 Having fit our model, we will test the predictions on our held out data.\n1glm.probs = predict(glm.fit,weeklyVal, type = \u0026#34;response\u0026#34;) 2glm.pred = rep(\u0026#34;Up\u0026#34;,length(glm.probs)) 3glm.pred[glm.probs\u0026lt;0.5]=\u0026#34;Down\u0026#34; 4glm.pred=factor(glm.pred) 5confusionMatrix(glm.pred,weeklyVal$Direction) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction Down Up 5# Down 9 5 6# Up 34 56 7# 8# Accuracy : 0.625 9# 95% CI : (0.5247, 0.718) 10# No Information Rate : 0.5865 11# P-Value [Acc \u0026gt; NIR] : 0.2439 12# 13# Kappa : 0.1414 14# 15# Mcnemar\u0026#39;s Test P-Value : 7.34e-06 16# 17# Sensitivity : 0.20930 18# Specificity : 0.91803 19# Pos Pred Value : 0.64286 20# Neg Pred Value : 0.62222 21# Prevalence : 0.41346 22# Detection Rate : 0.08654 23# Detection Prevalence : 0.13462 24# Balanced Accuracy : 0.56367 25# 26# \u0026#39;Positive\u0026#39; Class : Down 27# We really aren\u0026rsquo;t doing very well with this single variable model as is evident.\ne) LDA models At this stage we could use MASS to get the lda function, but it would be better to just switch to using caret. Note that the caret prediction is a label by default, so thresholding needs to be specified differently if required.\n1lda.fit=train(Direction~Lag2,data=weeklyTrain,method=\u0026#34;lda\u0026#34;) 2summary(lda.fit) 1# Length Class Mode 2# prior 2 -none- numeric 3# counts 2 -none- numeric 4# means 2 -none- numeric 5# scaling 1 -none- numeric 6# lev 2 -none- character 7# svd 1 -none- numeric 8# N 1 -none- numeric 9# call 3 -none- call 10# xNames 1 -none- character 11# problemType 1 -none- character 12# tuneValue 1 data.frame list 13# obsLevels 2 -none- character 14# param 0 -none- list 1predict(lda.fit,weeklyVal) %\u0026gt;% confusionMatrix(weeklyVal$Direction) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction Down Up 5# Down 9 5 6# Up 34 56 7# 8# Accuracy : 0.625 9# 95% CI : (0.5247, 0.718) 10# No Information Rate : 0.5865 11# P-Value [Acc \u0026gt; NIR] : 0.2439 12# 13# Kappa : 0.1414 14# 15# Mcnemar\u0026#39;s Test P-Value : 7.34e-06 16# 17# Sensitivity : 0.20930 18# Specificity : 0.91803 19# Pos Pred Value : 0.64286 20# Neg Pred Value : 0.62222 21# Prevalence : 0.41346 22# Detection Rate : 0.08654 23# Detection Prevalence : 0.13462 24# Balanced Accuracy : 0.56367 25# 26# \u0026#39;Positive\u0026#39; Class : Down 27# f) QDA models 1qda.fit=train(Direction~Lag2,data=weeklyTrain,method=\u0026#34;qda\u0026#34;) 2summary(qda.fit) 1# Length Class Mode 2# prior 2 -none- numeric 3# counts 2 -none- numeric 4# means 2 -none- numeric 5# scaling 2 -none- numeric 6# ldet 2 -none- numeric 7# lev 2 -none- character 8# N 1 -none- numeric 9# call 3 -none- call 10# xNames 1 -none- character 11# problemType 1 -none- character 12# tuneValue 1 data.frame list 13# obsLevels 2 -none- character 14# param 0 -none- list 1predict(qda.fit,weeklyVal) %\u0026gt;% confusionMatrix(weeklyVal$Direction) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction Down Up 5# Down 0 0 6# Up 43 61 7# 8# Accuracy : 0.5865 9# 95% CI : (0.4858, 0.6823) 10# No Information Rate : 0.5865 11# P-Value [Acc \u0026gt; NIR] : 0.5419 12# 13# Kappa : 0 14# 15# Mcnemar\u0026#39;s Test P-Value : 1.504e-10 16# 17# Sensitivity : 0.0000 18# Specificity : 1.0000 19# Pos Pred Value : NaN 20# Neg Pred Value : 0.5865 21# Prevalence : 0.4135 22# Detection Rate : 0.0000 23# Detection Prevalence : 0.0000 24# Balanced Accuracy : 0.5000 25# 26# \u0026#39;Positive\u0026#39; Class : Down 27# This is quite possibly the worst of the lot. As is evident, the model just predicts Up no matter what.\ng) KNN caret tends to over-zealously retrain models and find the best possible parameters. In this case that is annoying and redundant so we will use the class library. We should really scale our data before using KNN though.\n1library(class) 2set.seed(1) 3knn.pred=knn(as.matrix(weeklyTrain$Lag2),as.matrix(weeklyVal$Lag2),weeklyTrain$Direction,k=1) 4confusionMatrix(knn.pred,weeklyVal$Direction) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction Down Up 5# Down 21 30 6# Up 22 31 7# 8# Accuracy : 0.5 9# 95% CI : (0.4003, 0.5997) 10# No Information Rate : 0.5865 11# P-Value [Acc \u0026gt; NIR] : 0.9700 12# 13# Kappa : -0.0033 14# 15# Mcnemar\u0026#39;s Test P-Value : 0.3317 16# 17# Sensitivity : 0.4884 18# Specificity : 0.5082 19# Pos Pred Value : 0.4118 20# Neg Pred Value : 0.5849 21# Prevalence : 0.4135 22# Detection Rate : 0.2019 23# Detection Prevalence : 0.4904 24# Balanced Accuracy : 0.4983 25# 26# \u0026#39;Positive\u0026#39; Class : Down 27# Clearly this model is not doing very well.\nh) Model Selection We will first get the ROC curves.\n1library(pROC) 1# Type \u0026#39;citation(\u0026#34;pROC\u0026#34;)\u0026#39; for a citation. 1# 2# Attaching package: \u0026#39;pROC\u0026#39; 1# The following objects are masked from \u0026#39;package:stats\u0026#39;: 2# 3# cov, smooth, var 1knnROC\u0026lt;-roc(predictor=as.numeric(knn.pred),response=weeklyVal$Direction,levels=rev(levels(weeklyVal$Direction))) 1# Setting direction: controls \u0026lt; cases 1logiROC\u0026lt;-roc(predictor=as.numeric(predict(glm.fit,weeklyVal)),response=weeklyVal$Direction) 1# Setting levels: control = Down, case = Up 1# Setting direction: controls \u0026gt; cases 1ldaROC\u0026lt;-roc(predictor=as.numeric(predict(lda.fit,weeklyVal)),response=weeklyVal$Direction) 1# Setting levels: control = Down, case = Up 1# Setting direction: controls \u0026lt; cases 1qdaROC\u0026lt;-roc(predictor=as.numeric(predict(qda.fit,weeklyVal)),response=weeklyVal$Direction) 1# Setting levels: control = Down, case = Up 2# Setting direction: controls \u0026lt; cases Now to plot them.\n1ggroc(list(KNN=knnROC,Logistic=logiROC,LDA=ldaROC,QDA=qdaROC))  Figure 8: ROC curves for Weekly data\n  To compare models with caret it is easy to refit the logistic and knn models in the caret formulation.\n1knnCaret=train(Direction~Lag2,data=weeklyTrain,method=\u0026#34;knn\u0026#34;) However, the KNN model is the best parameter model.\n1resmod \u0026lt;- resamples(list(lda=lda.fit, qda=qda.fit, KNN=knnCaret)) 2summary(resmod) 1# 2# Call: 3# summary.resamples(object = resmod) 4# 5# Models: lda, qda, KNN 6# Number of resamples: 25 7# 8# Accuracy 9# Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s 10# lda 0.5043228 0.5344353 0.5529101 0.5500861 0.5683060 0.5846995 0 11# qda 0.5044248 0.5204360 0.5307263 0.5326785 0.5462428 0.5777778 0 12# KNN 0.4472222 0.5082873 0.5240642 0.5168327 0.5302198 0.5485714 0 13# 14# Kappa 15# Min. 1st Qu. Median Mean 3rd Qu. Max. 16# lda -0.02618939 -0.003638168 0.005796908 0.007801904 0.01635328 0.05431238 17# qda -0.06383592 -0.005606123 0.000000000 -0.003229697 0.00000000 0.03606344 18# KNN -0.11297539 0.004168597 0.024774647 0.016171229 0.04456142 0.07724439 19# NA\u0026#39;s 20# lda 0 21# qda 0 22# KNN 0 1bwplot(resmod)  Figure 9: Caret plots for comparison\n  1dotplot(resmod)  Kappa or Cohen\u0026rsquo;s Kappa is essentially classification accuracy, normalized at the baseline of random chance. It is a more useful measure to use on problems that have imbalanced classes. There\u0026rsquo;s more on model selection here.\ni) Further Tuning Do note the caret defaults.\n1fitControl \u0026lt;- trainControl(# 10-fold CV 2method = \u0026#34;repeatedcv\u0026#34;, 3number = 10, 4# repeated ten times 5repeats = 10) Logistic 1glm2.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=weeklyDat, family=binomial) 23glm2.probs = predict(glm2.fit,weeklyVal, type = \u0026#34;response\u0026#34;) 4glm2.pred = rep(\u0026#34;Up\u0026#34;,length(glm2.probs)) 5glm2.pred[glm2.probs\u0026lt;0.5]=\u0026#34;Down\u0026#34; 6glm2.pred=factor(glm2.pred) 7confusionMatrix(glm2.pred,weeklyVal$Direction) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction Down Up 5# Down 17 13 6# Up 26 48 7# 8# Accuracy : 0.625 9# 95% CI : (0.5247, 0.718) 10# No Information Rate : 0.5865 11# P-Value [Acc \u0026gt; NIR] : 0.24395 12# 13# Kappa : 0.1907 14# 15# Mcnemar\u0026#39;s Test P-Value : 0.05466 16# 17# Sensitivity : 0.3953 18# Specificity : 0.7869 19# Pos Pred Value : 0.5667 20# Neg Pred Value : 0.6486 21# Prevalence : 0.4135 22# Detection Rate : 0.1635 23# Detection Prevalence : 0.2885 24# Balanced Accuracy : 0.5911 25# 26# \u0026#39;Positive\u0026#39; Class : Down 27# QDA 1qdaCaret=train(Direction~Lag2+Lag4,data=weeklyTrain,method=\u0026#34;qda\u0026#34;,trainControl=fitControl) 1summary(qdaCaret) 1# Length Class Mode 2# prior 2 -none- numeric 3# counts 2 -none- numeric 4# means 4 -none- numeric 5# scaling 8 -none- numeric 6# ldet 2 -none- numeric 7# lev 2 -none- character 8# N 1 -none- numeric 9# call 4 -none- call 10# xNames 2 -none- character 11# problemType 1 -none- character 12# tuneValue 1 data.frame list 13# obsLevels 2 -none- character 14# param 1 -none- list 1predict(qdaCaret,weeklyVal) %\u0026gt;% confusionMatrix(weeklyVal$Direction) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction Down Up 5# Down 9 14 6# Up 34 47 7# 8# Accuracy : 0.5385 9# 95% CI : (0.438, 0.6367) 10# No Information Rate : 0.5865 11# P-Value [Acc \u0026gt; NIR] : 0.863079 12# 13# Kappa : -0.0217 14# 15# Mcnemar\u0026#39;s Test P-Value : 0.006099 16# 17# Sensitivity : 0.20930 18# Specificity : 0.77049 19# Pos Pred Value : 0.39130 20# Neg Pred Value : 0.58025 21# Prevalence : 0.41346 22# Detection Rate : 0.08654 23# Detection Prevalence : 0.22115 24# Balanced Accuracy : 0.48990 25# 26# \u0026#39;Positive\u0026#39; Class : Down 27# LDA 1ldaCaret=train(Direction~Lag2+Lag1+Year,data=weeklyTrain,method=\u0026#34;lda\u0026#34;,trainControl=fitControl) 1summary(ldaCaret) 1# Length Class Mode 2# prior 2 -none- numeric 3# counts 2 -none- numeric 4# means 6 -none- numeric 5# scaling 3 -none- numeric 6# lev 2 -none- character 7# svd 1 -none- numeric 8# N 1 -none- numeric 9# call 4 -none- call 10# xNames 3 -none- character 11# problemType 1 -none- character 12# tuneValue 1 data.frame list 13# obsLevels 2 -none- character 14# param 1 -none- list 1predict(ldaCaret,weeklyVal) %\u0026gt;% confusionMatrix(weeklyVal$Direction) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction Down Up 5# Down 20 19 6# Up 23 42 7# 8# Accuracy : 0.5962 9# 95% CI : (0.4954, 0.6913) 10# No Information Rate : 0.5865 11# P-Value [Acc \u0026gt; NIR] : 0.4626 12# 13# Kappa : 0.1558 14# 15# Mcnemar\u0026#39;s Test P-Value : 0.6434 16# 17# Sensitivity : 0.4651 18# Specificity : 0.6885 19# Pos Pred Value : 0.5128 20# Neg Pred Value : 0.6462 21# Prevalence : 0.4135 22# Detection Rate : 0.1923 23# Detection Prevalence : 0.3750 24# Balanced Accuracy : 0.5768 25# 26# \u0026#39;Positive\u0026#39; Class : Down 27# KNN Honestly, again, this should be scaled. Plot KNN with the best parameters.\n1plot(knnCaret)  Figure 10: KNN statistics\n  Evidently, the accuracy increases with an increase in the number of neighbors considered.\n1plot(knnCaret, print.thres = 0.5, type=\u0026#34;S\u0026#34;)  Figure 11: Visualizing thresholds for KNN\n  However this shows that we don\u0026rsquo;t actually get much of an increase in accuracy anyway.\nQuestion 4.11 - Pages 171-172 In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set.\n(a) Create a binary variable, mpg01 , that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.\n(b) Explore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01 ? Scatter-plots and boxplots may be useful tools to answer this question. Describe your findings.\n(c) Split the data into a training set and a test set.\n(d) Perform LDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?\n(e) Perform QDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?\n(f) Perform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?\n(g) Perform KNN on the training data, with several values of \\(K\\), in order to predict mpg01 . Use only the variables that seemed most associated with mpg01 in (b). What test errors do you obtain? Which value of \\(K\\) seems to perform the best on this data set?\nAnswer 1autoDat\u0026lt;-ISLR::Auto a) Binary Variable 1autoDat$mpg %\u0026gt;% sort() %\u0026gt;% median() 1# [1] 22.75 Now we can get a new variable from that.\n1newDat=autoDat 2newDat$mpg01 \u0026lt;- ifelse(autoDat$mpg\u0026lt;autoDat$mpg %\u0026gt;% sort() %\u0026gt;% median(),0,1) %\u0026gt;% factor() Note that the ifelse command takes a truthy function, value when false, value when true, but does not return a factor automatically so we piped it to factor to ensure it is factorial.\nb) Visual Exploration Some box-plots:\n1newDat %\u0026gt;% pivot_longer(-c(mpg01,name),names_to=\u0026#34;Params\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=mpg01,y=Value)) + 2geom_boxplot() + 3facet_wrap(~ Params, scales = \u0026#34;free_y\u0026#34;)  Figure 12: Box plots\n  With some scatter plots as well:\n1newDat %\u0026gt;% pivot_longer(-c(mpg01,name,weight),names_to=\u0026#34;Params\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=weight,y=Value,color=mpg01)) + 2geom_point() + 3facet_wrap(~ Params, scales = \u0026#34;free_y\u0026#34;)  Figure 13: Scatter plots\n  Clearly, origin, year and cylinder are essentially not very relevant numerically for the regression lines and confidence intervals.\n1newDat %\u0026gt;% select(-year,-origin,-cylinders) %\u0026gt;% pivot_longer(-c(mpg01,name,mpg),names_to=\u0026#34;Params\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=mpg,y=Value,color=mpg01)) + 2geom_point() + 3geom_smooth(method=lm) + 4facet_wrap(~ Params, scales = \u0026#34;free_y\u0026#34;)  c) Train-Test Split We can split our data very easily with caret. It is important to remember that for factors, random sampling occurs within each class to preserve the overall class distribution of the data.\n1set.seed(1984) 2trainInd \u0026lt;- createDataPartition(newDat$mpg01, # Factor, so class sampling 3p=0.7, # 70-30 train-test 4list=FALSE, # No lists 5times=1) # No bootstrap 6autoTrain\u0026lt;-newDat[trainInd,] 7autoTest\u0026lt;-newDat[-trainInd,] d) LDA with Significant Variables Whenever I see significant I think correlation, so let\u0026rsquo;s take a look at that.\n1newDat %\u0026gt;% select(-mpg01,-name) %\u0026gt;% cor 1# mpg cylinders displacement horsepower weight 2# mpg 1.0000000 -0.7776175 -0.8051269 -0.7784268 -0.8322442 3# cylinders -0.7776175 1.0000000 0.9508233 0.8429834 0.8975273 4# displacement -0.8051269 0.9508233 1.0000000 0.8972570 0.9329944 5# horsepower -0.7784268 0.8429834 0.8972570 1.0000000 0.8645377 6# weight -0.8322442 0.8975273 0.9329944 0.8645377 1.0000000 7# acceleration 0.4233285 -0.5046834 -0.5438005 -0.6891955 -0.4168392 8# year 0.5805410 -0.3456474 -0.3698552 -0.4163615 -0.3091199 9# origin 0.5652088 -0.5689316 -0.6145351 -0.4551715 -0.5850054 10# acceleration year origin 11# mpg 0.4233285 0.5805410 0.5652088 12# cylinders -0.5046834 -0.3456474 -0.5689316 13# displacement -0.5438005 -0.3698552 -0.6145351 14# horsepower -0.6891955 -0.4163615 -0.4551715 15# weight -0.4168392 -0.3091199 -0.5850054 16# acceleration 1.0000000 0.2903161 0.2127458 17# year 0.2903161 1.0000000 0.1815277 18# origin 0.2127458 0.1815277 1.0000000 1newDat %\u0026gt;% length 1# [1] 10 Now lets quickly see what it looks like with correlated values removed.\n1corrCols2=newDat %\u0026gt;% select(-mpg01,-name) %\u0026gt;% cor %\u0026gt;% findCorrelation(cutoff=0.85) 2newRed\u0026lt;-newDat[-c(corrCols2)] 3newRed %\u0026gt;% summary 1# mpg weight acceleration year origin 2# Min. : 9.00 Min. :1613 Min. : 8.00 Min. :70.00 Min. :1.000 3# 1st Qu.:17.00 1st Qu.:2225 1st Qu.:13.78 1st Qu.:73.00 1st Qu.:1.000 4# Median :22.75 Median :2804 Median :15.50 Median :76.00 Median :1.000 5# Mean :23.45 Mean :2978 Mean :15.54 Mean :75.98 Mean :1.577 6# 3rd Qu.:29.00 3rd Qu.:3615 3rd Qu.:17.02 3rd Qu.:79.00 3rd Qu.:2.000 7# Max. :46.60 Max. :5140 Max. :24.80 Max. :82.00 Max. :3.000 8# 9# name mpg01 10# amc matador : 5 0:196 11# ford pinto : 5 1:196 12# toyota corolla : 5 13# amc gremlin : 4 14# amc hornet : 4 15# chevrolet chevette: 4 16# (Other) :365 Inherent in this discussion is the fact that I consider what is correlated to mpg to be a good indicator of what will help mpg01 for obvious reasons.\nNow we can just use the columns we found with findCorrelation.\n1corrCols2 %\u0026gt;% print 1# [1] 3 4 2 1names(newDat) 1# [1] \u0026#34;mpg\u0026#34; \u0026#34;cylinders\u0026#34; \u0026#34;displacement\u0026#34; \u0026#34;horsepower\u0026#34; \u0026#34;weight\u0026#34; 2# [6] \u0026#34;acceleration\u0026#34; \u0026#34;year\u0026#34; \u0026#34;origin\u0026#34; \u0026#34;name\u0026#34; \u0026#34;mpg01\u0026#34; 1autoLDA=train(mpg01~cylinders+displacement+horsepower,data=autoTrain,method=\u0026#34;lda\u0026#34;) 2valScoreLDA=predict(autoLDA,autoTest) Now we can check the statistics.\n1confusionMatrix(valScoreLDA,autoTest$mpg01) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction 0 1 5# 0 56 2 6# 1 2 56 7# 8# Accuracy : 0.9655 9# 95% CI : (0.9141, 0.9905) 10# No Information Rate : 0.5 11# P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 12# 13# Kappa : 0.931 14# 15# Mcnemar\u0026#39;s Test P-Value : 1 16# 17# Sensitivity : 0.9655 18# Specificity : 0.9655 19# Pos Pred Value : 0.9655 20# Neg Pred Value : 0.9655 21# Prevalence : 0.5000 22# Detection Rate : 0.4828 23# Detection Prevalence : 0.5000 24# Balanced Accuracy : 0.9655 25# 26# \u0026#39;Positive\u0026#39; Class : 0 27# That is an amazingly accurate model.\n1auto_ldaROC\u0026lt;-roc(predictor=as.numeric(valScoreLDA),response=autoTest$mpg01,levels=levels(autoTest$mpg01)) 1# Setting direction: controls \u0026lt; cases 1ggroc(auto_ldaROC)  e) QDA with Significant Variables Same deal as before.\n1autoQDA=train(mpg01~cylinders+displacement+horsepower,data=autoTrain,method=\u0026#34;qda\u0026#34;) 2valScoreQDA=predict(autoQDA,autoTest) Now we can check the statistics.\n1confusionMatrix(valScoreQDA,autoTest$mpg01) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction 0 1 5# 0 56 2 6# 1 2 56 7# 8# Accuracy : 0.9655 9# 95% CI : (0.9141, 0.9905) 10# No Information Rate : 0.5 11# P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 12# 13# Kappa : 0.931 14# 15# Mcnemar\u0026#39;s Test P-Value : 1 16# 17# Sensitivity : 0.9655 18# Specificity : 0.9655 19# Pos Pred Value : 0.9655 20# Neg Pred Value : 0.9655 21# Prevalence : 0.5000 22# Detection Rate : 0.4828 23# Detection Prevalence : 0.5000 24# Balanced Accuracy : 0.9655 25# 26# \u0026#39;Positive\u0026#39; Class : 0 27# 1auto_qdaROC\u0026lt;-roc(predictor=as.numeric(valScoreQDA),response=autoTest$mpg01,levels=levels(autoTest$mpg01)) 1# Setting direction: controls \u0026lt; cases 1ggroc(auto_qdaROC)  OK, this is weird enough to check if it isn\u0026rsquo;t some sort of artifact.\n1autoQDA2=train(mpg01~horsepower, data=autoTrain,method=\u0026#39;qda\u0026#39;) 2valScoreQDA2=predict(autoQDA2, autoTest) 3confusionMatrix(valScoreQDA2,autoTest$mpg01) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction 0 1 5# 0 42 3 6# 1 16 55 7# 8# Accuracy : 0.8362 9# 95% CI : (0.7561, 0.8984) 10# No Information Rate : 0.5 11# P-Value [Acc \u0026gt; NIR] : 4.315e-14 12# 13# Kappa : 0.6724 14# 15# Mcnemar\u0026#39;s Test P-Value : 0.005905 16# 17# Sensitivity : 0.7241 18# Specificity : 0.9483 19# Pos Pred Value : 0.9333 20# Neg Pred Value : 0.7746 21# Prevalence : 0.5000 22# Detection Rate : 0.3621 23# Detection Prevalence : 0.3879 24# Balanced Accuracy : 0.8362 25# 26# \u0026#39;Positive\u0026#39; Class : 0 27# OK, so the model isn\u0026rsquo;t completely creepily correct all the time. In this case we should probably think about what is going on. I would think it is because of the nature of the train-test split we performed. We have ensured during the sampling of our data that the train and test sets contain the SAME distribution (assumed). So that\u0026rsquo;s why our training result and test results are both incredibly good. They\u0026rsquo;re essentially the same thing.\nIn fact, this is the perfect time to consider a validation set, just to see what the models are really doing. Won\u0026rsquo;t get into it right now though.\nf) Logistic with Significant Variables 1glmAuto.fit=glm(mpg01~cylinders+displacement+horsepower, data=autoTrain, family=binomial) 1glmAuto.probs = predict(glmAuto.fit,autoTest, type = \u0026#34;response\u0026#34;) 2glmAuto.pred = rep(1,length(glmAuto.probs)) 3glmAuto.pred[glmAuto.probs\u0026lt;0.5]=0 4glmAuto.pred=factor(glmAuto.pred) 5confusionMatrix(glmAuto.pred,autoTest$mpg01) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction 0 1 5# 0 56 4 6# 1 2 54 7# 8# Accuracy : 0.9483 9# 95% CI : (0.8908, 0.9808) 10# No Information Rate : 0.5 11# P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 12# 13# Kappa : 0.8966 14# 15# Mcnemar\u0026#39;s Test P-Value : 0.6831 16# 17# Sensitivity : 0.9655 18# Specificity : 0.9310 19# Pos Pred Value : 0.9333 20# Neg Pred Value : 0.9643 21# Prevalence : 0.5000 22# Detection Rate : 0.4828 23# Detection Prevalence : 0.5172 24# Balanced Accuracy : 0.9483 25# 26# \u0026#39;Positive\u0026#39; Class : 0 27# g) KNN Modeling Scale the parameters later.\n1knnAuto=train(mpg01~cylinders+displacement+horsepower,data=autoTrain,method=\u0026#34;knn\u0026#34;) Plot KNN with the best parameters.\n1plot(knnCaret)  Evidently, the accuracy increases with an increase in the number of neighbors considered.\n1plot(knnAuto, print.thres = 0.5, type=\u0026#34;S\u0026#34;)  So we can see that \\(5\\) neighbors is a good compromise.\nQuestion 4.12 - Pages 172-173 This problem involves writing functions.\n(a) Write a function, Power() , that prints out the result of raising 2 to the 3rd power. In other words, your function should compute 2^3 and print out the results.\nHint: Recall that x^a raises x to the power a. Use the print() function to output the result.\n(b) Create a new function, Power2() , that allows you to pass any two numbers, x and a , and prints out the value of x^a . You can do this by beginning your function with the line\n1Power2=function(x,a){} You should be able to call your function by entering, for instance,\n1Power2(3,8) on the command line. This should output the value of \\(3^8\\), namely, \\(6,651\\).\n(c) Using the Power2() function that you just wrote, compute \\(10^3\\), \\(8^{17}\\), and \\(131^3\\).\n(d) Now create a new function, Power3(), that actually returns the result x^a as an R object, rather than simply printing it to the screen. That is, if you store the value x^a in an object called result within your function, then you can simply return() this result, using the following line:\n1return(result) The line above should be the last line in your function, before the } symbol.\n(e) Now using the Power3() function, create a plot of \\(f(x)=x^2\\). The x-axis should display a range of integers from \\(1\\) to \\(10\\), and the y-axis should display \\(x^2\\) . Label the axes appropriately, and use an appropriate title for the figure. Consider displaying either the x-axis, the y-axis, or both on the log-scale. You can do this by using log=‘‘x’’, log=‘‘y’’, or log=‘‘xy’’ as arguments to the plot() function.\n(f) Create a function, PlotPower() , that allows you to create a plot of x against x^a for a fixed a and for a range of values of x. For instance, if you call\n1PlotPower (1:10 ,3) then a plot should be created with an x-axis taking on values \\(1,2,\u0026hellip;,10\\) and a y-axis taking on values \\(1^3,2^3,\u0026hellip;,10^3\\)\nAnswer a) Create a Squaring Function 1Power=function(x){print(2^x)} 2Power(3) 1# [1] 8 b) Generalizing Power to arbitrary numbers 1Power2=function(x,a){print(x^a)} 1Power2(3,8) 1# [1] 6561 c) Random Testing of Power2 1Power2(10,3) 1# [1] 1000 1Power2(8,17) 1# [1] 2.2518e+15 1Power2(131,2) 1# [1] 17161 d) Return a value 1Power3=function(x,a){return(x^a)} e) Plot something with Power3 Actually now would be a good place to introduce LaTeX labeling.\n1#install.packages(\u0026#34;latex2exp\u0026#34;) 2library(latex2exp) No log scale.\n1qplot(x=seq(1,10),y=Power3(seq(1,10),2)) + ggtitle(\u0026#34;Function without a log scale\u0026#34;) + 2geom_point() + xlab(\u0026#34;X\u0026#34;) + ylab(TeX(\u0026#34;$X^2$\u0026#34;))  With a log scale.\n1qplot(x=seq(1,10),y=Power3(seq(1,10),2)) + ggtitle(\u0026#34;Function with a log scale\u0026#34;) + 2geom_point() + xlab(\u0026#34;X\u0026#34;) + ylab(TeX(\u0026#34;$X^2$\u0026#34;)) + scale_y_log10()  f) PlotPower Function 1PlotPower=function(xrange,pow){return(qplot(x=xrange,y=Power3(xrange,pow)))} 1plotter\u0026lt;-PlotPower(1:10,3) 2plotter  The R Cookbook is quite neat for some simple tasks like this.\nQuestion 4.13 - Pages 173 Using the Boston data set, fit classification models in order to predict whether a given suburb has a crime rate above or below the median. Explore logistic regression, LDA, and KNN models using various subsets of the predictors. Describe your findings.\nAnswer OK, to speed this up, I will simply run through all the work done on the Auto set. Recall that details about this data-set are also here.\n1boston\u0026lt;-MASS::Boston  Check unique values   1boston %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) 1# crim zn indus chas nox rm age dis rad tax 2# 504 26 76 2 81 446 356 412 9 66 3# ptratio black lstat medv 4# 46 357 455 229 CHAS is of course something which should be a factor, and with RAD having only \\(9\\) levels, I\u0026rsquo;m inclined to make it a factor as well.\n1boston\u0026lt;-boston %\u0026gt;% mutate(rad=factor(rad),chas=factor(chas))  Make a median variable   1boston$highCrime\u0026lt;- ifelse(boston$crim\u0026lt;boston$crim %\u0026gt;% median(),0,1) %\u0026gt;% factor()  Take a look at the data  Some box-plots:\n1boston %\u0026gt;% pivot_longer(-c(rad,chas,highCrime),names_to=\u0026#34;Param\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=highCrime,y=Value,fill=chas)) + 2geom_boxplot()+ 3facet_wrap(~Param,scales=\u0026#34;free_y\u0026#34;)  It is surprising, but evidently the CHAS variable is strangely relevant. 1 implies the tract bounds the river, otherwise 0.\n Correlations   1boston %\u0026gt;% select(-c(rad,chas,highCrime)) %\u0026gt;% cor 1# crim zn indus nox rm age 2# crim 1.0000000 -0.2004692 0.4065834 0.4209717 -0.2192467 0.3527343 3# zn -0.2004692 1.0000000 -0.5338282 -0.5166037 0.3119906 -0.5695373 4# indus 0.4065834 -0.5338282 1.0000000 0.7636514 -0.3916759 0.6447785 5# nox 0.4209717 -0.5166037 0.7636514 1.0000000 -0.3021882 0.7314701 6# rm -0.2192467 0.3119906 -0.3916759 -0.3021882 1.0000000 -0.2402649 7# age 0.3527343 -0.5695373 0.6447785 0.7314701 -0.2402649 1.0000000 8# dis -0.3796701 0.6644082 -0.7080270 -0.7692301 0.2052462 -0.7478805 9# tax 0.5827643 -0.3145633 0.7207602 0.6680232 -0.2920478 0.5064556 10# ptratio 0.2899456 -0.3916785 0.3832476 0.1889327 -0.3555015 0.2615150 11# black -0.3850639 0.1755203 -0.3569765 -0.3800506 0.1280686 -0.2735340 12# lstat 0.4556215 -0.4129946 0.6037997 0.5908789 -0.6138083 0.6023385 13# medv -0.3883046 0.3604453 -0.4837252 -0.4273208 0.6953599 -0.3769546 14# dis tax ptratio black lstat medv 15# crim -0.3796701 0.5827643 0.2899456 -0.3850639 0.4556215 -0.3883046 16# zn 0.6644082 -0.3145633 -0.3916785 0.1755203 -0.4129946 0.3604453 17# indus -0.7080270 0.7207602 0.3832476 -0.3569765 0.6037997 -0.4837252 18# nox -0.7692301 0.6680232 0.1889327 -0.3800506 0.5908789 -0.4273208 19# rm 0.2052462 -0.2920478 -0.3555015 0.1280686 -0.6138083 0.6953599 20# age -0.7478805 0.5064556 0.2615150 -0.2735340 0.6023385 -0.3769546 21# dis 1.0000000 -0.5344316 -0.2324705 0.2915117 -0.4969958 0.2499287 22# tax -0.5344316 1.0000000 0.4608530 -0.4418080 0.5439934 -0.4685359 23# ptratio -0.2324705 0.4608530 1.0000000 -0.1773833 0.3740443 -0.5077867 24# black 0.2915117 -0.4418080 -0.1773833 1.0000000 -0.3660869 0.3334608 25# lstat -0.4969958 0.5439934 0.3740443 -0.3660869 1.0000000 -0.7376627 26# medv 0.2499287 -0.4685359 -0.5077867 0.3334608 -0.7376627 1.0000000 Now, unsurprisingly, there\u0026rsquo;s nothing which is really strongly correlated here for some reason.\n Train test splits   1set.seed(1984) 2trainIndCri \u0026lt;- createDataPartition(boston$highCrime, # Factor, so class sampling 3p=0.7, # 70-30 train-test 4list=FALSE, # No lists 5times=1) # No bootstrap 6bostonTrain\u0026lt;-boston[trainIndCri,] 7bostonTest\u0026lt;-boston[-trainIndCri,]  Make a bunch of models   1glmBos.fit=glm(highCrime~., data=bostonTrain, family=binomial) 1# Warning: glm.fit: algorithm did not converge 1# Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred 1glmBos.probs = predict(glmBos.fit,bostonTest, type = \u0026#34;response\u0026#34;) 2glmBos.pred = rep(1,length(glmBos.probs)) 3glmBos.pred[glmBos.probs\u0026lt;0.5]=0 4glmBos.pred=factor(glmBos.pred) 5confusionMatrix(glmBos.pred,bostonTest$highCrime) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction 0 1 5# 0 68 6 6# 1 7 69 7# 8# Accuracy : 0.9133 9# 95% CI : (0.8564, 0.953) 10# No Information Rate : 0.5 11# P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 12# 13# Kappa : 0.8267 14# 15# Mcnemar\u0026#39;s Test P-Value : 1 16# 17# Sensitivity : 0.9067 18# Specificity : 0.9200 19# Pos Pred Value : 0.9189 20# Neg Pred Value : 0.9079 21# Prevalence : 0.5000 22# Detection Rate : 0.4533 23# Detection Prevalence : 0.4933 24# Balanced Accuracy : 0.9133 25# 26# \u0026#39;Positive\u0026#39; Class : 0 27# 1bostonLDA=train(highCrime~.,data=bostonTrain,method=\u0026#39;lda\u0026#39;) 2bostonQDA=train(highCrime~tax+crim,data=bostonTrain,method=\u0026#39;qda\u0026#39;) 3bostonKNN=train(highCrime~.,data=bostonTrain,preProcess = c(\u0026#34;center\u0026#34;,\u0026#34;scale\u0026#34;),method=\u0026#39;knn\u0026#39;) 1bLDAp=predict(bostonLDA,bostonTest) 2bQDAp=predict(bostonQDA,bostonTest) 3bKNNp=predict(bostonKNN,bostonTest) 1confusionMatrix(bLDAp,bostonTest$highCrime) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction 0 1 5# 0 72 6 6# 1 3 69 7# 8# Accuracy : 0.94 9# 95% CI : (0.8892, 0.9722) 10# No Information Rate : 0.5 11# P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 12# 13# Kappa : 0.88 14# 15# Mcnemar\u0026#39;s Test P-Value : 0.505 16# 17# Sensitivity : 0.9600 18# Specificity : 0.9200 19# Pos Pred Value : 0.9231 20# Neg Pred Value : 0.9583 21# Prevalence : 0.5000 22# Detection Rate : 0.4800 23# Detection Prevalence : 0.5200 24# Balanced Accuracy : 0.9400 25# 26# \u0026#39;Positive\u0026#39; Class : 0 27# 1confusionMatrix(bQDAp,bostonTest$highCrime) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction 0 1 5# 0 73 5 6# 1 2 70 7# 8# Accuracy : 0.9533 9# 95% CI : (0.9062, 0.981) 10# No Information Rate : 0.5 11# P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 12# 13# Kappa : 0.9067 14# 15# Mcnemar\u0026#39;s Test P-Value : 0.4497 16# 17# Sensitivity : 0.9733 18# Specificity : 0.9333 19# Pos Pred Value : 0.9359 20# Neg Pred Value : 0.9722 21# Prevalence : 0.5000 22# Detection Rate : 0.4867 23# Detection Prevalence : 0.5200 24# Balanced Accuracy : 0.9533 25# 26# \u0026#39;Positive\u0026#39; Class : 0 27# 1confusionMatrix(bKNNp,bostonTest$highCrime) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction 0 1 5# 0 74 6 6# 1 1 69 7# 8# Accuracy : 0.9533 9# 95% CI : (0.9062, 0.981) 10# No Information Rate : 0.5 11# P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 12# 13# Kappa : 0.9067 14# 15# Mcnemar\u0026#39;s Test P-Value : 0.1306 16# 17# Sensitivity : 0.9867 18# Specificity : 0.9200 19# Pos Pred Value : 0.9250 20# Neg Pred Value : 0.9857 21# Prevalence : 0.5000 22# Detection Rate : 0.4933 23# Detection Prevalence : 0.5333 24# Balanced Accuracy : 0.9533 25# 26# \u0026#39;Positive\u0026#39; Class : 0 27# Clearly in this particular case, an LDA model seems to be working out the best for this data when trained on all the parameters, though Logistic Regression is doing quite well too.\n Notes on KNN   1plot(bostonKNN)  1plot(bostonKNN, print.thres = 0.5, type=\u0026#34;S\u0026#34;)   Comparison  Finally, we will quickly plot some indicative measures.\n1knnBosROC\u0026lt;-roc(predictor=as.numeric(bKNNp),response=bostonTest$highCrime) 1# Setting levels: control = 0, case = 1 1# Setting direction: controls \u0026lt; cases 1logiBosROC\u0026lt;-roc(predictor=as.numeric(glmBos.probs),response=bostonTest$highCrime) 1# Setting levels: control = 0, case = 1 2# Setting direction: controls \u0026lt; cases 1ldaBosROC\u0026lt;-roc(predictor=as.numeric(bLDAp),response=bostonTest$highCrime) 1# Setting levels: control = 0, case = 1 2# Setting direction: controls \u0026lt; cases 1qdaBosROC\u0026lt;-roc(predictor=as.numeric(bQDAp),response=bostonTest$highCrime) 1# Setting levels: control = 0, case = 1 2# Setting direction: controls \u0026lt; cases 1ggroc(list(KNN=knnBosROC,Logistic=logiBosROC,LDA=ldaBosROC,QDA=qdaBosROC))  Figure 14: plot of chunk unnamed-chunk-87\n  OK, one of the reasons why these models do so well is because they are all assuming an equal distribution of train and test classes, and they use crim itself as a predictor. This is no doubt a strong reason why these models uniformly perform so well. I\u0026rsquo;d say 5 is the best option.\n  James, G., Witten, D., Hastie, T., \u0026amp; Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Berlin, Germany: Springer Science \u0026amp; Business Media.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n A good introduction to the caret and skimr packages is here\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/islr-ch4/","tags":["solutions","R","ISLR"],"title":"ISLR :: Classification"},{"categories":["personal"],"contents":"Background  Pandora doesn\u0026rsquo;t work outside the states I keep forgetting how to set-up proxychains  Proxychains Technically this article expects proxychains-ng, which seems to be the more up-to-date fork of the original proxychains.\n  Install proxychains-ng\n1# I am on archlinux.. 2sudo pacman -S proxychains-ng   Copy the configuration to the $HOME directory\n1cp /etc/proxychains.conf .   Edit said configuration to add some US-based proxy\n  In my particular case, I don\u0026rsquo;t keep the tor section enabled.\n1tail $HOME/proxychains.conf 1# 2# proxy types: http, socks4, socks5 3# ( auth types supported: \u0026#34;basic\u0026#34;-http \u0026#34;user/pass\u0026#34;-socks ) 4# 5[ProxyList] 6# add proxy here ... 7# meanwile 8# defaults set to \u0026#34;tor\u0026#34; 9# socks4 127.0.0.1 9050 I actually use Windscribe for my VPN needs, and they have a neat SOCKS5 proxy setup. This works out to a line like socks5 $IP $PORT $USERNAME $PASS being added. The default generator gives you a pretty server name, but to get the IP I use ping $SERVER and put that in the conf file. Any online DNS resolver will also do the trick, like this one.\nPandora I use the excellent pianobar frontend.\n  Get pianobar\n1sudo pacman -S pianobar   Use it with proxychains\n1proxychains pianobar   Profit\n  I also like setting up some defaults to make life easier:\n1mkdir -p ~/.config/pianobar 2vim ~/.config/pianobar/config I normally set the following (inspired by the ArchWiki):\n1audio_quality = {high, medium, low} 2autostart_station = $ID 3password = \u0026#34;$PASS\u0026#34; 4user = \u0026#34;$emailID\u0026#34; The autostart_station ID can be obtained by inspecting the terminal output during an initial run. I usually set it to the QuickMix station.\n","permalink":"https://rgoswami.me/posts/pandora-proxychains/","tags":["tools","workflow"],"title":"Pandora and Proxychains"},{"categories":["programming"],"contents":"Background  I dislike Jupyter notebooks (and JupyterHub) a lot EIN is really not much of a solution either  In the past I have written some posts on TeX with JupyterHub and discussed ways to use virtual Python with JupyterHub in a more reasonable manner.\nHowever, I personally found that EIN was a huge pain to work with, and I mostly ended up working with the web-interface anyway.\nIt is a bit redundant to do so, given that at-least for my purposes, the end result was a LaTeX document. Breaking down the rest of my requirements went a bit like this:\n What exports well to TeX? Org, Markdown, anything which goes into pandoc What displays code really well? LaTeX, Markdown, Org What allows easy visualization of code snippets? Rmarkdown, RStudio, JupyterHub, Org with babel  Clearly, orgmode is the common denominator, and ergo, a perfect JupyterHub alternative.\nSetup Throughout this post I will assume the following structure:\n1tree tmp 2mkdir -p tmp/images 3touch tmp/myFakeJupyter.org    tmp        ├── images     └── myFakeJupyter.org     1 directory, 1 file    As is evident, we have a folder tmp which will have all the things we need for dealing with our setup.\nVirtual Python Without waxing too eloquent on the whole reason behind doing this, since I will rant about virtual python management systems elsewhere, here I will simply describe my preferred method, which is using poetry.\n1# In a folder above tmp 2poetry init 3poetry add numpy matplotlib scipy pandas The next part is optional, but a good idea if you figure out using direnv and have configured layout_poetry as described here:\n1# Same place as the poetry files 2echo \u0026#34;layout_poetry()\u0026#34; \u0026gt;\u0026gt; .envrc Note:\n We can nest an arbitrary number of the tmp structures under a single place we define the poetry setup I prefer using direnv to ensure that I never forget to hook into the right environment  Orgmode This is not an introduction to org, however in particular, there are some basic settings to keep in mind to make sure the set-up works as expected.\nIndentation Python is notoriously weird about whitespace, so we will ensure that our export process does not mangle whitespace and offend the python interpreter. We will have the following line at the top of our orgmode file:\n1# -*- org-src-preserve-indentation: t; org-edit-src-content: 0; -*- Note:\n this post is actually generating the file being discussed here by  tangling the file\n You can get the whole file here  TeX Settings These are also basically optional, but at the very least you will need the following:\n1#+author: Rohit Goswami 2#+title: Whatever 3#+subtitle: Wittier line about whatever 4#+date: \\today 5#+OPTIONS: toc:nil I actually use a lot of math using the TeX input mode in Emacs, so I like the following settings for math:\n1# For math display 2#+LATEX_HEADER: \\usepackage{amsfonts} 3#+LATEX_HEADER: \\usepackage{unicode-math} There are a bunch of other settings which may be used, but these are the bare minimum, more on that would be in a snippet anyway.\nNote:\n rendering math in the orgmode file in this manner requires that we use XeTeX to compile the final file  Org-Python We essentially need to ensure that:\n Babel uses our virtual python The same session is used for each block  We will get our poetry python pretty easily:\n1which python Now we will use this as a common header-arg passed into the property drawer to make sure we don\u0026rsquo;t need to set them in every code block.\nWe can use the following structure in our file:\n1\\* Python Stuff 2:PROPERTIES: 3:header-args: :python /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python :session One :results output :exports both 4:END: 5Now we can simply work with code as we normally would 6\\#+BEGIN_SRC python 7print(\u0026#34;Hello World\u0026#34;) 8\\#+END_SRC Note:\n For some reason, this property needs to be set on every heading (as of Feb 13 2020) In the actual file you will want to remove extraneous \\ symbols:  \\* → * \\#+BEGIN_SRC → #+BEGIN_SRC \\#+END_SRC → #+END_SRC    Python Images and Orgmode To view images in orgmode as we would in a JupyterLab notebook, we will use a slight trick.\n  We will ensure that the code block returns a file object with the arguments\n  The code block should end with a print statement to actually generate the file name\nSo we want a code block like this:\n   1#+BEGIN_SRC python :results output file :exports both 2import matplotlib.pyplot as plt 3from sklearn.datasets.samples_generator import make_circles 4X, y = make_circles(100, factor=.1, noise=.1) 5plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\u0026#39;autumn\u0026#39;) 6plt.xlabel(\u0026#39;x1\u0026#39;) 7plt.ylabel(\u0026#39;x2\u0026#39;) 8plt.savefig(\u0026#39;images/plotCircles.png\u0026#39;, dpi = 300) 9print(\u0026#39;images/plotCircles.png\u0026#39;) # return filename to org-mode 10#+end_src Which would give the following when executed:\n1#+RESULTS: 2[[file:images/plotCircles.png]] Since that looks pretty ugly, this will actually look like this:\n1import matplotlib.pyplot as plt 2from sklearn.datasets.samples_generator import make_circles 3X, y = make_circles(100, factor=.1, noise=.1) 4plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\u0026#39;autumn\u0026#39;) 5plt.xlabel(\u0026#39;x1\u0026#39;) 6plt.ylabel(\u0026#39;x2\u0026#39;) 7plt.savefig(\u0026#39;images/plotCircles.png\u0026#39;, dpi = 300) 8print(\u0026#39;images/plotCircles.png\u0026#39;) # return filename to org-mode  Bonus A better way to simulate standard jupyter workflows is to just specify the properties once at the beginning.\n1#+PROPERTY: header-args:python :python /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python :session One :results output :exports both This setup circumvents having to set the properties per sub-tree, though for very large projects, it is useful to use different processes.\nConclusions  The last step is of course to export the file as to a TeX file and then compile that with something like latexmk -pdfxe -shell-escape file.tex  There are a million and one variations of this of course, but this is enough to get started.\nThe whole file is also reproduced here.\nComments The older commenting system was implemented with utteranc.es as seen below.\n ","permalink":"https://rgoswami.me/posts/jupyter-orgmode/","tags":["tools","emacs","workflow","orgmode"],"title":"Replacing Jupyter with Orgmode"},{"categories":["programming"],"contents":"Background  I end up writing about using poetry a lot I almost always use direnv in real life too I don\u0026rsquo;t keep writing mini scripts in my .envrc  Honestly there\u0026rsquo;s nothing here anyone using the direnv wiki will find surprising, but then it is still neat to link back to.\nSetting Up Poetry This essentially works by simply modifying the global .direnvrc which essentially gets sourced by every local .envrc anyway.\n1vim $HOME/.direnvrc So what we put in there is the following snippet derived from other snippets on the wiki, and is actually now there too.\n1# PUT this here 2layout_poetry() { 3if [[ ! -f pyproject.toml ]]; then 4log_error \u0026#39;No pyproject.toml found. Use `poetry new` or `poetry init` to create one first.\u0026#39; 5exit 2 6fi 78local VENV=$(dirname $(poetry run which python)) 9export VIRTUAL_ENV=$(echo \u0026#34;$VENV\u0026#34; | rev | cut -d\u0026#39;/\u0026#39; -f2- | rev) 10export POETRY_ACTIVE=1 11PATH_add \u0026#34;$VENV\u0026#34; 12} Now we can just make .envrc files with layout_poetry and everything will just work™.\n","permalink":"https://rgoswami.me/posts/poetry-direnv/","tags":["tools","direnv","workflow","python"],"title":"Poetry and Direnv"},{"categories":["notes"],"contents":"Background As a member of several large organizations, I get a lot of github notifications. Not all of these are of relevance to me. This is especially true of psuedo-monorepo style repositories like the JOSS review system and especially the exercism community.\n I recently (re-)joined the exercism community as a maintainer for the C++ lessons after having been a (sporadic) teacher This was largely in response to a community call to action as the group needed new blood to usher in v3 of the exercism project  Anyway, I have since found that at the small cost of possibly much of my public repo data, I can manage my notifications better with Octobox\nOctobox  It appears to be free for now It syncs on demand (useful) I can search things quite easily They have a neat logo There appear to be many features I probably won\u0026rsquo;t use  It looks like this:\n Figure 1: Octobox Stock Photo\n  ","permalink":"https://rgoswami.me/posts/ghnotif/","tags":["tools","github","workflow"],"title":"Taming Github Notifications"},{"categories":["personal"],"contents":"Why this site exists I have a lot of online presences. I have been around (or at-least, lurking) for over ten years. Almost as long as I have been programming. Anyway, I have a penchant lately for using emacs and honestly there isn\u0026rsquo;t very good support for org-mode files. There are options recently with gatsby as well, but this seemed kinda neat.\nWhat \u0026rsquo;this\u0026rsquo; is  This site is built by Hugo The posts are generated with ox-hugo The theme is based of this excellent one by Djordje Atlialp, which in turn is based off of this theme by panr  My modifications are here    What is here  Mostly random thoughts I don\u0026rsquo;t mind people knowing Some tech stuff which isn\u0026rsquo;t coherent enough to be put in any form with references Emacs specific workflows which I might want to write about more than short notes on the config  What isn\u0026rsquo;t here  Some collections should and will go to my grimoire My doom-emacs configuration Academic stuff is better tracked on Publons or Google Scholar or my pages hosted by my favorite IITK group or UI group  ","permalink":"https://rgoswami.me/posts/rationale/","tags":["ramblings","explanations"],"title":"Site Rationale"},{"categories":["programming"],"contents":"Chapter II - Statistical Learning All the questions are as per the ISL seventh printing of the First edition 1.\nQuestion 2.8 - Pages 54-55 This exercise relates to the College data set, which can be found in the file College.csv. It contains a number of variables for \\(777\\) different universities and colleges in the US. The variables are\n Private : Public/private indicator Apps : Number of applications received Accept : Number of applicants accepted Enroll : Number of new students enrolled Top10perc : New students from top 10 % of high school class Top25perc : New students from top 25 % of high school class F.Undergrad : Number of full-time undergraduates P.Undergrad : Number of part-time undergraduates Outstate : Out-of-state tuition Room.Board : Room and board costs Books : Estimated book costs Personal : Estimated personal spending PhD : Percent of faculty with Ph.D.\u0026rsquo;s Terminal : Percent of faculty with terminal degree S.F.Ratio : Student/faculty ratio perc.alumni : Percent of alumni who donate Expend : Instructional expenditure per student Grad.Rate : Graduation rate  Before reading the data into R, it can be viewed in Excel or a text editor.\n(a) Use the read.csv() function to read the data into R . Call the loaded data college. Make sure that you have the directory set to the correct location for the data.\n(b) Look at the data using the fix() function. You should notice that the ﬁrst column is just the name of each university. We don\u0026rsquo;t really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands:\n1rownames(college)=college[,1] 2fix(college) You should see that there is now a row.names column with the name of each university recorded. This means that R has given each row a name corresponding to the appropriate university. R will not try to perform calculations on the row names. However, we still need to eliminate the ﬁrst column in the data where the names are stored. Try:\n1college=college[,-1] 2fix(college) (c)\n Use the summary() function to produce a numerical summary of the variables in the data set. Use the pairs() function to produce a scatterplot matrix of the ﬁrst ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix A using A[,1:10] . Use the plot() function to produce side-by-side boxplots of Outstate versus Private . Create a new qualitative variable, called Elite , by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top \\(10%\\) of their high school classes exceeds \\(50%\\).   1Elite = rep(\u0026#34;No\u0026#34;, nrow(college)) 2Elite [college$Top10perc \u0026gt;50]=\u0026#34;Yes\u0026#34; 3Elite = as.factor (Elite) 4college = data.frame (college, Elite) Use the summary() function to see how many elite univer- sities there are. Now use the plot() function to produce side-by-side boxplots of Outstate versus Elite .\n Use the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative vari- ables. You may fnd the command par(mfrow=c(2,2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways. Continue exploring the data, and provide a brief summary of what you discover.  Answer Instead of reading in data, for ISLR in particular we can load the ISLR library which is on CRAN and contains the data-sets required for the book.\n1install.packages(\u0026#34;ISLR\u0026#34;) Thus, we can now read it in as library(\u0026quot;ISLR\u0026quot;)\nThe remaining sections are meant to be executed, and are marked as such, with r in {}.\n(c)\nWe will load the dataset once for the whole document.\n1library(\u0026#34;ISLR\u0026#34;)  Usage of the summary() function   1summary(ISLR::College) 1## Private Apps Accept Enroll Top10perc 2## No :212 Min. : 81 Min. : 72 Min. : 35 Min. : 1.00 3## Yes:565 1st Qu.: 776 1st Qu.: 604 1st Qu.: 242 1st Qu.:15.00 4## Median : 1558 Median : 1110 Median : 434 Median :23.00 5## Mean : 3002 Mean : 2019 Mean : 780 Mean :27.56 6## 3rd Qu.: 3624 3rd Qu.: 2424 3rd Qu.: 902 3rd Qu.:35.00 7## Max. :48094 Max. :26330 Max. :6392 Max. :96.00 8## Top25perc F.Undergrad P.Undergrad Outstate 9## Min. : 9.0 Min. : 139 Min. : 1.0 Min. : 2340 10## 1st Qu.: 41.0 1st Qu.: 992 1st Qu.: 95.0 1st Qu.: 7320 11## Median : 54.0 Median : 1707 Median : 353.0 Median : 9990 12## Mean : 55.8 Mean : 3700 Mean : 855.3 Mean :10441 13## 3rd Qu.: 69.0 3rd Qu.: 4005 3rd Qu.: 967.0 3rd Qu.:12925 14## Max. :100.0 Max. :31643 Max. :21836.0 Max. :21700 15## Room.Board Books Personal PhD 16## Min. :1780 Min. : 96.0 Min. : 250 Min. : 8.00 17## 1st Qu.:3597 1st Qu.: 470.0 1st Qu.: 850 1st Qu.: 62.00 18## Median :4200 Median : 500.0 Median :1200 Median : 75.00 19## Mean :4358 Mean : 549.4 Mean :1341 Mean : 72.66 20## 3rd Qu.:5050 3rd Qu.: 600.0 3rd Qu.:1700 3rd Qu.: 85.00 21## Max. :8124 Max. :2340.0 Max. :6800 Max. :103.00 22## Terminal S.F.Ratio perc.alumni Expend 23## Min. : 24.0 Min. : 2.50 Min. : 0.00 Min. : 3186 24## 1st Qu.: 71.0 1st Qu.:11.50 1st Qu.:13.00 1st Qu.: 6751 25## Median : 82.0 Median :13.60 Median :21.00 Median : 8377 26## Mean : 79.7 Mean :14.09 Mean :22.74 Mean : 9660 27## 3rd Qu.: 92.0 3rd Qu.:16.50 3rd Qu.:31.00 3rd Qu.:10830 28## Max. :100.0 Max. :39.80 Max. :64.00 Max. :56233 29## Grad.Rate 30## Min. : 10.00 31## 1st Qu.: 53.00 32## Median : 65.00 33## Mean : 65.46 34## 3rd Qu.: 78.00 35## Max. :118.00  Usage of pairs()   1tenColl \u0026lt;- ISLR::College[,1:10] # For getting the first ten columns 2pairs(tenColl) # Scatterplot  Figure 1: Pairs\n   Boxplot creation with plot()   1plot(ISLR::College$Private,ISLR::College$Outstate,xlab=\u0026#34;Private\u0026#34;,ylab=\u0026#34;Outstate\u0026#34;)  Figure 2: Boxplots\n   Binning and plotting   1college=ISLR::College 2Elite=rep(\u0026#34;No\u0026#34;,nrow(college)) 3Elite[college$Top10perc\u0026gt;50]=\u0026#34;Yes\u0026#34; 4Elite=as.factor(Elite) 5college\u0026lt;-data.frame(college,Elite) 6summary(college$Elite) 1## No Yes 2## 699 78 1plot(college$Outstate,college$Elite,xlab=\u0026#34;Outstate\u0026#34;,ylab=\u0026#34;Elite\u0026#34;)  Figure 3: Plotting Outstate and Elite\n   Histograms with hist()   1par(mfrow=c(2,2)) 2hist(college$Enroll) 3hist(college$perc.alumni, col=2) 4hist(college$Personal, col=3, breaks=10) 5hist(college$PhD, breaks=10)  Figure 4: Histogram\n  1hist(college$Top10perc, col=\u0026#34;blue\u0026#34;) 2hist(college$Outstate, col=23)  Figure 5: Colored Histogram\n   Explorations (graphical)  \\(0\\) implies the faculty have PhDs. It is clear that people donate more when faculty do not have terminal degrees.\n1plot(college$Terminal-college$PhD, college$perc.alumni)  Figure 6: Terminal degrees and alumni\n  High tuition correlates to high graduation rate.\n1plot(college$Expend, college$Grad.Rate)  Figure 7: Tuiton and graduation\n  Low acceptance implies a low student to faculty ratio.\n1plot(college$Accept / college$Apps, college$S.F.Ratio)  Figure 8: Acceptance and Student/Faculty ratio\n  Question 2.9 - Page 56 This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.\n(a) Which of the predictors are quantitative, and which are qualitative?\n(b) What is the range of each quantitative predictor? You can answer this using the range() function.\n(c) What is the mean and standard deviation of each quantitative predictor?\n(d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?\n(e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.\n(f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.\nAnswer Once again, since the dataset is loaded from the library, we will simply start manipulating it.\n1# Clean data 2autoDat\u0026lt;-na.omit(ISLR::Auto) # renamed for convenience (a) To determine weather the variables a qualitative or quantitative we can either inspect the variables by eye, or query the dataset.\n1summary(autoDat) # Observe the output for variance 1## mpg cylinders displacement horsepower weight 2## Min. : 9.00 Min. :3.000 Min. : 68.0 Min. : 46.0 Min. :1613 3## 1st Qu.:17.00 1st Qu.:4.000 1st Qu.:105.0 1st Qu.: 75.0 1st Qu.:2225 4## Median :22.75 Median :4.000 Median :151.0 Median : 93.5 Median :2804 5## Mean :23.45 Mean :5.472 Mean :194.4 Mean :104.5 Mean :2978 6## 3rd Qu.:29.00 3rd Qu.:8.000 3rd Qu.:275.8 3rd Qu.:126.0 3rd Qu.:3615 7## Max. :46.60 Max. :8.000 Max. :455.0 Max. :230.0 Max. :5140 8## 9## acceleration year origin name 10## Min. : 8.00 Min. :70.00 Min. :1.000 amc matador : 5 11## 1st Qu.:13.78 1st Qu.:73.00 1st Qu.:1.000 ford pinto : 5 12## Median :15.50 Median :76.00 Median :1.000 toyota corolla : 5 13## Mean :15.54 Mean :75.98 Mean :1.577 amc gremlin : 4 14## 3rd Qu.:17.02 3rd Qu.:79.00 3rd Qu.:2.000 amc hornet : 4 15## Max. :24.80 Max. :82.00 Max. :3.000 chevrolet chevette: 4 16## (Other) :365 1str(autoDat) # Directly find find out 1## \u0026#39;data.frame\u0026#39;: 392 obs. of 9 variables: 2## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... 3## $ cylinders : num 8 8 8 8 8 8 8 8 8 8 ... 4## $ displacement: num 307 350 318 304 302 429 454 440 455 390 ... 5## $ horsepower : num 130 165 150 150 140 198 220 215 225 190 ... 6## $ weight : num 3504 3693 3436 3433 3449 ... 7## $ acceleration: num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... 8## $ year : num 70 70 70 70 70 70 70 70 70 70 ... 9## $ origin : num 1 1 1 1 1 1 1 1 1 1 ... 10## $ name : Factor w/ 304 levels \u0026#34;amc ambassador brougham\u0026#34;,..: 49 36 231 14 161 141 54 223 241 2 ... From the above view, we can see that there is only one listed as a qualitative variable or factor, and that is name. However, we can also do this in a cleaner manner or at-least in a different manner with a function.\n1findFactors \u0026lt;- sapply(autoDat,is.factor) 2findFactors 1## mpg cylinders displacement horsepower weight acceleration 2## FALSE FALSE FALSE FALSE FALSE FALSE 3## year origin name 4## FALSE FALSE TRUE Though only name is listed as a qualitative variable, we note that origin seems to be almost qualitative as well.\n1length(unique(autoDat$origin)) 1## [1] 3 1unique(autoDat$origin) 1## [1] 1 3 2 Infact we can check that nothing else has this property by repeated application of sapply, though a pipe would be more satisfying\n1getUniq\u0026lt;-sapply(autoDat, unique) 2getLengths\u0026lt;-sapply(getUniq,length) 3getLengths 1## mpg cylinders displacement horsepower weight acceleration 2## 127 5 81 93 346 95 3## year origin name 4## 13 3 301 This is really nicer with pipes\n1library(dplyr) 1## 2## Attaching package: \u0026#39;dplyr\u0026#39; 1## The following objects are masked from \u0026#39;package:stats\u0026#39;: 2## 3## filter, lag 1## The following objects are masked from \u0026#39;package:base\u0026#39;: 2## 3## intersect, setdiff, setequal, union 1autoDat %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) 1## mpg cylinders displacement horsepower weight acceleration 2## 127 5 81 93 346 95 3## year origin name 4## 13 3 301 At any rate, we know now that origin and name are probably qualitative, and the rest are quantitative.\n(b) Using range()\nA nice feature of the dataset we have is that the suspected qualitative variables are at the end of the dataset. So we can simply select the first \\(7\\) rows and go nuts on them.\n1autoDat[,1:7] %\u0026gt;% sapply(range) # or sapply(autoDat[,1:7],range) 1## mpg cylinders displacement horsepower weight acceleration year 2## [1,] 9.0 3 68 46 1613 8.0 70 3## [2,] 46.6 8 455 230 5140 24.8 82 Once again, more elegant with pipes and subset()\n1autoDat %\u0026gt;% subset(select=-c(name,origin)) %\u0026gt;% sapply(range) 1## mpg cylinders displacement horsepower weight acceleration year 2## [1,] 9.0 3 68 46 1613 8.0 70 3## [2,] 46.6 8 455 230 5140 24.8 82 1# Even simpler with dplyr 2autoDat %\u0026gt;% select(-name,-origin) %\u0026gt;% sapply(range) 1## mpg cylinders displacement horsepower weight acceleration year 2## [1,] 9.0 3 68 46 1613 8.0 70 3## [2,] 46.6 8 455 230 5140 24.8 82 (c) Mean and standard deviation\n1noFactors \u0026lt;- autoDat %\u0026gt;% select(-name,-origin) 2noFactors %\u0026gt;% sapply(mean) 1## mpg cylinders displacement horsepower weight acceleration 2## 23.445918 5.471939 194.411990 104.469388 2977.584184 15.541327 3## year 4## 75.979592 1noFactors %\u0026gt;% sapply(sd) 1## mpg cylinders displacement horsepower weight acceleration 2## 7.805007 1.705783 104.644004 38.491160 849.402560 2.758864 3## year 4## 3.683737 (d) Removing observations 10-85 and testing.\n1noFactors[-(10:85),] %\u0026gt;% sapply(mean) 1## mpg cylinders displacement horsepower weight acceleration 2## 24.404430 5.373418 187.240506 100.721519 2935.971519 15.726899 3## year 4## 77.145570 1noFactors[-(10:85),] %\u0026gt;% sapply(sd) 1## mpg cylinders displacement horsepower weight acceleration 2## 7.867283 1.654179 99.678367 35.708853 811.300208 2.693721 3## year 4## 3.106217 (e) Plots for determining relationships\n1par(mfrow=c(2,2)) 2plot(autoDat$weight, autoDat$horsepower) 3plot(autoDat$weight, autoDat$acceleration) 4plot(autoDat$displacement, autoDat$acceleration) 5plot(autoDat$cylinders, autoDat$acceleration)  Figure 9: Relationship determination\n   Evidently horsepower is directly proportional to weight but acceleration is inversely proportional to weight Acceleration is also inversely proportional to displacement Cylinders are a poor measure, not surprising since there are only \\(5\\) values  (f) Choosing predictors for gas mileage mpg\nLet us recall certain key elements of the quantitative aspects of the dataset.\n1summary(noFactors) # To understand the spread 1## mpg cylinders displacement horsepower weight 2## Min. : 9.00 Min. :3.000 Min. : 68.0 Min. : 46.0 Min. :1613 3## 1st Qu.:17.00 1st Qu.:4.000 1st Qu.:105.0 1st Qu.: 75.0 1st Qu.:2225 4## Median :22.75 Median :4.000 Median :151.0 Median : 93.5 Median :2804 5## Mean :23.45 Mean :5.472 Mean :194.4 Mean :104.5 Mean :2978 6## 3rd Qu.:29.00 3rd Qu.:8.000 3rd Qu.:275.8 3rd Qu.:126.0 3rd Qu.:3615 7## Max. :46.60 Max. :8.000 Max. :455.0 Max. :230.0 Max. :5140 8## acceleration year 9## Min. : 8.00 Min. :70.00 10## 1st Qu.:13.78 1st Qu.:73.00 11## Median :15.50 Median :76.00 12## Mean :15.54 Mean :75.98 13## 3rd Qu.:17.02 3rd Qu.:79.00 14## Max. :24.80 Max. :82.00 1getLengths # To get the number of unique values 1## mpg cylinders displacement horsepower weight acceleration 2## 127 5 81 93 346 95 3## year origin name 4## 13 3 301 From this we can assert easily that the number of cylinders is not of much interest for predictions of the mileage.\n1par(mfrow=c(3,2)) 2plot(noFactors$mpg,noFactors$horsepower) 3plot(noFactors$mpg,noFactors$weight) 4plot(noFactors$mpg,noFactors$displacement) 5plot(noFactors$mpg,noFactors$acceleration) 6plot(noFactors$mpg,noFactors$year)  Figure 10: Predictions\n   So now we know that the mileage increases when horsepower is low, weight is low, displacement is low and acceleration is high  Where low represents an inverse response and high represents a direct response.\n It is also clear that the mileage increases every year  Chapter III - Linear Regression Question 3.9 - Page 122 This question involves the use of multiple linear regression on the Auto data set.\n(a) Produce a scatterplot matrix which includes all of the variables in the data set.\n(b) Compute the matrix of correlations between the variables using the function cor() . You will need to exclude the name variable, cor() which is qualitative.\n(c) Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output. For instance:\n Is there a relationship between the predictors and the response? Which predictors appear to have a statistically significant relationship to the response? What does the coefficient for the year variable suggest?  (d) Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?\n(e) Use the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?\n(f) Try a few different transformations of the variables, such as \\(\\log{X}\\), \\(\\sqrt{X}\\), \\(X^2\\).Comment on your ﬁndings.\nAnswer Once again, we will use the dataset from the library.\n1cleanAuto \u0026lt;- na.omit(autoDat) 2summary(cleanAuto) # Already created above, so no need to do na.omit again 1## mpg cylinders displacement horsepower weight 2## Min. : 9.00 Min. :3.000 Min. : 68.0 Min. : 46.0 Min. :1613 3## 1st Qu.:17.00 1st Qu.:4.000 1st Qu.:105.0 1st Qu.: 75.0 1st Qu.:2225 4## Median :22.75 Median :4.000 Median :151.0 Median : 93.5 Median :2804 5## Mean :23.45 Mean :5.472 Mean :194.4 Mean :104.5 Mean :2978 6## 3rd Qu.:29.00 3rd Qu.:8.000 3rd Qu.:275.8 3rd Qu.:126.0 3rd Qu.:3615 7## Max. :46.60 Max. :8.000 Max. :455.0 Max. :230.0 Max. :5140 8## 9## acceleration year origin name 10## Min. : 8.00 Min. :70.00 Min. :1.000 amc matador : 5 11## 1st Qu.:13.78 1st Qu.:73.00 1st Qu.:1.000 ford pinto : 5 12## Median :15.50 Median :76.00 Median :1.000 toyota corolla : 5 13## Mean :15.54 Mean :75.98 Mean :1.577 amc gremlin : 4 14## 3rd Qu.:17.02 3rd Qu.:79.00 3rd Qu.:2.000 amc hornet : 4 15## Max. :24.80 Max. :82.00 Max. :3.000 chevrolet chevette: 4 16## (Other) :365 (a) Scatterplot\n1pairs(cleanAuto)  Figure 11: Scatterplot\n  (b) Correlation matrix. For this we exclude the qualitative variables either by using select or by using the existing noFactors dataset\n1# A full set 2ISLR::Auto %\u0026gt;% na.omit %\u0026gt;% select(-name,-origin) %\u0026gt;% cor 1## mpg cylinders displacement horsepower weight 2## mpg 1.0000000 -0.7776175 -0.8051269 -0.7784268 -0.8322442 3## cylinders -0.7776175 1.0000000 0.9508233 0.8429834 0.8975273 4## displacement -0.8051269 0.9508233 1.0000000 0.8972570 0.9329944 5## horsepower -0.7784268 0.8429834 0.8972570 1.0000000 0.8645377 6## weight -0.8322442 0.8975273 0.9329944 0.8645377 1.0000000 7## acceleration 0.4233285 -0.5046834 -0.5438005 -0.6891955 -0.4168392 8## year 0.5805410 -0.3456474 -0.3698552 -0.4163615 -0.3091199 9## acceleration year 10## mpg 0.4233285 0.5805410 11## cylinders -0.5046834 -0.3456474 12## displacement -0.5438005 -0.3698552 13## horsepower -0.6891955 -0.4163615 14## weight -0.4168392 -0.3091199 15## acceleration 1.0000000 0.2903161 16## year 0.2903161 1.0000000 (c) Multiple Linear Regression\n1# Fit against every variable 2lm.fit=lm(mpg~.,data=noFactors) 3summary(lm.fit) 1## 2## Call: 3## lm(formula = mpg ~ ., data = noFactors) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -8.6927 -2.3864 -0.0801 2.0291 14.3607 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) -1.454e+01 4.764e+00 -3.051 0.00244 ** 12## cylinders -3.299e-01 3.321e-01 -0.993 0.32122 13## displacement 7.678e-03 7.358e-03 1.044 0.29733 14## horsepower -3.914e-04 1.384e-02 -0.028 0.97745 15## weight -6.795e-03 6.700e-04 -10.141 \u0026lt; 2e-16 *** 16## acceleration 8.527e-02 1.020e-01 0.836 0.40383 17## year 7.534e-01 5.262e-02 14.318 \u0026lt; 2e-16 *** 18## --- 19## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 20## 21## Residual standard error: 3.435 on 385 degrees of freedom 22## Multiple R-squared: 0.8093, Adjusted R-squared: 0.8063 23## F-statistic: 272.2 on 6 and 385 DF, p-value: \u0026lt; 2.2e-16 1# Fit against one variable 2noFactors %\u0026gt;% lm(mpg~horsepower,data=.) %\u0026gt;% summary 1## 2## Call: 3## lm(formula = mpg ~ horsepower, data = .) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -13.5710 -3.2592 -0.3435 2.7630 16.9240 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 39.935861 0.717499 55.66 \u0026lt;2e-16 *** 12## horsepower -0.157845 0.006446 -24.49 \u0026lt;2e-16 *** 13## --- 14## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 15## 16## Residual standard error: 4.906 on 390 degrees of freedom 17## Multiple R-squared: 0.6059, Adjusted R-squared: 0.6049 18## F-statistic: 599.7 on 1 and 390 DF, p-value: \u0026lt; 2.2e-16 1noFactors %\u0026gt;% lm(mpg~year,data=.) %\u0026gt;% summary 1## 2## Call: 3## lm(formula = mpg ~ year, data = .) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -12.0212 -5.4411 -0.4412 4.9739 18.2088 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) -70.01167 6.64516 -10.54 \u0026lt;2e-16 *** 12## year 1.23004 0.08736 14.08 \u0026lt;2e-16 *** 13## --- 14## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 15## 16## Residual standard error: 6.363 on 390 degrees of freedom 17## Multiple R-squared: 0.337, Adjusted R-squared: 0.3353 18## F-statistic: 198.3 on 1 and 390 DF, p-value: \u0026lt; 2.2e-16 1noFactors %\u0026gt;% lm(mpg~acceleration,data=.) %\u0026gt;% summary 1## 2## Call: 3## lm(formula = mpg ~ acceleration, data = .) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -17.989 -5.616 -1.199 4.801 23.239 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 4.8332 2.0485 2.359 0.0188 * 12## acceleration 1.1976 0.1298 9.228 \u0026lt;2e-16 *** 13## --- 14## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 15## 16## Residual standard error: 7.08 on 390 degrees of freedom 17## Multiple R-squared: 0.1792, Adjusted R-squared: 0.1771 18## F-statistic: 85.15 on 1 and 390 DF, p-value: \u0026lt; 2.2e-16 1noFactors %\u0026gt;% lm(mpg~weight,data=.) %\u0026gt;% summary 1## 2## Call: 3## lm(formula = mpg ~ weight, data = .) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -11.9736 -2.7556 -0.3358 2.1379 16.5194 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 46.216524 0.798673 57.87 \u0026lt;2e-16 *** 12## weight -0.007647 0.000258 -29.64 \u0026lt;2e-16 *** 13## --- 14## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 15## 16## Residual standard error: 4.333 on 390 degrees of freedom 17## Multiple R-squared: 0.6926, Adjusted R-squared: 0.6918 18## F-statistic: 878.8 on 1 and 390 DF, p-value: \u0026lt; 2.2e-16 1noFactors %\u0026gt;% lm(mpg~displacement,data=.) %\u0026gt;% summary 1## 2## Call: 3## lm(formula = mpg ~ displacement, data = .) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -12.9170 -3.0243 -0.5021 2.3512 18.6128 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 35.12064 0.49443 71.03 \u0026lt;2e-16 *** 12## displacement -0.06005 0.00224 -26.81 \u0026lt;2e-16 *** 13## --- 14## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 15## 16## Residual standard error: 4.635 on 390 degrees of freedom 17## Multiple R-squared: 0.6482, Adjusted R-squared: 0.6473 18## F-statistic: 718.7 on 1 and 390 DF, p-value: \u0026lt; 2.2e-16   Clearly there is a relationship between the predictors and variables, mostly as described previously, with the following broad trends:\n Inversely proportional to Horsepower, Weight, and Displacement    The predictors which have a relationship to the response are (based on R squared values): \\[ all \u0026gt; weight \u0026gt; displacement \u0026gt; horsepower \u0026gt; year \u0026gt; acceleration \\] However, things lower than horsepower are not statistically significant.\n  The visual analysis of the year variable suggests that the mileage grows every year. However, it is clear from the summary, that there is no statistical significance of year when used to fit a single parameter linear model. We note that when we compare this to the multiple linear regression analysis, we see that the year factor accounts for \\(0.7508\\) of the total, that is, the cars become more efficient every year\n  (d) Lets plot these\n1par(mfrow=c(2,2)) 2noFactors %\u0026gt;% lm(mpg~horsepower,data=.) %\u0026gt;% plot(main=\u0026#34;Mileage v/s Horsepower\u0026#34;)  1noFactors %\u0026gt;% lm(mpg~weight,data=.) %\u0026gt;% plot(main=\u0026#34;Mileage v/s Weight\u0026#34;)  1noFactors %\u0026gt;% lm(mpg~year,data=.) %\u0026gt;% plot(main=\u0026#34;Mileage v/s Year\u0026#34;)  1noFactors %\u0026gt;% lm(mpg~acceleration,data=.) %\u0026gt;% plot(main=\u0026#34;Mileage v/s Acceleration\u0026#34;)  1noFactors %\u0026gt;% lm(mpg~displacement,data=.) %\u0026gt;% plot(main=\u0026#34;Mileage v/s Displacement\u0026#34;)  1noFactors %\u0026gt;% lm(mpg~.,data=.) %\u0026gt;% plot(main=\u0026#34;Mileage Multiple Regression\u0026#34;)  Form this we can see that the fit is not very accurate as there is a clear curve to the residuals. The 14th point has high leverage, though it is of a small magnitude. Thus it is not expected to have affected the plot too much.\nWe know that an observation with a studentized residual greater than \\(3\\) in absolute value are possible outliers. Hence we must plot this.\n1# Predict and get the plot 2fitPlot \u0026lt;- noFactors %\u0026gt;% lm(mpg~.,data=.) 3# See residuals 4plot(xlab=\u0026#34;Prediction\u0026#34;,ylab=\u0026#34;Studentized Residual\u0026#34;,x=predict(fitPlot),y=rstudent(fitPlot))  1# Try a linear fit of studentized residuals 2par(mfrow=c(2,2)) 3plot(lm(predict(fitPlot)~rstudent(fitPlot)))  Clearly the studentized residuals are nonlinear w.r.t the prediction. Also, some points are above the absolute value of \\(3\\) so they might be outliers, in keeping with the leverage plot.\n(e) Interaction effects\nWe recall that x*y corresponds to x+y+x:y\n1# View the correlation matrix 2cleanAuto %\u0026gt;% select(-name,-origin) %\u0026gt;% cor 1## mpg cylinders displacement horsepower weight 2## mpg 1.0000000 -0.7776175 -0.8051269 -0.7784268 -0.8322442 3## cylinders -0.7776175 1.0000000 0.9508233 0.8429834 0.8975273 4## displacement -0.8051269 0.9508233 1.0000000 0.8972570 0.9329944 5## horsepower -0.7784268 0.8429834 0.8972570 1.0000000 0.8645377 6## weight -0.8322442 0.8975273 0.9329944 0.8645377 1.0000000 7## acceleration 0.4233285 -0.5046834 -0.5438005 -0.6891955 -0.4168392 8## year 0.5805410 -0.3456474 -0.3698552 -0.4163615 -0.3091199 9## acceleration year 10## mpg 0.4233285 0.5805410 11## cylinders -0.5046834 -0.3456474 12## displacement -0.5438005 -0.3698552 13## horsepower -0.6891955 -0.4163615 14## weight -0.4168392 -0.3091199 15## acceleration 1.0000000 0.2903161 16## year 0.2903161 1.0000000 1summary(lm(mpg~weight*displacement*year,data=noFactors[(10:85),])) 1## 2## Call: 3## lm(formula = mpg ~ weight * displacement * year, data = noFactors[(10:85), 4## ]) 5## 6## Residuals: 7## Min 1Q Median 3Q Max 8## -5.3020 -0.9055 0.0966 0.8912 3.7049 9## 10## Coefficients: 11## Estimate Std. Error t value Pr(\u0026gt;|t|) 12## (Intercept) 3.961e+02 2.578e+02 1.537 0.129 13## weight -1.030e-01 1.008e-01 -1.021 0.311 14## displacement -1.587e+00 1.308e+00 -1.213 0.229 15## year -4.889e+00 3.623e+00 -1.349 0.182 16## weight:displacement 3.926e-04 3.734e-04 1.051 0.297 17## weight:year 1.317e-03 1.418e-03 0.929 0.356 18## displacement:year 2.150e-02 1.846e-02 1.165 0.248 19## weight:displacement:year -5.287e-06 5.253e-06 -1.007 0.318 20## 21## Residual standard error: 1.8 on 68 degrees of freedom 22## Multiple R-squared: 0.922, Adjusted R-squared: 0.914 23## F-statistic: 114.9 on 7 and 68 DF, p-value: \u0026lt; 2.2e-16 1summary(lm(mpg~weight*displacement*year,data=noFactors)) 1## 2## Call: 3## lm(formula = mpg ~ weight * displacement * year, data = noFactors) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -9.6093 -1.6472 -0.0531 1.2289 14.5604 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) -8.437e+01 3.128e+01 -2.697 0.0073 ** 12## weight 8.489e-03 1.322e-02 0.642 0.5212 13## displacement 3.434e-01 1.969e-01 1.744 0.0820 . 14## year 1.828e+00 4.127e-01 4.430 1.23e-05 *** 15## weight:displacement -6.589e-05 5.055e-05 -1.303 0.1932 16## weight:year -2.433e-04 1.744e-04 -1.395 0.1638 17## displacement:year -5.566e-03 2.674e-03 -2.082 0.0380 * 18## weight:displacement:year 1.144e-06 6.823e-07 1.677 0.0944 . 19## --- 20## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 21## 22## Residual standard error: 2.951 on 384 degrees of freedom 23## Multiple R-squared: 0.8596, Adjusted R-squared: 0.8571 24## F-statistic: 336 on 7 and 384 DF, p-value: \u0026lt; 2.2e-16  Adding the interaction effects of the \\(3\\) most positive R value terms improves the existing prediction to be better than that obtained by considering all effects. We note that the best model is obtained by removing the range identified in chapter 2.  (f) Nonlinear transformations\n1summary(lm(mpg~weight*displacement*year+I(year^2),data=noFactors[(10:85),])) 1## 2## Call: 3## lm(formula = mpg ~ weight * displacement * year + I(year^2), 4## data = noFactors[(10:85), ]) 5## 6## Residuals: 7## Min 1Q Median 3Q Max 8## -5.1815 -0.8235 0.0144 1.0076 3.9420 9## 10## Coefficients: 11## Estimate Std. Error t value Pr(\u0026gt;|t|) 12## (Intercept) -4.205e+03 1.810e+03 -2.324 0.0232 * 13## weight -8.800e-02 9.709e-02 -0.906 0.3680 14## displacement -1.030e+00 1.276e+00 -0.807 0.4225 15## year 1.238e+02 5.026e+01 2.464 0.0163 * 16## I(year^2) -9.000e-01 3.506e-01 -2.567 0.0125 * 17## weight:displacement 2.471e-04 3.634e-04 0.680 0.4988 18## weight:year 1.113e-03 1.365e-03 0.815 0.4177 19## displacement:year 1.368e-02 1.800e-02 0.760 0.4501 20## weight:displacement:year -3.254e-06 5.111e-06 -0.637 0.5264 21## --- 22## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 23## 24## Residual standard error: 1.73 on 67 degrees of freedom 25## Multiple R-squared: 0.929, Adjusted R-squared: 0.9205 26## F-statistic: 109.6 on 8 and 67 DF, p-value: \u0026lt; 2.2e-16 1summary(lm(mpg~.-I(log(acceleration^2)),data=noFactors[(10:85),])) 1## 2## Call: 3## lm(formula = mpg ~ . - I(log(acceleration^2)), data = noFactors[(10:85), 4## ]) 5## 6## Residuals: 7## Min 1Q Median 3Q Max 8## -6.232 -1.470 -0.211 1.075 7.088 9## 10## Coefficients: 11## Estimate Std. Error t value Pr(\u0026gt;|t|) 12## (Intercept) 41.3787633 24.1208720 1.715 0.0907 . 13## cylinders 0.0863161 0.6112822 0.141 0.8881 14## displacement -0.0148491 0.0103249 -1.438 0.1549 15## horsepower -0.0158500 0.0151259 -1.048 0.2984 16## weight -0.0039125 0.0008546 -4.578 2.02e-05 *** 17## acceleration -0.1473786 0.1438220 -1.025 0.3091 18## year -0.0378187 0.3380266 -0.112 0.9112 19## --- 20## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 21## 22## Residual standard error: 2.262 on 69 degrees of freedom 23## Multiple R-squared: 0.8751, Adjusted R-squared: 0.8642 24## F-statistic: 80.55 on 6 and 69 DF, p-value: \u0026lt; 2.2e-16   The best model I found was still the one without the non-linear transformation but with removed outliers and additional interaction effects of displacement,=year= and weight\n  A popular approach is to use a log transform for both the inputs and the outputs\n   1summary(lm(log(mpg)~.,data=noFactors[(10:85),])) 1## 2## Call: 3## lm(formula = log(mpg) ~ ., data = noFactors[(10:85), ]) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -0.285805 -0.052358 -0.001456 0.066521 0.209739 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 3.886e+00 1.028e+00 3.781 0.000328 *** 12## cylinders -1.771e-02 2.604e-02 -0.680 0.498669 13## displacement -1.540e-04 4.399e-04 -0.350 0.727314 14## horsepower -2.343e-03 6.444e-04 -3.636 0.000529 *** 15## weight -1.960e-04 3.641e-05 -5.383 9.51e-07 *** 16## acceleration -1.525e-02 6.128e-03 -2.489 0.015224 * 17## year 4.138e-03 1.440e-02 0.287 0.774703 18## --- 19## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 20## 21## Residual standard error: 0.09636 on 69 degrees of freedom 22## Multiple R-squared: 0.919, Adjusted R-squared: 0.912 23## F-statistic: 130.5 on 6 and 69 DF, p-value: \u0026lt; 2.2e-16 1summary(lm(log(mpg)~log(weight*displacement*year),data=noFactors[(10:85),])) 1## 2## Call: 3## lm(formula = log(mpg) ~ log(weight * displacement * year), data = noFactors[(10:85), 4## ]) 5## 6## Residuals: 7## Min 1Q Median 3Q Max 8## -0.41121 -0.04107 0.01266 0.07791 0.21056 9## 10## Coefficients: 11## Estimate Std. Error t value Pr(\u0026gt;|t|) 12## (Intercept) 8.91995 0.26467 33.70 \u0026lt;2e-16 *** 13## log(weight * displacement * year) -0.34250 0.01508 -22.71 \u0026lt;2e-16 *** 14## --- 15## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 16## 17## Residual standard error: 0.1158 on 74 degrees of freedom 18## Multiple R-squared: 0.8745, Adjusted R-squared: 0.8728 19## F-statistic: 515.6 on 1 and 74 DF, p-value: \u0026lt; 2.2e-16 Question 3.10 - Page 123 This question should be answered using the Carseats data set.\n(a) Fit a multiple regression model to predict Sales using Price, Urban, and US.\n(b) Provide an interpretation of each coefficient in the model. Be careful\u0026mdash;some of the variables in the model are qualitative!\n(c) Write out the model in equation form, being careful to handle the qualitative variables properly.\n(d) For which of the predictors can you reject the null hypothesis \\(H_0:\\beta_j=0\\)?\n(e) On the basis of your response to the previous question, ﬁt a smaller model that only uses the predictors for which there is evidence of association with the outcome.\n(f) How well do the models in (a) and (e) fit the data?\n(g) Using the model from (e), obtain \\(95%\\) confidence intervals for the coefficient(s).\n(h) Is there evidence of outliers or high leverage observations in the model from (e)?\nAnswer Load the dataset (and clean it)\n1cleanCarSeats \u0026lt;- na.omit(ISLR::Carseats) Obtain summary statistics\n1cleanCarSeats %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) 1## Sales CompPrice Income Advertising Population Price 2## 336 73 98 28 275 101 3## ShelveLoc Age Education Urban US 4## 3 56 9 2 2 1str(cleanCarSeats) 1## \u0026#39;data.frame\u0026#39;: 400 obs. of 11 variables: 2## $ Sales : num 9.5 11.22 10.06 7.4 4.15 ... 3## $ CompPrice : num 138 111 113 117 141 124 115 136 132 132 ... 4## $ Income : num 73 48 35 100 64 113 105 81 110 113 ... 5## $ Advertising: num 11 16 10 4 3 13 0 15 0 0 ... 6## $ Population : num 276 260 269 466 340 501 45 425 108 131 ... 7## $ Price : num 120 83 80 97 128 72 108 120 124 124 ... 8## $ ShelveLoc : Factor w/ 3 levels \u0026#34;Bad\u0026#34;,\u0026#34;Good\u0026#34;,\u0026#34;Medium\u0026#34;: 1 2 3 3 1 1 3 2 3 3 ... 9## $ Age : num 42 65 59 55 38 78 71 67 76 76 ... 10## $ Education : num 17 10 12 14 13 16 15 10 10 17 ... 11## $ Urban : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 2 2 2 1 2 2 1 1 ... 12## $ US : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 2 2 1 2 1 2 1 2 ... 1summary(cleanCarSeats) 1## Sales CompPrice Income Advertising 2## Min. : 0.000 Min. : 77 Min. : 21.00 Min. : 0.000 3## 1st Qu.: 5.390 1st Qu.:115 1st Qu.: 42.75 1st Qu.: 0.000 4## Median : 7.490 Median :125 Median : 69.00 Median : 5.000 5## Mean : 7.496 Mean :125 Mean : 68.66 Mean : 6.635 6## 3rd Qu.: 9.320 3rd Qu.:135 3rd Qu.: 91.00 3rd Qu.:12.000 7## Max. :16.270 Max. :175 Max. :120.00 Max. :29.000 8## Population Price ShelveLoc Age Education 9## Min. : 10.0 Min. : 24.0 Bad : 96 Min. :25.00 Min. :10.0 10## 1st Qu.:139.0 1st Qu.:100.0 Good : 85 1st Qu.:39.75 1st Qu.:12.0 11## Median :272.0 Median :117.0 Medium:219 Median :54.50 Median :14.0 12## Mean :264.8 Mean :115.8 Mean :53.32 Mean :13.9 13## 3rd Qu.:398.5 3rd Qu.:131.0 3rd Qu.:66.00 3rd Qu.:16.0 14## Max. :509.0 Max. :191.0 Max. :80.00 Max. :18.0 15## Urban US 16## No :118 No :142 17## Yes:282 Yes:258 18## 19## 20## 21## We can see that:\n Urban, US and ShelveLoc are factors with 2,2 and 3 levels respectively Education has only 9 unique values so we might as well consider it to be a factor too if we need to  (a) Multiple Regression Model\nFit it to things\n1summary(lm(Sales~.,data=cleanCarSeats)) 1## 2## Call: 3## lm(formula = Sales ~ ., data = cleanCarSeats) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -2.8692 -0.6908 0.0211 0.6636 3.4115 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 5.6606231 0.6034487 9.380 \u0026lt; 2e-16 *** 12## CompPrice 0.0928153 0.0041477 22.378 \u0026lt; 2e-16 *** 13## Income 0.0158028 0.0018451 8.565 2.58e-16 *** 14## Advertising 0.1230951 0.0111237 11.066 \u0026lt; 2e-16 *** 15## Population 0.0002079 0.0003705 0.561 0.575 16## Price -0.0953579 0.0026711 -35.700 \u0026lt; 2e-16 *** 17## ShelveLocGood 4.8501827 0.1531100 31.678 \u0026lt; 2e-16 *** 18## ShelveLocMedium 1.9567148 0.1261056 15.516 \u0026lt; 2e-16 *** 19## Age -0.0460452 0.0031817 -14.472 \u0026lt; 2e-16 *** 20## Education -0.0211018 0.0197205 -1.070 0.285 21## UrbanYes 0.1228864 0.1129761 1.088 0.277 22## USYes -0.1840928 0.1498423 -1.229 0.220 23## --- 24## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 25## 26## Residual standard error: 1.019 on 388 degrees of freedom 27## Multiple R-squared: 0.8734, Adjusted R-squared: 0.8698 28## F-statistic: 243.4 on 11 and 388 DF, p-value: \u0026lt; 2.2e-16 1summary(lm(Sales~US*Price*Urban,data=cleanCarSeats)) 1## 2## Call: 3## lm(formula = Sales ~ US * Price * Urban, data = cleanCarSeats) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -6.7952 -1.6659 -0.0984 1.6119 7.2433 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 13.456350 1.727210 7.791 6.03e-14 *** 12## USYes 2.049051 2.322591 0.882 0.378 13## Price -0.061657 0.014875 -4.145 4.17e-05 *** 14## UrbanYes -0.651545 2.071401 -0.315 0.753 15## USYes:Price -0.001567 0.019972 -0.078 0.937 16## USYes:UrbanYes -1.122034 2.759662 -0.407 0.685 17## Price:UrbanYes 0.010793 0.017796 0.606 0.545 18## USYes:Price:UrbanYes 0.001288 0.023619 0.055 0.957 19## --- 20## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 21## 22## Residual standard error: 2.473 on 392 degrees of freedom 23## Multiple R-squared: 0.2467, Adjusted R-squared: 0.2333 24## F-statistic: 18.34 on 7 and 392 DF, p-value: \u0026lt; 2.2e-16 1summary(lm(Sales~US+Price+Urban,data=cleanCarSeats)) 1## 2## Call: 3## lm(formula = Sales ~ US + Price + Urban, data = cleanCarSeats) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -6.9206 -1.6220 -0.0564 1.5786 7.0581 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 13.043469 0.651012 20.036 \u0026lt; 2e-16 *** 12## USYes 1.200573 0.259042 4.635 4.86e-06 *** 13## Price -0.054459 0.005242 -10.389 \u0026lt; 2e-16 *** 14## UrbanYes -0.021916 0.271650 -0.081 0.936 15## --- 16## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 17## 18## Residual standard error: 2.472 on 396 degrees of freedom 19## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2335 20## F-statistic: 41.52 on 3 and 396 DF, p-value: \u0026lt; 2.2e-16 (b) Interpret stuff\nTo interpret the data, we need to determine which of the models fits the data best, we will use anova() to test this:\n1lmCarSAll\u0026lt;-lm(Sales~.,data=cleanCarSeats) 2lmCarStimesPUU\u0026lt;-lm(Sales~US*Price*Urban,data=cleanCarSeats) 3lmCarSplusPUU\u0026lt;-lm(Sales~US+Price+Urban,data=cleanCarSeats) 4anova(lmCarSAll,lmCarStimesPUU,lmCarSplusPUU) 1## Analysis of Variance Table 2## 3## Model 1: Sales ~ CompPrice + Income + Advertising + Population + Price + 4## ShelveLoc + Age + Education + Urban + US 5## Model 2: Sales ~ US * Price * Urban 6## Model 3: Sales ~ US + Price + Urban 7## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) 8## 1 388 402.83 9## 2 392 2397.10 -4 -1994.27 480.2082 \u0026lt; 2.2e-16 *** 10## 3 396 2420.83 -4 -23.73 5.7149 0.0001772 *** 11## --- 12## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 1anova(lmCarStimesPUU,lmCarSplusPUU) 1## Analysis of Variance Table 2## 3## Model 1: Sales ~ US * Price * Urban 4## Model 2: Sales ~ US + Price + Urban 5## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) 6## 1 392 2397.1 7## 2 396 2420.8 -4 -23.734 0.9703 0.4236 Remember that it is not possible to use anova() unless the same variables are present in all the models being tested, so it is meaningless to use anova for lmCarSAll along with the others, because we can\u0026rsquo;t change the interaction model to get only the main effects.\n We note that due to the low value of the F-statistic and the non-zero value of the p-value we cannot disregard the null hypothesis, or in other words, the models are basically the same in terms of their performance.  This means that I would like to continue with the simpler model, since the increase in R squared is too small to account for dealing with the additional factors.\n We see immediately, that there is a positive correlation only with being in the US Increases in price and being in an urban area actually decrease the sales, which is not surprising since being in the an urban area is probably correlated to a higher price, which we can check immediately   1summary(lm(Price~Urban,data=cleanCarSeats)) 1## 2## Call: 3## lm(formula = Price ~ Urban, data = cleanCarSeats) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -92.514 -15.514 1.205 14.595 74.486 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 114.076 2.180 52.330 \u0026lt;2e-16 *** 12## UrbanYes 2.438 2.596 0.939 0.348 13## --- 14## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 15## 16## Residual standard error: 23.68 on 398 degrees of freedom 17## Multiple R-squared: 0.002211, Adjusted R-squared: -0.0002965 18## F-statistic: 0.8817 on 1 and 398 DF, p-value: 0.3483 We see that our assumption is validated. Being in an urban area has a low t-statistic for a positive increase on the slope\n Returning to our previous model, we note that there is a high value of the p-value of the t-statistic for Urban being true, this means there isn\u0026rsquo;t a real relationship between being in an urban area and the sales. This makes intuitive sense as well  note t-test is essentially a linear model with one variable, that is, if we want to find out if there is a relation between having a store in an urban area, we could sum all the urban yes and divide by the number of observations and compare that to the sum of all the urban no divided by the number of observations which is essentially the t-test again.\n Price is significant, and has an inverse relation with the sales, so we should keep that in mind  (c) In Equation Form:\n\\[ Sales=1.200573*USYes - 0.054459*Price - 0.021916*UrbanYes + 13.043469 \\]\n(e) Other models\n We know from our case-study on testing the full multiple linear regression for Sales that there are definitely more important variables being ignored. However, we also know that Urban is not significant, so we can use a smaller model.   1lmCarSplusPU\u0026lt;-lm(Sales~US+Price, data=cleanCarSeats) (f) Comparison of models\n1summary(lmCarSplusPU) 1## 2## Call: 3## lm(formula = Sales ~ US + Price, data = cleanCarSeats) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -6.9269 -1.6286 -0.0574 1.5766 7.0515 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 13.03079 0.63098 20.652 \u0026lt; 2e-16 *** 12## USYes 1.19964 0.25846 4.641 4.71e-06 *** 13## Price -0.05448 0.00523 -10.416 \u0026lt; 2e-16 *** 14## --- 15## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 16## 17## Residual standard error: 2.469 on 397 degrees of freedom 18## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2354 19## F-statistic: 62.43 on 2 and 397 DF, p-value: \u0026lt; 2.2e-16 1anova(lmCarSplusPUU,lmCarSplusPU) 1## Analysis of Variance Table 2## 3## Model 1: Sales ~ US + Price + Urban 4## Model 2: Sales ~ US + Price 5## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) 6## 1 396 2420.8 7## 2 397 2420.9 -1 -0.03979 0.0065 0.9357 As expected, the low value of the F statistic and the high p-value for the anova() test asserts that the null hypothesis cannot be neglected, thus there are no differences between the model with the insignificant parameter, which is also seen in the R squared value, which is the same for both models\n(g) Confidence Intervals\n1confint(lmCarSplusPU) 1## 2.5 % 97.5 % 2## (Intercept) 11.79032020 14.27126531 3## USYes 0.69151957 1.70776632 4## Price -0.06475984 -0.04419543 1confint(lmCarSplusPUU) 1## 2.5 % 97.5 % 2## (Intercept) 11.76359670 14.32334118 3## USYes 0.69130419 1.70984121 4## Price -0.06476419 -0.04415351 5## UrbanYes -0.55597316 0.51214085  ☐ Look into trying to plot this with ggplot  (h) Outliers\n We will first check the leverage plots   1par(mfrow=c(2,2)) 2plot(lmCarSplusPU)  Figure 12: Leverage Plots\n  We can see there is a point with high leverage, but it has a low residual. In any case we should check further.\n Now we will check the studentized residuals to see if they are greater than 3   1# See residuals 2plot(xlab=\u0026#34;Prediction\u0026#34;,ylab=\u0026#34;Studentized Residual\u0026#34;,x=predict(lmCarSplusPU),y=rstudent(lmCarSplusPU))  Figure 13: Studentized residuals\n  Thus I would say there are no outliers in our dataset, as none of our datapoints have an absolute studentized residual above 3.\n  James, G., Witten, D., Hastie, T., \u0026amp; Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Berlin, Germany: Springer Science \u0026amp; Business Media.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/islr-ch2-ch3/","tags":["solutions","R","ISLR"],"title":"ISLR :: Multiple Linear Regression"},{"categories":null,"contents":"Hi.\nI\u0026rsquo;m Rohit Goswami, better known across the web as HaoZeke. I\u0026rsquo;m not the first of my name, which is why instead of rgoswami, I occasionally use rg0swami when I need to be identified by something closer to my name.\nThe actual username is a throwback to back when people liked being anonymous (and with multiple personalities) online, so that ought to give an idea of how old I am. A curriculum vitae is available here.\nIt is difficult to keep this section short and not let it spill into an unstructured memoir. For a while I considered trying to consolidate my online presences but that turned out to be completely impossible without a series of posts and avatars1. I did however eventually set up a sporadically updated collection of web-links involving me. There is also, a separate list of quotes, or other things I have something to say about.\nTangible Positions  As of January 2021, I have been awarded a three year doctoral fellowship from the Icelandic Research Fund (Rannís) for my project on \u0026ldquo;Magnetic interactions of itinerant electrons modeled using Bayesian machine learning\u0026rdquo; tenable at the Science Institute with Prof. Hannes Jónsson as my supervisor and Prof. Birgir Hrafnkelsson as my co-supervisor As of August 2021, I have joined Quansight Labs as a Software Engineer reporting to (Dr. Ralf Gommers \u0026ndash;\u0026gt; Dr. Melissa Weber Mendonça)  My work initially revolved around f2py through the CZI NumPy project with Dr. Melissa Weber Mendonça and Dr. Pearu Peterson    Intangible Positions This is a set of things which are primarily online and/or voluntary in a non-academic sense.\n I maintain within NumPy, mostly parts of f2py I serve on the board of the Nordic Research Software Engineers association I administer and design a bunch of websites, mostly verified on Keybase I am a certified Software Carpentries instructor I officially maintain, for the Software Carpentries, the lesson on R (r-novice-inflammation) I also maintain some packages on the AUR (ArchLinux User Repository) I hone coursework development and teaching with univ.ai I maintain(ed) the official LineageOS image for the Xperia Z5 Dual  Historical Connections What follows is a more informal set of places I am or have been associated with or are of significance to me2. Many of these are tangible, but better ordered in this manner.\nReykjavík  I collaborated closely with the brilliant Dr. Ondřej Čertík over the course of both on my GSoD 2020 long project for SymEngine and over GSoC 2021 on the LFortran compiler I was a Fellow at the Institute for Pure and Applied Mathematics (IPAM) for its Spring 2021 program on “Tensor Methods and Emerging Applications to the Physical and Data Sciences” I am associated with the reputed Jonsson group of the Science Institute at the University of Iceland, where I benefit from the guidance of the erudite and inspiring Prof. Hannes Jonsson My doctoral committee is here, which includes the very excellent inputs of Dr. Elvar Jonsson I have also benefited from sitting in on some formal coursework here, which has been a fascinatingly useful experience  Kanpur  I retain a close association with the fantastic Femtolab at IIT Kanpur under Prof. Debabrata Goswami, who has provided constant guidance throughout my career I am the co-lead developer of the FOSS scientific d-SEAMS software suite for graph theoretic approaches to structure determination of molecular dynamics simulations, along with my exceptional co-lead Amrita Goswami of the CNS Lab under Prof. Jayant K. Singh at IITK I worked with the Nair group as part of the Summer Undergraduate in Research Excellence (SURGE) program, also at IITK Harcourt Butler Technical Institute (HBTI) Kanpur, or the Harcourt Butler Technological University, as it is now called, was where I trained to be a chemical engineer  Bombay  I spent a formative summer under Prof. Rajarshi Chakrabarti of the IIT Bombay Chemistry department, who has been instrumental in developing my interests I also spent some time discussing experiments with Prof. Rajdip Bandyopadhyaya of the IIT Bombay Chemical Engineering department during an industrial internship in fragnance compounding at the R\u0026amp;D department of KEVA Ltd. under Dr. Debojit Chakrabarty  Bangalore  At IISc, I had the good fortune to meet Prof. Hannes Jonsson at a summer workshop on Rare events At the BIC, I undertook formal machine learning and artificial intelligence training under Harvard\u0026rsquo;s Dr. Rahul Dave and Dr. Pavlos Protopapas as part of the univ.ai summer course  Chennai  I spent a very fruitful summer on quantum tomography under Prof. Sibashish Ghosh at the Institute for Mathematical Sciences (IMSc Chennai)  Avatars I thought it might be of use to list a few of my more official visages. This is mostly to ensure people do not confuse me with a Sasquatch3. These mugshots are exactly that, mugshots for profile icons4.\n Figure 1: A collage of mugshots, shuffled and not ordered by date to confuse people trying to kill me\n  Donations If you\u0026rsquo;ve gotten this far, you might also want to check out the following5:\n Patreon Librepay    I didn\u0026rsquo;t think it would be necessary, but just in case it isn\u0026rsquo;t clear, people listed here are not necessarily all references or anything, this is a personal list of people associated with each city, not a cover letter\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n I grew up on the verdant and beautiful TIFR Mumbai campus, and completed high school and undergraduate stuff while playing with peacocks and things on the IIT Kanpur campus\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This is not a replacement for an Instagram feed or a Facebook wall, or even a ResearchGate or Publons or ORCID page; all of which I do sporadically remember I have\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Made with the Mountain Tapir Collage Maker\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n There won\u0026rsquo;t ever be any content behind paywalls though\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/about/","tags":null,"title":"About"},{"categories":["personal"],"contents":" An attempt to re-claim and verify my digital presence.\n Background I mentioned on my about page, that it is nigh impossible to keep track of every digital trace there is of me. That said, it is really not even a countable infinite set yet, so it is a good idea to get started before it gets much worse. This is minimally curated, and will only be sporadically updated, so take everything here with a grain of salt. I honestly have no idea why anyone who is not me would like to see this, other than to prove one of these with respect to the rest1.\nProfiles Professional  University of Iceland FemtoLab Google Knowledge Panel  Voluntary  The Carpentries  Instructor, Maintainer, CarpentyrCon2020 Committee Task Force, CarpentryCon2020 Speaker   IGDORE Univ.ai TeX Users Group  TUG 2020 Conference Committee member TUG Zulip Admin   Maintainer for packages on:  ArchLinux (the Arch User Repository (AUR)) Nixpkgs Spack MELPA PyPI    Academic  Publons PeerJ Google Scholar OrCiD Loop OSF  Societies  American Institute of Chemical Engineers IEEE  Communities  Figshare  Misc  Github Gitlab Goodreads Keybase  Pages and Articles By Me  Articles written for the Student Paper of the University of Iceland Everything on any of my many websites Write-up on research with a ChemE undergraduate degree for the American Institute of Chemical Engineers (AIChE) Young Professionals Committee (YPC) Hackernoon article on Locking and Encrypting Apps with Encfs The Water, Chemicals and more with Computers for Chemistry (WC3m) course website My GSoD (Google Season of Docs) 2020 completion report for SymEngine  Collaborative Carpentries Posts  A collaborative post on [[https://carpentries.org/blog/2020/08/r-4-migration/][migrating to R 4.0 for The  Carpentries]]\nTech Conferences These are tech talks, for academic presentations, my CV is a better guide.\n Reproducible Scalable Workflows with Nix, Papermill and Renku at PyCon India 2020 Reproducible Environments with the Nix Packaging System at CarpentryCon 2020  Lightning Talks  Nix from the dark ages (without Root) at NixCon 2020  Mentioning Me Volunteering  TUGBOAT issue for TUG 2020 proceedings  Several mentions as a conference organizer    Lists  Google Summer of Code (GSoC) 2021 - LFortran and Computational Chemistry 2021 List of projects funded by the Icelandic Research Fund Google Season of Docs (GSoD) 2020 - Symengine Spring 2021 Institute of Pure and Applied Mathematics (IPAM) Fellow for “Tensor Methods and Emerging Applications to the Physical and Data Sciences” 2020 Categorifications in Representation Theory (participant) Heilbronn Annual Conference 2020 (participant) CIRM - Jean-Morlet Chair - Quasi-Monte Carlo Methods and Applications (info) [participant] CIRM Mathematical Methods of Modern Statistics 2 (info) [participant] Kavli IPMU conference on \u0026ldquo;The McKay correspondence, mutation and related topics\u0026rdquo; (participant) Gallery of RARE 2019 at IISc Probablistic Data Analysis (University of Turku) AiiDA Virtual Tutotrial 2020 FortranCon 2020 Author Stan Math 3.3.0 contributor CECAM Participant for:  CECAM-DE-SMSM: (Machine) learning how to coarse-grain CECAM-DE-SMSM: ESPResSo and Python: Versatile Tools for Soft Matter Research   Fortran Newsletter Mentions:  April 2021: LFortran contributor March 2021: LFortran contributor May 2021: GSoC acceptance    Teaching  IOP Workshop on C++ II - Libraries and Simulations April 13, 2020 (instructor) (materials) Water, Chemicals and more with Computers for Chemistry (WC3m) July 28-August 28, 2020 (co-lead)  2020 MegaCodeRefinery (helper)\nCarpentries  Data Carpentry Ecology Social Sciences with R at CMU March 29-31, 2021 (instructor) Network of the National Library of Medicine (NNLM) LC Workshop March 16-19, 2021 (instructor) SWC workshop at the Eindhoven University of Technology (TU/e) March 16-17, 23-24, 2021 (instructor) Data Analysis and Visualization in Python for Ecologists at UCSB February 18-19, 2021 (instructor) Triangle Research Library Network Library Carpentry Workshop December 1-4, 2020 (instructor) Data Carpentry Ecology with R at CMU November 11-13, 2020 (instructor) Data Carpentry Workshop at ITEP November 9-10, 2020 (instructor) Sciware: Git and GitHub at the Flatiron Institute September 24 and October 1, 2020 (instructor) Data Carpentry Workshop for Social Sciences Georgia Gwinnett College September 8-11, 2020 (instructor) Online Data Carpentry Workshop SADiLaR, South Africa 29 June - 3 July, 2020 (instructor) Data Carpentry Ecology for Biotech Partners June 22-July 2, 2020 (instructor)  Quotes  Quoted in a Stanford Daily Article on CS106A Code in Place Fortran Monthly Newsletter (June 2020) Emacs News  2021-07-27: for Doom Emacs and Language Servers 2020-06-22: for Temporary LaTeX Documents with Orgmode 2020-06-15: for Emacs for Nix-R 2020-05-11: for An Orgmode Note Workflow 2020-05-04: for Pandoc to Orgmode with Babel 2020-04-27: for Using Mathematica with Orgmode 2020-04-13: for my dotDoom doom-emacs configuration 2020-04-06: for Replacing Jupyter with Orgmode    Videos Of Me  Everything on my YouTube channel Discussion session for the CS196A Code in Place AMA with the students Panelist for the CarpentryCon 2020 session on lessons learnt from remote teaching Student Presentation at UI on Scrum for Software Quality Management CarpentryCon 2020 session recording on \u0026ldquo;Reproducible Environments with the Nix Packaging System\u0026rdquo;  Including Me This category involves recordings where I asked questions, and therefore technically involve me in a sense.\n I appear in the audience of this clip on neural networks and regularization Code in Place AMA with Stanford CS Lecturers I appear (audibly) to ask a question for the TUG2020 closing seminar by John MacFarlane  HPC Carpentry  A guided tour  Fortran Maintainers Monthly Calls  June 2020  IAS TML Lecture Questions I\u0026rsquo;ve been sitting in on these for a while thanks to Ke Li, but this section lists some of the lectures I asked a question in\n \u0026ldquo;What Do Models Learn?\u0026rdquo; by Aleksander Mądry \u0026ldquo;Langevin Dynamics in Machine Learning\u0026rdquo; by Michael Jordan \u0026ldquo;Graph Nets: The Next Generation\u0026rdquo; by Max Welling \u0026ldquo;Priors for Semantic Variables\u0026rdquo; by Yoshua Bengio \u0026ldquo;A Blueprint of Standardized and Composable Machine Learning\u0026rdquo; by Eric Xing    If you do think you have seen me somewhere not on this list, drop me an email\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/rg-collection-weblinks/","tags":["ramblings"],"title":"Collection of WebLinks"},{"categories":null,"contents":"","permalink":"https://rgoswami.me/search/","tags":null,"title":"Search"}]