
@book{adamsFortran90Handbook1992,
  title = {Fortran 90 Handbook: Complete {{ANSI ISO}} Reference},
  shorttitle = {Fortran 90 Handbook},
  editor = {Adams, Jeanne C.},
  date = {1992},
  publisher = {{Intertext Publ. [u.a.]}},
  location = {{New York, NY}},
  annotation = {OCLC: 246553971},
  isbn = {978-0-07-000406-1},
  langid = {english},
  pagetotal = {740}
}

@article{backusHistoryFortranII1998,
  title = {The History of {{Fortran I}}, {{II}}, and {{III}}},
  author = {Backus, J.},
  date = {1998-10},
  journaltitle = {IEEE Annals of the History of Computing},
  volume = {20},
  pages = {68--78},
  issn = {1934-1547},
  doi = {10.1109/85.728232},
  abstract = {The article discusses attitudes about "automatic programming", the economics of programming, and existing programming systems, all in the early 1950s. It describes the formation of the Fortran group, its knowledge of existing systems, its plans for Fortran, and the development of the language in 1954. It describes the development of the optimizing compiler for Fortran I, of various language manuals, and of Fortran II and III. It concludes with remarks about later developments and the impact of Fortran and its successors on programming today.},
  eventtitle = {{{IEEE Annals}} of the {{History}} of {{Computing}}},
  keywords = {Algorithms,Automatic programming,Computer aided instruction,Computer languages,Decision support systems,Environmental economics,History,Optimizing compilers,Program processors,Programming profession},
  number = {4}
}

@inproceedings{benediktssonFORTRANIIFirst2009,
  title = {{{FORTRAN II}} – {{The First Computer Language Used}} at the {{University}} of {{Iceland}}},
  booktitle = {History of {{Nordic Computing}} 2},
  author = {Benediktsson, Oddur},
  editor = {Impagliazzo, John and Järvi, Timo and Paju, Petri},
  date = {2009},
  pages = {149--155},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-03757-3_16},
  abstract = {At the end of World War II, people considered Iceland an underdeveloped country. The use of IBM punched card systems started in 1949. The first computers appeared in 1964. Then the University of Iceland acquired an IBM 1620 “scientific” computer. The first computer language used to instruct engineers and scientists was FORTRAN II. The subsequent development gives an interesting picture of the advance of computer technology in Iceland.},
  isbn = {978-3-642-03757-3},
  keywords = {FORTRAN II,IBM 1620,programming education},
  langid = {english},
  series = {{{IFIP Advances}} in {{Information}} and {{Communication Technology}}}
}

@article{certikDftatomRobustGeneral2013,
  title = {Dftatom: {{A}} Robust and General {{Schrödinger}} and {{Dirac}} Solver for Atomic Structure Calculations},
  shorttitle = {Dftatom},
  author = {Čertík, Ondřej and Pask, John E. and Vackář, Jiří},
  date = {2013-07-01},
  journaltitle = {Computer Physics Communications},
  shortjournal = {Computer Physics Communications},
  volume = {184},
  pages = {1777--1791},
  issn = {0010-4655},
  doi = {10.1016/j.cpc.2013.02.014},
  url = {https://www.sciencedirect.com/science/article/pii/S0010465513000714},
  urldate = {2021-04-26},
  abstract = {A robust and general solver for the radial Schrödinger, Dirac, and Kohn–Sham equations is presented. The formulation admits general potentials and meshes: uniform, exponential, or other defined by nodal distribution and derivative functions. For a given mesh type, convergence can be controlled systematically by increasing the number of grid points. Radial integrations are carried out using a combination of asymptotic forms, Runge–Kutta, and implicit Adams methods. Eigenfunctions are determined by a combination of bisection and perturbation methods for robustness and speed. An outward Poisson integration is employed to increase accuracy in the core region, allowing absolute accuracies of 10−8 Hartree to be attained for total energies of heavy atoms such as uranium. Detailed convergence studies are presented and computational parameters are provided to achieve accuracies commonly required in practice. Comparisons to analytic and current-benchmark density-functional results for atomic number Z=1–92 are presented, verifying and providing a refinement to current benchmarks. An efficient, modular Fortran 95 implementation, dftatom, is provided as open source, including examples, tests, and wrappers for interface to other languages; wherein particular emphasis is placed on the independence (no global variables), reusability, and generality of the individual routines. Program summary Program title:dftatom Catalogue identifier: AEPA\_v1\_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEPA\_v1\_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: MIT license No. of lines in distributed program, including test data, etc.: 14122 No. of bytes in distributed program, including test data, etc.: 157453 Distribution format: tar.gz Programming language: Fortran 95 with interfaces to Python and C. Computer: Any computer with a Fortran 95 compiler. Operating system: Any OS with a Fortran 95 compiler. RAM: 500~MB Classification: 2.1. External routines: Numpy (http://www.numpy.org/) and Cython (http://cython.org/) Nature of problem: Solution of the Schrödinger, Dirac, and Kohn–Sham equations of Density Functional Theory for isolated atoms. Solution method: Radial integrations are carried out using a combination of asymptotic forms, Runge–Kutta, and implicit Adams methods. Eigenfunctions are determined by a combination of bisection and perturbation methods. An outward Poisson integration is employed to increase accuracy in the core region. Self-consistent field equations are solved by adaptive linear mixing. Restrictions: Spherical symmetry Unusual features: Radial integrators work for general potentials and meshes. No restriction to Coulombic or self-consistent potentials; no restriction to uniform or exponential meshes. Outward Poisson integration. Fallback to bisection for robustness. Running time: For uranium, non-relativistic density functional calculation execution time is around 0.6~s for 10−6 a.u. accuracy in total energy on an Intel Core i7 1.46~GHz processor.},
  keywords = {Atomic structure,Density functional theory,Dirac equation,Electronic structure,Fortran 95,Kohn–Sham equations,Schrödinger equation,Shooting method},
  langid = {english},
  number = {7}
}

@article{cipraBest20thCentury,
  title = {The {{Best}} of the 20th {{Century}}: {{Editors Name Top}} 10 {{Algorithms}}},
  author = {Cipra, Barry A},
  pages = {2},
  langid = {english}
}

@book{clermanModernFortranStyle2012,
  title = {Modern {{Fortran}}: Style and Usage},
  shorttitle = {Modern {{Fortran}}},
  author = {Clerman, Norman S. and Spector, Walter},
  date = {2012},
  publisher = {{Cambridge University Press}},
  location = {{New York}},
  annotation = {OCLC: ocn743298762},
  isbn = {978-0-521-51453-8 978-0-521-73052-5},
  keywords = {FORTRAN (Computer program language)},
  pagetotal = {334}
}

@book{cooperEngineeringCompiler2011a,
  title = {Engineering a {{Compiler}}},
  author = {Cooper, Keith and Torczon, Linda},
  date = {2011-01-18},
  publisher = {{Elsevier}},
  abstract = {This entirely revised second edition of Engineering a Compiler is full of technical updates and new material covering the latest developments in compiler technology. In this comprehensive text you will learn important techniques for constructing a modern compiler. Leading educators and researchers Keith Cooper and Linda Torczon combine basic principles with pragmatic insights from their experience building state-of-the-art compilers. They will help you fully understand important techniques such as compilation of imperative and object-oriented languages, construction of static single assignment forms, instruction scheduling, and graph-coloring register allocation.In-depth treatment of algorithms and techniques used in the front end of a modern compilerFocus on code optimization and code generation, the primary areas of recent research and developmentImprovements in presentation including conceptual overviews for each chapter, summaries and review questions for sections, and prominent placement of definitions for new termsExamples drawn from several different programming languages},
  eprint = {_tgh4bgQ6PAC},
  eprinttype = {googlebooks},
  isbn = {978-0-08-091661-3},
  keywords = {Computers / Computer Architecture,Computers / Computer Engineering,Computers / General,Computers / Languages / General,Computers / Operating Systems / General,Computers / Programming / Compilers,Computers / Software Development & Engineering / General},
  langid = {english},
  pagetotal = {825}
}

@online{hansonEarlyExperienceASDL1998,
  title = {Early {{Experience}} with {{ASDL}} in Lcc},
  author = {Hanson, David R.},
  date = {1998-10-13},
  url = {http://arxiv.org/abs/cs/9810013},
  urldate = {2021-07-05},
  abstract = {The Abstract Syntax Description Language (ASDL) is a language for specifying the tree data structures often found in compiler intermediate representations. The ASDL generator reads an ASDL specification and generates code to construct, read, and write instances of the trees specified. Using ASDL permits a compiler to be decomposed into semi-independent components that communicate by reading and writing trees. Each component can be written in a different language, because the ASDL generator can emit code in several languages, and the files written by ASDL-generated code are machine- and language-independent. ASDL is part of the National Compiler Infrastructure project, which seeks to reduce dramatically the overhead of computer systems research by making it much easier to build highquality compilers. This paper describes dividing lcc, a widely used retargetable C compiler, into two components that communicate via trees defined in ASDL. As the first use of ASDL in a ‘real’ compiler, this experience reveals much about the effort required to retrofit an existing compiler to use ASDL, the overheads involved, and the strengths and weaknesses of ASDL itself and, secondarily, of lcc.},
  archiveprefix = {arXiv},
  eprint = {cs/9810013},
  eprinttype = {arxiv},
  keywords = {Computer Science - Programming Languages,Computer Science - Software Engineering,D.3.4},
  langid = {english}
}

@report{J318007r1F2018,
  title = {J3/18-007r1 ({{F2018 Interpretation Document}})},
  url = {https://j3-fortran.org/doc/year/18/18-007r1.pdf},
  urldate = {2021-06-28},
  abstract = {9th October 2018 16:46}
}

@report{J321007Draft,
  title = {J3/21-007 ({{Draft Fortran}} 202x)},
  url = {https://j3-fortran.org/doc/year/21/21-007.pdf},
  urldate = {2021-06-28},
  abstract = {16th December 2020 12:00}
}

@report{J397007R295,
  title = {J3/97-{{007R2}} (95 {{Interpretation Document}})},
  url = {https://j3-fortran.org/doc/year/18/18-007r1.pdf},
  urldate = {2021-06-28},
  abstract = {October 21, 1997 12:28 pm}
}

@article{khedkerGCCTranslationSequence,
  title = {{{GCC Translation Sequence}} and {{Gimple IR}}},
  author = {Khedker, Uday},
  pages = {70},
  langid = {english}
}

@article{klintEngineeringDisciplineGrammarware2005,
  title = {Toward an Engineering Discipline for Grammarware},
  author = {Klint, Paul and Lämmel, Ralf and Verhoef, Chris},
  date = {2005-07-01},
  journaltitle = {ACM Transactions on Software Engineering and Methodology},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  volume = {14},
  pages = {331--380},
  issn = {1049-331X},
  doi = {10.1145/1072997.1073000},
  url = {https://doi.org/10.1145/1072997.1073000},
  urldate = {2021-07-05},
  abstract = {Grammarware comprises grammars and all grammar-dependent software. The term grammar is meant here in the sense of all established grammar formalisms and grammar notations including context-free grammars, class dictionaries, and XML schemas as well as some forms of tree and graph grammars. The term grammar-dependent software refers to all software that involves grammar knowledge in an essential manner. Archetypal examples of grammar-dependent software are parsers, program converters, and XML document processors. Despite the pervasive role of grammars in software systems, the engineering aspects of grammarware are insufficiently understood. We lay out an agenda that is meant to promote research on increasing the productivity of grammarware development and on improving the quality of grammarware. To this end, we identify the problems with the current grammarware practices, the barriers that currently hamper research, and the promises of an engineering discipline for grammarware, its principles, and the research challenges that have to be addressed.},
  keywords = {automated software engineering,best practices,generic language technology,grammar-dependent software,grammars,Grammarware,language processing,metamodeling,model-driven development,parsers,software evolution,software transformation},
  number = {3}
}

@book{levineLinkersLoaders2000,
  title = {Linkers and Loaders},
  author = {Levine, John R.},
  date = {2000},
  publisher = {{Morgan Kaufmann}},
  location = {{San Francisco}},
  isbn = {978-1-55860-496-4},
  keywords = {Assembling (Electronic computers),Linkers (Computer programs),Loaders (Computer programs)},
  pagetotal = {256}
}

@book{lyonUsingAnsFortran1980,
  title = {Using {{Ans Fortran}}},
  author = {Lyon, G. E.},
  date = {1980},
  publisher = {{National Bureau of Standards}},
  abstract = {This FORTRAN volume presents, in order: a set of quick and clear reference charts for ANS FORTRAN 66 syntax; observations on using only standard FORTRAN 65 features; instructions on circumventing and extending FORTRAN 66 with the least harm; an appraisal of the new FORTRAN 77 in terms of FORTRAN 65 constructs. Although the chapters comprise much material that has appeared in other technical memoranda or published articles, heavily recast sections have been re-refereed. The four chapters address programmers concerned with FORTRAN transportability, managers engaged in programming standards, and other practitioners interested in system influences upon languages. Since the text touches upon several general programming aspects (input/output, storage allocation, storage lifetimes and protection, control structures), the volume's appeal will extend beyond the immediate FORTRAN community.},
  eprint = {8ymHAQAACAAJ},
  eprinttype = {googlebooks},
  langid = {english},
  pagetotal = {109}
}

@article{merrillGENERICGIMPLENew,
  title = {{{GENERIC}} and {{GIMPLE}}: {{A New Tree Representation}} for {{Entire Functions}}},
  author = {Merrill, Jason},
  pages = {10},
  langid = {english}
}

@article{papaevripidesExploitingMixedBinaries2021,
  title = {Exploiting {{Mixed Binaries}}},
  author = {Papaevripides, Michalis and Athanasopoulos, Elias},
  date = {2021-02},
  journaltitle = {ACM Transactions on Privacy and Security},
  shortjournal = {ACM Trans. Priv. Secur.},
  volume = {24},
  pages = {1--29},
  issn = {2471-2566, 2471-2574},
  doi = {10.1145/3418898},
  url = {https://dl.acm.org/doi/10.1145/3418898},
  urldate = {2021-07-07},
  abstract = {Unsafe programming systems are still very popular, despite the shortcomings due to several published memory-corruption vulnerabilities. Toward defending memory corruption, compilers have started to employ advanced software hardening such as Control-flow Integrity (CFI) and SafeStack. However, there is a broad interest for realizing compilers that impose memory safety with no heavy runtime support (e.g., garbage collection). Representative examples of this category are Rust and Go, which enforce memory safety primarily statically at compile time.                            Software hardening and Rust/Go are promising directions for defending memory corruption, albeit combining the two is questionable. In this article, we consider hardened               mixed               binaries, i.e., machine code that has been produced from different compilers and, in particular, from               hardened               C/C++ and Rust/Go (e.g., Mozilla Firefox, Dropbox, npm, and Docker). Our analysis is focused on Mozilla Firefox, which outsources significant code to Rust and is open source with known public vulnerabilities (with assigned CVE). Furthermore, we extend our analysis in mixed binaries that leverage Go, and we derive similar results.                                         The attacks explored in this article               do not               exploit Rust or Go binaries that depend on some legacy (vulnerable) C/C++ code. In contrast, we explore how Rust/Go compiled code can stand as a vehicle for bypassing hardening in C/C++ code. In particular, we discuss CFI and SafeStack, which are available in the latest Clang. Our assessment concludes that CFI can be completely nullified through Rust or Go code by constructing much simpler attacks than state-of-the-art CFI bypasses.},
  langid = {english},
  number = {2}
}

@online{UsingASDLDescribe,
  title = {Using {{ASDL}} to Describe {{ASTs}} in Compilers - {{Eli Bendersky}}'s Website},
  url = {https://eli.thegreenplace.net/2014/06/04/using-asdl-to-describe-asts-in-compilers/},
  urldate = {2021-07-05}
}

@article{wangZephyrAbstractSyntax,
  title = {The {{Zephyr Abstract Syntax Description Language}}},
  author = {Wang, Daniel C and Appel, Andrew W and Korn, Jeff L and Serra, Christopher S},
  pages = {16},
  abstract = {The Zephyr1 Abstract Syntax Description Language (ASDL) describes the abstract syntax of compiler intermediate representations (IRs) and other tree-like data structures. Just as the lexical and syntactic structures of programming languages are described with regular expressions and context free grammars, ASDL provides a concise notation for describing the abstract syntax of programming languages. Tools can convert ASDL descriptions into the appropriate data-structure definitions and functions to convert the data-structures to or from a standard flattened representation. This makes it easier to build compiler components that interoperate.},
  langid = {english}
}

@article{zhouEmpiricalStudyOptimization2021,
  title = {An Empirical Study of Optimization Bugs in {{GCC}} and {{LLVM}}},
  author = {Zhou, Zhide and Ren, Zhilei and Gao, Guojun and Jiang, He},
  date = {2021-04-01},
  journaltitle = {Journal of Systems and Software},
  shortjournal = {Journal of Systems and Software},
  volume = {174},
  pages = {110884},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2020.110884},
  url = {https://www.sciencedirect.com/science/article/pii/S0164121220302740},
  urldate = {2021-07-09},
  abstract = {Optimizations are the fundamental component of compilers. Bugs in optimizations have significant impacts, and can cause unintended application behavior and disasters, especially for safety-critical domains. Thus, an in-depth analysis of optimization bugs should be conducted to help developers understand and test the optimizations in compilers. To this end, we conduct an empirical study to investigate the characteristics of optimization bugs in two mainstream compilers, GCC and LLVM. We collect about 57K and 22K bugs of GCC and LLVM, and then exhaustively examine 8,771 and 1,564 optimization bugs of the two compilers, respectively. The results reveal the following five characteristics of optimization bugs: (1) Optimizations are the buggiest component in both compilers except for the C++ component; (2) the value range propagation optimization and the instruction combine optimization are the buggiest optimizations in GCC and LLVM, respectively; the loop optimizations in both GCC and LLVM are more bug-prone than other optimizations; (3) most of the optimization bugs in both GCC and LLVM are misoptimization bugs, accounting for 57.21\% and 61.38\% respectively; (4) on average, the optimization bugs live over five months, and developers take 11.16 months for GCC and 13.55 months for LLVM to fix an optimization bug; in both GCC and LLVM, many confirmed optimization bugs have lived for a long time; (5) the bug fixes of optimization bugs involve no more than two files and three functions on average in both compilers, and around 99\% of them modify no more than 100 lines of code, while 90\% less than 50 lines of code. Our study provides a deep understanding of optimization bugs for developers and researchers. This could provide useful guidance for the developers and researchers to better design the optimizations in compilers. In addition, the analysis results suggest that we need more effective techniques and tools to test compiler optimizations. Moreover, our findings are also useful to the research of automatic debugging techniques for compilers, such as automatic compiler bug isolation techniques.},
  keywords = {Bug characteristics,Compiler optimization bugs,Compiler reliability,Compiler testing,Empirical study},
  langid = {english}
}


