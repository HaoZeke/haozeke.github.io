#+author: Rohit Goswami

#+hugo_base_dir: ../
#+hugo_front_matter_format: yaml
#+hugo_front_matter_key_replace: description>summary
#+hugo_auto_set_lastmod: t
#+bibliography: biblio/refs.bib

#+seq_todo: TODO DRAFT DONE
#+seq_todo: TEST__TODO | TEST__DONE

#+property: header-args :eval never-export

#+startup: logdone indent overview inlineimages

* DONE About
:PROPERTIES:
:EXPORT_HUGO_SECTION: /
:EXPORT_FILE_NAME: about
:EXPORT_DATE: 1995-08-10
:END:

#+begin_description
A short historical recollection of some *thoughts* and _stuff_.
#+end_description

*Hi.*

I'm [[https://orcid.org/0000-0002-2393-8056][Rohit Goswami]], better known across the web as ~HaoZeke~. I'm not the
first of my name, which is why instead of ~rgoswami~, I occasionally use ~rg0swami~ when I need to be
identified by something closer to my name.

The actual username is a throwback to back when people *liked* being anonymous (and with multiple personalities)
online, so that ought to give an idea of how old I am. A curriculum vitae is
[[https://github.com/HaoZeke/CV/blob/master/RG_Latest-cv.pdf][available here]].

It is difficult to keep this section short and not let it spill
into an unstructured memoir. For a while I considered trying to consolidate my
online presences but that turned out to be completely impossible without a
series of posts and avatars[fn:notrefs]. I did however eventually set up a
sporadically updated [[Collection of WebLinks][collection of web-links]] involving me. There is also, a separate list of quotes, or other things I have something to say about.
** Intangible Positions
This is a set of things which are primarily online and/or voluntary in a
non-academic sense.
- I administer and design a bunch of websites, mostly verified [[https://keybase.io/HaoZeke][on Keybase]]
- I am a certified Software Carpentries [[https://carpentries.org/instructors/#HaoZeke][instructor]]
- I [[https://carpentries.org/maintainers/#HaoZeke][officially maintain]], for the Software Carpentries, the lesson on R ([[https://github.com/swcarpentry/r-novice-inflammation][r-novice-inflammation]])
- I [[https://aur.archlinux.org/packages/?SeB=m&K=HaoZeke][also maintain]] some packages on the AUR (ArchLinux User Repository)
- I hone coursework development and teaching [[https://www.univ.ai/team/rohit-goswami][with univ.ai]]
- I [[https://forum.xda-developers.com/xperia-z5/orig-development/cm-14-1-lineageos-t3536846][maintain(ed)]] the official LineageOS image for the Xperia Z5 Dual
** Historical Places
What follows is a more informal set of places I am or have been associated with or are of significance to
me[fn:growingUp].
*** Reykjavík
- As of 2021, I have been awarded a three year doctoral fellowship [[https://en.rannis.is/news/allocation-from-the-icelandic-research-fund-for-the-financial-year-2021-1][from the Icelandic Research Fund (Rannís)]] for my project on "Magnetic interactions of itinerant electrons modeled using Bayesian machine learning" tenable at the Science Institute with [[https://notendur.hi.is//~hj/indexE.html][Prof. Hannes Jónsson]] as my supervisor and [[https://english.hi.is/staff/birgirhr][Prof. Birgir Hrafnkelsson]] as my co-supervisor
- I am a Fellow at the Institute for Pure and Applied Mathematics (IPAM) for its Spring 2021 program on “[[http://www.ipam.ucla.edu/programs/long-programs/tensor-methods-and-emerging-applications-to-the-physical-and-data-sciences/?tab=participant-list][Tensor Methods and Emerging Applications to the Physical and Data Sciences]]”
- I [[https://english.hi.is/staff/rog32][am associated]] with the [[https://notendur.hi.is/hj/researchgroup.html][reputed Jonsson group]] of the [[http://raunvisindastofnun.hi.is/the_science_institute][Science Institute]] at the
  [[https://english.hi.is/school_of_engineering_and_natural_sciences][University of Iceland]], where I benefit from the
  guidance of the erudite and inspiring [[https://notendur.hi.is/hj/indexE.html][Prof. Hannes Jonsson]]
- My doctoral committee is here, which includes the very excellent inputs of
  [[https://english.hi.is/staff/elvarorn][Dr. Elvar Jonsson]]
- I have also benefited from sitting in on some formal coursework here, which
  has been a fascinatingly useful experience
*** Kanpur
- I [[https://femtolab.science/people/rohit][retain a close association]] with the fantastic [[https://femtolab.science/][Femtolab]] at [[http://home.iitk.ac.in/~dgoswami/][IIT Kanpur]] under
  [[https://femtolab.science/people/dgoswami][Prof. Debabrata Goswami]], who has provided constant guidance throughout my career
- I am the co-lead developer of the FOSS scientific [[https://dseams.info][d-SEAMS software suite]] for
  [[https://wiki.dseams.info/#citation][graph theoretic approaches to structure determination]] of molecular dynamics
  simulations, along with my exceptional co-lead [[https://www.researchgate.net/profile/Amrita_Goswami2][Amrita Goswami]] of the CNS Lab
  under Prof. Jayant K. Singh at IITK
- I worked with the Nair group as part of the [[http://surge.iitk.ac.in/AnnualReport/report2017.pdf][Summer Undergraduate in Research Excellence]] (SURGE) program, also at IITK
- Harcourt Butler Technical Institute (HBTI) Kanpur, or the [[http://hbtu.ac.in/][Harcourt Butler Technological University]], as it is now called, was where I trained to be a chemical engineer
*** Bombay
- I [[https://rajarshichakrabarti.wixsite.com/rajarshichakrabarti/team][spent a formative summer]] under [[https://rajarshichakrabarti.wixsite.com/rajarshichakrabarti][Prof. Rajarshi Chakrabarti]] of the IIT Bombay
  Chemistry department, who has been instrumental in developing my interests
- I also spent some time discussing experiments with [[https://www.che.iitb.ac.in/online/faculty/rajdip-bandyopadhyaya][Prof. Rajdip Bandyopadhyaya]]
  of the IIT Bombay Chemical Engineering department during an industrial
  internship in fragnance compounding at the R&D department of KEVA Ltd. under
  [[https://in.linkedin.com/in/debojit-chakrabarty-b9a2262][Dr. Debojit Chakrabarty]]
*** Bangalore
- At IISc, I had the good fortune to meet Prof. Hannes Jonsson at a summer
  workshop [[https://chemeng.iisc.ac.in/rare-events/index.html][on Rare events]]
- At the [[http://bangaloreinternationalcentre.org/][BIC]], I undertook formal machine learning and artificial intelligence
  training under Harvard's [[https://www.extension.harvard.edu/faculty-directory/rahul-dave][Dr. Rahul Dave]] and [[https://iacs.seas.harvard.edu/people/pavlos-protopapas][Dr. Pavlos Protopapas]] as part of the [[https://univ.ai][univ.ai]]
  summer course
*** Chennai
- I spent a very fruitful summer on quantum tomography under [[https://www.imsc.res.in/~sibasish/qis.html][Prof. Sibashish Ghosh]] at the [[https://www.imsc.res.in/][Institute for Mathematical Sciences]] (IMSc Chennai)
** Avatars
I thought it might be of use to list a few of my more official visages. This is
mostly to ensure people do not confuse me with a Sasquatch[fn:notpersonal].
These mugshots are exactly that, mugshots for profile icons[fn:mountaintapir].

#+caption: A collage of mugshots, shuffled and not ordered by date to confuse people trying to kill me
[[file:images/avatarCollage.jpg]]

** Donations
If you've gotten this far, you might also want to check out the
following[fn:patreon]:
- [[https://www.patreon.com/rgoswami][Patreon]]
- [[https://liberapay.com/rohit][Librepay]]

[fn:patreon] There won't ever be any content behind paywalls though
[fn:growingUp] I grew up on the verdant and beautiful [[https://www.tifr.res.in/][TIFR Mumbai]] campus, and
completed high school and undergraduate stuff while playing with peacocks and things on
the [[https://www.iitk.ac.in][IIT Kanpur]] campus
[fn:notrefs] I didn't think it would be necessary, but just in case it isn't
clear, people listed here are not necessarily all references or anything, this is
a personal list of people associated with each city, not a cover letter
[fn:notpersonal] This is not a replacement for [[https://www.instagram.com/rg0swami/][an Instagram feed]] or a [[https://www.facebook.com/rg0swami][Facebook
wall]], or even a [[https://www.researchgate.net/profile/Rohit_Goswami2][ResearchGate]] or [[https://publons.com/researcher/2911170/rohit-goswami/][Publons]] or [[https://orcid.org/0000-0002-2393-8056][ORCID]] page; all of which I do sporadically remember I have
[fn:mountaintapir] Made with the [[https://github.com/tttppp/mountain_tapir][Mountain Tapir Collage Maker]]
* TODO Collection of Quotes
From Statistical Rethinking:
#+begin_quote
An engineer, a physicist, and a statistician go hunting with a bow and arrow. The physicist underestimates ("let us ignore windspeed") and his arrow falls short. The engineer adds a fudge factor and his shot goes wide. The statistician averages the two and claims they have bagged the animal.
#+end_quote
Illustrates the usage of stats.

From some talk by Cederic Villani:
#+begin_quote
A mathematician and a statistician see two people go into a building, one comes out. The statistician asserts there is a 50-50 chance of someone coming out, and the mathematician says if one person goes in, no one comes out.
#+end_quote
Illustrates the concept of rigor.

* DONE Collection of WebLinks :@personal:ramblings:
CLOSED: [1995-08-10]
:PROPERTIES:
:EXPORT_FILE_NAME: rg-collection-weblinks
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments false
:END:
#+BEGIN_QUOTE
An attempt to re-claim and verify my digital presence.
#+END_QUOTE
** Background
I mentioned [[About][on my about page]], that it is nigh impossible to keep track of every
digital trace there is of me. That said, it is really not even a countable
infinite set yet, so it is a good idea to get started before it gets much worse.
This is minimally curated, and will only be sporadically updated, so take
everything here with a grain of salt. I honestly have no idea why anyone who is
not me would like to see this, other than to prove one of these with respect to
the rest[fn:whereswaldo].
** Profiles
*** Professional
- [[https://english.hi.is/staff/rog32][University of Iceland]]
- [[https://femtolab.science/people/rohit][FemtoLab]]
- [[https://g.co/kgs/Y9fpDC][Google Knowledge Panel]]
*** Voluntary
- The Carpentries
  - [[https://carpentries.org/instructors#HaoZeke][Instructor]], [[https://carpentries.org/maintainers#HaoZeke][Maintainer]], [[https://2020.carpentrycon.org/task-force/][CarpentyrCon2020 Committee Task Force]], [[https://2020.carpentrycon.org/schedule/#session-10][CarpentryCon2020 Speaker]]
- [[https://forum.igdore.org/t/rohit-goswami/656][IGDORE]]
- [[https://www.univ.ai/teams/rohit-goswami][Univ.ai]]
- TeX Users Group
  - [[http://tug.org/tug2020/][TUG 2020 Conference Committee member]]
  - TUG Zulip Admin
- Maintainer for packages on:
  - ArchLinux (the [[https://aur.archlinux.org/account/HaoZeke][Arch User Repository]] (AUR))
  - [[https://github.com/NixOS/nixpkgs/search?q=haozeke][Nixpkgs]]
  - [[https://github.com/spack/spack/pulls?q=is%3Apr+author%3AHaoZeke][Spack]]
  - [[https://melpa.org/#/?q=HaoZeke][MELPA]]
  - [[https://pypi.org/user/rgoswami/][PyPI]]
*** Academic
- [[https://publons.com/researcher/2911170/rohit-goswami/][Publons]]
- [[https://peerj.com/rgoswami/][PeerJ]]
- [[http://scholar.google.com/citations?user=36gIdJMAAAAJ&hl=en][Google Scholar]]
- [[https://orcid.org/0000-0002-2393-8056][OrCiD]]
- [[https://loop.frontiersin.org/people/829611/overview][Loop]]
- [[https://osf.io/c47v3/][OSF]]
# **** Software
# I am particularly proud of my contributions to to:
# - d-SEAMS
*** Societies
- [[https://engage.aiche.org/network/community-directory/profile?UserKey=8f135c8b-747d-4f79-95fd-84975458e3bd][American Institute of Chemical Engineers]]
- [[https://ieeexplore.ieee.org/author/37086865956][IEEE]]
*** Communities
- [[https://figshare.com/authors/Rohit_Goswami/5453063][Figshare]]
*** Misc
- [[https://github.com/HaoZeke][Github]]
- [[https://gitlab.com/HaoZeke][Gitlab]]
- [[https://www.goodreads.com/user/show/33462912-rohit-goswami][Goodreads]]
- [[https://keybase.io/HaoZeke][Keybase]]
** Pages and Articles
*** By Me
- Everything on [[https://keybase.io/HaoZeke][any of my many websites]]
- [[https://www.aiche.org/community/sites/committees/young-professionals/blog/fundamental-research-chemical-engineering-undergrad][Write-up on research with a ChemE undergraduate degree]] for the American Institute of Chemical Engineers (AIChE) Young Professionals Committee (YPC)
- Hackernoon article on [[https://hackernoon.com/locking-and-encrypting-apps-with-encfs-c1484e77f479][Locking and Encrypting Apps with Encfs]]
- The Water, Chemicals and more with Computers for Chemistry (WC3m) [[https://wc3m.github.io/][course website]]
- My [[https://github.com/symengine/symengine/wiki/GSoD-2020-Report-Rohit-Goswami:-SymEngine][GSoD (Google Season of Docs) 2020 completion report]] for SymEngine
**** Collaborative Carpentries Posts
- A collaborative post on [[https://carpentries.org/blog/2020/08/r-4-migration/][migrating to R 4.0 for The
Carpentries]]
**** Tech Conferences
These are tech talks, for academic presentations, [[https://github.com/haozeke/cv][my CV is a better guide]].
- Reproducible Scalable Workflows with Nix, Papermill and Renku at [[https://in.pycon.org/2020/][PyCon India 2020]]
- Reproducible Environments with the Nix Packaging System at [[https://2020.carpentrycon.org/schedule/#session-10][CarpentryCon 2020]]
**** Lightning Talks
- Nix from the dark ages (without Root) at [[https://2020.nixcon.org/][NixCon 2020]]
*** Mentioning Me
**** Volunteering
- [[https://www.tug.org/TUGboat/Contents/contents41-2.html][TUGBOAT issue for TUG 2020]] proceedings
  - Several mentions as a conference organizer
**** Lists
- [[https://summerofcode.withgoogle.com/projects/#5790375215628288][Google Summer of Code (GSoC) 2021 - LFortran and Computational Chemistry]]
- [[https://en.rannis.is/news/allocation-from-the-icelandic-research-fund-for-the-financial-year-2021-1][2021 List of projects funded by the Icelandic Research Fund]]
- [[https://developers.google.com/season-of-docs/docs/participants/project-sympy-rohitgoswami][Google Season of Docs (GSoD) 2020 - Symengine]]
- Spring 2021 [[http://www.ipam.ucla.edu/programs/long-programs/tensor-methods-and-emerging-applications-to-the-physical-and-data-sciences/?tab=participant-list][Institute of Pure and Applied Mathematics (IPAM) Fellow]] for “Tensor Methods and Emerging Applications to the Physical and Data Sciences”
- [[https://sites.google.com/view/2020crt/home?authuser=0][2020 Categorifications in Representation Theory]] ([[https://sites.google.com/view/2020crt/registered-participants?authuser=0][participant]])
- [[https://web-eur.cvent.com/event/16fae83c-5b72-4690-a368-829688f0d14b/summary][Heilbronn Annual Conference 2020]] ([[https://web-eur.cvent.com/event/16fae83c-5b72-4690-a368-829688f0d14b/websitePage:1faf08a0-dcdd-4a2c-b4b8-d9d445202377?RefId=HIMR][participant]])
- [[https://www.cirm-math.fr/Listes/liste_pre_verif.php?id_renc=2255&num_semaine=0][CIRM - Jean-Morlet Chair - Quasi-Monte Carlo Methods and Applications]] ([[https://www.chairejeanmorlet.com/2255.html][info]]) [participant]
- [[https://www.cirm-math.fr/Listes/liste_pre_verif.php?id_renc=2146&num_semaine=0][CIRM Mathematical Methods of Modern Statistics 2]] ([[https://www.cirm-math.com/cirm-virtual-event-2146.html][info]]) [participant]
- Kavli IPMU conference on "[[https://indico.ipmu.jp/event/314/overview][The McKay correspondence, mutation and related topics]]" ([[https://indico.ipmu.jp/event/314/registrations/participants][participant]])
- [[https://chemeng.iisc.ac.in/rare-events/gallery.html][Gallery of RARE 2019]] at IISc
- [[https://gitlab.com/openresearchlabs/probabilistic_data_analysis_2020/-/blob/master/session7.md][Probablistic Data Analysis]] (University of Turku)
- [[https://twitter.com/aiidateam/status/1281323480290660352][AiiDA Virtual Tutotrial 2020]]
- [[https://tcevents.chem.uzh.ch/event/12/contributions/45/author/73][FortranCon 2020 Author]]
- [[https://github.com/stan-dev/math/releases/tag/v3.3.0][Stan Math 3.3.0 contributor]]
- CECAM Participant for:
  - [[https://www.cecam.org/workshop-details/26#participant_tab][CECAM-DE-SMSM: (Machine) learning how to coarse-grain]]
  - [[https://www.cecam.org/workshop-details/28#participant_tab][CECAM-DE-SMSM: ESPResSo and Python: Versatile Tools for Soft Matter Research]]
- Fortran Newsletter Mentions:
  - [[https://fortran-lang.org/newsletter/2021/04/01/Fortran-Newsletter-April-2021/][April 2021]] :: LFortran contributor
  - [[https://fortran-lang.org/newsletter/2021/03/01/Fortran-Newsletter-March-2021/][March 2021]] :: LFortran contributor
  - [[https://fortran-lang.org/newsletter/2021/05/18/Welcome-GSoC-students/][May 2021]] :: GSoC acceptance
**** Teaching
- [[https://www.iopconferences.org/iop/frontend/reg/thome.csp?pageID=1032921&eventID=1664][IOP Workshop on C++ II - Libraries and Simulations]] :: April 13, 2020 (instructor) ([[Talk Supplements for IOP's C++ Workshop][materials]])
- [[https://wc3m.github.io/][Water, Chemicals and more with Computers for Chemistry (WC3m)]] :: July 28-August 28, 2020 (co-lead)
- [[https://coderefinery.github.io/2020-05-25-online/][2020 MegaCodeRefinery]] (helper)
***** Carpentries
- [[https://annajiat.github.io/2021-03-29-cmu-online/][Data Carpentry Ecology Social Sciences with R at CMU]] :: March 29-31, 2021 (instructor)
- [[https://haozeke.github.io/2021-03-16-utah-online/][Network of the National Library of Medicine (NNLM) LC Workshop]] :: March 16-19, 2021 (instructor)
- [[https://4turesearchdata-carpentries.github.io/2021-03-16-tudelft-online/][SWC workshop at the Eindhoven University of Technology (TU/e)]] :: March 16-17, 23-24, 2021 (instructor)
- [[https://ucsbcarpentry.github.io/2021-02-18-ucsb-online/][Data Analysis and Visualization in Python for Ecologists at UCSB]] :: February 18-19, 2021 (instructor)
- [[https://bkmgit.github.io/2020-12-01-TRLN-NNLM-online/][Triangle Research Library Network Library Carpentry Workshop]] :: December 1-4, 2020 (instructor)
- [[https://sichongp.github.io/2020-11-11-cmu-online/][Data Carpentry Ecology with R at CMU]] :: November 11-13, 2020 (instructor)
- [[https://marwahaha.github.io/2020-11-04-ITEP-online/][Data Carpentry Workshop at ITEP]] :: November 9-10, 2020 (instructor)
- [[https://flatironinstitute.github.io/sciware-swc-2020-09-git/][Sciware: Git and GitHub at the Flatiron Institute]] :: September 24 and October 1, 2020 (instructor)
- [[https://nairps.github.io/2020-09-08-ggc-SBDH-online/][Data Carpentry Workshop for Social Sciences Georgia Gwinnett College]] :: September 8-11, 2020 (instructor)
- [[https://sadilar.github.io/2020-06-29-SA-ONLINE/][Online Data Carpentry Workshop SADiLaR, South Africa]] :: 29 June - 3 July, 2020 (instructor)
- [[https://smcclatchy.github.io/2020-06-22-biotechPartners-pm/][Data Carpentry Ecology for Biotech Partners]] :: June 22-July 2, 2020 (instructor)
**** Quotes
- Quoted in a [[https://www.stanforddaily.com/2020/06/08/code-in-place-makes-cs-accessible-to-thousands-worldwide/][Stanford Daily Article on CS106A Code in Place]]
- [[https://fortran-lang.org/newsletter/2020/06/01/Fortran-Newsletter-June-2020/][Fortran Monthly Newsletter (June 2020)]]
- Emacs News
  - [[https://sachachua.com/blog/2020/05/2020-05-11-emacs-news/][2020-06-22]] :: for [[Temporary LaTeX Documents with Orgmode][Temporary LaTeX Documents with Orgmode]]
  - [[https://sachachua.com/blog/2020/05/2020-05-11-emacs-news/][2020-06-15]] :: for [[Emacs for Nix-R][Emacs for Nix-R]]
  - [[https://sachachua.com/blog/2020/05/2020-05-11-emacs-news/][2020-05-11]] :: for [[An Orgmode Note Workflow][An Orgmode Note Workflow]]
  - [[https://sachachua.com/blog/2020/05/2020-05-04-emacs-news/][2020-05-04]] :: for [[Pandoc to Orgmode with Babel][Pandoc to Orgmode with Babel]]
  - [[https://sachachua.com/blog/2020/04/2020-04-27-emacs-news/][2020-04-27]] :: for [[Using Mathematica with Orgmode][Using Mathematica with Orgmode]]
  - [[https://sachachua.com/blog/2020/04/2020-04-13-emacs-news/][2020-04-13]] :: for [[https://dotdoom.rgoswami.me][my dotDoom doom-emacs configuration]]
  - [[https://sachachua.com/blog/2020/04/2020-04-06-emacs-news/][2020-04-06]] :: for [[Replacing Jupyter with Orgmode][Replacing Jupyter with Orgmode]]
** Videos
*** Of Me
- Everything [[https://www.youtube.com/channel/UC9f_UGqNsa60kmNGj_WkLPw?][on my YouTube channel]]
- [[https://www.youtube.com/watch?v=Q1St1VT43sc&feature=youtu.be][Discussion session]] for the CS196A Code in Place AMA with the students
- Panelist for the CarpentryCon 2020 session on [[https://pad.carpentries.org/cchome-teaching-online][lessons learnt from remote teaching]]
- Student Presentation at UI on [[https://hi.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=6cda865b-c58a-4a79-89a3-ab9300d6ede6][Scrum for Software Quality Management]]
- CarpentryCon 2020 [[https://www.youtube.com/watch?v=np6TPzME0aA][session recording]] on "[[https://2020.carpentrycon.org/schedule/#session-10][Reproducible Environments with the Nix Packaging System]]"
*** Including Me
This category involves recordings where I asked questions, and therefore
technically involve me in a sense.
- I appear [[https://www.linkedin.com/posts/univ-ai_neuralnetwork-regularisation-ucla-activity-6689748037511782400-gMSx][in the audience of this clip]] on neural networks and regularization
- Code in Place AMA with [[https://www.youtube.com/watch?v=J7S_SJ2Adi4&list=PLcil-m27rjnZ2f85ao8_EauguYhzkzSQA&index=5&t=0s][Stanford CS Lecturers]]
- I [[http://www.youtube.com/watch?v=-Y_OYhoHOb8][appear (audibly)]] to ask a question for the TUG2020 closing seminar by John MacFarlane
**** HPC Carpentry
- [[https://pad.carpentries.org/hpccarpentry-tour][A guided tour]]
**** Fortran Maintainers Monthly Calls
- [[https://www.youtube.com/watch?v=i-gRNGRzugc][June 2020]]
**** IAS TML Lecture Questions
I've been sitting in on these for a while thanks to [[https://www.math.ias.edu/~ke.li/][Ke Li]], but this section lists some of the lectures I asked a question in
  - "[[https://video.ias.edu/tml/2020/0609-AleksanderMadry][What Do Models Learn?]]" by Aleksander Mądry
  - "[[https://youtu.be/QTnjqdxG99c?t=3879][Langevin Dynamics in Machine Learning]]" by Michael Jordan
  - "[[https://youtu.be/Wx8J-Kw3fTA?t=4814][Graph Nets: The Next Generation]]" by Max Welling
  - "[[https://youtu.be/IbKWTF4MzMY?t=3787][Priors for Semantic Variables]]" by Yoshua Bengio
  - "A Blueprint of Standardized and Composable Machine Learning" by Eric Xing

[fn:whereswaldo] If you do think you have seen me somewhere not on this list, drop me an email
* DONE Search
:PROPERTIES:
:EXPORT_HUGO_SECTION: /
:EXPORT_FILE_NAME: search
:END:
#+begin_src yaml :front_matter_extra t
layout: "search"
outputs:
  - html
  - json
sitemap:
  priority: 0.1
#+end_src
#+begin_description
Search in full-text, the entire contents of the site.
#+end_description
* DONE Categories
:PROPERTIES:
:EXPORT_HUGO_SECTION: /categories/
:EXPORT_FILE_NAME: _index.md
:END:
#+begin_src yaml :front_matter_extra t
mainlist: True
#+end_src
* DONE Tags
:PROPERTIES:
:EXPORT_HUGO_SECTION: /tags/
:EXPORT_FILE_NAME: _index.md
:END:
#+begin_src yaml :front_matter_extra t
mainlist: True
#+end_src
* DONE Site Rationale :@personal:ramblings:explanations:
:PROPERTIES:
:EXPORT_FILE_NAME: rationale
:EXPORT_DATE: 2020-02-11 23:28
:END:
** Why this site exists
I have a lot of online presences. I have been around (or at-least, lurking) for
over ten years. Almost as long as I have been programming. Anyway, I have a
penchant lately for using ~emacs~ and honestly there isn't very good support for
~org-mode~ files. There are options recently with ~gatsby~ as well, but this
seemed kinda neat.
** What 'this' is
- This site is [[http://gohugo.io/][built by Hugo]]
- The posts are [[https://ox-hugo.scripter.co/][generated with ox-hugo]]
- The theme is based of this [[https://github.com/rhazdon/hugo-theme-hello-friend-ng][excellent one]] by Djordje Atlialp, which in turn is based off of this [[https://github.com/panr/hugo-theme-hello-friend][theme by panr]]
    - My modifications [[https://github.com/HaoZeke/hugo-theme-hello-friend-ng-hz][are here]]
** What is here
- Mostly random thoughts I don't mind people knowing
- Some tech stuff which isn't coherent enough to be put in any form with
  references
- Emacs specific workflows which I might want to write about more than [[https://dotdoom.rgoswami.me/][short
  notes on the config]]
** What isn't here
- Some collections should and will go to my [[https://grimoire.science][grimoire]]
- My [[https://dotdoom.rgoswami.me/][doom-emacs configuration]]
- Academic stuff is better tracked on [[https://publons.com/researcher/2911170/rohit-goswami/][Publons]] or [[https://scholar.google.co.in/citations?user=36gIdJMAAAAJ&hl=en][Google Scholar]] or my pages
  hosted by my favorite [[https://femtolab.science/people/rohit][IITK group]] or [[https://www.hi.is/starfsfolk/rog32][UI group]]
* DONE Taming Github Notifications :@notes:tools:github:workflow:
:PROPERTIES:
:EXPORT_FILE_NAME: ghNotif
:EXPORT_DATE: 2020-02-12 11:36
:END:
** Background
As a member of several large organizations, I get a lot of github notifications.
Not all of these are of relevance to me. This is especially true of
~psuedo-monorepo~ style repositories like the [[https://github.com/openjournals/joss-reviews][JOSS review system]] and
*especially* the [[https://github.com/exercism/v3/][exercism community]].

- I recently (re-)joined the [[https://exercism.io/][exercism community]] as a maintainer for the C++
  lessons after having been a (sporadic) teacher
- This was largely in response to a community call to action as the group needed
  new blood to usher in *v3* of the exercism project

Anyway, I have since found that at the small cost of possibly much of my public
repo data, I can manage my notifications better with [[https://octobox.io/][Octobox]]

** Octobox
- It appears to be free for now
- It syncs on demand (useful)
- I can search things quite easily
- They have a neat logo
- There appear to be many features I probably won't use

It looks like this:

#+caption: Octobox Stock Photo
[[file:images/octoboxSample.png]]
* DONE Poetry and Direnv :@programming:tools:direnv:workflow:python:
:PROPERTIES:
:EXPORT_FILE_NAME: poetry-direnv
:EXPORT_DATE: 2020-02-13 21:36
:END:
** Background
- I end up writing about using [[https://python-poetry.org/][poetry]] a lot
- I almost always [[https://direnv.net/][use direnv]] in real life too
- I don't keep writing mini scripts in my ~.envrc~

Honestly there's nothing here anyone using the [[https://github.com/direnv/direnv/wiki/Python][direnv wiki]] will find surprising,
but then it is still neat to link back to.

** Setting Up Poetry
This essentially works by simply modifying the global ~.direnvrc~ which
essentially gets sourced by every local ~.envrc~ anyway.
#+BEGIN_SRC sh
vim $HOME/.direnvrc
#+END_SRC
So what we put in there is the following snippet derived from other snippets [[https://github.com/direnv/direnv/wiki/Python][on
the wiki]], and is actually now there too.

#+BEGIN_SRC bash
# PUT this here
layout_poetry() {
  if [[ ! -f pyproject.toml ]]; then
    log_error 'No pyproject.toml found.  Use `poetry new` or `poetry init` to create one first.'
    exit 2
  fi

  local VENV=$(dirname $(poetry run which python))
  export VIRTUAL_ENV=$(echo "$VENV" | rev | cut -d'/' -f2- | rev)
  export POETRY_ACTIVE=1
  PATH_add "$VENV"
}
#+END_SRC

Now we can just make ~.envrc~ files with ~layout_poetry~ and everything will
/just work™/.

* DONE Replacing Jupyter with Orgmode :@programming:tools:emacs:workflow:orgmode:
:PROPERTIES:
:EXPORT_FILE_NAME: jupyter-orgmode
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:EXPORT_DATE: 2020-02-13 22:36
:END:
** Background
- I dislike Jupyter notebooks (and [[https://jupyter.org/][JupyterHub]]) a lot
- [[https://tkf.github.io/emacs-ipython-notebook/][EIN]] is really not much of a solution either

In the past I have written some posts on [[https://grimoire.science/latex-and-jupyterhub/][TeX with JupyterHub]] and discussed ways
to use virtual [[https://grimoire.science/python-and-jupyterhub/][Python with JupyterHub]] in a more reasonable manner.

However, I personally found that EIN was a huge pain to work with, and I mostly
ended up working with the web-interface anyway.

It is a bit redundant to do so, given that at-least for my purposes, the end
result was a LaTeX document. Breaking down the rest of my requirements went a
bit like this:

- What exports well to TeX? :: *Org*, Markdown, anything which goes into pandoc
- What displays code really well? :: LaTeX, Markdown, *Org*
- What allows easy visualization of code snippets? :: Rmarkdown, RStudio,
  JupyterHub, *Org* with babel

Clearly, [[https://orgmode.org/manual/][orgmode]] is the common denominator, and ergo, a perfect JupyterHub alternative.
** Setup
Throughout this post I will assume the following structure:
#+BEGIN_SRC bash :exports both
tree tmp
mkdir -p tmp/images
touch tmp/myFakeJupyter.org
#+END_SRC

#+RESULTS:
| tmp |                   |   |      |
| ├── | images            |   |      |
| └── | myFakeJupyter.org |   |      |
| 1   | directory,        | 1 | file |

As is evident, we have a folder ~tmp~ which will have all the things we need for
dealing with our setup.

*** Virtual Python
Without waxing too eloquent on the whole reason behind doing this, since I will
rant about virtual python management systems elsewhere, here I will simply
describe my preferred method, which is [[https://python-poetry.org/][using poetry]].

#+BEGIN_SRC bash
# In a folder above tmp
poetry init
poetry add numpy matplotlib scipy pandas
#+END_SRC

The next part is optional, but a good idea if you figure out [[https://direnv.net/][using direnv]] and
have configured ~layout_poetry~ as [[https://rgoswami.me/posts/poetry-direnv][described here]]:
#+BEGIN_SRC bash
# Same place as the poetry files
echo "layout_poetry()" >> .envrc
#+END_SRC

*Note:*
- We can nest an arbitrary number of the ~tmp~ structures under a single place
  we define the poetry setup
- I prefer using ~direnv~ to ensure that I never forget to hook into the right environment
** Orgmode
This is not an introduction to org, however in particular, there are some basic
settings to keep in mind to make sure the set-up works as expected.

*** Indentation
Python is notoriously weird about whitespace, so we will ensure that our export
process does not mangle whitespace and offend the python interpreter. We will
have the following line at the top of our ~orgmode~ file:

#+BEGIN_SRC orgmode :tangle tmp/myFakeJupyter.org :exports code
# -*- org-src-preserve-indentation: t; org-edit-src-content: 0; -*-
#+END_SRC

*Note:*
- this post is actually generating the file being discussed here by
[[https://orgmode.org/manual/Extracting-Source-Code.html][tangling the file]]
- You can get the [[https://github.com/HaoZeke/haozeke.github.io/blob/src/content-org/tmp/myFakeJupyter.org][whole file here]]
*** TeX Settings
These are also basically optional, but at the very least you will need the
following:

#+BEGIN_SRC orgmode :tangle tmp/myFakeJupyter.org
#+author: Rohit Goswami
#+title: Whatever
#+subtitle: Wittier line about whatever
#+date: \today
#+OPTIONS: toc:nil
#+END_SRC

I actually use a lot of math using the ~TeX~ input mode in Emacs, so I like the
following settings for math:

#+BEGIN_SRC orgmode :tangle tmp/myFakeJupyter.org
# For math display
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{unicode-math}
#+END_SRC

There are a bunch of other settings which may be used, but these are the bare
minimum, more on that would be in a snippet anyway.

*Note:*
- rendering math in the ~orgmode~ file in this manner requires that we
 use ~XeTeX~ to compile the final file
*** Org-Python
We essentially need to ensure that:
- Babel uses our virtual python
- The same session is used for each block

We will get our poetry python pretty easily:
#+BEGIN_SRC bash
which python
#+END_SRC

#+RESULTS:
: /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python

Now we will use this as a common ~header-arg~ passed into the property drawer to
make sure we don't need to set them in every code block.

We can use the following structure in our file:

#+BEGIN_SRC orgmode :tangle tmp/myFakeJupyter.org :exports code
\* Python Stuff
  :PROPERTIES:
  :header-args:    :python /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python :session One :results output :exports both
  :END:
Now we can simply work with code as we normally would
\#+BEGIN_SRC python
print("Hello World")
\#+END_SRC
#+END_SRC

*Note:*
- For some reason, this property needs to be set on *every* heading (as of Feb 13 2020)
- In the actual file you will want to remove extraneous  \ symbols:
  - \* → *
  - \#+BEGIN_SRC → #+BEGIN_SRC
  - \#+END_SRC → #+END_SRC
*** Python Images and Orgmode
To view images in ~orgmode~ as we would in a JupyterLab notebook, we will use a
slight trick.
- We will ensure that the code block returns a file object with the arguments
- The code block should end with a print statement to actually generate the file
  name

 So we want a code block like this:

#+begin_example
#+BEGIN_SRC python :results output file :exports both
import matplotlib.pyplot as plt
from sklearn.datasets.samples_generator import make_circles
X, y = make_circles(100, factor=.1, noise=.1)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plt.xlabel('x1')
plt.ylabel('x2')
plt.savefig('images/plotCircles.png', dpi = 300)
print('images/plotCircles.png') # return filename to org-mode
#+end_src
#+end_example

Which would give the following when executed:

#+begin_example
#+RESULTS:
[[file:images/plotCircles.png]]
#+end_example

Since that looks pretty ugly, this will actually look like this:

#+BEGIN_SRC python :results output file :exports both
import matplotlib.pyplot as plt
from sklearn.datasets.samples_generator import make_circles
X, y = make_circles(100, factor=.1, noise=.1)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plt.xlabel('x1')
plt.ylabel('x2')
plt.savefig('images/plotCircles.png', dpi = 300)
print('images/plotCircles.png') # return filename to org-mode
#+end_src

[[file:tmp/images/plotCircles.png]]

*** Bonus
A better way to simulate standard ~jupyter~ workflows is to just specify the
properties once at the beginning.

#+BEGIN_SRC orgmode
#+PROPERTY: header-args:python :python /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python :session One :results output :exports both
#+END_SRC

This setup circumvents having to set the properties per sub-tree, though for
very large projects, it is useful to use different processes.
** Conclusions
- The last step is of course to export the file as to a ~TeX~ file and then
  compile that with something like ~latexmk -pdfxe -shell-escape file.tex~

There are a million and one variations of this of course, but this is enough to
get started.

The whole file is also [[https://github.com/HaoZeke/haozeke.github.io/blob/src/content-org/tmp/myFakeJupyter.org][reproduced here]].

** Comments
The older commenting system was implemented with [[https://utteranc.es][utteranc.es]] as seen below.
@@html:<script src="https://utteranc.es/client.js" repo="haozeke/haozeke.github.io" issue-term="pathname" theme="photon-dark" label="Utterance 💬" crossorigin="anonymous" async></script>@@

* TODO Orgmode and Hugo :@programming:tools:emacs:webdev:hugo:
:PROPERTIES:
:EXPORT_FILE_NAME: hugo-orgmode
:END:
** Background
- This is about the site you are reading
- It is also a partial rant
- It has a lot to do with web development in general
* DONE Switching to Colemak :@personal:workflow:explanations:
:PROPERTIES:
:EXPORT_FILE_NAME: colemak-switch
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:EXPORT_DATE: 2020-02-29 14:06
:EXPORT_HUGO_AUTO_SET_LASTMOD: t
:END:
#+BEGIN_QUOTE
Thoughts on and rationale behind leaving QWERTY and touch typing in general. Followed [[Refactoring Dotfiles For Colemak][by this post]] on refactoring my Dotfiles.
#+END_QUOTE

** Background
I just realized that it has been over two years since I switched from QWERTY to
Colemak but somehow never managed to write about it. It was a major change in my
life, and it took forever to get acclimatized to. I do not think I'll ever again be
in a position to make such a change in my life again, but it was definitely
worth it.
** Touch Typing
My interest in touch typing in I decided to digitize my notes for posterity, during the
last two years of my undergraduate studies back in Harcourt Butler Technical
Institute (HBTI) Kanpur, India. in one of my many instances of yak shaving, I
realized I could probably consume and annotate a lot more content by typing
faster. Given that at that stage I was already a fast talker, it seemed like a
natural extension. There was probably an element of nostalgia involved as well.
That and the end of a bachelors involves the thesis, which generally involves a
lot of typing.

There were (and are) some fantastic resources for learning to touch type
nowadays, I personally used:
- [[https://www.typing.com/][Typing.com]] :: This is short, but a pretty good basic setup. The numbering and
  special characters are a bit much to take in at the level of practice you get
  by completing all the previous exercises, but eventually they make for a good workout.
- [[https://www.typingclub.com/][TypingClub]] :: This is what I ended up working my way through. It is
  comprehensive, beautiful, and fun.

Also, later, I ended up using [[https://www.keybr.com/][keybr]] a lot, simply because typing gibberish is a
good way of practicing, and it is independent of the keyboard layout.

Just to foreshadow things, the enemy facing me at this point was the layout
itself[fn:img] .

https://www.keyboard-design.com/kb-images/qwerty-kla.jpg

** Alternate layouts
Having finally broken into the giddy regimes of 150+ wpm, I was ecstatic, and
decided to start working my way through some longer reports. However, I quickly
realized I was unable to type for more than a couple of minutes without getting
terribly cramped. Once it got to the point of having to visit a physiotherapist,
I had to call it quits. At that stage, relearning the entire touch typing
corpus, given that I already was used to QWERTY, seemed pretty bleak.

It took forever, and I ended up applying my choices to my phone keyboard as
well, which presumably helped me in terms of increasing familiarity, had the
unintended effect of making me seem distant to people I was close to, since my
verbose texts suddenly devolved to painful one-liners.

The alternative layouts I tried were:

- [[https://www.dvorak-keyboard.com/][DVORAK]] :: At the time, TypingClub only supported QWERTY and DVORAK, so it was
  pretty natural for me to try it out. There are also some [[https://www.dvzine.org/][very nice comics
  about it]]. I remember that it was pretty neat, with
  a good even distribution, until I tried coding. The placement of the
  semicolons make it impossible to use while programming. I would still say it
  makes for a comfortable layout, as long as special characters are not required.

https://www.keyboard-design.com/kb-images/dvorak-kla.jpg

- [[http://mkweb.bcgsc.ca/carpalx][CarpalX]] :: I experimented with the entire carpalx family, but I was unable to get
  used to it. I liked QFMLWY best. I do recommend reading the training methodology, especially if
  anyone is interested in numerical optimization in general. More importantly,
  though it was relatively easy to set up on my devices and operating systems,
  the fact that it wasn't natively supported meant a lot of grief whenever I
  inevitably had to use a public computer.

https://www.keyboard-design.com/kb-images/qgmlwy-kla.jpg

- Colemak :: Eventually I decided to go with [[https://colemak.com/][Colemak]], especially since it is
  widely available. Nothing is easier than ~setxkbmap us -variant colemak -option grp:alt_shift_toggle~ on public machines and it's easy on Windows as
  well. Colemak seems like a good compromise. I personally have not been able to
  reach the same speeds I managed with QWERTY, even after a year, but then
  again, I can be a lot more consistent, and it hurts less. Nowadays, Colemak
  has made its way onto most typing sites as well, including TypingClub

https://www.keyboard-design.com/kb-images/colemak-kla.jpg

*** What about VIM?
- DVORAK makes it impossible, so do most other layouts, but there are some
  tutorials purporting to help use vim movement with DVORAK
- Colemak isn't any better, but the fact of the matter is that once you know VIM
  on QWERTY, and have separately internalized colemak or something else, hitting
  keys is just hitting keys

+All that said, I still occasionally simply remap HJKL (QWERTY movement) to HNEI (Colemak analog) when it is feasible.+
*update:* I actually ended up refactoring my entire Dotfiles to use more Colemak native bindings, as described [[Refactoring Dotfiles For Colemak][in this post]].
** Conclusion
Changing layouts was a real struggle. Watching my WPM drop back to lower than
hunt and peck styles was pretty humiliating, especially since the reports kept
coming in, and more than once I switched to QWERTY. However, since then, I have
managed to stay on course. I guess if I think about it, it boils down to a few
scattered thoughts:
- Typing is kinda like running a marathon, knowing how it is done and doing it
  are two different things
- Tell *everyone*, so people can listen to you lament your reduced speed and not
  hate you for replying slowly
- Practice everyday, because, well, it works out in the long run, even when you
  plateau
- Alternate shifts! That's really something which should show up more in
  tutorials, especially for listicles, not changing the shifts will really hurt
- Try and get a mechanical keyboard (like the [[https://www.annepro.net/][Anne Pro 2]] or the [[https://www.coolermaster.com/catalog/peripheral/keyboards/masterkeys-pro-l-white/][Coolermaster Masterkeys]]), they're fun and easy to change layouts on

** Comments
The older commenting system was implemented with [[https://utteranc.es][utteranc.es]] as seen below.
@@html:<script src="https://utteranc.es/client.js" repo="haozeke/haozeke.github.io" issue-term="pathname" theme="photon-dark" label="Utterance 💬" crossorigin="anonymous" async></script>@@

[fn:img] The images are [[https://www.keyboard-design.com/best-keyboard-layouts.html][from here]], where there's also an effort based metric
used to score keyboard layouts.
* TODO Replacing Rstudio with Emacs :@programming:tools:emacs:workflow:R:
:PROPERTIES:
:EXPORT_FILE_NAME: rstudio-emacs
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:EXPORT_DATE: 2020-02-15 04:38
:END:
** Background
RStudio is one of the best IDEs around, in that it is essentially a text editor
and terminal with some pretty printing and object viewing functionality. It is
really great, but it is also relatively resource intensive. It turns out that
thanks to Emacs ESS, it is possible to circumvent Rstudio completely in favor of
an Emacs-native workflow.
* TODO Role models and colleges
* TODO My current courses
* TODO Rude college admissions
* DONE Pandora and Proxychains :@personal:tools:workflow:
:PROPERTIES:
:EXPORT_FILE_NAME: pandora-proxychains
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :comments true
:EXPORT_DATE: 2020-02-15 05:28
:END:
** Background
- Pandora doesn't work outside the states
- I keep forgetting how to set-up ~proxychains~
** Proxychains
Technically this article [[https://github.com/rofl0r/proxychains-ng][expects proxychains-ng]], which seems to be the more
up-to-date fork of the original ~proxychains~.

1. Install ~proxychains-ng~
   #+BEGIN_SRC bash
# I am on archlinux..
sudo pacman -S proxychains-ng
   #+END_SRC
2. Copy the configuration to the ~$HOME~ directory
   #+BEGIN_SRC bash
cp /etc/proxychains.conf .
   #+END_SRC
3. Edit said configuration to add some US-based proxy

In my particular case, I don't keep the tor section enabled.
#+BEGIN_SRC bash :exports both :results raw
tail $HOME/proxychains.conf
#+END_SRC

#+RESULTS:
#+begin_example
#
#       proxy types: http, socks4, socks5
#        ( auth types supported: "basic"-http  "user/pass"-socks )
#
[ProxyList]
# add proxy here ...
# meanwile
# defaults set to "tor"
# socks4 	127.0.0.1 9050
#+end_example

I actually use [[https://windscribe.com][Windscribe]] for my VPN needs, and they have a neat [[https://windscribe.com/getconfig/socks][SOCKS5 proxy
setup]]. This works out to a line like ~socks5 $IP $PORT $USERNAME $PASS~ being
added. The default generator gives you a pretty server name, but to get the IP
I use ~ping $SERVER~ and put that in the ~conf~ file.
** Pandora
I use the excellent ~pianobar~ frontend.
1. Get [[https://github.com/PromyLOPh/pianobar][pianobar]]
   #+BEGIN_SRC bash
sudo pacman -S pianobar
   #+END_SRC
2. Use it with ~proxychains~
   #+BEGIN_SRC bash
proxychains pianobar
   #+END_SRC
3. Profit

I also like setting up some defaults to make life easier:
#+BEGIN_SRC bash
mkdir -p ~/.config/pianobar
vim ~/.config/pianobar/config
#+END_SRC
I normally set the following (inspired by the [[https://wiki.archlinux.org/index.php/Pianobar][ArchWiki]]):
#+BEGIN_SRC conf
audio_quality = {high, medium, low}
autostart_station = $ID
password = "$PASS"
user = "$emailID"
#+END_SRC

The ~autostart_station ID~ can be obtained by inspecting the terminal output
during an initial run. I usually set it to the QuickMix station.
* Bojack Horseman :@personal:thoughts:random:review:TV:
:PROPERTIES:
:EXPORT_FILE_NAME: bojack-horseman
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :comments false
:EXPORT_DATE: 2020-02-27 22:28
:END:
** Background
For a while I was worried about writing about a TV show here. I thought it might
be frivolous, or worse, might outweigh the other kinds of articles I would like
to write. However, like most things, that which is ignored just grows, so it is
easier to just write and forget about it.
** The Show
Much has been said about how Bojack Horseman is one of the best shows ever, and
they're all correct. For that matter I won't be going into the details of how
every episode ties together a tapestry of lives in a meaningful way, or any of
that. The show was amazingly poignant. The characters felt real. Which actually
leads me to the real issue.
** The End
The end of Bojack was *good*. It was the way it was meant to be. For a
slice-of-life show, it is a natural conclusion. It isn't necessary that any
catharsis occurs or that the characters change or become better or all that
jazz. It isn't about giving the viewers closure. It is simply about a window
onto the lives of (fictional) characters being shut. To that end, I disliked
attempts to bring closure in the show itself.

One of the main reasons why I felt strongly enough to write this, is simply
because when I looked around, the prevailing opinion was that the main character
should have been killed off, _for his sins_. This strikes me as a very flippant
attitude to take. It reeks of people trying to make the show a cautionary tale,
which is frankly speaking a weird approach to take towards any fictional story.
The idea that the character should be redeemed also seemed equally weak, for
much the same reasons.

The fact that the characters are hypocrites, and that none of them are as good
or bad as they make themselves out to be is one of the best parts of the show.

** Conclusion
That's actually all I have to say about this. I thought of adding relevant memes
or listing episodes or name dropping sites, but this isn't buzzfeed. The show is
incredible, and there are far better ways of proving that. Bust out your
favorite search engine + streaming content provider / digital piracy eye-patch
and give it a whirl. The only thing I'd suggest is watching everything in order,
it's just that kind of show.

* TODO The Morpho Language :@programming:review:
:PROPERTIES:
:EXPORT_FILE_NAME: morpho-lang
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:END:
* TODO Towards DOOM-Emacs :@programming:workflow:review:
:PROPERTIES:
:EXPORT_FILE_NAME: towards-doom-emacs
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments false
:END:
** Background
[[https://dotdoom.rgoswami.me/][My doom-emacs configuration]] gets a rather insane number of views every month.
Statistically, it accounts for 90% of the traffic to [[https://grimoire.science][my other site]], and that is
essentially around three times time traffic on the rest of my presences,
combined. I followed a pretty standard path to finally reach doom-emacs.
However, before delving into it, I thought I'd discuss the chronological aspects
of my road to doom. In a nutshell it was just:

Word → Notepad++ → Sublime Text 3 → VIM → Emacs (Spacemacs) → Emacs (doom-emacs)
* DONE Provisioning Dotfiles on an HPC :@programming:workflow:projects:hpc:
:PROPERTIES:
:EXPORT_FILE_NAME: prov-dots
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:EXPORT_DATE: 2020-03-16 00:06
:END:
** Background
[[https://github.com/HaoZeke/Dotfiles][My dotfiles]] turned 4 years old a few months ago (since 9th Jan 2017) and remains one of my most
frequently updated projects for obvious reasons. Going through the changes
reminds me of a whole of posts I never got around to writing.

Anyway, recently I gained access to another HPC cluster, with a standard configuration
(bash, old CentOS) and decided to track my provisioning steps. This is really a
very streamlined experience by now, since I've used the same setup across scores
of machines. This is actually also a generic intro to configuring user setups on
HPC (high performance cluster) machines, if one is inclined to read it in that
manner. To that end, sections of this post involve restrictions relating to user
privileges which aren't normally part of most Dotfile setups.
*** Aside
- Dotfiles define most people who maintain them
- No two sets are ever exactly alike
- They fall somewhere between winging it for each machine and using something
  like [[https://www.habitat.sh/learn/][Chef]] or [[https://www.ansible.com/][Ansible]]
- Tracking dotfiles is really close to having a sort of out-of-context journal

Before I settled on using [[https://github.com/kobus-v-schoor/dotgit][the fabulous dotgit]], I considered several
alternatives, most notably [[https://www.gnu.org/software/stow/][GNU stow]].
** Preliminaries
It is important to note the environment into which I had to get my
setup.
*** SSH Setup
- The very first thing to do is to use a new ~ssh-key~
#+BEGIN_SRC bash
export myKey="someName"
ssh-keygen -f $HOME/.ssh/$myKey
# I normally don't set a password
ssh-add $HOME/.ssh/$myKey
ssh-copy-id $myHPC
# myHPC being an IP address
#+END_SRC
I more often than not tend to back this up with a cutesy alias, also because I
do not always get my username of choice on these machines. So in
~$HOME/.ssh/config~ I use:
#+BEGIN_SRC conf
Host myHPC
 Hostname 127.0.0.1
 User somethingIgot
 IdentityFile ~/.ssh/myKey
#+END_SRC
*** Harvesting Information
- I normally use [[https://github.com/dylanaraps/neofetch][neofetch]] on new machines
#+BEGIN_SRC bash
mkdir -p $HOME/Git/Github
cd $HOME/Git/Github
git clone https://github.com/dylanaraps/neofetch.git
cd neofetch
./neofetch
#+END_SRC

#+caption: Neofetch Output
[[file:images/sampleHPC.png]]

Where the top has been tastefully truncated. Just for context, the latest ~bash~
as of this writing is ~v5.0.16~ so, that's not too bad, given that ~neofetch~
works for ~bash~ ≥ 3.2

** TODO Circumventing User Restrictions with Nix
- A post in and of itself would be required to explain why and how users are
  normally restricted from activities in cluster nodes
- Here, we leverage the [[https://nixos.org/nix/manual/#chap-installation][nix-package management system]] to circumvent these
- User installation of ~nix~ is sadly non-trivial, so this might be of some use[fn:nixUsr]
*** Testing nix-user-chroot
1. We will first check namespace support
#+BEGIN_SRC bash
# Errored out
unshare --user --pid echo YES
# Worked!
zgrep CONFIG_USER_NS /boot/config-$(uname -r)
# CONFIG_USER_NS=y
#+END_SRC

Thankfully we have support for namespaces, so we can continue with ~nix-user-chroot~.

2. Since we definitely do not have ~rustup~ or ~rustc~ on the HPC, we will use [[https://github.com/nix-community/nix-user-chroot/releases][a
   prebuilt binary]] of ~nix-user-chroot~

#+BEGIN_SRC bash
cd $HOME && wget -O nix-user-chroot  https://github.com/nix-community/nix-user-chroot/releases/download/1.0.2/nix-user-chroot-bin-1.0.2-x86_64-unknown-linux-musl
#+END_SRC

3. Similar to [[https://nixos.wiki/wiki/Nix_Installation_Guide#Installing_without_root_permissions][the wiki example]], we will use ~$HOME/.nix~

#+BEGIN_SRC bash
cd ~/
chmod +x nix-user-chroot
mkdir -m 0755 ~/.nix
./nix-user-chroot ~/.nix bash -c 'curl https://nixos.org/nix/install | sh'
#+END_SRC

- Only, this *doesn't work*

Turns out that since ~unshare~ is too old, ~nix-user-chroot~ won't work either.

*** Using PRoot
PRoot is pretty neat in general, they even have a [[https://proot-me.github.io/][nice website describing it]].
0. Set a folder up for local installations (this is normally done by my
   Dotfiles, but we might as well have one here too)
#+BEGIN_SRC bash
mkdir -p $HOME/.local/bin
export PATH=$PATH:$HOME/.local/bin
#+END_SRC
1. Get a binary from the [[https://gitlab.com/proot/proot/-/jobs][GitLab artifacts]]
#+BEGIN_SRC bash
cd $HOME
mkdir tmp
cd tmp
wget -O artifacts.zip https://gitlab.com/proot/proot/-/jobs/452350181/artifacts/download
unzip artifacts.zip
mv dist/proot $HOME/.local/bin
#+END_SRC
2. Bind and install ~nix~
#+BEGIN_SRC bash
mkdir ~/.nix
export PROOT_NO_SECCOMP=1
proot -b ~/.nix:/nix
export PROOT_NO_SECCOMP=1
curl https://nixos.org/nix/install | sh
#+END_SRC

If you're very unlucky, like I was, you may be greeted by a lovely little error
message along the lines of:

#+begin_example
/nix/store/ddmmzn4ggz1f66lwxjy64n89864yj9w9-nix-2.3.3/bin/nix-store: /opt/ohpc/pub/compiler/gcc/5.4.0/lib64/libstdc++.so.6: version `GLIBCXX_3.4.22' not found (required by /nix/store/c0b76xh2za9r9r4b0g3iv4x2lkw1zzcn-aws-sdk-cpp-1.7.90/lib/libaws-cpp-sdk-core.so)
#+end_example

Which basically is as bad as it sounds. At this stage, we need a newer compiler
to even get ~nix~ up and running, but can't without getting an OS update. This
chicken and egg situation calls for the drastic measure of leveraging ~brew~
first[fn:brewStuff].

#+BEGIN_SRC bash
sh -c "$(curl -fsSL https://raw.githubusercontent.com/Linuxbrew/install/master/install.sh)"
#+END_SRC

Note that nothing in this section suggests the best way is not to lobby your
sys-admin to install ~nix~ system-wide in multi-user mode.
** Giving Up with Linuxbrew
- Somewhere around this point, [[https://docs.brew.sh/Homebrew-on-Linux][linuxbrew]] is a good idea
- More on this later
** Shell Stuff
~zsh~ is my shell of choice, and is what my ~Dotfiles~ expect and work best with.
- I did end up making a quick change to update the ~dotfiles~ with a target
  which includes a snippet to transition to ~zsh~ from the default ~bash~ shell
** Dotfiles
The actual installation steps basically tracks [[https://github.com/HaoZeke/Dotfiles][the readme instructions]].

#+BEGIN_SRC bash
git clone https://github.com/kobus-v-schoor/dotgit.git
mkdir -p ~/.bin
cp -r dotgit/old/bin/dotgit* ~/.bin
cat dotgit/old/bin/bash_completion >> ~/.bash_completion
rm -rf dotgit
# echo 'export PATH="$PATH:$HOME/.bin"' >> ~/.bashrc
echo 'export PATH="$PATH:$HOME/.bin"' >> ~/.zshrc
#+END_SRC

[fn:nixUsr] Much of this section is directly adapted from [[https://nixos.wiki/wiki/Nix_Installation_Guide#Installing_without_root_permissions][the NixOS wiki]]
[fn:brewStuff] This used to be called linuxbrew, but the [[https://docs.brew.sh/Homebrew-on-Linux][new site]] makes it clear
that it's all one ~brew~ now.
* DONE Shorter Posts :@notes:tools:rationale:workflow:ideas:
:PROPERTIES:
:EXPORT_FILE_NAME: shortpost
:EXPORT_DATE: 2020-03-16 00:16
:END:
** Background
Sometime this year, I realized that I no longer have access to a lot of my older
communication. This included, a lot of resources I enjoyed and shared with the
people who were around me at that point in time. To counter this, I have decided
to opt for shorter posts, even if they don't always include the same level of
detail I would prefer to provide.

*** Alternatives
- I have an automated system based around IFTTT combined with Twitter, Diigo,
  and even Pocket
- However, that doesn't really tell me much, and trawling through a massive glut
  of data is often pointless as well
- There's always Twitter, but I don't really care to hear the views of others
  when I want to revisit my own ideas
** Conclusions
- I will be making shorter posts here, like the random one on [[https://rgoswami.me/posts/ghnotif/][octobox]]
* DONE D3 for Git :@notes:tools:rationale:workflow:ideas:
:PROPERTIES:
:EXPORT_FILE_NAME: d3git
:EXPORT_DATE: 2020-03-16 00:17
:END:
** Background
- I have had a lot of discussions regarding the teaching of ~git~
- This is mostly as a part of [[https://static.carpentries.org/maintainers/#HaoZeke][the SoftwareCarpentries]], or in view of my
  [[https://www.univ.ai/teams/rohit-goswami][involvement with univ.ai]], or simply in every public space I am associated with
- Without getting into my views, I just wanted to keep this resource in mind
** The site
- Learning ~git~ is a highly contentious thing
- People seem to be fond of GUI tools, especially since on non *nix systems, it
  seems that there is a lot of debate surrounding obtaining the ~git~ utility in
  the first place

One of the best ways of understanding (without installing stuff) the mental
models required for working with ~git~ is [[https://onlywei.github.io/explain-git-with-d3/#checkout][this site]]

#+caption: A screenshot of the site
[[file:images/d3git.png]]

- However, as is clear, this is not exactly a replacement for a good old command-line.

- It does make for a good resource for teaching with slides, or for generating
  other static visualizations, where live coding is not an option
* DONE Trees and Bags :@notes:theory:statistics:math:
:PROPERTIES:
:EXPORT_FILE_NAME: trees-and-bags
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:EXPORT_HUGO_CUSTOM_FRONT_MATTER+: :link-citations true
:EXPORT_HUGO_PANDOC_CITATIONS: t
:EXPORT_DATE: 2020-03-26 00:28
:END:
# :EXPORT_HUGO_CUSTOM_FRONT_MATTER+: :nocite '(@hastieElementsStatisticalLearning2009)

#+BEGIN_QUOTE
  Explain why using bagging for prediction trees generally improves
  predictions over regular prediction trees.
#+END_QUOTE


** Introduction

Bagging (or Bootstrap Aggregation) is one of the most commonly used
ensemble method for improving the prediction of trees. We will broadly
follow a historical development trend to understand the process. That
is, we will begin by considering the Bootstrap method. This in turn
requires knowledge of the Jacknife method, which is understandable from
a simple bias variance perspective. Finally we will close out the
discussion by considering the utility and trade-offs of the Bagging
technique, and will draw attention to the fact that the Bagging method
was contrasted to another popular ensemble method, namely the Random
Forest method, in the previous section.

Before delving into the mathematics, recall that the approach taken by
bagging is given as per @cichoszDataMiningAlgorithms2015 to be:

- create base models with *bootstrap* samples of the training set
- combine models by unweighted voting (for classification) or by
  averaging (for regression)

The reason for covering the Jacknife method is to develop an intuition
relating to the sampling of data described in the following table:

| Data-set   Size  per   sample | Estimator         |
| Reduces                       | Jacknife          |
| Remains    the   same         | Bootstrap         |
| Increases                     | data-augmentation |

** Bias Variance Trade-offs

We will recall, for this discussion, the bias variance trade off which
is the basis of our model accuracy estimates (for regression) as per the
formulation of @jamesIntroductionStatisticalLearning2013.

\begin{equation}
E(y₀-\hat{f}(x₀))²=\mathrm{Var}(\hat{f}(x₀))+[\mathrm{Bias(\hat{f(x₀)})}]²+\mathrm{Var}(ε)
\end{equation}

Where:

- $E(y_{0}-\hat{f}(x_{0}))²$ is the expected test MSE, or the average
  test MSE if $f$ is estimated with a large number of training sets and
  tested at each $x₀$
- The variance is the amount by which our approximation $\hat{f}$ will
  change if estimated by a different training set, or the *flexibility*
  error
- The bias is the (reducible) *approximation* error, caused by not
  fitting to the training set exactly
- $\mathrm{Var}(ε)$ is the *irreducible* error

We will also keep in mind, going forward the following requirements of a
good estimator:

- Low variance AND low bias
- Typically, the variance increases while the bias decreases as we use
  more flexible methods (i.e. methods which fit the training set
  better[fn:smooth])

Also for the rest of this section, we will need to recall from
@hastieElementsStatisticalLearning2009, that the bias is given by:

\begin{equation}
[E(\hat{f_{k}}(x₀)-f(x₀)]²
\end{equation}

Where the expectation averages over the randomness in the training data.

To keep things in perspective, recall from
@hastieElementsStatisticalLearning2009:

#+CAPTION: Test and training error as a function of model complexity
[[file:images/biasVar.png]]

** Jacknife Estimates
    :PROPERTIES:
    :CUSTOM_ID: jacknife-estimates
    :END:

We will model our discussion on the work of
@efronJackknifeBootstrapOther1982. Note that:

- The $\hat{θ}$ symbol is an estimate of the true quantity $θ$
- This is defined by the estimate being $\hat{θ}=θ(\hat{F})$
- $\hat{F}$ is the empirical probability distribution, defined by mass
  $1/n$ at $xᵢ ∀ i∈I$, i is from 1 to n

The points above establishes our bias to be given by
$E_Fθ(\hat{F})-θ(F)$ such that $E_F$ is the expectation under x₁⋯xₙ~F.

To derive the Jacknife estimate $(\tilde{θ})$ we will simply
sequentially delete points xᵢ (changing $\hat{F}$), and recompute our
estimate $\hat{θ}$, which then simplifies to:

\begin{equation}
\tilde{θ}\equiv n\hat{θ}-(\frac{n-1}{n})∑_{i=1}ⁿ\hat{θ}
\end{equation}

In essence, the Jacknife estimate is obtained by making repeated
estimates on increasingly smaller data-sets. This intuition lets us
imagine a method which actually makes estimates on larger data-sets
(which is the motivation for data augmentation) or, perhaps not so
intuitively, on estimates on data-sets of the same size.

** Bootstrap Estimates

Continuing with the same notation, we will note that the bootstrap is
obtained by draw random data-sets with replacement from the training
data, where each sample is the same size as the original; as noted by @hastieElementsStatisticalLearning2009.

We will consider the bootstrap estimate for the standard deviation of
the $\hat{θ}$ operator, which is denoted by $σ(F,n,\hat{\theta})=σ(F)$

The bootstrap is simple the standard deviation at the approximate F,
i.e., at $F=\hat{F}$:

\begin{equation}
\hat{\mathrm{SD}}=\sigma(\hat{F})
\end{equation}

Since we generally have no closed form analytical form for $σ(F)$ we
must use a Monte Carlo algorithm:

1. Fit a non parametric maximum likelihood estimate (MLE) of F,
   i.e. $\hat{F}$
2. Draw a sample from $\hat{F}$ and calculate the estimate of $\hat{θ}$
   on that sample, say, $\hat{θ}^*$
3. Repeat 2 to get multiple (say B) replications of $\hat{θ}^*$

Now we know that as $B→∞$ then our estimate would match $σ(\hat{F})$
perfectly, however, since that itself is an estimate of the value we are
actually interested in, in practice there is no real point using a very
high B value.

Note that in actual practice we simply use the given training data with
repetition and do not actually use an MLE of the approximate true
distribution to generate samples. This causes the bootstrap estimate to
be unreasonably good, since there is always significant overlap between
the training and test samples during the model fit. This is why cross
validation demands non-overlapping data partitions.

*** Connecting Estimates

The somewhat surprising result can be proved when $\hat{θ}=θ(\hat{F}$ is
a quadratic functional, namely:

\begin{equation}\hat{\mathrm{Bias}}_{boot}=\frac{n-1}{n} \hat{\mathrm{Bias}}_{jack}\end{equation}

In practice however, we will simply recall that the Jacknife tends to
overestimate, and the Bootstrap tends to underestimation.

** Bagging

Bagging, is motivated by using the bootstrap methodology to improve the
estimate or prediction directly, instead of using it as a method to
asses the accuracy of an estimate. It is a representative of the
so-called parallel ensemble methods where the base learners are
generated in parallel. As such, the motivation is to reduce the error by
exploiting the independence of base learners (true for mathematically
exact bootstrap samples, but not really true in practice).

Mathematically the formulation of @hastieElementsStatisticalLearning2009
establishes a connection between the Bayesian understanding of the
bootstrap mean as a posterior average, however, here we will use a more
heuristic approach.

We have noted above that the bagging process simply involves looking at
different samples in differing orders. This has some stark repercussions
for tree-based methods, since the trees are grown with a /greedy/
approach.

- Bootstrap samples may cause different trees to be produced
- This causes a reduction in the *variance*, especially when not too
  many samples are considered
- Averaging, reduces variance while leaving bias unchanged

Practically, these separate trees being averaged allows for varying
importance values of the variables to be calculated.

In particular, following @hastieElementsStatisticalLearning2009, it is
possible to see that the MSE tends to decrease by bagging.

\begin{align}
 E_P[Y-\hat{f}^*(x)]² & = & E_P[Y-f*{ag}(x)+f^*_{ag}(x)-\hat{f}^*(x)]² \\
& = & E_P[Y-f^*_{ag}(x)]²+E_P[\hat{f}^*(x)-f^*_{ag}(x)]² ≥ E_P[Y-f^*_{ag}(x)]²
\end{align}

Where:

- The training observations are independently drawn from a distribution
  $P$
- $f_{ag}(x)=E_P\hat{f}^*(x)$ is the ideal aggregate estimator

For the formulation above, we assume that $f_{ag}$ is a true bagging
estimate, which draws samples from the actual population. The upper
bound is obtained from the variance of the $\hat{f}^*(x)$ around the
mean, $f_{ag}$

Practically, we should note the following:

- The regression trees are deep
- The greedy algorithm growing the trees cause them to be unstable
  (sensitive to changes in input data)
- Each tree has a high variance, and low bias
- Averaging these trees reduces the variance

Missing from the discussion above is how exactly the training and test
sets are used in a bagging algorithm, as well as an estimate for the
error for each base learner. This has been reported in the code above as
the OOB error, or out of bag error. We have, as noted by
@zhouEnsembleMethodsFoundations2012 and @breimanBaggingPredictors1996
the following considerations.

- Given $m$ training samples, the probability that the iᵗʰ sample is
  selected 0,1,2... times is approximately Poisson distributed with
  $λ=1$
- The probability of the iᵗʰ example will occur at least once is then
  $1-(1/e)≈0.632$
- This means for each base learner, there are around $36.8$ % original
  training samples which have not been used in its training process

The goodness can thus be estimated using these OOB error, which is
simply an estimate of the error of the base tree on the OOB samples.

As a final note, random forests are conceptually easily understood by combining
bagging with subspace sampling, which is why in most cases and packages, we used
bagging as a special case of random forests, i.e. when no subspace sampling is
performed, random forests algorithms perform bagging.


[fn:smooth] This is mostly true for reasonably smooth true functions
* TODO d-SEAMS got published
* TODO Why I don't cure cancer
With the coronavirus pandemic going on, some of the louder rabble of the
academic community (as evinced by Twitter) have been calling for the shut down
of non-essential work. The real reason why it doesn't matter if there's a
pandemic going on is simply because the work keeps you up anyway. Working on
projects you love is like carrying a pandemic around with you all the time. It
is impossible to let go of in the first place. Understandably not everyone works
like this, and there are as many reasons to be on a project as there are people
probably.
* TODO Machine Learning is not the future
- I dislike machine learning in terms of scientific achievement
- Competitions are no way to bring a field forward
- State of the art on a month to month basis is a very poor way of understanding
  any field
- The ability to provide direct industrial applications is probably why this is
  so popular

This kind of behavior would be pretty unthinkable in other fields. The push to
clear a benchmark simply discards the basic ideas behind learning a subject in
the first place.
* TODO The net is not for socializing
- I used to go online to be an idea, an embodiment of an idea
Nowadays we bring our selves to the internet and I don't think that is as
liberating as the older format.
* DONE Analytics: Google to Goat :@notes:tools:rationale:workflow:webdev:
CLOSED: [2020-04-09 Thu 17:17]
:PROPERTIES:
:EXPORT_FILE_NAME: goat-google
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments false
:EXPORT_DATE: 2020-04-09 11:17
:END:
** Background
Like a lot of my tech based rants, this was brought on by a recent [[https://news.ycombinator.com/item?id=22813168][Hacker News
post]]. I won't go into why the product listed there is a hollow faux FOSS
rip-off. I won't discuss how the 'free' analytics option, like many others are
just hobby projects taking pot shots at other projects. Or how insanely
overpriced most alternatives are.

I will however discuss why and how I transitioned to using the awesome Goat
Counter.
** Google Analytics
I would like to point out that it is OK to start out with Google Analytics. It
is easy, and free, and scales well. There are reasons not to, but it is a good
starting point.

*** Pros
- Google Analytics is free, truly free
- The metrics are very detailed
- It is easy to set up
*** Cons
- Privacy concerns
- Blocked by people
- Easy to obsess over metrics

** Goat Counter
As with most Hacker News posts, the article itself was nothing compared to the
excellent comment thread. It was there that I came across people praising [[https://www.goatcounter.com/][Goat Counter]].

*** Pros
- Is open sourced ([[https://github.com/zgoat/goatcounter][here on Github]])
- Super lightweight
- Anonymous statistics
- Easy to share
*** Cons
- Has an upper limit on free accounts (10k a month)
- I am not very fond of Go
** Conclusions
I might eventually go back to GA, if I go over the 10k page view limit. Then
again, I might not. It might be more like, I only care about the first 10k
people who make it to my site.

**UPDATE:** This site has since shifted to Clicky, for [[Analytics II: Goat to Clicky][reasons outlined here]]
* TODO Fediverse Thoughts
** Background
I recently decided to take a half day off. Naturally I began looking into things
I've never seen before. I then ran into the delightful fediverse again.
* DONE On-boarding for Code in Place :@notes:ideas:teaching:cs106a:
CLOSED: [2020-04-10 Fri 16:01]
:PROPERTIES:
:EXPORT_FILE_NAME: scp-onboarding
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:END:
** Background
A few weeks ago, I ended up recording a video for the [[https://compedu.stanford.edu/codeinplace/announcement/][Stanford CS106A: Code in Place]]
initiative (which can be [[https://youtu.be/J0ULMEtM00w][found here]]). I heard back a while ago, and am now to lead a section for the
course!

I'll probably be making a series of short posts as this process continues.
** On-Boarding
This was very reminiscent of the [[http://carpentries.github.io/instructor-training/][Carpentries instructor training]], which makes
sense, given how well thought out that experience was.

We started out with a pre-presentation where people were able to just spitball
and connect, which is pretty neat.

One of the interesting parts of this, was the idea of *interactive recorded
lectures*, where the professors will be watching lectures with the students. The
entire slide deck [[https://docs.google.com/presentation/d/12DFKzJWYunNbVMdJ3PbMlS3ZutSoPlMnyzXpFcKyHJc/edit#slide=id.p2][is here]].

The other great idea for this kind of long course was the idea of having a Tea
room and a Teachers lounge where people can just tune in to chat.
*** Caveats
A couple of things which keep cropping up for online teaching in general are the
following:
- Zoom does not have persistent chats, so an auxiliary tool like an [[https://board.net][Etherpad]] is great
* DONE Small Section On-boarding :@notes:teaching:cs106a:
CLOSED: [2020-04-14 Tue 02:48]
:PROPERTIES:
:EXPORT_FILE_NAME: scp-smallgrp
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:END:
** Background
As I mentioned in my [[https://rgoswami.me/posts/scp-onboarding/][last post]], I'm leading a section for [[https://compedu.stanford.edu/codeinplace/announcement/][Stanford CS106A: Code
in Place]]. I did also mention I'd try to keep a set of short notes on the
process. So there[fn:videos].
** The Training
Given the overwhelming number of students, and section leaders, the small groups
are for fostering a community of teachers.

# Arun Kulshrestha is the section leader. He graduated a while ago from Stanford
# and was a section leader too.

- [ ] Consider allowing for daisy chaining during introductions
- [ ] Discussions are the primary take-away
- [ ] Only the instructor should be coding during the session

*** Core components
- Clarity
- Content
- Atmosphere
- Section management
- Correctness
*** Sectional Details
- Check in at the start
- Notice the space
- Check in regularly
- Avoid negative phrases
- Establish norms and the general culture
*** Zoom Norms
- Have people introduce themselves
- Mute people when they aren't talking
- Raise hands
- Try to use icebreakers which respect privacy
*** Materials
Here's some of the stuff which, being as it was open-sourced, I suppose is OK to
put here[fn:help].
- [[https://docs.google.com/document/d/1PPei3a5yORmKW1KusD4kearBUzZZM8DYCG7X0NY1oaM/preview][Section Leader Training]]
- [[https://docs.google.com/document/d/1VTnPA7dMwqpoE_Dl-jWL32g99P_ey4g-NmF7OEzhqR8/preview][Section Leaders' Guide to Virtual Sections]]
- [[https://docs.google.com/document/d/1lHdnwAB17iLyvASZbWrIZz4PVy9zMmHjxGBGPwXNDs4/preview#heading=h.7dq0u3orjv9z][Some Zoom Icebreakers]]
[fn:help] If you know otherwise, let me know in the comments
[fn:videos] As you may know, the official playlist [[https://www.youtube.com/channel/UCWw34Ie0yNe96myEZ5RLHhg][is here]]
* DONE CS106A Section Meeting I :@notes:teaching:cs106a:
CLOSED: [2020-04-17 Fri 22:54]
:PROPERTIES:
:EXPORT_FILE_NAME: scp-smallgrp-meet1
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:END:
** Background
As I mentioned [[https://rgoswami.me/posts/scp-onboarding/][earlier]], I'm leading a section for [[https://compedu.stanford.edu/codeinplace/announcement/][Stanford CS106A: Code
in Place]]. I did also mention I'd try to keep a set of short notes on the
process. I finally had my first section meeting!
** Preparation
I went through the following:
- Sent out a welcome message
- Detailed the workflow
- Set up a HackMD instance
- Set up some slides in ~beamer~[fn:whyslides]

However, after that, I was still concerned since I didn't get much of a response on the ice-breakers for EdStem. Thankfully, everyone showed up.
** Teaching
- I had a fabulous session, and we went through a variety of concepts.
- Didn't spend much time on icebreakers, but did get a feel for where the students stand on the functional vs imperative programming paradigms
- Possibly because of working through two different approaches, the 40 minute long session went on for two hours and fifteen minutes.
- Some students had more of a background than the others, thankfully computational thinking is not normally taught very well

** Conclusion

- The notes are [[https://hackmd.io/tqd-a5SlSbK_NAtnSdqEPA][visible here]], and the session was [[https://youtu.be/rLak1v4k4o0][recorded here]][fn:whatnow]
- It was fun, and I hope the students enjoyed it as much as I did.
- I will probably expand this in terms of the concepts covered, to give the students more of an overview of what was covered

[fn:whyslides] Even though most of the session was supposed to be live, it was still helpful to show I was interested enough to set up slides
[fn:whatnow] As always, advice is much appreciated (and moderated)
* DONE CS106A Small Group Training :@notes:teaching:cs106a:
CLOSED: [2020-04-22 Wed 07:01]
:PROPERTIES:
:EXPORT_FILE_NAME: scp-smallgrp-trainig
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments false
:END:
** Background
As I mentioned [[https://rgoswami.me/posts/scp-onboarding/][earlier]], I'm leading a section for [[https://compedu.stanford.edu/codeinplace/announcement/][Stanford CS106A: Code
in Place]]. This post relates to the notes and thoughts garnered during the small group training session[fn:whenpost].
** Reflections
*** Demographics
Redacted. Did not use breakout meetings due to privacy issues.
*** Engagement and Participation
- Some people were more active (skewed responses)
- Some of the more rudimentary questions might have been suppressed
*** Highlighted Moments
- Covering multiple perspectives
- Different mental models
*** Challenges and Transformations
- Technical debt was an issue
- Lack of engagement
- Went on for too long
For me in particular:
#+BEGIN_QUOTE
It took over two hours, and though most people stayed on, not everyone was engaged.
#+END_QUOTE
** Scenarios
These are to be dealt with as per the [[https://docs.google.com/document/d/13RZzvY_9WTR_sjo_Y4oBNchsAWAv_z6kSJ9395snANU/preview][guidelines here]]. Since different groups covered different scenarios, not all of these have answers here.
*** Ensuring Engagement
#+BEGIN_QUOTE
You have some students who didn't participate at all in the section. What do you do?
#+END_QUOTE
*** Effective Communication
#+BEGIN_QUOTE
What might not be effective about the policy, “Students should just tell me if I say something that offends them”?
#+END_QUOTE
*** Sharing Experiences
#+BEGIN_QUOTE
You just finished your section and are staying behind to answer questions from your students. A couple students asked what it’s like studying/working in an engineering/tech field.

What things might you want to keep in mind when answering their questions?
#+END_QUOTE
*** Time Management
#+BEGIN_QUOTE
Section went way over time due to lots of questions being asked by students. What are some time management strategies you can use moving forward?
#+END_QUOTE
*** Homework Assists
#+BEGIN_QUOTE
A sectionee posts in your Ed group, “I am a little bit frustrated because I don't really know where to start on the first assignment. A little hint would be very helpful.” How do you respond?
#+END_QUOTE
*** Debugging
#+BEGIN_QUOTE
A  sectionee shows you the following buggy code for printing all the elements in a list:

my_lst = ['apple', 'banana', 'carrot']
i = 0
while len(my_lst) > 0:
  print(my_lst[i])
  i = i + 1

They explain that the code works (it prints all the elements in the right order) but then throws a weird error: “IndexError: list index out of range.”  How would you help them find their bug?
#+END_QUOTE
*** Quitting
#+BEGIN_QUOTE
You have a student who is already discouraged by how difficult the first assignment is and has told you they don’t feel cut out for CS.  What do you say to them?
#+END_QUOTE
1. Provide encouragement
2. Give examples of hardship faced
3. Be positive and make sure they don’t feel worse, even if they do follow through and quit
4. “You’re not the first”
5. Takes a lot of time. Doesn’t happen overnight
6. Ask them why they don’t feel cut out and try to solve that problem

*** Looking up issues
#+BEGIN_QUOTE
Why might it be problematic to say something like, “It’s easy to download X or look up the answer to Y”? Why might those statements not be true?
#+END_QUOTE
1. Difficulty in backgrounds (language barriers)
2. They might not be able to understand stackoverflow.com until they learn more CS
3. They might not know where to look online (lack of domain expertise)
4. Dependencies (for downloads)
5. Makes them feel bad if they don’t end up finding it easy

[fn:whenpost] This post was created on the day of training, 21-04-20, but will be posted later
* DONE Using Mathematica with Orgmode :@programming:tools:emacs:workflow:orgmode:
CLOSED: [2020-04-26 Sun 20:01]
:PROPERTIES:
:EXPORT_FILE_NAME: org-mathematica
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
** Background
I have been wanting to find a workflow which allows me to bypass writing a lot of TeX by hand for a while now. To that end I looked into using a computer algebra system (CAS). Naturally, my first choice was the [[http://maxima.sourceforge.net/][FOSS Maxima]] (also because it uses Lisp under the hood). However, for all the reasons [[http://thingwy.blogspot.com/2015/07/maxima-versus-mathematica-should-i-go.html][listed here]], relating to its accuracy, which have not been fixed even though the post was over 5 years ago, I ended up having to go with the closed source [[https://www.wolfram.com/mathematica/][Mathematica]].
** Packages
Support for Mathematica in modern orgmode is mainly through the use of [[https://github.com/emacsmirror/org/blob/master/contrib/lisp/ob-mathematica.el][ob-mathematica]], which is the official org-babel extension (from ~contrib~) for working with Mathematica. However, ~ob-mathematica~ relies on the now-defunct ~mma~ package for font-locking, which is less than ideal. Thankfully, there exists the excellent [[https://github.com/kawabata/wolfram-mode][wolfram-mode]] package which happens to be in MELPA as well. Finally, since the default return type of a ~mathematica~ block is an input-string meant to be used in another ~mathematica~ block, which is not useful when we work with ~org-babel~, we will use the excellent ~mash.pl~ utility [[http://ai.eecs.umich.edu/people/dreeves/mash/][from here]], as suggested by the ~ob-mathematica~ package to sanitize our output and set a unifying path.

So to recap, use your favorite manager to get:
- [x] ~ob-mathematica~ (in contrib)
- [x] ~wolfram-mode~ ([[https://melpa.org/#/wolfram-mode][MELPA]])
- [x] ~mash.pl~ ([[http://ai.eecs.umich.edu/people/dreeves/mash][from here]])[fn:aboutmash]

After obtaining the packages, the configuration is then simply[fn:fullconf]:

#+begin_src emacs-lisp
;; Load mathematica from contrib
(org-babel-do-load-languages 'org-babel-load-languages
                             (append org-babel-load-languages
                                     '((mathematica . t))
                                     ))
;; Sanitize output and deal with paths
(setq org-babel-mathematica-command "~/.local/bin/mash")
;; Font-locking
(add-to-list 'org-src-lang-modes '("mathematica" . wolfram))
;; For wolfram-mode
(setq mathematica-command-line "~/.local/bin/mash")
#+end_src

** Results
*** LaTeX
Now we are in a position to simply evaluate content with font-locking. We will test our set up with an example lifted from the ~ob-mathematica~ [[https://github.com/analyticd/wy-els/blob/master/ob-mathematica.el][source-code]].

#+NAME: example-table
#+caption: A table
  | 1 | 4 |
  | 2 | 4 |
  | 3 | 6 |
  | 4 | 8 |
  | 7 | 0 |

#+BEGIN_SRC mathematica :var x=example-table :results latex
(1+Transpose@x) // TeXForm
#+END_SRC

#+RESULTS:
#+begin_export latex
\left(
\begin{array}{ccccc}
 2 & 3 & 4 & 5 & 8 \\
 5 & 5 & 7 & 9 & 1 \\
\end{array}
\right)
#+end_export

Where our header-line (with ~#+begin_src~) is:
#+BEGIN_SRC orgmode
mathematica :var x=example-table :results latex
#+END_SRC

*** Sanity Checks

We can also test the example from the [[http://thingwy.blogspot.com/2015/07/maxima-versus-mathematica-should-i-go.html][blog post]] earlier to test basic mathematical sanity.

#+BEGIN_SRC mathematica :results raw
Limit[Log[b - a + I eta], eta -> 0, Direction -> -1,Assumptions -> {a > 0, b > 0, a > b}]
TeXForm[Limit[Log[b - a + I eta], eta -> 0, Direction -> 1,Assumptions -> {a > 0, b > 0, a > b}]]
#+END_SRC

#+RESULTS:
$(I*Pi + Log[a - b])*\log (a-b)-i \pi$

*** Inline Math

Note that we can now also write fractions, integrals and other cumbersome TeX objects a lot faster with this syntax, like src_mathematica[:exports none :results raw]{Integrate[x^2,x] // TeXForm} $\frac{x^3}{3}$. Where we are using the following snippet:

#+BEGIN_SRC orgmode
src_mathematica[:exports none :results raw]{Integrate[x^2,x] // TeXForm}
#+END_SRC

*** Plots

For plots, the standard ~orgmode~ rules apply, that is, we have to export to a file and return the name through our code snippet. Consider:
#+BEGIN_SRC mathematica :results file
p=Plot[Sin[x], {x, 0, 6 Pi},Frame->True];
Export["images/sine.png",p];
Print["images/sine.png"]
#+END_SRC

#+RESULTS:
#+caption: An exported Mathematica image
[[file:images/sine.png]]

Where we have used ~mathematica :results file~ as our header line.

** Comments
The older commenting system was implemented with [[https://utteranc.es][utteranc.es]] as seen below.
@@html:<script src="https://utteranc.es/client.js" repo="haozeke/haozeke.github.io" issue-term="pathname" theme="photon-dark" label="Utterance 💬" crossorigin="anonymous" async></script>@@

[fn:aboutmash] As noted in the comments, it is nicer to rename ~mash.pl~ to ~mash~
[fn:whydoom] The dark side has cookies
[fn:fullconf] For reference, my whole config [[https://dotdoom.rgoswami.me][is here]]
* TODO Thoughts on Chemical Engineering :@notes:ramblings:explanations:
:PROPERTIES:
:EXPORT_FILE_NAME: cheme-thoughts
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments false
:END:
** Background
** Mass and Energy Balances
From the first lecture on the topic, I came away with an implicit understanding, that though reactor design might be the heart of a chemical engineering plant, the soul would be mass and energy balances.
* DONE An Orgmode Note Workflow :@programming:tools:emacs:workflow:orgmode:
CLOSED: [2020-05-10 Sun 15:01]
:PROPERTIES:
:EXPORT_FILE_NAME: org-note-workflow
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
** Background
One of the main reasons to use ~orgmode~ is definitely to get a better note
taking workflow. Closely related to blogging or writing, the ideal note workflow
is one which lets you keep a bunch of throwaway ideas and also somehow have
access to them in a coherent manner. This will be a long post, and it is a
work-in-progress, so, keep that in mind. Since this is mainly me[fn:bojackme]
work-shopping my technique, the philosophy will come in a later post probably.
This workflow is documented more sparsely in my [[https://dotdoom.rgoswami.me/config.html#text-3][config file here]], in the
~noteYoda~ section[fn:whoyoda]. Some parts of this post also include mini video
clips for clarity[fn:howrec].

The entire workflow will end up being something like this[fn:notrepro]:

{{< youtube UWB6ZABRVq0 >}}

** Concept
While working through ideas, it actually was more useful to describe the
workflow I want, and then implement it, instead of relying on the canned
approaches of each package. So the basics of the ideology are listed below.
*** Reference Management
Reference management is one of the main reasons to consider a plain-text setup, and mine is no different. The options most commonly seen are:
- Mendeley :: This is a great option, and the most mobile friendly of the bunch. Sadly, the price tiers aren't very friendly so I have to give it a hard pass.
- Jabref :: This is fun, but really more of a per-project management system, but it works well for that. The fact that it is Java based was a major issue for me.
- Zotero :: This is what I personally use and recommend. More on that in a later post.
*** Notes
The idea is to be able to create notes for all kinds of content. Specifically,
papers or books, along with webpages. This then requires a separate system for
each which is described by:
- Search Engine :: The search engine is key, both in terms of accessibility and scalability. It is assumed that there will be many notes, and that they will have a wide variety of content. The search interface must then simply allow us to narrow down our candidates in a meaningful manner.
- Contextual Representation :: This aspect of the workflow deals with representations, which should transcend the usage of tags or categories. In particular, it would be nice to be able to visualize the flow of ideas, each represented by a note.
- Backlinks :: In particular, by backlinks at this point we are referring to the ability to link to a ~pdf~ or a website with a unique key such that notes can be added or removed at will.
- Storage :: Not actually part of the workflow in the same way, since it will be handled at the system level, it is worth nothing, that in this workflow Zotero is used to export a master *bib* file and keeps it updated, while the notes themselves are version controlled[fn:nodrop].
The concepts above will be handled by the following packages.

| Concept   | Package                      | Note                                                   |
|-----------+------------------------------+--------------------------------------------------------|
| Search    | deft                         | Has a great interface                                  |
| Context   | org-roam                     | Allows the export of graphiz mindmaps                  |
| Backlinks | org-roam, org-ref, org-noter | Covers websites, bibliographies, and pdfs respectively |

A key component in this workflow is actually facilitated by the fabulous
~org-roam-bibtex~ [[https://github.com/org-roam/org-roam-bibtex][or ORB]]. The basic idea is to ensure meaningful templates which
interpolate smoothly with ~org-roam~, ~org-ref~, ~helm-bibtex~, and
~org-capture~.
*** Basic Variables
Given the packages we will be using, some variable settings are in order, namely:
#+BEGIN_SRC emacs-lisp
(setq
   org_notes (concat (getenv "HOME") "/Git/Gitlab/Mine/Notes/")
   zot_bib (concat (getenv "HOME") "/GDrive/zotLib.bib")
   org-directory org_notes
   deft-directory org_notes
   org-roam-directory org_notes
   )
#+END_SRC
** Search
For the search setup, the ~doom-emacs~ ~deft~ setup, by adding ~+deft~ in my
~init.el~, worked out of the box for me. For those who do not use
~doom~[fn:whynot], the following should suffice:
#+BEGIN_SRC emacs-lisp
(use-package deft
  :commands deft
  :init
  (setq deft-default-extension "org"
        ;; de-couples filename and note title:
        deft-use-filename-as-title nil
        deft-use-filter-string-for-filename t
        ;; disable auto-save
        deft-auto-save-interval -1.0
        ;; converts the filter string into a readable file-name using kebab-case:
        deft-file-naming-rules
        '((noslash . "-")
          (nospace . "-")
          (case-fn . downcase)))
  :config
  (add-to-list 'deft-extensions "tex")
  )
#+END_SRC
For more about the ~doom-emacs~ defaults, check [[https://github.com/hlissner/doom-emacs/search?q=deft&unscoped_q=deft][the Github repo]]. The other
aspect of interacting with the notes is via the ~org-roam~ interface and will be
covered below.
** Bibliography
Since I will be using ~org-ref~, it makes no sense to load or work with the
~+biblio~ module at the moment. Thus this section is actually ~doom~ agnostic.
The basic tools of bibliographic management from the ~emacs~ end are the
venerable ~helm-bibtex~ ([[https://github.com/tmalsburg/helm-bibtex][repo here]]) and ~org-ref~ ([[https://github.com/jkitchin/org-ref/][repo here]]). In order to make
this guide complete, I will also describe the [[https://www.zotero.org/][Zotero]] settings I have.
*** Zotero
Without getting too deep into the weeds here, the basic requirements are:
- [x] [[https://zotero.org][Zotero]]
- [x] The [[https://retorque.re/zotero-better-bibtex/][better bibtex extension]]
The idea is to then have one top level ~.bib~ file in some handy location which
you will set up to sync automatically. To make life easier, there is a tiny
recording of the next steps.

{{< youtube iDRpIo7mcKE >}}

** Helm-Bibtex
This [[https://github.com/tmalsburg/helm-bibtex][venerable package]] is really good at interfacing with a variety of
externally formatted bibliographic managers.
#+BEGIN_SRC emacs-lisp
(setq
 bibtex-completion-notes-path "/home/haozeke/Git/Gitlab/Mine/Notes/"
 bibtex-completion-bibliography "/home/haozeke/GDrive/zotLib.bib"
 bibtex-completion-pdf-field "file"
 bibtex-completion-notes-template-multiple-files
 (concat
  "#+TITLE: ${title}\n"
  "#+ROAM_KEY: cite:${=key=}\n"
  "* TODO Notes\n"
  ":PROPERTIES:\n"
  ":Custom_ID: ${=key=}\n"
  ":NOTER_DOCUMENT: %(orb-process-file-field \"${=key=}\")\n"
  ":AUTHOR: ${author-abbrev}\n"
  ":JOURNAL: ${journaltitle}\n"
  ":DATE: ${date}\n"
  ":YEAR: ${year}\n"
  ":DOI: ${doi}\n"
  ":URL: ${url}\n"
  ":END:\n\n"
  )
 )
#+END_SRC
~doom-emacs~ users like me might want to wrap the above in a nice ~after!
org-ref~ expression, but it doesn't really matter.
*** Explanation
To break-down aspects of the configuration snippet above:
- The template includes the ~orb-process-file-field~ function to allow selecting the ~pdf~ to be used with ~org-noter~
- The ~file~ field is specified to work with the ~.bib~ file generated by Zotero
- ~helm-bibtex~ allows for any of the keys in a ~.bib~ file to be used in a template, and an overly expressive one is more useful
- The ~ROAM_KEY~ is defined to ensure that cite backlinks work correctly with ~org-roam~
- As I prefer to have one notes file per ~pdf~, I have only configured the ~bibtex-completion-notes-template-multiple-files~ variable
** Org-Ref
As discussed above, this just makes citations much more meaningful in ~orgmode~.
#+BEGIN_SRC emacs-lisp
(use-package org-ref
    :config
    (setq
         org-ref-completion-library 'org-ref-ivy-cite
         org-ref-get-pdf-filename-function 'org-ref-get-pdf-filename-helm-bibtex
         org-ref-default-bibliography (list "/home/haozeke/GDrive/zotLib.bib")
         org-ref-bibliography-notes "/home/haozeke/Git/Gitlab/Mine/Notes/bibnotes.org"
         org-ref-note-title-format "* TODO %y - %t\n :PROPERTIES:\n  :Custom_ID: %k\n  :NOTER_DOCUMENT: %F\n :ROAM_KEY: cite:%k\n  :AUTHOR: %9a\n  :JOURNAL: %j\n  :YEAR: %y\n  :VOLUME: %v\n  :PAGES: %p\n  :DOI: %D\n  :URL: %U\n :END:\n\n"
         org-ref-notes-directory "/home/haozeke/Git/Gitlab/Mine/Notes/"
         org-ref-notes-function 'orb-edit-notes
    ))
#+END_SRC
An essential aspect of this configuration is just that most of heavy lifting in
terms of the notes are palmed off to ~helm-bibtex~.
*** Explanation
To break-down aspects of the configuration snippet above:
- The ~org-ref-get-pdf-filename-function~ simply uses the ~helm-bibtex~ settings to find the ~pdf~
- The default bibliography and notes directory are set to the same location as all the ~org-roam~ files, to encourage a flat hierarchy
- The ~org-ref-notes-function~ simply ensures that, like the ~helm-bibtex~ settings, I expect one file per ~pdf~, and that I would like to use my ~org-roam~ template instead of the ~org-ref~ or ~helm-bibtex~ one
Note that for some reason,
the format specifiers for ~org-ref~ are *not* the keys in ~.bib~ but are
instead, the following[fn:wherereforg]:
#+BEGIN_SRC bash
In the format, the following percent escapes will be expanded.
%l   The BibTeX label of the citation.
%a   List of author names, see also `reftex-cite-punctuation'.
%2a  Like %a, but abbreviate more than 2 authors like Jones et al.
%A   First author name only.
%e   Works like %a, but on list of editor names.  (%2e and %E work as well)
It is also possible to access all other BibTeX database fields:
%b booktitle     %c chapter        %d edition    %h howpublished
%i institution   %j journal        %k key        %m month
%n number        %o organization   %p pages      %P first page
%r address       %s school         %u publisher  %t title
%v volume        %y year
%B booktitle, abbreviated          %T title, abbreviated
%U url
%D doi
%S series        %N note
%f pdf filename
%F absolute pdf filename
Usually, only %l is needed.  The other stuff is mainly for the echo area
display, and for (setq reftex-comment-citations t).
%< as a special operator kills punctuation and space around it after the
string has been formatted.
A pair of square brackets indicates an optional argument, and RefTeX
will prompt for the values of these arguments.
#+END_SRC
** Indexing Notes
This part of the workflow builds on the concepts best known as the [[https://www.zettelkasten.de/][Zettelkasten method]]. More details about the philosophy behind ~org-roam~ is [[https://www.orgroam.com/][here]].
*** Org-Roam
The first part of this interface is essentially just the ~doom-emacs~
configuration, adapted for those who don't believe in the dark side below.
#+BEGIN_SRC emacs-lisp
(use-package org-roam
  :hook (org-load . org-roam-mode)
  :commands (org-roam-buffer-toggle-display
             org-roam-find-file
             org-roam-graph
             org-roam-insert
             org-roam-switch-to-buffer
             org-roam-dailies-date
             org-roam-dailies-today
             org-roam-dailies-tomorrow
             org-roam-dailies-yesterday)
  :preface
  ;; Set this to nil so we can later detect whether the user has set a custom
  ;; directory for it, and default to `org-directory' if they haven't.
  (defvar org-roam-directory nil)
  :init
  :config
  (setq org-roam-directory (expand-file-name (or org-roam-directory "roam")
                                             org-directory)
        org-roam-verbose nil  ; https://youtu.be/fn4jIlFwuLU
        org-roam-buffer-no-delete-other-windows t ; make org-roam buffer sticky
        org-roam-completion-system 'default
)

  ;; Normally, the org-roam buffer doesn't open until you explicitly call
  ;; `org-roam'. If `+org-roam-open-buffer-on-find-file' is non-nil, the
  ;; org-roam buffer will be opened for you when you use `org-roam-find-file'
  ;; (but not `find-file', to limit the scope of this behavior).
  (add-hook 'find-file-hook
    (defun +org-roam-open-buffer-maybe-h ()
      (and +org-roam-open-buffer-on-find-file
           (memq 'org-roam-buffer--update-maybe post-command-hook)
           (not (window-parameter nil 'window-side)) ; don't proc for popups
           (not (eq 'visible (org-roam-buffer--visibility)))
           (with-current-buffer (window-buffer)
             (org-roam-buffer--get-create)))))

  ;; Hide the mode line in the org-roam buffer, since it serves no purpose. This
  ;; makes it easier to distinguish among other org buffers.
  (add-hook 'org-roam-buffer-prepare-hook #'hide-mode-line-mode))


;; Since the org module lazy loads org-protocol (waits until an org URL is
;; detected), we can safely chain `org-roam-protocol' to it.
(use-package org-roam-protocol
  :after org-protocol)


(use-package company-org-roam
  :after org-roam
  :config
  (set-company-backend! 'org-mode '(company-org-roam company-yasnippet company-dabbrev)))
#+END_SRC
Once again, for more details, check the [[https://github.com/hlissner/doom-emacs/search?q=roam&unscoped_q=roam][Github repo]].
*** Org-Roam-Bibtex
The configuration required is:
#+BEGIN_SRC emacs-lisp
 (use-package org-roam-bibtex
  :after (org-roam)
  :hook (org-roam-mode . org-roam-bibtex-mode)
  :config
  (setq org-roam-bibtex-preformat-keywords
   '("=key=" "title" "url" "file" "author-or-editor" "keywords"))
  (setq orb-templates
        '(("r" "ref" plain (function org-roam-capture--get-point)
           ""
           :file-name "${slug}"
           :head "#+TITLE: ${=key=}: ${title}\n#+ROAM_KEY: ${ref}

- tags ::
- keywords :: ${keywords}

\n* ${title}\n  :PROPERTIES:\n  :Custom_ID: ${=key=}\n  :URL: ${url}\n  :AUTHOR: ${author-or-editor}\n  :NOTER_DOCUMENT: %(orb-process-file-field \"${=key=}\")\n  :NOTER_PAGE: \n  :END:\n\n"

           :unnarrowed t))))
#+END_SRC
Where most of the configuration is essentially the template again. Like ~helm-bibtex~, [[https://github.com/org-roam/org-roam-bibtex][ORB]] allows taking arbitrary keys from the ~.bib~ file.
** Org Noter
The final aspect of a ~pdf~ workflow is simply ensuring that every ~pdf~ is
associated with notes. The philosophy of ~org-noter~ is [[https://github.com/weirdNox/org-noter][best described here]].
Only minor tweaks should be required to get this working with ~interleave~ as
well.
#+BEGIN_SRC emacs-lisp
(use-package org-noter
  :after (:any org pdf-view)
  :config
  (setq
   ;; The WM can handle splits
   org-noter-notes-window-location 'other-frame
   ;; Please stop opening frames
   org-noter-always-create-frame nil
   ;; I want to see the whole file
   org-noter-hide-other nil
   ;; Everything is relative to the main notes file
   org-noter-notes-search-path (list org_notes)
   )
  )
#+END_SRC
Evidently, from my configuration, it appears that I decided to use [[https://github.com/weirdNox/org-noter][org-noter]] over
the more commonly described [[https://github.com/rudolfochrist/interleave][interleave]] because it has better support for working
with multiple documents linked to one file.
** Org-Protocol
I will only cover the bare minimum relating to the use of ~org-capture~ here,
because eventually I intend to handle a lot more cases with [[https://github.com/abo-abo/orca][orca]]. Note that this
part of the workflow has more to do with using ~org-roam~ with websites than
~pdf~ files.
*** Templates
This might get complicated but I am only trying to get the bare minimum for
~org-protocol~ right now.
#+BEGIN_SRC emacs-lisp
;; Actually start using templates
(after! org-capture
  ;; Firefox and Chrome
  (add-to-list 'org-capture-templates
               '("P" "Protocol" entry ; key, name, type
                 (file+headline +org-capture-notes-file "Inbox") ; target
                 "* %^{Title}\nSource: %u, %c\n #+BEGIN_QUOTE\n%i\n#+END_QUOTE\n\n\n%?"
                 :prepend t ; properties
                 :kill-buffer t))
  (add-to-list 'org-capture-templates
               '("L" "Protocol Link" entry
                 (file+headline +org-capture-notes-file "Inbox")
                 "* %? [[%:link][%(transform-square-brackets-to-round-ones \"%:description\")]]\n"
                 :prepend t
                 :kill-buffer t))
)
#+END_SRC
** Conclusions
At this point, many might argue that since by the end, only one template is
called, defining the rest were pointless. They would be right, however, this is
just how my configuration evolved. Feel free to cannibalize this for your
personal benefit. Eventually I plan to expand this into something with
~org-journal~ as well, but not right now.

** Comments
The older commenting system was implemented with [[https://utteranc.es][utteranc.es]] as seen below.
@@html:<script src="https://utteranc.es/client.js" repo="haozeke/haozeke.github.io" issue-term="pathname" theme="photon-dark" label="Utterance 💬" crossorigin="anonymous" async></script>@@

[fn:notrepro] The video uses ~org-ref-notes-function-many-files~ as the ~org-ref-notes-function~ so the template looks a little different
[fn:nodrop] For some strange reason a lot of online posts suggested Dropbox for syncing notes, which makes no sense to me, it is always better to have version control and ignore rules
[fn:wherereforg] Where these are from the [[https://github.com/jkitchin/org-ref/blob/875371a63430544e446db7a76b44b33c7e20a8bd/org-ref-utils.el][org-ref documentation]]
[fn:howrec] Recorded with [[https://www.maartenbaert.be/simplescreenrecorder/][SimpleScreenRecorder]], cut with [[https://github.com/mifi/lossless-cut][LosslessCut]], uploaded to [[https://www.youtube.com/][YouTube]], and embedded with a [[https://gohugo.io/extras/shortcodes/#youtube][Hugo shortcode]]
[fn:bojackme] Rohit Goswami that is, from the [[https://rgoswami.me][landing page]]; obviously
[fn:whoyoda] This is a reference to my fantastic pet, named Yoda
[fn:whynot] Therefore clearly proving that the cookies of the dark side have no power in the holy text editor war
* DONE Pandoc to Orgmode with Babel :@programming:tools:emacs:workflow:orgmode:
CLOSED: [2020-05-02 Sat 16:39]
:PROPERTIES:
:EXPORT_FILE_NAME: org-pandoc-babel
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
** Background
One of the best things about writing in ~orgmode~ is that we can embed and
execute arbitrary code snippets. However, not all languages have an exporter,
for obvious reasons. Somewhat surprisingly, there is no way to call [[https://pandoc.org/MANUAL.html][pandoc]] on embedded snippets, which feels like a waste, especially when a whole bunch of documentation formats can be converted to ~orgmode~ with it.

Consider the following beautifully highlighted snippet of an ~rst~ (ReStructured Text) [[https://sublime-and-sphinx-guide.readthedocs.io/en/latest/tables.html][list table]].
#+BEGIN_SRC rest
.. list-table:: Title
   :widths: 25 25 50
   :header-rows: 1

   ,* - Heading row 1, column 1
     - Heading row 1, column 2
     - Heading row 1, column 3
   ,* - Row 1, column 1
     -
     - Row 1, column 3
   ,* - Row 2, column 1
     - Row 2, column 2
   - Row 2, column 3
#+END_SRC

Trying to run this will generate the sort of obvious error:
#+BEGIN_SRC emacs-lisp
org-babel-execute-src-block: No org-babel-execute function for rst!
#+END_SRC
** Writing an Exporter
For this post, I will be focusing on ~rst~, but this can be defined for any of the ~pandoc~ back-ends. The approach was inspired by [[https://github.com/hoelterhof/ob-markdown/blob/master/ob-markdown.el][ob-markdown]].

#+BEGIN_SRC emacs-lisp
(defun org-babel-execute:rst (body params)
  "Execute a block of rst code with org-babel.
This function is called by `org-babel-execute-src-block'."
  (let* ((result-params (split-string (or (cdr (assoc :results params)) "")))
	 (in-file (org-babel-temp-file "rst-"))
	 (cmdline (cdr (assoc :cmdline params)))
	 (to (cdr (assoc :to params)))
	 (template (cdr (assoc :template params)))
	 (cmd (concat "pandoc"
		      " -t  org"
		      " -i " (org-babel-process-file-name in-file)
		      " -f rst "
		      " " cmdline)))
    (with-temp-file in-file (insert body))
    (message cmd)
    (shell-command-to-string cmd))) ;; Send to results

(defun org-babel-prep-session:rst (session params)
  "Return an error because rst does not support sessions."
  (error "rst does not support sessions"))
#+END_SRC

** Trying it out
With that done, it is pretty trivial to re-run the above example.

#+BEGIN_SRC rest :exports both :results raw
.. list-table:: Title
   :widths: 25 25 50
   :header-rows: 1

   ,* - Heading row 1, column 1
     - Heading row 1, column 2
     - Heading row 1, column 3
   ,* - Row 1, column 1
     -
     - Row 1, column 3
   ,* - Row 2, column 1
     - Row 2, column 2
   - Row 2, column 3
#+END_SRC

#+RESULTS:
| Heading row 1, column 1 | Heading row 1, column 2 | Heading row 1, column 3 |
|-------------------------+-------------------------+-------------------------|
| Row 1, column 1         |                         | Row 1, column 3         |
| Row 2, column 1         | Row 2, column 2         |                         |
|                         |                         |                         |
#+CAPTION: Title

Note that we have used ~rst :exports both :results raw~ as the header argument.

** Conclusions
Will probably follow this up with an actual package, which should handle the entire spectrum of ~pandoc~ back-ends.
* DONE Refactoring Dotfiles For Colemak :@programming:workflow:
CLOSED: [2020-05-02 Sat 20:30]
:PROPERTIES:
:EXPORT_FILE_NAME: colemak-dots-refactor
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A more actionable follow up to [[Switching to Colemak][my personal recollections]] relating to my switch to Colemak.
#+END_QUOTE

** Background
I have, in the past written about how I [[Switching to Colemak][made the switch]] to Colemak. However,
until recently, I was still trying to mimic the VIM keybindings from QWERTY.
This is a post where I discuss the changes I made to ensure that I never have to
stretch my fingers in odd ways again. The main idea is expressed well by [[https://github.com/jooize/vim-colemak][vim-colemak]].
#+BEGIN_SRC shell
Colemak layout:                  |                 QWERTY layout:
`12345 67890-=     Move around:  |  (instead of)   `12345 67890-=
 qwfpg jluy;[]\         e        |       k          qwert yuiop[]\
 arstd HNEIo'         h   i      |     h   l        asdfg HJKL;'
 zxcvb km,./            n        |       j          zxcvb nm,./
#+END_SRC
** Sudoers
It is important to note that the ~sudo~ command does not automatically pick up on your keyboard layout. It is best to set this explicitly. Use ~visudo~ and un-comment ~Defaults env_keep += "LANG LANGUAGE LINGUAS LC_* _XKB_CHARSET"~, or:
#+begin_src bash
su
echo 'Defaults env_keep += "LANG LANGUAGE LINGUAS LC_* _XKB_CHARSET"' >> /etc/sudoers
#+end_src
** Emacs
Though I have [[https://github.com/hlissner/doom-emacs/issues/783#issuecomment-535805347][mentioned publicly]], that I was using the regular QWERTY motion
keys, I realized I had actually started to use the mouse more often, simply
because it was a pain to navigate. Thankfully, ~emacs~ has [[https://github.com/wbolster/evil-colemak-basics][evil-colemak-basics]],
which is fabulous. For reference, these make it really easy for QWERTY users to
make the switch if they're previously used to VIM bindings.
#+BEGIN_SRC rst :results raw :exports results
.. list-table::
   :header-rows: 1

   * - Colemak
     - Qwerty
     - Action
     - States
     - At Qwerty position?
     - Remarks

   * - ``h``, ``n``, ``e``, ``i``
     - ``h``, ``j``, ``k``, ``l``
     - navigate
     - ``mnvo``
     - yes
     -

   * - ``k``, ``K``
     - ``n``, ``N``
     - search next/previous
     - ``mnvo``
     - yes
     -

   * - ``u``, ``U``
     - ``i``, ``I``
     - insert
     - ``_nv_``
     - yes
     -

   * - ``l``
     - ``u``
     - undo
     - ``_nv_``
     - yes
     -

   * - ``N``
     - ``J``
     - join lines
     - ``_nv_``
     - yes
     -

   * - ``E``
     - ``K``
     - lookup
     - ``mnv_``
     - yes
     -

   * - ``u``
     - ``i``
     - inner text object keymap
     - ``___o``
     - yes
     -

   * - ``f``, ``F``
     - ``e``, ``E``
     - jump to end of word
     - ``mnvo``
     - yes
     - with ``t-f-j`` rotation

   * - ``t``, ``T``
     - ``f``, ``f``
     - jump to character
     - ``mnvo``
     - yes
     - with ``t-f-j`` rotation

   * - ``j``, ``J``
     - ``t``, ``T``
     - jump until character
     - ``mnvo``
     - no
     - with ``t-f-j`` rotation

   * - ``j``, ``J``
     - ``e``, ``E``
     - jump to end of word
     - ``mnvo``
     - no
     - without ``t-f-j`` rotation
#+END_SRC

#+RESULTS:
| Colemak            | Qwerty             | Action                   | States | At Qwerty position? | Remarks                  |
|--------------------+--------------------+--------------------------+--------+---------------------+--------------------------|
| =h=, =n=, =e=, =i= | =h=, =j=, =k=, =l= | navigate                 | =mnvo= | yes                 |                          |
| =k=, =K=           | =n=, =N=           | search next/previous     | =mnvo= | yes                 |                          |
| =u=, =U=           | =i=, =I=           | insert                   | =_nv_= | yes                 |                          |
| =l=                | =u=                | undo                     | =_nv_= | yes                 |                          |
| =N=                | =J=                | join lines               | =_nv_= | yes                 |                          |
| =E=                | =K=                | lookup                   | =mnv_= | yes                 |                          |
| =u=                | =i=                | inner text object keymap | =___o= | yes                 |                          |
| =f=, =F=           | =e=, =E=           | jump to end of word      | =mnvo= | yes                 | with =t-f-j= rotation    |
| =t=, =T=           | =f=, =f=           | jump to character        | =mnvo= | yes                 | with =t-f-j= rotation    |
| =j=, =J=           | =t=, =T=           | jump until character     | =mnvo= | no                  | with =t-f-j= rotation    |
| =j=, =J=           | =e=, =E=           | jump to end of word      | =mnvo= | no                  | without =t-f-j= rotation |

Where the table above is from the fantastic [[https://github.com/wbolster/evil-colemak-basics][readme]].

I still had some issues, mostly relating to searching in buffers, so I ended
using ~swiper-isearch~ more which is a bonus too.

*** Visual Lines
Since I tend to keep ~visual-line-mode~ all the time, it makes sense to actually swap working with lines and visual lines.
To work this through this needs [[https://github.com/YourFin/evil-better-visual-line/][evil-better-visual-line]].
#+BEGIN_SRC emacs-lisp
(use-package! evil-better-visual-line
  :after evil-colemak-basics
  :config
  (evil-better-visual-line-on)
  (map! :map evil-colemak-basics-keymap
        (:nvm "n" 'evil-better-visual-line-next-line
         :nvm "e" 'evil-better-visual-line-previous-line
         :nvm "g n" 'evil-next-line
         :nvm "g e" 'evil-previous-line))
)
#+END_SRC
*** Pdf-Tools
For my ~doom-emacs~ configuration, I also set the following map:
#+BEGIN_SRC emacs-lisp
(after! pdf-view
  (add-hook! 'pdf-view-mode-hook (evil-colemak-basics-mode -1))
 (map!
   :map pdf-view-mode-map
   :n "g g"          #'pdf-view-first-page
   :n "G"            #'pdf-view-last-page
   :n "N"            #'pdf-view-next-page-command
   :n "E"            #'pdf-view-previous-page-command
   :n "e"            #'evil-collection-pdf-view-previous-line-or-previous-page
   :n "n"            #'evil-collection-pdf-view-next-line-or-next-page
 )
#+END_SRC
Where the most important thing is the hook which removes the
~evil-colemak-basics~ binding. Since it is a single mode and hook, ~after-hook!~
is the same as ~after-hook~[fn:attribute].
*** Window Management
Somehow these are not part of the ~evil-colemak~ defaults.
#+BEGIN_SRC emacs-lisp
(after! evil
  (map! :map evil-window-map
        (:leader
         (:prefix ("w" . "Select Window")
          :n :desc "Left"  "h" 'evil-window-left
          :n :desc "Up"    "e" 'evil-window-up
          :n :desc "Down"  "n" 'evil-window-down
          :n :desc "Right" "i" 'evil-window-right
          ))
        ))
#+END_SRC
*** Search
Harmonizing with Vimium.
#+BEGIN_SRC emacs-lisp
(after! evil (map! :map evil-motion-state-map
                   (:n :desc "Previous match" "K" 'evil-ex-search-previous
                    :n :desc "Next match" "k" 'evil-ex-search-next
                    :n :desc "Forward search" "/" 'evil-search-forward
                    )
                   ))
#+END_SRC
*** Page Movement
Though this is more of a personal preference, I find it more natural to bind N
and E to page-wise movement instead of join lines and lookup, since I almost
never use those commands, and the movement keys echo what I expect elsewhere.
#+BEGIN_SRC emacs-lisp
(after! evil
  (map! :map evil-colemak-basics-keymap
      :nv "N" 'evil-scroll-page-up
      :nv "E" 'evil-scroll-page-down)
  )
#+END_SRC
*** Evil Org
Annoyingly, ~evil-org-mode~ had a map which kept overriding all my other
settings. Thankfully it has a helper variable to set movement. I also do not
need this anyway, at-least not by default.
#+BEGIN_SRC emacs-lisp
(after! org
  (remove-hook 'org-mode-hook 'evil-org-mode)
  (setq evil-org-movement-bindings
        '((up . "e") (down . "n")
          (left . "h") (right . "i"))
        )
)
#+END_SRC
** Vimium
I use the excellent [[https://vimium.github.io/][vimium]] to make Chrome be a little less annoying. Luckily [[https://github.com/philc/vimium/wiki/colemak][the
Wiki]] seems to have a reasonable suggestion for colemak. The basic idea is to
migrate the underlying keys directly to ensure very few manual changes are
required.
#+BEGIN_SRC conf
mapkey n j
mapkey N J
mapkey e k
mapkey E K
mapkey i l
mapkey I L
mapkey k n
mapkey K N
mapkey l i
mapkey L I
mapkey j e
mapkey J E
#+END_SRC
** Tridactyl
I still use the [[https://github.com/tridactyl/tridactyl/][fantastic tridactyl]] for Firefox when I can. However, the bindings are slightly more involved, since there is no equivalent for the ~mapkey~ which ~Vimium~ has.
#+begin_src conf
" Rebinds for colemak
" hjkl --> hnei
bind h scrollpx -50
bind n scrollline 10
bind e scrollline -10
bind i scrollpx 50
" HJKL --> HNEI
bind H back
bind N tabprev
bind E tabnext
bind I forward
#+end_src
** Vim
For a lot of terminal edits, ~vim~ is still my editor of choice, and [[https://github.com/jooize/vim-colemak][vim-colemak]] works without any trouble [[https://github.com/HaoZeke/Dotfiles/commit/d6df2743822e6fcf4761d569600a8e9a802a7af4][in my configuration]].
** Zsh
To ensure uniform bindings, I used to use ~bindkey -v~ but will need some minor
changes to that set up. I based this part of my configuration off the bindings
of [[https://github.com/bunnyfly/dotfiles/blob/master/zshrc][bunnyfly]].
#+BEGIN_SRC shell
bindkey -v
# Colemak.
  bindkey -M vicmd "h" backward-char
  bindkey -M vicmd "n" down-line-or-history
  bindkey -M vicmd "e" up-line-or-history
  bindkey -M vicmd "i" forward-char
  bindkey -M vicmd "s" vi-insert
  bindkey -M vicmd "S" vi-insert-bol
  bindkey -M vicmd "k" vi-repeat-search
  bindkey -M vicmd "K" vi-rev-repeat-search
  bindkey -M vicmd "l" beginning-of-line
  bindkey -M vicmd "L" end-of-line
  bindkey -M vicmd "j" vi-forward-word-end
  bindkey -M vicmd "J" vi-forward-blank-word-end

# Sane Undo, Redo, Backspace, Delete.
  bindkey -M vicmd "u" undo
  bindkey -M vicmd "U" redo
  bindkey -M vicmd "^?" backward-delete-char
  bindkey -M vicmd "^[[3~" delete-char

# Keep ctrl+r searching
  bindkey -M viins '^R' history-incremental-pattern-search-forward
  bindkey -M viins '^r' history-incremental-pattern-search-backward
#+END_SRC
** Zathura
There is no better ~pdf~ viewer than [[https://pwmt.org/projects/zathura/][zathura]], and it also works for ~djvu~ and
friends. As a plus point, it normally has very reasonable ~vim~ bindings, and an
excellent configuration system, so we will leverage that. The best part is that
we can just add to it using ~include zathuraColemak~ or whatever so as to be
minimally invasive.

#+BEGIN_SRC vim
map h scroll left
map n scroll down
map e scroll up
map i scroll right

map N scroll half-down
map E scroll half-up

map k search forward
map K search backward

# For TOC navigation
map [index] o toggle_index

# hjkl →  hnei
map [index] n navigate_index down
map [index] e navigate_index up
map [index] h navigate_index collapse
map [index] i navigate_index expand

map [index] H navigate_index collapse-all
map [index] I navigate_index expand-all
#+END_SRC
Zathura is a complicated beast, however, and my [[https://github.com/HaoZeke/Dotfiles/blob/master/dotfiles/colemak/.zshColemak][full configuration]] contains a
lot more information.
** i3
I have some bindings set up in terms of $left $right $up and $down, so it was
simple to re-bind them.
#+BEGIN_SRC vim
set $left h
set $down n
set $up e
set $right i
#+END_SRC
** MailMate
Sadly, one of the email clients I do use regularly of late is MailMate. It supports a rather rich set of keybindings placed, e.g. with ~"Library/Application Support/MailMate/Resources/KeyBindings/ColemakVIM.plist"~ configured as follows:
#+begin_src json
	"c"	= "newMessage:";
	"/"	= "searchAllMessages:";
	"n"	= "nextMessage:";
	"e"	= "previousMessage:";
	"h" = "collapseThread:";
	"i" = "expandThread:";
	"H" = "rootOfThread:";
	"I" = "lastOfThread:";
	"N" = "nextThread:";
	"E" = "previousThread:";
	"o"	= "openMessages:";
	"x" = ( "deleteMessage:", "nextMessage:" ); // Defaults to going to the previous message
	"a"	= "archive:";
	"s"	= "toggleFlag:";
	"!"	= "moveToJunk:";
	"r"	= "reply:";
	"R"	= "replyAll:";
	"f"	= "forwardMessage:";
	"^s"	= "saveDocument:";
#+end_src
** Conclusions
That seems to be it for now. If I think of more programs I use regularly which
allow VIM bindings, or keybindings in general, I'll probably just update this
post. My full dotfiles are [[https://github.com/HaoZeke/Dotfiles][present here]], and now include a ~colemak~ target.
[fn:attribute] The hook fix was suggested by the fantastic [[https://github.com/hlissner/][hlissner]] on the super friendly doom Discord server.
* TODO Reclaiming The Web With RSS :@notes:ramblings:workflow:tools:
:PROPERTIES:
:EXPORT_FILE_NAME: reclaim-web-rss
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
** Background
I came across some fascinating sites recently, which didn't bother to have an RSS feed. I've been an RSS fan for as long as I can remember, so, that's really about it.
** Context
*** Do I need an RSS reader?
If you follow a bunch of talented, but sporadic content creators, then *yes*. If you plan on subscribing to news-feeds and consolidated feeds which spam you with a million links per minute, then *no*. To clarify, the feed reader setup is essentially a mechanism by which the audience opts-in to post subscriptions, but without putting the onus on the creator to harvest user-emails or any other data[fn:privacy].
*** RSS? Atom? asrtinuo?
One of the words in the title is not a feed. That said, the concept of a feed, given the rise of Facebook, is probably not hard for anyone to understand anymore. The specifications are a little different, though in practice from an end-user perspective, there are few (if any) differences.
**** Halp?
There are excellent guidelines for every static site generator, and
** Programs
First of all, *no one* really needs to have an RSS subscription. This actually applies to *every* provider, from Feedly, Inoreader, The Old Reader, etc. to even FOSS projects like Miniflux[fn:supportfoss]. Locally, there are a couple of options, but I will only cover things I have used personally.
*** RSSOwl
RSSOwl was fantastic. It looked great, ran smoothly, and had nifty desktop integration. That is, until it stopped working. Though it is a Java package so it is always possible to downgrade your JRE and JDK, sadly this is probably not worth the effort.
*** News Flash
A successor to the well-known Feedbin, this seems to check most of the boxes for me. Though it does not come with a tray icon, KDocker is quick to step into that breach.
** Personal Workflow
While consuming content, there are only a couple of things to do.
** Conclusions
I am not sure there will ever be a resurgence of RSS feeds, given the rise of newsletters. It would be nice to see more people get back to RSS though.
- [x] Don't be the product
- [x] Don't be beholden to search engine rankings
- [x] Support content creators, not SEO specialists

[fn:supportfoss] That said, you should always try to support FOSS products, so if you really feel the need to have a central server, go FOSS
[fn:privacy] That last bit, is probably why companies like [[http://xanadu.ai/][Xanadu]] and others prefer to have a mailing list....

* DONE Compton to Picom and Zoom Glitches :@notes:workflow:tools:
CLOSED: [2020-05-12 Tue 01:32]
:PROPERTIES:
:EXPORT_FILE_NAME: compton-zoom-shadow
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
** Background
I [[https://rgoswami.me/tags/cs106a/][have been leading]] the fantastic section 881 as a virtual section leader for
the [[https://compedu.stanford.edu/codeinplace/v1/#/course][Stanford CS106A: Code in Place]] initiative for the past four weeks. I have
also spent a lot of time on Zoom, sharing my screen. Fun fact. My screen shares
look like this:

#+DOWNLOADED: screenshot @ 2020-05-12 01:05:38
#+caption: Zoom screen share with weird overlay
[[file:images/Background/2020-05-12_01-05-38_screenshot.png]]

This post is about hunting down what caused this amazing zoom
glitch[fn:whatglitch] and how I finally fixed it.
** Tiling Windows and Compositors
For reasons best left to another post, I use the fabulous [[https://i3wm.org/][i3 window manager]],
with colemak keybindings [[i3][described here]]. Recall that, [[https://en.wikipedia.org/wiki/Compositing_window_manager][from Wikipedia]]:
#+BEGIN_QUOTE
A compositing window manager is a window manager that provides applications with an off-screen buffer for each window. The window manager composites the window buffers into an image representing the screen and writes the result into the display memory.
#+END_QUOTE

For reasons I can no longer recall, ~compton~ has been a traditional aspect of my
workflow. As per my last update back in April last year; my configuration [[https://github.com/HaoZeke/Dotfiles/blob/8213473f313aa8fb9eb4c22d6b36a801b1584df6/dotfiles/archLinux/.config/compton.conf][is here]].
** Compton to Picom
Some time ago (actually [[https://github.com/yshui/picom/issues/222][many months ago]]), ~compton~
itself transitioned over to ~picom~, but remained largely compatible with my old
configuration[fn:archwikiplug].
To be clear, the transition was largely painless, with ample warnings in the
terminal showing up; along with very reasonable fallbacks. The key aspect of my ~compton.conf~ which caused the shadowing was:
#+BEGIN_SRC shell
shadow = true;
shadow-radius = 5;
shadow-offset-x = -5;
shadow-offset-y = -5;
shadow-opacity = 0.5;
#+END_SRC
The corrective measure was simply to set ~shadow-opacity~ to nothing; that is:
#+BEGIN_SRC shell
shadow-opacity = 0.0;
#+END_SRC
The rest of the configuration [[https://github.com/HaoZeke/Dotfiles/blob/master/dotfiles/archLinux/.config/picom.conf][is here]]; and contains a lot more, mostly
pertaining to opacity and other pretty effects[fn:plugdots].
** Conclusion
Finally we have achieved the goal of having normal screen sharing capabilities;
as seen below:

#+DOWNLOADED: screenshot @ 2020-05-12 01:13:37
#+caption: Just in time to see an excellent pun
[[file:images/Conclusion/2020-05-12_01-13-37_screenshot.png]]

The struggle was real, though the cause was trivial, and really highlights the
need to always know your system packages. In this case, no doubt my students
would have preferred not having to suffer through the darkness of my
screen[fn:bojackplug]. This has been a rather trivial post, but one to keep in
mind none-the-less.

** Comments
The older commenting system was implemented with [[https://utteranc.es][utteranc.es]] as seen below.
@@html:<script src="https://utteranc.es/client.js" repo="haozeke/haozeke.github.io" issue-term="pathname" theme="photon-dark" label="Utterance 💬" crossorigin="anonymous" async></script>@@

[fn:whatglitch] To be clear, none of the windows were the glitch. The issue was the darkened overlay
[fn:bojackplug] Though it might have also served as a metaphor for *darkness*
[fn:plugdots] The rest of [[https://github.com/HaoZeke/Dotfiles][my Dotfiles]], managed by the [[https://github.com/kobus-v-schoor/dotgit][excellent dotgit]] are also worth a look

[fn:archwikiplug] As always, the [[https://wiki.archlinux.org/index.php/Picom][ArchLinux Wiki]] is a great place for more information

* TODO Fancy Unicode XeTeX in Orgmode :@notes:tools:emacs:workflow:orgmode:
:PROPERTIES:
:EXPORT_FILE_NAME: fancy-unicode-orgmode
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:

\begin{align}
\nonumber W_{r\rightarrow\infty}=&-\int_{r}^{\infty}\!F\,\mathrm{d}y=-    \int_r^\infty \!     \dfrac{1}{4\pi \epsilon_0} \dfrac{q^2}{\alpha^2}     \dfrac{\alpha^3}{y^3}\left(1-    \dfrac{\alpha^2}    {y^2}\right)^{-2}\,\mathrm{d}y\\
=&-\dfrac{1}{4\pi \epsilon_0} \dfrac{q^2}{\alpha^2}\alpha^3     \underbrace{\int_r^\infty     \! y^{-3} \left(1-\dfrac{\alpha^2}    {y^2}\right)^{-2} \,\mathrm{d}y}_{I} \label{eq:WcondI}
\end{align}

* DONE LosslessCut, Zoom and an AMA for CS106A :@notes:teaching:cs106a:tools:
CLOSED: [2020-05-20 Wed 20:21]
:PROPERTIES:
:EXPORT_FILE_NAME: losslesscut-zoom-ama
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments false
:END:
** Background
I recently had the opportunity to take part in an AMA (ask me anything) session
for the CS106A students on Machine Learning for the Physical Sciences. This is a
post about the technical issues, and also includes a video if you read through.
** Zoom and LosslessCut
Zoom recordings are one of the nicer ways to deal with switching windows and
screen sharing, especially after fixing the [[Compton to Picom and Zoom Glitches][dark screen glitch]]. However, though
[[https://github.com/mifi/lossless-cut][LosslessCut]] works really well to get cut-points, exporting and merging the file
into one caused a bunch of glitches.
** Enter Handbrake
To not beat around the bush, the solution was to simply encode the Zoom
recording with [[https://handbrake.fr/][Handbrake]] before using LosslessCut[fn:whatsettings]. Since the
conversion takes a while, it is also neat to note that you can directly export
the cut points made with LosslessCut on the original video, then import them
onto the newly encoded file.
** Conclusions
I am not really sure how this will turn out, but it is a useful thing to keep in
mind. The introductory video turned out to be:

{{< youtube aOuqgyHHOK4 >}}

[fn:whatsettings] For me, the Vimeo Youtube HQ 1080p60 preset worked out well
* TODO FOSS Maintenance and Me :@personal:ramblings:explanations:thoughts:
Write about being the [[https://github.com/chriskempson/base16/issues/225#issuecomment-639739616][new maintainer]] for the base16-zathura thing and the AUR packages I maintain.
* DONE Nix with R and devtools :@programming:tools:nix:workflow:R:
CLOSED: [2020-06-06 Sat 05:49]
:PROPERTIES:
:EXPORT_FILE_NAME: nix-r-devtools
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
This post discusses briefly, the ~nix-shell~ environment for reproducible
programming. In particular, there is an emphasis on extensions for installing
and working with packages not in [[https://cran.r-project.org/web/packages/][CRAN]], i.e. packages off Github which are
normally installed with ~devtools~.
#+END_QUOTE

** Background
The entire [[https://nixos.org/][nix ecosystem]] is fantastic, and is the main packaging
system used by [[https://dseams.info][d-SEAMS]] as well.
Recently I began working through the [[https://xcelab.net/rm/statistical-rethinking/][excellent second edition]] of "Statistical
Rethinking" by [[https://twitter.com/rlmcelreath][Richard McElreath]][fn:whatwhy].

Unfortunately, the ~rethinking~ package which is a major component of the book
itself depends on the V8 engine for some reason. The reigning AUR[fn:whutaur]
package ([[https://aur.archlinux.org/packages/v8-r/][V8-r]]) broke with a [[https://aur.archlinux.org/packages/v8-r/#comment-749561][fun error message]] I couldn't be bothered to deal
with. Ominously, the [[https://pastebin.com/CbdCMZ8d][rest of the logs]] prominently featured ~Warning: Running
gclient on Python 3.~. Given that older ~python~ versions have been permanently
retired, this seemed like a bad thing to deal with[fn:sonothing]. In any case,
having weaned off non-nix dependency tools for ~python~ and friends, it seemed
strange to not do the same for R.

The standard installation for the package entails obtaining ~rstan~ (which is trivial with ~nixpkgs~) and then using:
#+BEGIN_SRC R
install.packages(c("coda","mvtnorm","devtools","loo","dagitty"))
library(devtools)
devtools::install_github("rmcelreath/rethinking")
#+END_SRC
We will break this down and work through this installation in Nix space.
** Nix and R
The standard approach to setting up a project ~shell.nix~ is simply by using the
~mkshell~ function. There are some common aspects to this workflow, with more
language specific details documented here. A simple first version might be:

#+BEGIN_SRC nix
let
    pkgs = import <nixpkgs> { };
in pkgs.mkShell {
    buildInputs = with pkgs; [
        zsh
        R
        rPackages.ggplot
        rPackages.data_table
    ];
    shellHook = ''
    echo "hello"
    '';
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

Where we note that we can install CRAN packages as easily as regular packages
(like R), except for the fact that they are kept in a ~pkgs.rPackages~
environment, as opposed to ~pkgs~. This is actually a common convention most
languages with central repos. The most interesting thing to note is that,
similar to the convention for ~nix-python~ setups, packages with a dot in the
name will be converted to having an underscore, i.e. ~data.table~ ->
~data_table~.

However, for the rethinking package, and many others, there is no current CRAN
package, and so the ~rPackages~ approach fails.

The ~LOCALE_ARCHIVE~ needs to be set for Linux machines, and is required for
working with other packages.

** Nix-R and Devtools
To work with non-CRAN packages, we need to modify our package setup a little. We
will also simplify our file to split the ~pkgs~ and the ~r-pkgs~.
*** Naive Approach
The naive approach works by using the ~shellHook~ to set ~R_LIBS_USER~ to save
user packages per-project.
#+BEGIN_SRC nix
{ pkgs ? import <nixpkgs> { } }:
with pkgs;
let
  my-r-pkgs = rWrapper.override {
    packages = with rPackages; [
      ggplot2
      knitr
      rstan
      tidyverse
      V8
      dagitty
      coda
      mvtnorm
      shape
      Rcpp
      tidybayes
    ];
  };
in mkShell {
  buildInputs = = with pkgs;[ git glibcLocales openssl openssh curl wget ];
  inputsFrom = [ my-r-pkgs ];
  shellHook = ''
    mkdir -p "$(pwd)/_libs"
    export R_LIBS_USER="$(pwd)/_libs"
  '';
  GIT_SSL_CAINFO = "${cacert}/etc/ssl/certs/ca-bundle.crt";
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

Note that here we will also need to set the ~GIT_SSL_CAINFO~ to prevent some
errors during the build process[fn:seenelsewhere].
*** Native Approach
The native approach essentially leverages the ~nix~ method for building ~R~
packages. This is the most reproducible of the lot, and also has the useful
property of storing the files in the ~nix-store~ so re-using packages across
different projects will not store, build or download the package again. The
values required can be calculated from ~nix-prefetch-git~ as follows:

#+BEGIN_SRC bash
nix-env -i nix-prefetch-git
nix-prefetch-git https://github.com/rmcelreath/rethinking.git
#+END_SRC

The crux of this approach is the following snippet[fn:defnfrm]:
#+BEGIN_SRC nix
(buildRPackage {
  name = "rethinking";
  src = fetchFromGitHub {
    owner = "rmcelreath";
    repo = "rethinking";
    rev = "d0978c7f8b6329b94efa2014658d750ae12b1fa2";
    sha256 = "1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0";
  };
  propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ];
 })
#+END_SRC

**** Project Shell
This formulation for some strange reason does not work from the shell or environment by default, but does work with ~nix-shell --run bash --pure~.
#+BEGIN_SRC nix
{ pkgs ? import <nixpkgs> { } }:
with pkgs;
let
  my-r-pkgs = rWrapper.override {
    packages = with rPackages; [
      ggplot2
      knitr
      rstan
      tidyverse
      V8
      dagitty
      coda
      mvtnorm
      shape
      Rcpp
      tidybayes
      (buildRPackage {
        name = "rethinking";
        src = fetchFromGitHub {
          owner = "rmcelreath";
          repo = "rethinking";
          rev = "d0978c7f8b6329b94efa2014658d750ae12b1fa2";
          sha256 = "1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0";
        };
        propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ];
      })
    ];
  };
in mkShell {
  buildInputs = with pkgs; [ git glibcLocales openssl which openssh curl wget my-r-pkgs ];
  shellHook = ''
    mkdir -p "$(pwd)/_libs"
    export R_LIBS_USER="$(pwd)/_libs"
    echo ${my-r-pkgs}/bin/R
  '';
  GIT_SSL_CAINFO = "${cacert}/etc/ssl/certs/ca-bundle.crt";
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

The reason behind this is simply that ~rWrapper~ forms an extra package which
has lower precedence than the user profile ~R~, which is documented in more
detail here on the [[https://nixos.wiki/wiki/R][NixOS wiki]].
**** User Profile
This is a more general approach which defines the environment for R with all the
relevant libraries and is described in the [[https://nixos.org/nixpkgs/manual/#r][nixpkgs manual]]. The following code
should be placed in ~$HOME/.config/nixpkgs/config.nix~:

#+BEGIN_SRC nix
{
  packageOverrides = super:
    let self = super.pkgs;
    in {
      rEnv = super.rWrapper.override {
        packages = with self.rPackages; [
          ggplot2
          knitr
          tidyverse
          tidybayes
          (buildRPackage {
            name = "rethinking";
            src = self.fetchFromGitHub {
              owner = "rmcelreath";
              repo = "rethinking";
              rev = "d0978c7f8b6329b94efa2014658d750ae12b1fa2";
              sha256 = "1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0";
            };
            propagatedBuildInputs =
              [ coda MASS mvtnorm loo shape rstan dagitty ];
          })
        ];
      };
    };
}
#+END_SRC

This snippet allows us to use our ~R~ as follows:
#+BEGIN_SRC bash
# Install things
nix-env -f "<nixpkgs>" -iA rEnv
# Fix locale
export LOCALE_ARCHIVE="$(nix-build --no-out-link "<nixpkgs>" -A glibcLocales)/lib/locale/locale-archive"
# Profit
R
#+END_SRC

Note that in this method, on Linux systems, the locale problem has to be fixed
with the explicit export. This means that this should be used mostly with
project level environments, instead of populating the global shell RC files.

*Update:* There is [[Emacs for Nix-R][another post]] with methods to reload this configuration automatically
** Conclusions
Of the methods described, the most useful method for working with packages not
hosted on CRAN is through the user-profile, while the ~shell.nix~ method is
useful in conjunction, for managing various projects. So the ideal approach is
then to use the user profile for installing anything which normally uses
~devtools~ and then use ~shell.nix~ for the rest.

Note that if the [[Project Shell]] is used with a [[User Profile]] as described in the
next section, all packages defined there can be dropped and then the project
shell does not need to execute ~R~ by default. The simplified ~shell.nix~ is
then simply:

#+BEGIN_SRC nix
{ pkgs ? import <nixpkgs> { } }:
with pkgs;
let
  my-r-pkgs = rWrapper.override {
    packages = with rPackages; [
      ggplot2
    ];
  };
in mkShell {
  buildInputs = with pkgs;[ git glibcLocales openssl which openssh curl wget my-r-pkgs ];
  inputsFrom = [ my-r-pkgs ];
  shellHook = ''
    mkdir -p "$(pwd)/_libs"
    export R_LIBS_USER="$(pwd)/_libs"
  '';
  GIT_SSL_CAINFO = "${cacert}/etc/ssl/certs/ca-bundle.crt";
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

The entire workflow for
~rethinking~ is [[Statistical Rethinking and Nix][continued here]].

[fn:whatwhy] As part of [[https://ugla.hi.is/kennsluskra/index.php?sid=&tab=nam&chapter=namskeid&id=71055920203][a summer course]] at the University of Iceland relating to their successful [[http://covid.hi.is/][COVID-19 model]]
[fn:whutaur] The [[https://wiki.archlinux.org/index.php/Arch_User_Repository][Arch User Repository]] is the port of first call for most ArchLinux users
[fn:sonothing] Though, like any good AUR user, I did post a bug report
[fn:seenelsewhere] This approach is also [[https://churchman.nl/tag/r/][discussed here]]
[fn:defnfrm] As discussed on [[https://github.com/NixOS/nixpkgs/issues/44290][this issue]], this [[https://stackoverflow.com/questions/55176609/how-to-install-r-and-packages-through-configuration-nix-and-how-to-add-packages][stackoverflow question]] and also [[https://github.com/rikhuijzer/nix-with-r/blob/master/default.nix][seen here]]

* DONE Statistical Rethinking and Nix :@programming:tools:nix:workflow:R:
CLOSED: [2020-06-07 Sun 04:24]
:PROPERTIES:
:EXPORT_FILE_NAME: rethinking-r-nix
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:

#+BEGIN_QUOTE
This post describes how to set up a transparent automated setup for reproducible
~R~ workflows using ~nixpkgs~, ~niv~, and ~lorri~. The explanatory example used
throughout the post is one of setting up the ~rethinking~ package and running
some examples
from the [[https://xcelab.net/rm/statistical-rethinking/][excellent second edition]] of "Statistical
Rethinking" by [[https://twitter.com/rlmcelreath][Richard McElreath]].
#+END_QUOTE

** Background
As detailed [[Nix with R and devtools][in an earlier post]][fn:wherehook], I had set up Nix to work with
non-CRAN packages. If the rest of this section is unclear, please refer back to [[Nix with R and devtools][the earlier post]].
*** Setup

For the remainder of the post, we will set up a basic project structure:
#+BEGIN_SRC bash
mkdir tryRnix/
#+END_SRC

#+RESULTS:

Now we will create a ~shell.nix~ as[fn:explaaiin]:
#+BEGIN_SRC  nix
# shell.nix
{ pkgs ? import <nixpkgs> { } }:
with pkgs;
let
  my-r-pkgs = rWrapper.override {
    packages = with rPackages; [
      ggplot2
      tidyverse
      tidybayes
      tidybayes.rethinking
      (buildRPackage {
        name = "rethinking";
        src = fetchFromGitHub {
          owner = "rmcelreath";
          repo = "rethinking";
          rev = "d0978c7f8b6329b94efa2014658d750ae12b1fa2";
          sha256 = "1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0";
        };
        propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ];
      })
    ];
  };
in mkShell {
  buildInputs = with pkgs; [ git glibcLocales openssl which openssh curl wget ];
  inputsFrom = [ my-r-pkgs ];
  shellHook = ''
    mkdir -p "$(pwd)/_libs"
    export R_LIBS_USER="$(pwd)/_libs"
  '';
  GIT_SSL_CAINFO = "${cacert}/etc/ssl/certs/ca-bundle.crt";
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

So we have:
#+BEGIN_SRC bash :exports both
tree tryRnix
#+END_SRC

#+RESULTS:
| tryRnix |              |   |      |
| └──     | shell.nix    |   |      |
| 0       | directories, | 1 | file |

*** Introspection
At this point:
- I was able to install packages (system and ~R~) arbitrarily
- I was able to use project specific folders
- Unlike ~npm~, ~pipenv~, ~poetry~, ~conda~ and friends, my system was not bloated by downloading and setting up the same packages every-time I used them in different projects

However, though this is a major step up from being chained to RStudio and my
system package manager, it is still perhaps not immediately obvious how this
workflow is reproducible. Admittedly, I have defined my packages in a nice
functional manner; but someone else might have a different upstream channel they
are tracking, and thus will have different packages. Indeed the only packages
which I could be sure of were the ~R~ packages I built from Github, since those
were tied to a hash. Finally, the setup described for each project is pretty
onerous, and it is not immediately clear how to leverage fantastic tools like
~direnv~ for working through this.
** Towards Reproducible Environments
The astute reader will have noticed that I mentioned that the ~R~ packages were
reproducible since they were tied to a *hash*, and might reasonable argue that
the entire Nix ecosystem is about *hashing* in the first place. Once we realize
that, the rest is relatively simple[fn:callbacktopost].
*** Niv and Pinning
[[https://github.com/nmattia/niv/][Niv]] essentially keeps track of the channel from which all the packages are installed. Setup is pretty minimal.
#+BEGIN_SRC bash
cd tryRnix/
nix-env -i niv
niv init
#+END_SRC

At this point, we have:
#+BEGIN_SRC bash :exports both
tree tryRnix
#+END_SRC

#+RESULTS:
| tryRnix |            |              |       |
| ├──     | nix        |              |       |
| │       | ├──        | sources.json |       |
| │       | └──        | sources.nix  |       |
| └──     | shell.nix  |              |       |
|         |            |              |       |
| 1       | directory, | 3            | files |

We will have to update our ~shell.nix~ to use the new sources.

#+BEGIN_SRC nix
let
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  stdenv = pkgs.stdenv;
  my-r-pkgs = pkgs.rWrapper.override {
    packages = with pkgs.rPackages; [
      ggplot2
      tidyverse
      tidybayes
    ];
  };
in pkgs.mkShell {
  buildInputs = with pkgs;[ git glibcLocales openssl which openssh curl wget my-r-pkgs ];
  shellHook = ''
    mkdir -p "$(pwd)/_libs"
    export R_LIBS_USER="$(pwd)/_libs"
  '';
  GIT_SSL_CAINFO = "${pkgs.cacert}/etc/ssl/certs/ca-bundle.crt";
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${pkgs.glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

We could inspect and edit these sources by hand, but it is much more convenient
to simply use ~niv~ again when we need to update these.

#+BEGIN_SRC bash
cd tryRnix/
niv update nixpkgs -b nixpkgs-unstable
#+END_SRC

At this stage we have a reproducible set of packages ready to use. However it is
still pretty annoying to have to go through the trouble of writing ~nix-shell~
and also waiting while it rebuilds when we change things.
*** Lorri and Direnv
[[Poetry and Direnv][In the past]], I have made my admiration for ~direnv~ very clear (especially for
~python-poetry~). However, though ~direnv~ does allow us to include arbitrary ~bash~ logic into our projects, it would be nice to have something which has some defaults for nix. Thankfully, the folks at TweagIO developed [[https://github.com/target/lorri][lorri]] to scratch that itch.

The basic setup is simple:

#+BEGIN_SRC bash
nix-env -i lorri
cd tryRnix/
lorri init
#+END_SRC

#+BEGIN_SRC bash :exports both
tree -a tryRnix/
#+END_SRC

#+RESULTS:
| tryRnix/ |            |              |       |
| ├──      | .envrc     |              |       |
| ├──      | nix        |              |       |
| │        | ├──        | sources.json |       |
| │        | └──        | sources.nix  |       |
| └──      | shell.nix  |              |       |
|          |            |              |       |
| 1        | directory, | 4            | files |

We can and should inspect the environment ~lorri~ wants us to load with ~direnv~ file:

#+BEGIN_SRC bash :exports both
cat tryRnix/.envrc
#+END_SRC

#+RESULTS:
: $(lorri direnv)

In and of itself that is not too descriptive, so we should run  that on our own first.

#+BEGIN_SRC bash
EVALUATION_ROOT="$HOME/.cache/lorri/gc_roots/407bd4df60fbda6e3a656c39f81c03c2/gc_root/shell_gc_root"

watch_file "/run/user/1000/lorri/daemon.socket"
watch_file "$EVALUATION_ROOT"

#!/usr/bin/env bash
# ^ shebang is unused as this file is sourced, but present for editor
# integration. Note: Direnv guarantees it *will* be parsed using bash.

function punt () {
    :
}

# move "origPreHook" "preHook" "$@";;
move() {
    srcvarname=$1 # example: varname might contain the string "origPATH"
    # drop off the source variable name
    shift

    destvarname=$1 # example: destvarname might contain the string "PATH"
    # drop off the destination variable name
    shift

    # like: export origPATH="...some-value..."
    export "${@?}";

    # set $original to the contents of the variable $srcvarname
    # refers to
    eval "$destvarname=\"${!srcvarname}\""

    # mark the destvarname as exported so direnv picks it up
    # (shellcheck: we do want to export the content of destvarname!)
    # shellcheck disable=SC2163
    export "$destvarname"

    # remove the export from above, ie: export origPATH...
    unset "$srcvarname"
}

function prepend() {
    varname=$1 # example: varname might contain the string "PATH"

    # drop off the varname
    shift

    separator=$1 # example: separator would usually be the string ":"

    # drop off the separator argument, so the remaining arguments
    # are the arguments to export
    shift

    # set $original to the contents of the the variable $varname
    # refers to
    original="${!varname}"

    # effectfully accept the new variable's contents
    export "${@?}";

    # re-set $varname's variable to the contents of varname's
    # reference, plus the current (updated on the export) contents.
    # however, exclude the ${separator} unless ${original} starts
    # with a value
    eval "$varname=${!varname}${original:+${separator}${original}}"
}

function append() {
    varname=$1 # example: varname might contain the string "PATH"

    # drop off the varname
    shift

    separator=$1 # example: separator would usually be the string ":"
    # drop off the separator argument, so the remaining arguments
    # are the arguments to export
    shift


    # set $original to the contents of the the variable $varname
    # refers to
    original="${!varname:-}"

    # effectfully accept the new variable's contents
    export "${@?}";

    # re-set $varname's variable to the contents of varname's
    # reference, plus the current (updated on the export) contents.
    # however, exclude the ${separator} unless ${original} starts
    # with a value
    eval "$varname=${original:+${original}${separator}}${!varname}"
}

varmap() {
    if [ -f "$EVALUATION_ROOT/varmap-v1" ]; then
        # Capture the name of the variable being set
        IFS="=" read -r -a cur_varname <<< "$1"

        # With IFS='' and the `read` delimiter being '', we achieve
        # splitting on \0 bytes while also preserving leading
        # whitespace:
        #
        #    bash-3.2$ printf ' <- leading space\0bar\0baz\0' \
        #                  | (while IFS='' read -d $'\0' -r x; do echo ">$x<"; done)
        #    > <- leading space<
        #    >bar<
        #    >baz<```
        while IFS='' read -r -d '' map_instruction \
           && IFS='' read -r -d '' map_variable \
           && IFS='' read -r -d '' map_separator; do
            unset IFS

            if [ "$map_variable" == "${cur_varname[0]}" ]; then
                if [ "$map_instruction" == "append" ]; then
                    append "$map_variable" "$map_separator" "$@"
                    return
                fi
            fi
        done < "$EVALUATION_ROOT/varmap-v1"
    fi


    export "${@?}"
}

function declare() {
    if [ "$1" == "-x" ]; then shift; fi

    # Some variables require special handling.
    #
    # - punt:    don't set the variable at all
    # - prepend: take the new value, and put it before the current value.
    case "$1" in
        # vars from: https://github.com/NixOS/nix/blob/92d08c02c84be34ec0df56ed718526c382845d1a/src/nix-build/nix-build.cc#L100
        "HOME="*) punt;;
        "USER="*) punt;;
        "LOGNAME="*) punt;;
        "DISPLAY="*) punt;;
        "PATH="*) prepend "PATH" ":" "$@";;
        "TERM="*) punt;;
        "IN_NIX_SHELL="*) punt;;
        "TZ="*) punt;;
        "PAGER="*) punt;;
        "NIX_BUILD_SHELL="*) punt;;
        "SHLVL="*) punt;;

        # vars from: https://github.com/NixOS/nix/blob/92d08c02c84be34ec0df56ed718526c382845d1a/src/nix-build/nix-build.cc#L385
        "TEMPDIR="*) punt;;
        "TMPDIR="*) punt;;
        "TEMP="*) punt;;
        "TMP="*) punt;;

        # vars from: https://github.com/NixOS/nix/blob/92d08c02c84be34ec0df56ed718526c382845d1a/src/nix-build/nix-build.cc#L421
        "NIX_ENFORCE_PURITY="*) punt;;

        # vars from: https://www.gnu.org/software/bash/manual/html_node/Bash-Variables.html (last checked: 2019-09-26)
        # reported in https://github.com/target/lorri/issues/153
        "OLDPWD="*) punt;;
        "PWD="*) punt;;
        "SHELL="*) punt;;

        # https://github.com/target/lorri/issues/97
        "preHook="*) punt;;
        "origPreHook="*) move "origPreHook" "preHook" "$@";;

        *) varmap "$@" ;;
    esac
}

export IN_NIX_SHELL=impure

if [ -f "$EVALUATION_ROOT/bash-export" ]; then
    # shellcheck disable=SC1090
    . "$EVALUATION_ROOT/bash-export"
elif [ -f "$EVALUATION_ROOT" ]; then
    # shellcheck disable=SC1090
    . "$EVALUATION_ROOT"
fi

unset declare

Jun 06 19:02:32.368 INFO lorri has not completed an evaluation for this project yet, expr: $HOME/Git/Github/WebDev/Mine/haozeke.github.io/content-org/tryRnix/shell.nix
Jun 06 19:02:32.368 WARN `lorri direnv` should be executed by direnv from within an `.envrc` file, expr: $HOME/Git/Github/WebDev/Mine/haozeke.github.io/content-org/tryRnix/shell.nix
#+END_SRC

Upon inspection, that seems to check out. So now we can enable this.

#+BEGIN_SRC bash
direnv allow
#+END_SRC

Additionally, we will need to stick to using a pure environment as much as
possible to prevent unexpected situations. So we set:

#+BEGIN_SRC bash
# .envrc
eval "$(lorri direnv)"
nix-shell --run bash --pure
#+END_SRC

There's still a catch though. We need to have ~lorri daemon~ running to make
sure the packages are built automatically without us having to exit the shell
and re-run things. We can [[https://github.com/target/lorri/blob/master/contrib/daemon.md][turn to the documentation]] for this. Essentially, we
need to have a user-level systemd socket file and service for ~lorri~.

#+BEGIN_SRC bash
# ~/.config/systemd/user/lorri.socket
[Unit]
Description=Socket for Lorri Daemon

[Socket]
ListenStream=%t/lorri/daemon.socket
RuntimeDirectory=lorri

[Install]
WantedBy=sockets.target
#+END_SRC

#+BEGIN_SRC bash
# ~/.config/systemd/user/lorri.service
[Unit]
Description=Lorri Daemon
Requires=lorri.socket
After=lorri.socket

[Service]
ExecStart=%h/.nix-profile/bin/lorri daemon
PrivateTmp=true
ProtectSystem=strict
ProtectHome=read-only
Restart=on-failure
#+END_SRC

With that we are finally ready to start working with our auto-managed,
reproducible environments.

#+BEGIN_SRC bash
systemctl --user daemon-reload && \
systemctl --user enable --now lorri.socket
#+END_SRC

** Rethinking
As promised, we will first test the setup to see that everything is working. Now
is also a good time to try the ~tidybayes.rethinking~ package. In order to use
it, we will need to define the ~rethinking~ package in a way so we can pass it
to the ~buildInputs~ for ~tidybayes.rethinking~. We will modify new ~shell.nix~
as follows:

#+BEGIN_SRC nix :tangle tryRnix/shell.nix
# shell.nix
let
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  stdenv = pkgs.stdenv;
  rethinking = with pkgs.rPackages;
    buildRPackage {
      name = "rethinking";
      src = pkgs.fetchFromGitHub {
        owner = "rmcelreath";
        repo = "rethinking";
        rev = "d0978c7f8b6329b94efa2014658d750ae12b1fa2";
        sha256 = "1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0";
      };
      propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ];
    };
  tidybayes_rethinking = with pkgs.rPackages;
    buildRPackage {
      name = "tidybayes.rethinking";
      src = pkgs.fetchFromGitHub {
        owner = "mjskay";
        repo = "tidybayes.rethinking";
        rev = "df903c88f4f4320795a47c616eef24a690b433a4";
        sha256 = "1jl3189zdddmwm07z1mk58hcahirqrwx211ms0i1rzbx5y4zak0c";
      };
      propagatedBuildInputs =
        [ dplyr tibble rlang MASS tidybayes rethinking rstan ];
    };
  rEnv = pkgs.rWrapper.override {
    packages = with pkgs.rPackages; [
      ggplot2
      tidyverse
      tidybayes
      devtools
      modelr
      cowplot
      ggrepel
      RColorBrewer
      purrr
      forcats
      rstan
      rethinking
      tidybayes_rethinking
    ];
  };
in pkgs.mkShell {
  buildInputs = with pkgs; [ git glibcLocales which ];
  inputsFrom = [ rEnv ];
  shellHook = ''
    mkdir -p "$(pwd)/_libs"
    export R_LIBS_USER="$(pwd)/_libs"
  '';
  GIT_SSL_CAINFO = "${pkgs.cacert}/etc/ssl/certs/ca-bundle.crt";
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${pkgs.glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

The main thing to note here is that we need the output of the derivation we
create here, i.e. we need to use ~inputsFrom~ and NOT ~buildInputs~ for ~rEnv~.

Let us try to get a nice graphic for the conclusion.

#+BEGIN_SRC R :tangle tryRnix/tesPlot.R
library(magrittr)
library(dplyr)
library(purrr)
library(forcats)
library(tidyr)
library(modelr)
library(tidybayes)
library(tidybayes.rethinking)
library(ggplot2)
library(cowplot)
library(rstan)
library(rethinking)
library(ggrepel)
library(RColorBrewer)

theme_set(theme_tidybayes())
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())


set.seed(5)
n = 10
n_condition = 5
ABC =
  tibble(
    condition = factor(rep(c("A","B","C","D","E"), n)),
    response = rnorm(n * 5, c(0,1,2,1,-1), 0.5)
  )

mtcars_clean = mtcars %>%
  mutate(cyl = factor(cyl))

m_cyl = ulam(alist(
    cyl ~ dordlogit(phi, cutpoint),
    phi <- b_mpg*mpg,
    b_mpg ~ student_t(3, 0, 10),
    cutpoint ~ student_t(3, 0, 10)
  ),
  data = mtcars_clean,
  chains = 4,
  cores = parallel::detectCores(),
  iter = 2000
)

cutpoints = m_cyl %>%
  recover_types(mtcars_clean) %>%
  spread_draws(cutpoint[cyl])

# define the last cutpoint
last_cutpoint = tibble(
  .draw = 1:max(cutpoints$.draw),
  cyl = "8",
  cutpoint = Inf
)

cutpoints = bind_rows(cutpoints, last_cutpoint) %>%
  # define the previous cutpoint (cutpoint_{j-1})
  group_by(.draw) %>%
  arrange(cyl) %>%
  mutate(prev_cutpoint = lag(cutpoint, default = -Inf))

fitted_cyl_probs = mtcars_clean %>%
  data_grid(mpg = seq_range(mpg, n = 101)) %>%
  add_fitted_draws(m_cyl) %>%
  inner_join(cutpoints, by = ".draw") %>%
  mutate(`P(cyl | mpg)` =
    # this part is logit^-1(cutpoint_j - beta*x) - logit^-1(cutpoint_{j-1} - beta*x)
    plogis(cutpoint - .value) - plogis(prev_cutpoint - .value)
  )


data_plot = mtcars_clean %>%
  ggplot(aes(x = mpg, y = cyl, color = cyl)) +
  geom_point() +
  scale_color_brewer(palette = "Dark2", name = "cyl")

fit_plot = fitted_cyl_probs %>%
  ggplot(aes(x = mpg, y = `P(cyl | mpg)`, color = cyl)) +
  stat_lineribbon(aes(fill = cyl), alpha = 1/5) +
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2")

png(filename="../images/rethinking.png")
plot_grid(ncol = 1, align = "v",
  data_plot,
  fit_plot
)
dev.off
#+END_SRC

Finally we will run this in our environment.

#+BEGIN_SRC bash
Rscript tesPlot.R
#+END_SRC

[[file:images/rethinking.png]]

** Conclusions
This post was really more of an exploratory follow up to the previous post, and
does not really work in isolation. Then again, at this point everything seems to
have worked out well. ~R~ with Nix has finally become a truly viable combination
for any and every analysis under the sun. Some parts of the workflow are still a
bit janky, but will probably resolve themselves over time.

*Update:* There is [[Emacs for Nix-R][a final part]] detailing automated ways of reloading the system configuration

[fn:wherehook] My motivations were laid out in the [[Nix with R and devtools][aforementioned post]], and will not be repeated
[fn:explaaiin] For why these are the way they are see the this is written, see the [[Nix with R and devtools][aforementioned post]]
[fn:callbacktopost] Christine Dodrill [[https://christine.website/blog/how-i-start-nix-2020-03-08][has a great write up]] on using these tools as well
* DONE Emacs for Nix-R :@programming:tools:nix:workflow:R:emacs:
CLOSED: [2020-06-10 Wed 00:12]
:PROPERTIES:
:EXPORT_FILE_NAME: emacs-nix-r
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A short post on my current set-up for ~R~ with ~nixpkgs~ and ~emacs~ to
auto-compile my system configuration.
#+END_QUOTE
** Background
This is my third post on working with ~nixpkgs~ and ~R~.

- [[Nix with R and devtools][Part I]] covered ways of working effectively with ~R~ and ~nixpkgs~
- [[Statistical Rethinking and Nix][Part II]] dealt with composing dependent ~devtools~ packages in a per-package environment, with a focus on ~rethinking~ and ~tidybayes.rethinking~

This final part is about automating the system-wide configuration using ~emacs~.
Specifically ~doom-emacs~. Naturally, this is the most optimal way to work with
~nix~ packages as well.
*** System Configuration
After experimenting with a per-project layout, I decided to use the full system
configuration instead of the per-project layout. So I simply set:
#+BEGIN_SRC nix
# $HOME/.config/nixpkgs/config.nix
{
  packageOverrides = super:
    let
      self = super.pkgs;
      rethinking = with self.rPackages;
        buildRPackage {
          name = "rethinking";
          src = self.fetchFromGitHub {
            owner = "rmcelreath";
            repo = "rethinking";
            rev = "d0978c7f8b6329b94efa2014658d750ae12b1fa2";
            sha256 = "1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0";
          };
          propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ];
        };
      tidybayes_rethinking = with self.rPackages;
        buildRPackage {
          name = "tidybayes.rethinking";
          src = self.fetchFromGitHub {
            owner = "mjskay";
            repo = "tidybayes.rethinking";
            rev = "df903c88f4f4320795a47c616eef24a690b433a4";
            sha256 = "1jl3189zdddmwm07z1mk58hcahirqrwx211ms0i1rzbx5y4zak0c";
          };
          propagatedBuildInputs =
            [ dplyr tibble rlang MASS tidybayes rethinking rstan ];
        };
    in {
      rEnv = super.rWrapper.override {
        packages = with self.rPackages; [
          tidyverse
          devtools
          modelr
          purrr
          forcats
          ####################
          # Machine Learning #
          ####################
          # MLR3
          mlr3
          mlr3viz
          mlr3learners
          mlr3pipelines
          # Plotting tools
          ggplot2
          cowplot
          ggrepel
          RColorBrewer
          # Stan Stuff
          rstan
          tidybayes
          # Text Utilities
          orgutils
          latex2exp
          kableExtra
          knitr
          data_table
          printr
          # Devtools Stuff
          rethinking
          tidybayes_rethinking
        ];
      };
    };
}
#+END_SRC
If any of these look strange, refer to the [[Nix with R and devtools][earlier posts]].
** Automation Pains
~direnv~, ~lorri~ and
~niv~ (the heroes of [[Statistical Rethinking and Nix][Part II]]) are not really useful for working with the system-wide configuration, but
an elegant solution still exists, which leverages ~firestarter~ and
~after-save-hooks~ in ~emacs~.
*** Firestarter
[[https://depp.brause.cc/firestarter/][Firestarter]] is my favorite method of working with shell commands after saving
things. My setup is simply:
#+begin_src emacs-lisp
; packages.el
(package! firestarter)
#+end_src
This is coupled with a simple configuration.
#+begin_src emacs-lisp
; config.el
(use-package! firestarter
  :ensure t
  :init
  (firestarter-mode)
  :config
  (setq firestarter-default-type t)
)
#+end_src
The default type corresponds to demanding the shell outupt for the commands.
*** Nix-R Stuff
To finalize this setup, we will need to modify our system configuration
slightly. For brevity, we simply note the following local variables.

#+BEGIN_SRC nix
# $HOME/.config/nixpkgs/config.nix
# Local Variables:
# firestarter: "nix-env -f '<nixpkgs>' -iA rEnv"
# firestarter-default-type: (quote failure)
# End:
#+END_SRC

The ~firestarter-default-type~ used here is to ensure that errors are displayed
in a buffer.

To check what is being installed (if anything) simply run:
#+BEGIN_SRC bash
nix-env -f "<nixpkgs>" -iA rEnv --dry-run
#+END_SRC
** Conclusion
This is my current setup. It works out better than most of my other attempts and
seems to be an optimal approach. The packages are versioned, everything is
automated, and I can reproduce changes across all my machines. Will stick with
this.

* DONE Temporary LaTeX Documents with Orgmode :@programming:tools:emacs:workflow:orgmode:
CLOSED: [2020-06-19 Fri 05:07]
:PROPERTIES:
:EXPORT_FILE_NAME: org-arb-tex
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A post on working with transient TeX templates in ~orgmode~ without modifying global configurations. This will also serve as a rudimentary introduction to TeX in ~orgmode~.
#+END_QUOTE
** Background
The sad reality of working in a field dominated by institutional actors which do not care for recognizing software development as a skill is that there are often a lot of ugly LaTeX templates[fn:thatsall]. In particular, often Universities have arbitrary LaTeX templates from the the dark days of 2010 something, which include gratuitous usage of say, ~natbib~ instead of ~biblatex~. In other situations, ~.cls~ files define separate document classes which are not covered by the ~orgmode~ defaults and need to be accounted for.
** Standard methods
Essentially for the exporter, the document is broken into[fn:halptex]:
- *document_class* :: This cannot be changed arbitrarily and [[https://orgmode.org/manual/LaTeX-specific-export-settings.html#LaTeX-specific-export-settings][has to be]] a valid element of ~org-latex-classes~
- *preamble* :: This section of the document is essentially everything before ~\begin{document}~ and after ~\documentclass{...}~
- *body* :: The rest of the document

We will briefly cover the standard methods of entering TeX in each of these sections, in reverse order since that is the direction in which the intuitive aspect decreases.
*** In-Body TeX
The method of writing TeX in ~orgmode~ for the document involves simply writing TeX directly, or wrapping the TeX markup in an export TeX block for font locking[fn:fontwhat]. Essentially, for a document snippet:
#+BEGIN_SRC latex :exports code
% #+BEGIN_SRC latex :exports code
\begin{align}
\pi(x) &= \sum_{n=1}^{\infty}\frac{\mu(n)}{n}\Pi(x^{\frac{1}{n}}) \\
       &= \Pi(x) -\frac{1}{2}\Pi(x^{\frac{1}{2}}) - \frac{1}{3}\Pi(x^{\frac{1}{3}}) - \frac{1}{5}\Pi(x^{\frac{1}{5}}) + \frac{1}{6} \Pi(x^{\frac{1}{6}}) -\cdots,
\end{align}
% #+END_SRC
#+END_SRC

Which will actually be rendered in a real document of course[fn:butwhatformula]:
# It is the Möbius inversion formula

\begin{align}
\pi(x) &= \sum_{n=1}^{\infty}\frac{\mu(n)}{n}\Pi(x^{\frac{1}{n}}) \\
       &= \Pi(x) -\frac{1}{2}\Pi(x^{\frac{1}{2}}) - \frac{1}{3}\Pi(x^{\frac{1}{3}}) - \frac{1}{5}\Pi(x^{\frac{1}{5}}) + \frac{1}{6} \Pi(x^{\frac{1}{6}}) -\cdots,
\end{align}

There is also the inline form of writing LaTeX with ~@@\sin{x}@@~ which is essentially $\sin{x}$.
*** Preamble
The main use of the preamble is to either add classes or modify class options for loaded packages like ~geometry~. Essentially, for ~orgmode~, anything prefixed with ~#+LATEX_HEADER:~ gets inserted in the preamble.
#+BEGIN_SRC org :exports code
#+LATEX_HEADER: \usepackage{amssymb,amsmath,MnSymbol}
#+LATEX_HEADER: \usepackage{unicode-math}
#+LATEX_HEADER: \usepackage{mathtools}
#+END_SRC
For larger documents, this gets quite annoying for loading packages. We will demonstrate a more aesthetically pleasant form later in this post.
*** LaTeX classes
Working with document classes is the least intuitive of all TeX manipulations, because for some reason, ~#+LATEX_CLASS:~ only accepts values defined in ~org-latex-classes~.

The standard approach to extending the ~orgmode~ TeX backend is to add lines like the following in ~init.el~ or, in my case[fn:whatwhy], ~config.org~:
#+BEGIN_SRC emacs-lisp
(add-to-list 'org-latex-classes
             '("koma-article" "\\documentclass{scrartcl}"
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}" . "\\paragraph*{%s}")
               ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
#+END_SRC

This is alright for often used classes like the ~koma-⋆~ family of LaTeX document-classes, but it is hardly ideal for one-off TeX templates which are meant for say, grant proposals[fn:grantwhat].
** Elisp to the rescue
The core idea is quite simple.
#+BEGIN_QUOTE
Since ~orgmode~ files are literate documents, and ~emacs~ is self-documenting and completely programmable, it should be possible to execute code to deterministically set the state of ~emacs~ before exporting the document to TeX.
#+END_QUOTE
Practically this has a few moving parts. In the following sections, assume that we have a ~.cls~ file which defines a document-class ~foo~ with a bunch of packages which conflict with our global configuration.
*** Adding Document Classes
Instead of adding the code snippet to our global configuration, we will now add it to the document directly with the comments indicating the appropriate environment.
#+BEGIN_SRC emacs-lisp :exports code  :results none :eval never
;; #+BEGIN_SRC emacs-lisp :exports none  :results none :eval always
(add-to-list 'org-latex-classes
             '("foo" "\\documentclass{foo}"
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}" . "\\paragraph*{%s}")
               ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
;; #+END_SRC
#+END_SRC

Where the header arguments simply ensure that the code and result do not show up in the document, and that the chunk is always evaluated by ~org-babel~.
*** Ensuring Purity
Since we want to stick to an external template defined in ~foo~ *and no other packages* we will need to clear the defaults we lovingly set globally for our convenience.
#+BEGIN_SRC emacs-lisp :exports code  :results none :eval never
;; #+BEGIN_SRC emacs-lisp :exports none  :results none :eval always
(setq org-latex-packages-alist 'nil)
(setq org-latex-minted-options 'nil) ;; Separately nullify minted
;; #+END_SRC
#+END_SRC
*** Pretty Packages
For packages we really would like to add, we can now leverage the ~elisp~ code instead of the ugly ~#+LATEX_HEADER:~ lines.
#+BEGIN_SRC emacs-lisp :exports code  :results none :eval never
;; #+BEGIN_SRC emacs-lisp :exports none  :results none :eval always
(setq org-latex-default-packages-alist
  '(("utf8" "inputenc"  t)
    ("normalem" "ulem"  t)
    (""     "mathtools"   t)
    ))
;; #+END_SRC
#+END_SRC
Note that for setting options, we will still need to use the ~#+LATEX_HEADER:~ syntax.
*** Automating With Hooks
At this stage, we have a chunk of ~elisp~ we can manually evaluate with ~org-babel~ before exporting with ~org-latex-export-to-pdf~ or ~org-latex-export-to-latex~. However, this can get old quickly, so we will instead have a ~before-save-hook~ to do this for us.
#+BEGIN_SRC org :exports code
# Local Variables:
# before-save-hook: org-babel-execute-buffer
# End:
#+END_SRC
**** Bonus Hook
In [[https://dotdoom.rgoswami.me/config.html#org13eed8a][my own configuration]], I have a function defined for an ~after-save-hook~ which generates the TeX file without having me deal with it. For a per-file configuration of this, or globally, the ~elisp~ is:
#+BEGIN_SRC emacs-lisp :exports code
(defun haozeke/org-save-and-export-latex ()
  (interactive)
  (if (eq major-mode 'org-mode)
    (org-latex-export-to-latex t)))
#+END_SRC
This indirection is required to call the function as a ~hook~. The additional ~t~ allows for asynchronous non blocking exports. Now this can be used as:
#+BEGIN_SRC org :exports code
# Local Variables:
# after-save-hook: haozeke/org-save-and-export-latex
# End:
#+END_SRC
** Conclusions
The entire file would look something like this (the ~elisp~ can be anywhere in the ~orgmode~ file):
#+BEGIN_SRC emacs-lisp :exports code  :results none :eval never
;; #+BEGIN_SRC emacs-lisp :exports none  :results none :eval always
(add-to-list 'org-latex-classes
             '("foo" "\\documentclass{foo}"
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}" . "\\paragraph*{%s}")
               ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
(setq org-latex-packages-alist 'nil)
(setq org-latex-default-packages-alist
  '(("utf8" "inputenc"  t)
    (""     "minted"   t)
    (""     "rotating"  nil)
    ("normalem" "ulem"  t)
    (""     "mathtools"   t)
    ))
;; #+END_SRC
#+END_SRC
#+BEGIN_SRC org :exports code
#+TITLE: Something
#+AUTHOR: Rohit Goswami
#+OPTIONS: toc:nil \n:nil
#+STARTUP: fninline
#+LATEX_COMPILER: xelatex
#+LATEX_CLASS: foo
#+LATEX_HEADER: \setlength\parindent{0pt}
#+LATEX_HEADER: \addbibresource{./biblio/refs.bib}

Blah blah document $\sin{x}$ stuff

# Local Variables:
# before-save-hook: org-babel-execute-buffer
# after-save-hook: haozeke/org-save-and-export-latex
# End:
#+END_SRC
This method could be extended to essentially resetting all ~emacs~ variables on a per-file basis (without ~file-local-variables~ and ~dir-local-variables~) or to potentially execute any ~elisp~ to make ~emacs~ do things, though I cannot really think of another realistic use-case. The method presented here is really general enough to work with any arbitrary LaTeX ~.cls~ file or other draconian measures.

[fn:thatsall] Of course there are more issues stemming from this toxic practice, but that's for another rant
[fn:whatwhy] I use [[https://github.com/hlissner/doom-emacs/][doom-emacs]] with my own [[https://dotdoom.rgoswami.me][literate configuration]]
[fn:fontwhat] Or syntax highlighting for most people
[fn:halptex] For more on document structure in TeX [[https://en.wikibooks.org/wiki/LaTeX/Document_Structure][read the wikibook]]
[fn:grantwhat] I haven't seen a grant proposal template I'd like to store for later, *ever*
[fn:butwhatformula] Props to anyone who recognizes that formula
* TODO Computational Science and Math
One of the earliest memories of feedback I recall was about my handwriting. To this day it remains an ugly scrawl, with legibility tending to nil in cursive.

* DONE LineageOS Maintainer Appreciation :@personal:ramblings:thoughts:
CLOSED: [2020-06-28 Sun 18:35]
:PROPERTIES:
:EXPORT_FILE_NAME: linos-maintainer-appre
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:END:
#+BEGIN_QUOTE
A post on a surprisingly heartwarming community appreciation effort.
#+END_QUOTE

** Background
As probably anyone who has asked me about my programming experience has heard, my first real foray into the FOSS community was being a [[https://forum.xda-developers.com/member.php?u=1964056][LineageOS co-maintainer]] (as HaoZeke) for the Xperia Z5 Dual. I haven't thought about the community all that much for a few years, mostly since XDA became pretty toxic, and Android development just got, less exciting.
** The Email
I recieved two of these from different accounts:
#+BEGIN_QUOTE
Thank you so much for your contribution to making my phone the phone it is!

Have a great day!
#+END_QUOTE

[[file:images/lineageOS.jpg]]
** Conclusion
This was completely unexpected, and really made my day. In reterospect this seems like something which should be made more explicit, more often.

* DONE Analytics II: Goat to Clicky :@notes:tools:rationale:workflow:webdev:
CLOSED: [2020-07-06 Mon 23:09]
:PROPERTIES:
:EXPORT_FILE_NAME: goat-clicky
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A follow up to [[Analytics: Google to Goat][my earlier post on analytics]], and on migrating from Goat Counter to Clicky.
#+END_QUOTE

** Background
A few days ago I recieved the following email:
#+BEGIN_QUOTE
Hi there!

I made some changes to the GoatCounter plans/pricing:

1. GoatCounter now has a "Starter" plan, this is €5/month, limited to 100k pageviews/month, comes with a custom domain, and allow commercial use. This is mostly the same as the "personal plus" plan there was before, except that it allows commercial use. If you had a "personal plus" for a custom domain before then you now have a Starter plan.

2. Starting on August 1st the data retention will be limited for the Free and Starter plans: the Free plan will be limited to 6 months, the Starter plan to 12 months, and the business plans remain unlimited. There is an export feature if you wish to retain your old pageviews.

Some background on this:

There seems to be a gap between "free for personal use" and "€15/month for commercial use". I've gotten quite a bit of feedback of small (potential) commercial users who just run a small website, where €15/month really is prohibitively expensive.

The entire idea behind making it free for personal use is that I'd like GoatCounter to be usable by as many people as possible, while also ensuring commercial users pay their fair share. Redistributing software is free, but developing it is not.

The general thinking is that larger businesses with several employees (who can easily afford €15/month) will have more than 100k pageviews/month, whereas for most startups and the like 100k should be more than enough.

The entire thing is a bit of a balancing act 😅 I may tweak the pricing further in the future based on additional experience and feedback.

As for the data retention: the biggest issue here is that some not-especially-active sites have had short bursts of millions of pageviews in a short time because they wrote or made something that got widely shared.

The hits/months limit isn't strictly enforced because I don't want to tell people to get a plan just because they wrote a popular article that got to the front page of HN, Reddit, Twitter, etc, and GoatCounter has no problem handling these levels of pageviews, so that's all fine.

But on the other hand, a million pageviews currently takes up about 400M of disk space including backups (although this could probably be reduced a bit with a more clever backup strategy). Disk space is pretty cheap, but it does add up.

It also means more effort on scaling GoatCounter; limiting the data retention is an easy way to reduce the pressure on this. It also gives people a bit more incentive to get a plan 😅

As always, self-hosting isn't affected by any of this. This just applies to the goatcounter.com service.

Feel free to let me know if you've got any questions or feedback.

Cheers,
Martin
#+END_QUOTE
** Personal Repercussions
I should point out that I unequivocally support Martin's decision. It is fair and equitable. That said, continuity is super important to me. I've mentioned earlier that for me, the daily limit is not much of an issue but I do like to look back at my collective history.
** Goat Counter
[[https://www.goatcounter.com/][Goat Counter]] is still definitely my go-to option for both self-hosting and shelling out, if the 15 euro fee is acceptable. Honestly, the best option is probably opening a PR or making a tool to aggregate metrics offline, since it is still possible to export the data. The major drawback is the six month window.
** Clicky
[[https://clicky.com/][Clicky]] is pretty great, and they have a good example of [[https://clicky.com/compare/google][what works out in their favor compared to Google Analytics]].
*** Pros
- Free tier has no time limit
- Has a nice dark theme
*** Cons
- Not open source
- Capped at 3K daily views
- Blocked by some VPN providers (Windscribe)
** Conclusions
It is unfortunate to have had to move, since this does imply losing the past eight months of metrics. Eventually I might even go back to steady dependable Google Analytics. Until this, Clicky will do.
* DONE Multiple Monitors with Touchscreens :@notes:tools:workflow:
CLOSED: [2020-07-11 Sat 22:45]
:PROPERTIES:
:EXPORT_FILE_NAME: multi-monitor-touch
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A short tutorial post on multiple screens for laptops with touch-support and ArchLinux. Also evolved into a long rant, with an Easter egg.
#+END_QUOTE
** Background
Of late, I have been attempting to move away from paper, for environmental reasons[fn:environ]. Years of touch typing in Colemak ([[Switching to Colemak][rationale]], [[Refactoring Dotfiles For Colemak][config changes]]) and a very customized [[https://dotdoom.rgoswami.me][Emacs setup]] (including [[Using Mathematica with Orgmode][mathematica]], [[Temporary LaTeX Documents with Orgmode][temporary latex templates]], [[Emacs for Nix-R][Nix]], and [[An Orgmode Note Workflow][org-roam annotations]]) have more or less kept me away from analog devices. One might even argue that my current website is essentially a set of tricks to move my life into ~orgmode~ completely.

However, there are still a few things I cannot do without a pointing device (and some kind of canvas). Scrawl squiggly lines on papers I'm reviewing. That and, scrawl weird symbols which don't actually follow a coherent mathematical notation but might be later written up in latex to prove a point. Also, and I haven't mastered any of the drawing systems (like [[https://en.wikibooks.org/wiki/LaTeX/PGF/TikZ][Tikz]]) yet, so for squiggly charts I rely on [[https://jamboard.google.com/][Jamboard]] (while teaching) and [[https://github.com/xournalpp/xournalpp][Xournal++]] for collaborations.

I also happen to have a [[https://www.notebookcheck.net/Lenovo-ThinkPad-X380-Yoga-i5-8250U-FHD-Convertible-Review.316442.0.html][ThinkPad X380]] (try ~sudo dmidecode -t system | grep Version~) which has an in-built stylus, and since Linux support for touchscreens from 2018 is known to be incredible, I coupled this with the [[https://www.lenovo.com/us/en/thinkvisionM14/][ThinkVision M14]] as a second screen.
** X-Windows and X-ternal Screens

We will define two separate solutions:
- [[https://github.com/Ventto/mons][mons]] :: Using arbitrary external monitors
- [[https://github.com/phillipberndt/autorandr][autorandr]] :: Setting up profiles for specific monitors

Finally we will leverage both to ensure a constant touchscreen area.

*** Autorandr
I use the [[https://github.com/phillipberndt/autorandr][python rewrite]] simply because that's the one which is in the [[https://www.archlinux.org/packages/community/any/autorandr/][ArchLinux community repo]]. To be honest, I came across this before I (re-)discovered ~mons~. The most relevant aspect of ~autorandr~ is using complicated configurations for different monitors, but it also does a mean job of running generic scripts as ~postswitch~ and ~prefix~ scripts.

*** Mons
~xrandr~ is awesome. Unfortunately, more often than not, I forget the commands to interact with it appropriately.
[[https://github.com/Ventto/mons][mons]] does my dirty work for me[fn:monsgood].

#+BEGIN_SRC bash
# -e is extend
mons -e left
# puts screen on the left
#+END_SRC

That and the similar ~right~ option, covers around 99% of all possible dual screen use-cases.

** Constant Touch

The problem is that by default, the entire combined screen area is assumed to be touch-enabled, which essentially means an awkward area of the screen which is dead to all input (since it doesn't exist). The key insight is that I never have more touch-enabled surfaces than my default screen, no matter how many extended screens are present. So the solution is:

#+BEGIN_QUOTE
Make sure the touch area is constant.
#+END_QUOTE

We need to figure out what the touch input devices are:

#+BEGIN_SRC bash :exports both
xinput --list
#+END_SRC

#+RESULTS:
| ⎡ Virtual core pointer                             | id=2  | [master pointer  (3)] |
| ⎜   ↳ Virtual core XTEST pointer                   | id=4  | [slave  pointer  (2)] |
| ⎜   ↳ Wacom Pen and multitouch sensor Finger touch | id=10 | [slave  pointer  (2)] |
| ⎜   ↳ Wacom Pen and multitouch sensor Pen stylus   | id=11 | [slave  pointer  (2)] |
| ⎜   ↳ ETPS/2 Elantech TrackPoint                   | id=14 | [slave  pointer  (2)] |
| ⎜   ↳ ETPS/2 Elantech Touchpad                     | id=15 | [slave  pointer  (2)] |
| ⎜   ↳ Wacom Pen and multitouch sensor Pen eraser   | id=17 | [slave  pointer  (2)] |
| ⎣ Virtual core keyboard                            | id=3  | [master keyboard (2)] |
| ↳ Virtual core XTEST keyboard                      | id=5  | [slave  keyboard (3)] |
| ↳ Power Button                                     | id=6  | [slave  keyboard (3)] |
| ↳ Video Bus                                        | id=7  | [slave  keyboard (3)] |
| ↳ Power Button                                     | id=8  | [slave  keyboard (3)] |
| ↳ Sleep Button                                     | id=9  | [slave  keyboard (3)] |
| ↳ Integrated Camera: Integrated C                  | id=12 | [slave  keyboard (3)] |
| ↳ AT Translated Set 2 keyboard                     | id=13 | [slave  keyboard (3)] |
| ↳ ThinkPad Extra Buttons                           | id=16 | [slave  keyboard (3)] |

At this point we will leverage ~autorandr~ to ensure that these devices are mapped to the primary (touch-enabled) screen with a ~postswitch~ script. This ~postswitch~ script needs to be:

#+BEGIN_SRC bash
#!/bin/sh
# .config/autorandr/postswitch
xinput --map-to-output 'Wacom Pen and multitouch sensor Finger touch' eDP1
xinput --map-to-output 'Wacom Pen and multitouch sensor Pen stylus' eDP1
xinput --map-to-output 'Wacom Pen and multitouch sensor Pen eraser' eDP1
notify-send "Screen configuration changed"
#+END_SRC

The last line of course is really more of an informative boast.

At this stage, we have the ability to set the touchscreens up by informing ~autorandr~ that our configuration has changed, through the command line for example:
#+BEGIN_SRC bash
autorandr --change
#+END_SRC

** Automatic Touch Configuration

Running a command manually on-change is the sort of thing which makes people think Linux is hard or un-intuitive. So we will instead make use of the incredibly powerful ~systemd~ framework for handling events.

Essentially, we combine our existing workflow with ~autorandr-launcher~ [[https://github.com/smac89/autorandr-launcher][from here]], and then set it all up as follows:

#+BEGIN_SRC bash
git clone https://github.com/smac89/autorandr-launcher.git
cd autorandr-launcher
sudo make install
sudo systemctl--user enable autorandr_launcher.service
#+END_SRC

** Conclusion
We now have a setup which ensures that the touch enabled area is constant, without any explicit manual interventions for when devices are added or removed. There isn't much else to say about this workflow here. Additional screens can be [[Old Laptops as Secondary Monitors][configured from older laptops described here]]. Separate posts can deal with how exactly I meld Zotero, ~org-roam~ and Xournal++ to wreak havoc on all kinds of documents. So, in-lieu of a conclusion, behold a recent scribble with this setup:

#+caption: From the planning of [[http://www.wavelf.org/ij6TzydE3kTSF8cwwIUj][this voluntary course]]
file:images/myXournalScrib.png

[fn:environ] Also because paper is difficult to keep track of and is essentially the antithesis of a computer oriented workflow.
[fn:monsgood] I actually planned a whole post called "An Ode to Mons", when I first found out about it.

* DONE Grant Proposals - I :@personal:ramblings:thoughts:academia:
CLOSED: [2020-07-18 Sat 20:31]
:PROPERTIES:
:EXPORT_FILE_NAME: grant-proposals
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:END:
#+BEGIN_QUOTE
Personal recollections of the academic grant writing process.
#+END_QUOTE
** Background
Of the many types of writing one undertakes in a typical academic career, grant writing stands out as a rather large anomaly. For the purposes of this post, we will note that an academic writing taxonomy would consist of roughly:
- Coursework and Assignments :: These are more or less comparative writing exercises, where the only main thing which is enforced is (or should be) plagiarism checks. In terms of locality in history, these are more or less focused on the past, with little to no original content.
- Peer Reviewed Articles :: Broadly, in this category we can lump society journals, some conference articles, and even reviews to some extent. These are hyper-local in time, with enough historical perspective to make the paper worthwhile for the journal/conference, and originality of content is a key highlight.
- Grants :: Grants are unique. They are both short (in terms of a prospectus) and also long, in that there are a huge number of auxiliary files to be added.
** Grants
  At some stage, every researcher who doesn't bow out of academia ends up faced with the prospect of proving to a funding agency that they are capable and well-adjusted enough to get money for an extended amount of time.
*** Lit Surveys
Unlike papers and reviews, though recent papers are important, it is more relevant to actually project where the field will be in upcoming years to ensure the deliverables are not out-dated. Additionally, it is best to link to widely cited literature, to ensure that the reviewers believe in your holistic understanding.
*** Deliverables
This part is fun to write, building towards a goal, is a special kind of write up. It allows one to really flesh out a research plan with realistic goals.
*** Extras
- For most applications, there is a budgetary requirement, mostly with a spreadsheet component.
- A Gantt chart is also often required
  - I started with [[https://teamgantt.com][teamgantt]], which was neat
  - Of course I eventually ended up regressing to ~orgmode~ and TeX using [[https://github.com/swillner/org-gantt/blob/master/org-gantt-manual.org][org-gantt]]
    - This actually would need a whole other post, but it is great
** Conclusions
If this post seemed short, it is probably because even though a lot else went into the proposal, until I hear otherwise next year, it would be presumptous to write more. That said, it was and is an enjoyable exercise.

* DONE A Short Guide to Statistical Rethinking² :@programming:R:SR2:solutions:workflow:
CLOSED: [2020-07-24 Fri 17:35]
:PROPERTIES:
:EXPORT_FILE_NAME: some-sol-sr2
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A meta post introducing my solutions to the fantastic [[https://xcelab.net/rm/statistical-rethinking/][excellent second edition]] of "Statistical
Rethinking" by [[https://twitter.com/rlmcelreath][Richard McElreath]], a.k.a. Statistical Rethinking². Also discusses strategies to keep up with the material, mostly meant for self-study groups.
#+END_QUOTE

** Background
As [[Statistical Rethinking and Nix][detailed previously]], I recently was part of a course centered around Bayesian modeling for the Icelandic COVID-19 pandemic. The Bayesian mindset needs no introduction, and this post is completely inadequete to explain why anyone should be interested (that's what the book is for!). That said, especially for self-paced study groups, it might help to have some structure.

** Solutions
These are meant to be sample solutions, and everyone *should* solve these for themselves. Each solution contains the packages used, as well as a colophon in the later posts to ensure reproduciblity.
Essentially this consists of four posts:
- [[https://rgoswami.me/posts/sr2-ch2-ch3-ch4/][Week I]] :: Covers the first four chapters {1,2,3,4}
- [[https://rgoswami.me/posts/sr2-ch5-ch6-ch7/][Week II]] :: Covers the next three chapters {5,6,7}
- [[https://rgoswami.me/posts/sr2-ch9-ch11-ch12/][Week III]] :: Covers five chapters {9,11,12}
- [[https://rgoswami.me/posts/sr2-ch13-ch14][Week IV]] :: The last five chapters {13,14}

More concisely:

| *Chapter*                                        | *Solutions* |
| 1. The Golem of Prague                           | N/A         |
| 2. Small Worlds and Large Worlds                 | [[https://rgoswami.me/posts/sr2-ch2-ch3-ch4/#chapter-ii-the-golem-of-prague][here]]        |
| 3. Sampling the Imaginary                        | [[https://rgoswami.me/posts/sr2-ch2-ch3-ch4/#chapter-iii-sampling-the-imaginary][here]]        |
| 4. Geocentric Models                             | [[https://rgoswami.me/posts/sr2-ch2-ch3-ch4/#chapter-iv-geocentric-models][here]]        |
| 5. The Many Variables & The Spurious Waffles     | [[https://rgoswami.me/posts/sr2-ch5-ch6-ch7/#chapter-v-the-many-variables-and-the-spurious-waffles][here]]        |
| 6. The Haunted DAG & The Causal Terror           | [[https://rgoswami.me/posts/sr2-ch5-ch6-ch7/#chapter-vi-the-haunted-dag-and-the-causal-terror][here]]        |
| 7. Ulysses’ Compass                              | [[https://rgoswami.me/posts/sr2-ch5-ch6-ch7/#chapter-vii-ulysses-compass][here]]        |
| 8. Conditional Manatees                          | N/A         |
| 9. Markov Chain Monte Carlo                      | [[https://rgoswami.me/posts/sr2-ch9-ch11-ch12/#chapter-ix-markov-chain-monte-carlo][here]]        |
| 10. Big Entropy and the Generalized Linear Model | N/A         |
| 11. God Spiked the Integers                      | [[https://rgoswami.me/posts/sr2-ch9-ch11-ch12/#chapter-xi-god-spiked-the-integers][here]]        |
| 12. Monsters and Mixtures                        | [[https://rgoswami.me/posts/sr2-ch9-ch11-ch12/#chapter-xii-monsters-and-mixtures][here]]        |
| 13. Models With Memory                           | [[https://rgoswami.me/posts/sr2-ch13-ch14/#chapter-xiii-models-with-memory][here]]        |
| 14. Adventures in Covariance                     | [[https://rgoswami.me/posts/sr2-ch13-ch14/#chapter-xiv-adventures-in-covariance][here]]        |
| 15. Missing Data and Other Opportunities         | TBA         |
| 16. Generalized Linear Madness                   | TBA         |
| 17. Horoscopes                                   | N/A         |

*** Pacing
The solutions compiled here were from an accelerated 4-week course covering the Statistical Rethinking² in four weeks. The book is more traditionally used in a full-semester course, so that should be kept in mind as well.
** Resources
These are highly opinionated and the following list is in no way complete.
*** Canonical Content
- [[https://xcelab.net/rm/statistical-rethinking/][Richard's Website]]
- The [[https://www.youtube.com/playlist?list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI][YouTube 2019 playlist]]
- [[https://twitter.com/rlmcelreath][Richard on Twitter]]
- The [[https://mc-stan.org/][Stan page]]
*** Additional Content
- An [[https://www.youtube.com/watch?v=eDMGDhyDxuY&list=PLFVtMSYAlp5-MsffvdbR-iqfHSaOBgwpn&index=2&t=19s][introduction to Frequentist and Bayesian statistics]] from LLNL by Kristin Lennox
- A [[https://rpubs.com/bgautijonsson/week4_covid][simple COVID-19 model]] for Iceland
- More [[https://covid.hi.is/english/][complete COVID-19 model]] for Iceland
- [[https://gitlab.com/openresearchlabs/probabilistic_data_analysis_2020/-/blob/master/annurev-statistics-031219-041300.pdf][Convergence Diagnostics for MCMC]]
- Betancourt's [[https://arxiv.org/pdf/1701.02434.pdf][Conceptual Introduction to HMC]]
*** Follow-up Courses
- [[https://github.com/avehtari/BDA_course_Aalto][Bayesian Data Analysis]]
** Conclusions
This has been a short meta post which is essentially meant to collect content posted with dates in the past. Though this is not exactly a complete reference for beginners, it might still help people.

* DONE Explorations with Backlight Controllers :@notes:tools:workflow:
CLOSED: [2020-08-01 Sat 20:00]
:PROPERTIES:
:EXPORT_FILE_NAME: expl-backlight-control
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A short post detailing the many trials and tribulations of setting brightness on multiple monitors in tandem.
#+END_QUOTE

** Background
As regular readers might know, I have a multi-screen setup, which accounts for having touch enabled on my primary laptop screen (detailed [[Multiple Monitors with Touchscreens][here]]). A failing of this setup was that I was not able to control the brightness of both monitors at the same time.
** Existing Stack
Since I use ~i3~, my brightness control is simply done with ~bindsym~ lines as follows[fn:wheredot]:
#+BEGIN_SRC conf
bindsym XF86MonBrightnessDown exec light -U 10
bindsym XF86MonBrightnessUp exec light -A 10
#+END_SRC
Note that to get the right ~bindsym~ I use [[https://github.com/wavexx/screenkey][screenkey]] with the ~keysyms~ preference. My software of choice was
Unfortunately, my existing setup (with ~light~, since that is in the Arch ~community~ repo) did not actually allow dimming external screens arbitarily. To be more exact,
#+BEGIN_SRC bash :results raw
light -h
#+END_SRC

#+begin_example
Usage:
  light [OPTIONS] [VALUE]

Commands:
  -H, -h      Show this help and exit
  -V          Show program version and exit
  -L          List available devices
  -A          Increase brightness by value
  -U          Decrease brightness by value
  -T          Multiply brightness by value (can be a non-whole number, ignores raw mode)
  -S          Set brightness to value
  -G          Get brightness
  -N          Set minimum brightness to value
  -P          Get minimum brightness
  -O          Save the current brightness
  -I          Restore the previously saved brightness

Options:
  -r          Interpret input and output values in raw mode (ignored for -T)
  -s          Specify device target path to use, use -L to list available
  -v          Specify the verbosity level (default 0)
                 0: Values only
                 1: Values, Errors.
                 2: Values, Errors, Warnings.
                 3: Values, Errors, Warnings, Notices.

Copyright (C) 2012 - 2018  Fredrik Haikarainen
This is free software, see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE
#+end_example

Clearly it is possible to target specific devices, but for arbitrary additions this is quite tough. Additionally, the project has been more or less been stuck in [[https://github.com/haikarainen/light/issues/37][maintainence mode for a while now]].
** Multi-head Configurations
*** Exposing Brightness
The first hurdle faced in o was actually getting the external monitor to expose the right controls. This is accomplished neatly with ~ddci-driver~ [[https://gitlab.com/ddcci-driver-linux/ddcci-driver-linux][found here]]. As demonstrated (courtesy of [[https://wiki.archlinux.org/index.php/Backlight#Backlight_utilities][the ArchWiki]]):
#+BEGIN_SRC bash
# Load module
sudo modprobe ddcci_backlight
# Check that it worked
sudo ddcutil capabilities | grep "Feature: 10"
sudo ddcutil getvcp 10
# Set brightness
sudo ddcutil setvcp 10 70
#+END_SRC
One of the obvious caveats of this technique is that ~sudo~ access or a dedicated ~polkit~ agent is required. My preferred method of loading this comes from [[https://aur.archlinux.org/packages/ddcci-driver-linux-dkms/][a comment]] on the ~ddcci-driver-linux-dkms~ page of the AUR:
#+BEGIN_SRC bash
# Put in /etc/systemd/system/ddcci-backlight.service:
# https://aur.archlinux.org/packages/ddcci-driver-linux-dkms/
# Placing "ddcci_backlight" into /etc/modules-load.d
# leads to hang on boot with external (HDMI) monitor
# connected to the laptop, so we need to add the module later.

# And ddcci_backlight can't detect monitor during sddm.service startup.

[Unit]
After=multi-user.target
Before=sddm.service

[Service]
Type=oneshot
ExecStart=/usr/bin/modprobe ddcci_backlight
ExecStop=/usr/bin/modprobe --remove ddcci_backlight
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
#+END_SRC
This is then activated with a standard ~systemctl enable ddcci_backlight.service~ command. At this point, the device interface should be exposed to most backlight controllers.
*** Xrandr
This is the most obvious of all methods, and does not even require the ~ddcci-driver~. We will simply tweak the brightness with ~xrandr~.
#+BEGIN_SRC bash
# Get devices
xrandr | grep " connected"
# Tweak
xrandr --output DP1 --brightness 0.2
#+END_SRC

Note that this is an in-exact method, since it actually adjusts the gamma value instead, and it effectively tints your screen rather than modifying the brightness.
*** Clight
[[https://github.com/FedeDP/Clight][clight]] is an excellent, highly performant alternative to redshift, but it tends to force the main screen brightness to zero. Nothing which can't be configured away, but in practice, I work late and tend to turn off the tint anyway. This requires a daemon to be run, as well as needing to be turned on manually for ~i3~. A very elegant additional feature gained by using ~clight~ is that external monitors are turned off automagically when lockscreens are activated.
*** Brillo
[[https://github.com/CameronNemo/brillo][brillo]] is one of the newer controllers, and is pretty actively developed. The interface is almost exactly like ~light~, and unlike ~clight~ there is no need to use a daemon. It meshes perfectly with a key-press based system like ~i3~ and also has controls for keyboard LEDS as well as for smoothed ramping up and down of the brightness. Most importantly, it features an ~-e~ flag which sets the brightness across all output devices. Essentially this means my configuration is simply modified to:
#+BEGIN_SRC conf
bindsym XF86MonBrightnessDown exec brillo -e -U 10
bindsym XF86MonBrightnessUp exec brillo -e -A 10
#+END_SRC
** Conclusions
*tl;dr* moving from ~light~ to ~brillo~ with ~ddcci-driver-linux-dkms~ was a fantastic idea.

[fn:wheredot] My dotfiles [[https://github.com/HaoZeke/Dotfiles][are here]]

* TODO Switching from Zplug to Zinit
* DONE HPC Dotfiles and LMod :@programming:workflow:projects:hpc:
CLOSED: [2020-08-09 Sun 02:29]
:PROPERTIES:
:EXPORT_FILE_NAME: hpc-dots-lmod
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
** Background
My move away from the powerful, but unimaginatively named HPC clusters of IITK [fn:reallythatbad] brought me in close contact with the Lua based [fn:mustisay] [[https://lmod.readthedocs.io/en/latest/index.html][lmod module system]]. Rather than fall into the rabbit hole of [[https://docs.brew.sh/Homebrew-on-Linux][brew]] we will leverage the existing system to add our new libraries. Not finding any good collections of these composable environments, and [[Provisioning Dotfiles on an HPC][having failed once before]] to install Nix as a user without admin access, I decided to [[https://github.com/HaoZeke/hzHPC_lmod][start my own collection of Lmod recipies]]. The rest of this post details the installation proceedure to be carried out in conjunction with the [[https://github.com/HaoZeke/hzHPC_lmod][hzHPC_lmod repo]].
** Setting Up
These are reproduced from the repo for completeness.
#+BEGIN_SRC bash
git clone https://github.com/kobus-v-schoor/dotgit.git
mkdir -p ~/.local/bin
cp -r dotgit/old/bin/dotgit* ~/.local/bin
cat dotgit/old/bin/bash_completion >> ~/.bash_completion
rm -rf dotgit
#+END_SRC
I actually strongly suggest using a target from [[https://github.com/HaoZeke/Dotfiles][my Dotfiles]] in conjunction with this, but it isn't really required, so:
#+BEGIN_SRC
~/.local/bin/dotgit restore hzhpc
#+END_SRC
Note that because of the suggested separation, I have not opted to setup a shell or even ensure that there are scripts here to help keep ~module~ in your path. Those are in [[https://github.com/HaoZeke/Dotfiles][my Dotfiles]].
If, you *opt to not use* these ~dotfiles~, then *do not* run the ~ml load~ commands.
** Basic Setup
We will first load an environment with basic packages (~autotools~). This is a stop-gap solution.
*** MicroMamba
We will set up a ~micromamba~ installation to get ~git~ and other base utilities.
#+begin_src bash
mkdir -p ~/.local/bin
export PATH=$HOME/.local/bin:$PATH
wget -qO- https://micromamba.snakepit.net/api/micromamba/linux-64/latest | tar -xvj bin/micromamba
mv bin/micromamba ~/.local/bin
rm -rf bin
micromamba shell init -s bash -p $HOME/micromamba
. ~/.bashrc
#+end_src
Now we can use ~mamba~ to get a temporary interactive environment
#+begin_src bash
# Spack requirements
PKGS="git patch curl make tar gzip unzip bzip2 xz zstd subversion lsof m4"
mkdir ~/.mamba_venvs
micromamba create -p ~/.mamba_venvs/intBase $PKGS
micromamba activate ~/.mamba_venvs/intBase
#+end_src
For subsequent logins we can simply run:
#+begin_src bash
export PATH=$HOME/.local/bin:$PATH
. ~/.bashrc
micromamba activate ~/.mamba_venvs/intBase
#+end_src
**** Configuration
A simple ~.condarc~ will suffice for the above.
#+begin_src yaml
channels:
  - conda-forge
  - defaults
channel_priority: disabled
#+end_src
*** Warning
Note that once the base packages have been installed...
#+begin_quote
We must *unload* our micromamba environment!!
#+end_quote
#+begin_src bash
micromamba deactivate
#+end_src
It is recommended to use a minimal set of ~micromamba~ packages.
** LMod Libraries
#+BEGIN_QUOTE
*Note that*: [[http://ihpc.is/garpur/][garpur]] and elja already has ~lmod~
#+END_QUOTE

The scripts in this post will also be part of the [[https://github.com/HaoZeke/haozeke.github.io/][repo]], but keep in mind that these are not meant to be robust ways to install anything, and every command should be run by hand because things will probably break badly. We keep all sources in ~$HOME/tmphpc~ and install everything relative to ~$HOME/.hpc~. Paths are managed by the ~lmod~ system.
#+begin_src bash
export hpcroot=$HOME/tmphpc
mkdir -p $hpcroot
#+end_src
Combining our paths with the [[https://lmod.readthedocs.io/en/latest/020_advanced.html][lmod manual]] gives rise to the following definiton (roughly the same for each one):
#+BEGIN_SRC bash
local home    = os.getenv("HOME")
local version = myModuleVersion()
local pkgName = myModuleName()
local pkg     = pathJoin(home,".hpc",pkgName,version,"bin")
prepend_path("PATH", pkg)
#+END_SRC
We will no longer bother with the module definitions for the rest of this post, as they are handled and documented [[https://github.com/HaoZeke/hzHPC_lmod/tree/master/dotfiles][in the repo]].
#+begin_src bash
# Setting up hzhpc modules
cd tmphpc
git clone https://github.com/kobus-v-schoor/dotgit.git
mkdir -p ~/.local/bin
cp -r dotgit/old/bin/dotgit* ~/.local/bin
cat dotgit/bin/bash_completion >> ~/.bash_completion
rm -rf dotgit
~/.local/bin/dotgit restore hzhpc
#+end_src
*** Patch
This is a basic utility we need before getting to anything else.
#+begin_src bash
cd $hpcroot
myprefix=$HOME/.hpc/patch/2.7.2
wget http://ftp.gnu.org/gnu/patch/patch-2.7.2.tar.gz
gzip -dc patch-2.7.2.tar.gz | tar xvf -
cd patch-2.7.2
./configure --prefix=$myprefix
make -j$(nproc) && make install
ml load patch
#+end_src
*** Help2man
#+begin_src bash
cd $hpcroot
myprefix=$HOME/.hpc/help2man/1.48.3
wget https://gnuftp.uib.no/help2man/help2man-1.48.3.tar.xz
tar xfv help2man-1.48.3.tar.xz
cd help2man-1.48.3
./configure --prefix=$myprefix   
make -j$(nproc)
make install
ml load help2man/1.48.3
#+end_src
*** Perl
This is essentially the setup from the [[https://learn.perl.org/installing/unix_linux.html][main docs]].
#+BEGIN_SRC bash :noeval :tangle codeSnips/lmod-help/getPerl.sh :eval never
# Get Perl
curl -L http://xrl.us/installperlnix | bash
cpanm --local-lib=~/perl5 local::lib && eval $(perl -I ~/perl5/lib/perl5/ -Mlocal::lib)
cpanm ExtUtils::MakeMaker --force # For git
cpanm Thread::Queue # For automake
ml load perl/5.28.0
#+END_SRC
*** Autotools
**** M4
This requires special considerations for ~glibc~ greater than ~2.28~ (true for compilers like ~gcc8~ and above).
#+begin_src bash
cd $HOME/tmphpc
myprefix=$HOME/.hpc/autotools
wget http://ftp.gnu.org/gnu/m4/m4-1.4.18.tar.gz
gzip -dc m4-1.4.18.tar.gz | tar xvf -
cd m4-1.4.18
# From http://git.openembedded.org/openembedded-core/commit/meta/recipes-devtools/m4/m4/m4-1.4.18-glibc-change-work-around.patch
# From https://askubuntu.com/a/1112101/690387
wget http://git.openembedded.org/openembedded-core/plain/meta/recipes-devtools/m4/m4/m4-1.4.18-glibc-change-work-around.patch
patch  -p 1  < m4-1.4.18-glibc-change-work-around.patch
./configure -C --prefix=$myprefix/m4/1.4.18 && make -j$(nproc) && make install
#+end_src
***** Patch contents
The patch is needed since ~glibc 2.28~ and above have   changed the nesting levels. The patch is effectively backported from the upstream repositories. Some more context [[https://github.com/buildroot/buildroot/commit/c48f8a64626c60bd1b46804b7cf1a699ff53cdf3#diff-0d8a48b477da9bfd28490aad8b4acd5efb319779f29bbdf56e2fb026bb0fbe52][is here]].
#+begin_src cpp
update for glibc libio.h removal in 2.28+

see
https://src.fedoraproject.org/rpms/m4/c/814d592134fad36df757f9a61422d164ea2c6c9b?branch=master

Upstream-Status: Backport [https://git.savannah.gnu.org/cgit/gnulib.git/commit/?id=4af4a4a718]
Signed-off-by: Khem Raj <raj.khem@gmail.com>

Index: m4-1.4.18/lib/fflush.c
===================================================================
--- m4-1.4.18.orig/lib/fflush.c
+++ m4-1.4.18/lib/fflush.c
@@ -33,7 +33,7 @@
 #undef fflush


-#if defined _IO_ftrylockfile || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */
+#if defined _IO_EOF_SEEN || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */

 /* Clear the stream's ungetc buffer, preserving the value of ftello (fp).  */
 static void
@@ -72,7 +72,7 @@ clear_ungetc_buffer (FILE *fp)

 #endif

-#if ! (defined _IO_ftrylockfile || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */)
+#if ! (defined _IO_EOF_SEEN || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */)

 # if (defined __sferror || defined __DragonFly__ || defined __ANDROID__) && defined __SNPT
 /* FreeBSD, NetBSD, OpenBSD, DragonFly, Mac OS X, Cygwin, Android */
@@ -148,7 +148,7 @@ rpl_fflush (FILE *stream)
   if (stream == NULL || ! freading (stream))
     return fflush (stream);

-#if defined _IO_ftrylockfile || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */
+#if defined _IO_EOF_SEEN || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */

   clear_ungetc_buffer_preserving_position (stream);

Index: m4-1.4.18/lib/fpending.c
===================================================================
--- m4-1.4.18.orig/lib/fpending.c
+++ m4-1.4.18/lib/fpending.c
@@ -32,7 +32,7 @@ __fpending (FILE *fp)
   /* Most systems provide FILE as a struct and the necessary bitmask in
      <stdio.h>, because they need it for implementing getc() and putc() as
      fast macros.  */
-#if defined _IO_ftrylockfile || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */
+#if defined _IO_EOF_SEEN || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */
   return fp->_IO_write_ptr - fp->_IO_write_base;
 #elif defined __sferror || defined __DragonFly__ || defined __ANDROID__
   /* FreeBSD, NetBSD, OpenBSD, DragonFly, Mac OS X, Cygwin, Android */
Index: m4-1.4.18/lib/fpurge.c
===================================================================
--- m4-1.4.18.orig/lib/fpurge.c
+++ m4-1.4.18/lib/fpurge.c
@@ -62,7 +62,7 @@ fpurge (FILE *fp)
   /* Most systems provide FILE as a struct and the necessary bitmask in
      <stdio.h>, because they need it for implementing getc() and putc() as
      fast macros.  */
-# if defined _IO_ftrylockfile || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */
+# if defined _IO_EOF_SEEN || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */
   fp->_IO_read_end = fp->_IO_read_ptr;
   fp->_IO_write_ptr = fp->_IO_write_base;
   /* Avoid memory leak when there is an active ungetc buffer.  */
Index: m4-1.4.18/lib/freadahead.c
===================================================================
--- m4-1.4.18.orig/lib/freadahead.c
+++ m4-1.4.18/lib/freadahead.c
@@ -25,7 +25,7 @@
 size_t
 freadahead (FILE *fp)
 {
-#if defined _IO_ftrylockfile || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */
+#if defined _IO_EOF_SEEN || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */
   if (fp->_IO_write_ptr > fp->_IO_write_base)
     return 0;
   return (fp->_IO_read_end - fp->_IO_read_ptr)
Index: m4-1.4.18/lib/freading.c
===================================================================
--- m4-1.4.18.orig/lib/freading.c
+++ m4-1.4.18/lib/freading.c
@@ -31,7 +31,7 @@ freading (FILE *fp)
   /* Most systems provide FILE as a struct and the necessary bitmask in
      <stdio.h>, because they need it for implementing getc() and putc() as
      fast macros.  */
-# if defined _IO_ftrylockfile || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */
+# if defined _IO_EOF_SEEN || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */
   return ((fp->_flags & _IO_NO_WRITES) != 0
           || ((fp->_flags & (_IO_NO_READS | _IO_CURRENTLY_PUTTING)) == 0
               && fp->_IO_read_base != NULL));
Index: m4-1.4.18/lib/fseeko.c
===================================================================
--- m4-1.4.18.orig/lib/fseeko.c
+++ m4-1.4.18/lib/fseeko.c
@@ -47,7 +47,7 @@ fseeko (FILE *fp, off_t offset, int when
 #endif

   /* These tests are based on fpurge.c.  */
-#if defined _IO_ftrylockfile || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */
+#if defined _IO_EOF_SEEN || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */
   if (fp->_IO_read_end == fp->_IO_read_ptr
       && fp->_IO_write_ptr == fp->_IO_write_base
       && fp->_IO_save_base == NULL)
@@ -123,7 +123,7 @@ fseeko (FILE *fp, off_t offset, int when
           return -1;
         }

-#if defined _IO_ftrylockfile || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */
+#if defined _IO_EOF_SEEN || __GNU_LIBRARY__ == 1 /* GNU libc, BeOS, Haiku, Linux libc5 */
       fp->_flags &= ~_IO_EOF_SEEN;
       fp->_offset = pos;
 #elif defined __sferror || defined __DragonFly__ || defined __ANDROID__
Index: m4-1.4.18/lib/stdio-impl.h
===================================================================
--- m4-1.4.18.orig/lib/stdio-impl.h
+++ m4-1.4.18/lib/stdio-impl.h
@@ -18,6 +18,12 @@
    the same implementation of stdio extension API, except that some fields
    have different naming conventions, or their access requires some casts.  */

+/* Glibc 2.28 made _IO_IN_BACKUP private.  For now, work around this
+   problem by defining it ourselves.  FIXME: Do not rely on glibc
+   internals.  */
+#if !defined _IO_IN_BACKUP && defined _IO_EOF_SEEN
+# define _IO_IN_BACKUP 0x100
+#endif

 /* BSD stdio derived implementations.  */
#+end_src
**** Auto{Conf,Make} and Libtool
This follows the standard approach outlined in the [[https://www.gnu.org/software/automake/faq/autotools-faq.html#How-can-I-install-software-below-my-home-directory_003f][GNU Autotools FAQ]]:
#+BEGIN_SRC bash :noeval :tangle codeSnips/lmod-help/getAutotools.sh :eval never
cd $hpcroot
myprefix=$HOME/.hpc/autotools
export PATH
wget http://ftp.gnu.org/gnu/autoconf/autoconf-2.69.tar.gz
wget http://ftp.gnu.org/gnu/automake/automake-1.16.2.tar.gz
wget http://ftp.gnu.org/gnu/libtool/libtool-2.4.6.tar.gz
wget http://ftp.gnu.org/gnu/gettext/gettext-0.20.tar.gz
wget https://gnuftp.uib.no/autoconf-archive/autoconf-archive-2021.02.19.tar.xz
tar xfv autoconf-archive-2021.02.19.tar.xz
gzip -dc autoconf-2.69.tar.gz | tar xvf -
gzip -dc automake-1.16.2.tar.gz | tar xvf -
gzip -dc libtool-2.4.6.tar.gz | tar xvf -
gzip -dc gettext-0.20.tar.gz | tar xvf -
cd autoconf-2.69
./configure -C --prefix=$myprefix/autoconf/2.69 && make -j$(nproc) && make install
cd ../automake-1.16.2
./configure -C --prefix=$myprefix/automake/1.16.2 --docdir=$myprefix/automake/1.16.2/share/doc/automake-1.16.2 && make -j$(nproc) && make install
cd ../autoconf-archive-2021.02.19
./configure -C --prefix=$myprefix/automake/1.16.2
cd ../libtool-2.4.6
./configure -C --disable-static --prefix=$myprefix/libtool/2.4.6 && make -j$(nproc) && make install
cd ../gettext-0.20
./configure -C --prefix=$myprefix/gettext/0.20 && make -j$(nproc) && make install
ml load autotools/autotools
#+END_SRC
*** GMP
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/gcc/gmp/6.2.0
export PATH
wget https://gmplib.org/download/gmp/gmp-6.2.0.tar.xz
tar xfv gmp-6.2.0.tar.xz
cd gmp-6.2.0
./configure --prefix=$myprefix    \
            --enable-cxx     \
            --docdir=$myprefix/doc/gmp-6.2.0
make -j$(nproc)
make install
ml load gcc/gmp/6.2.0
#+END_SRC
*** MPFR
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/gcc/mpfr/4.1.0
export PATH
wget https://www.mpfr.org/mpfr-current/mpfr-4.1.0.tar.xz
tar xfv mpfr-4.1.0.tar.xz
cd mpfr-4.1.0
./configure --prefix=$myprefix    \
            --enable-thread-safe     \
            --with-gmp=$HOME/.hpc/gcc/gmp/6.2.0 \
            --docdir=$myprefix/doc/mpfr-4.1.0
make -j$(nproc)
make install
ml load gcc/mpfr/4.1.0
#+END_SRC
*** MPC
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/gcc/mpc/1.2.0
export PATH
wget https://ftp.gnu.org/gnu/mpc/mpc-1.2.0.tar.gz
tar xfv mpc-1.2.0.tar.gz
cd mpc-1.2.0
./configure --prefix=$myprefix    \
            --with-gmp=$HOME/.hpc/gcc/gmp/6.2.0 \
            --with-mpfr=$HOME/.hpc/gcc/mpfr/4.1.0 \
            --docdir=$myprefix/doc/mpc-1.2.0
make -j$(nproc)
make install
ml load gcc/mpc/1.2.0
#+END_SRC
*** GCC 9.2.0
#+BEGIN_SRC bash :noeval :tangle codeSnips/lmod-help/getGCC920.sh :eval never
cd $hpcroot
ml load gcc/gmp gcc/mpfr gcc/mpc
myprefix=$HOME/.hpc/gcc/9.2.0
wget https://ftp.gnu.org/gnu/gcc/gcc-9.2.0/gcc-9.2.0.tar.xz
tar xfv gcc-9.2.0.tar.xz
cd gcc-9.2.0
case $(uname -m) in
  x86_64)
    sed -e '/m64=/s/lib64/lib/' \
        -i.orig gcc/config/i386/t-linux64
  ;;
esac
mkdir -p build                                         &&
cd    build                                            &&

SED=sed                               \
../configure --prefix=$myprefix            \
             --enable-languages=c,c++,fortran \
             --disable-multilib       \
             --with-gmp=$HOME/.hpc/gcc/gmp/6.2.0 \
             --with-mpfr=$HOME/.hpc/gcc/mpfr/4.1.0 \
             --with-mpc=$HOME/.hpc/gcc/mpc/1.2.0 \
             --disable-bootstrap      \
             --with-system-zlib
export PATH
unset LIBRARY_PATH
export LIBRARY_PATH=/usr/lib64/
mkdir -p -- .deps
make -j$(nproc)
make install
ml load gcc/9.2.0
#+END_SRC
*** Pkg-Config
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/pkg-config/0.29.2
wget https://pkg-config.freedesktop.org/releases/pkg-config-0.29.2.tar.gz
gzip -dc pkg-config-0.29.2.tar.gz | tar xvf -
cd pkg-config-0.29.2
./configure --prefix=$myprefix --with-internal-glib --disable-host-tool --docdir=$myprefix/share/doc/pkg-config-0.29.2
mkdir $myprefix/lib
make -j $(nproc)
make install
ml load pkg-config/0.29.2
#+END_SRC
*** Zlib
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/zlib/1.2.11
wget http://zlib.net/zlib-1.2.11.tar.gz
gzip -dc zlib-1.2.11.tar.gz | tar xvf -
cd zlib-1.2.11
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load zlib/1.2.11
#+END_SRC
*** XZ Utils
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/xz/5.2.5
wget https://tukaani.org/xz/xz-5.2.5.tar.gz
gzip -dc xz-5.2.5.tar.gz | tar xvf -
cd xz-5.2.5
./configure --prefix=$myprefix --enable-threads=yes
make -j $(nproc)
make install
ml load xz/5.2.5
#+END_SRC
*** OpenSSL
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/openssl/1.1.1d
wget https://www.openssl.org/source/openssl-1.1.1d.tar.gz
gzip -dc openssl-1.1.1d.tar.gz | tar xvf -
cd openssl-1.1.1d
./config --prefix=$myprefix --openssldir=$myprefix/etc/ssl shared zlib-dynamic
make -j $(nproc)
make install
ml load openssl/1.1.1d
#+END_SRC
*** cURL
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/curl/7.76.0
wget https://curl.haxx.se/download/curl-7.76.0.tar.xz
tar xfv  curl-7.76.0.tar.xz
cd curl-7.76.0
./configure --prefix=$myprefix \
    --disable-static \
    --enable-threaded-resolver \
    --with-openssl
make -j $(nproc)
make install
ml load curl/7.76.0
#+END_SRC
*** Git
This is very similar to the previous approach. However, since by default the system ~perl~ was being picked up, some slight changes have been made.
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/git/2.9.5
PATH=$myprefix/bin:$PATH
export PATH
wget https://mirrors.edge.kernel.org/pub/software/scm/git/git-2.9.5.tar.gz
gzip -dc git-2.9.5.tar.gz | tar xvf -
cd git-2.9.5
./configure --with-perl=$(which perl) --with-curl=$(which curl) -C --prefix=$myprefix
make -j $(nproc)
make install
ml load git/2.9.5
ml unload openssl # System will suffice
ml unload curl # System will suffice
#+END_SRC
**** Caveat
Also, for TRAMP, we would prefer having a more constant path, so we can set up a symlink:
#+BEGIN_SRC bash
mkdir ~/.hpc/bin
ln ~/.hpc/git/2.9.5/bin/git ~/.hpc/bin/git
#+END_SRC
*** Boost
The [[https://www.boost.org/][boost website]] is utterly incomprehensible. As is the documentation. Also, fun fact, the move from ~svn~ makes things worse. This is best installed with the standard compiler present, since the B2 engine detection seems to be very hit and miss. Thankfully, a quick dive into the slightly [[https://github.com/boostorg/wiki/wiki/Getting-Started%3A-Overview][better Github wiki]] led to this nugget:

#+BEGIN_SRC bash
cd $hpcroot
git clone --recursive https://github.com/boostorg/boost.git
cd boost
git checkout tags/boost-1.76.0 # or whatever branch you want to use
./bootstrap.sh
./b2 headers
#+END_SRC

This means we're almost done!

#+BEGIN_SRC bash
./b2
./b2 install --prefix=$HOME/.hpc/boost/boost-1.76.0
ml load boost/boost-1.76.0
#+END_SRC
*** Cmake
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/cmake/3.20.1
wget https://github.com/Kitware/CMake/releases/download/v3.20.1/cmake-3.20.1.tar.gz
gzip -dc cmake-3.20.1.tar.gz | tar xvf -
cd cmake-3.20.1
CXXFLAGS="-std=c++11" CC=$(which gcc) CXX=$(which g++) ./bootstrap --prefix=$myprefix --system-libs
make -j $(nproc)
make install
ml load cmake/3.20.1
#+END_SRC
*** GNU-Make
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/make/4.3
wget http://ftp.gnu.org/gnu/make/make-4.3.tar.gz
gzip -dc make-4.3.tar.gz | tar xvf -
cd make-4.3
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load make/4.3
#+END_SRC
*** Brotli
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/brotli/1.0.1
git clone https://github.com/bagder/libbrotli
cd libbrotli
libtoolize
aclocal
autoheader
./autogen.sh
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load brotli/1.0.1
#+END_SRC
*** ncurses
We will need to manually ensure the paths for ~pkg-config~ are in a feasible location.
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/ncurses/6.2
wget https://invisible-mirror.net/archives/ncurses/ncurses-6.2.tar.gz
gzip -dc ncurses-6.2.tar.gz | tar xvf -
cd ncurses-6.2
./configure --prefix=$myprefix --enable-widec --enable-pc-files --with-shared
make -j $(nproc)
make install
mkdir pkgconfig
cp misc/formw.pc misc/menuw.pc misc/ncurses++w.pc misc/ncursesw.pc misc/panelw.pc pkgconfig/
mv pkgconfig $myprefix/lib/
ml load ncurses/6.2
#+END_SRC
*** texinfo
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/texinfo/6.7
wget http://ftp.gnu.org/gnu/texinfo/texinfo-6.7.tar.gz
gzip -dc texinfo-6.7.tar.gz | tar xvf -
cd texinfo-6.7
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load texinfo/6.7
#+END_SRC
*** gperf
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/gperf/3.1
wget http://ftp.gnu.org/gnu/gperf/gperf-3.1.tar.gz
gzip -dc gperf-3.1.tar.gz | tar xvf -
cd gperf-3.1
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load gperf/3.1
#+END_SRC
*** libseccomp
There is a bug, which requires modifying ~src/system.c~ to change ~__NR_seccomp~ to ~_nr_seccomp~.
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/libseccomp/2.5.0
git clone https://github.com/seccomp/libseccomp
cd libseccomp
git checkout tags/v2.5.0
./autogen.sh
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load libseccomp/2.5.0
#+END_SRC
Alternatively, it is easier to work with an older version.
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/libseccomp/2.4.4
wget https://github.com/seccomp/libseccomp/releases/download/v2.4.4/libseccomp-2.4.4.tar.gz
tar xfv libseccomp-2.4.4.tar.gz
cd libseccomp-2.4.4
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load libseccomp/2.4.4
#+END_SRC
*** BDWGC
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/bdwgc/8.0.4
wget https://github.com/ivmai/bdwgc/releases/download/v8.0.4/gc-8.0.4.tar.gz
gzip -dc gc-8.0.4.tar.gz | tar xvf -
cd gc-8.0.4
./configure --prefix=$myprefix --enable-cplusplus
make -j $(nproc)
make install
ml load bdwgc/8.0.4
#+END_SRC
*** pcre
We will prep both ~pcre2~ and ~pcre~.
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/pcre2/10.35
wget https://ftp.pcre.org/pub/pcre/pcre2-10.35.tar.gz
gzip -dc pcre2-10.35.tar.gz | tar xvf -
cd pcre2-10.35
./configure --prefix=$myprefix \
            --enable-pcre2-16  \
            --enable-pcre2-32  \
            --enable-pcre2grep-libz
make -j $(nproc)
make install
ml load pcre2/10.35
#+END_SRC

#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/pcre/8.44
wget https://ftp.pcre.org/pub/pcre/pcre-8.44.tar.gz
gzip -dc pcre-8.44.tar.gz | tar xvf -
cd pcre-8.44
./configure --prefix=$myprefix \
            --enable-pcre-16  \
            --enable-pcre-32  \
            --enable-pcregrep-libz
make -j $(nproc)
make install
ml load pcre/8.44
#+END_SRC
*** bison
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/bison/3.7.1
wget http://ftp.gnu.org/gnu/bison/bison-3.7.1.tar.gz
gzip -dc bison-3.7.1.tar.gz | tar xvf -
cd bison-3.7.1
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load bison/3.7.1
#+END_SRC
*** flex
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/flex/2.6.4
wget https://github.com/westes/flex/releases/download/v2.6.4/flex-2.6.4.tar.gz
gzip -dc flex-2.6.4.tar.gz | tar xvf -
cd flex-2.6.4
./configure --prefix=$myprefix 
make -j $(nproc)
make install
ml load flex/2.6.4
#+END_SRC
*** jq
We first need the ~oniguruma~ [[https://github.com/kkos/oniguruma][regular expression library]].
#+begin_src bash
cd $hpcroot
myprefix=$HOME/.hpc/oniguruma/6.9.7.1
git clone git@github.com:kkos/oniguruma.git
cd oniguruma
git checkout tags/v6.9.7.1
libtoolize
autoreconf -i
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load oniguruma/6.9.7.1
#+end_src
We will actually use the binary.
#+BEGIN_SRC bash
cd $HOME/.local/bin
wget https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64
chmod +x jq-linux64
mv jq-linux64 jq
#+END_SRC
*** bzip2
Needed to manually configure it as shown [[https://github.com/samtools/htslib/issues/696#issuecomment-387405020][here]]
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/bzip2/1.0.8
wget https://www.sourceware.org/pub/bzip2/bzip2-1.0.8.tar.gz
gzip -dc bzip2-1.0.8.tar.gz | tar xvf -
cd bzip2-1.0.8
make -f Makefile-libbz2_so
ln -sf libbz2.so.1.0 libbz2.so
mkdir -p $myprefix/include
mkdir -p $myprefix/lib
cp -avf bzlib.h $myprefix/include
cp -avf libbz2.so* $myprefix/lib
make install PREFIX=$myprefix
ml load bzip2/1.0.8
#+END_SRC
Actually [[https://github.com/libimobiledevice/sbmanager/issues/1#issuecomment-575051418][there are a set of patches]] for ~1.0.6~ which include ~pkg-config~ support so we will use those as well.
#+begin_src bash
cd $hpcroot
myprefix=$HOME/.hpc/bzip2/1.0.6
wget ftp://sourceware.org/pub/bzip2/bzip2-1.0.6.tar.gz
tar xfz bzip2-1.0.6.tar.gz
cd bzip2-1.0.6
# patches.list: https://gist.github.com/steakknife/0ee85c93495ab9f9cff5e21ee12fb25b
wget https://gist.githubusercontent.com/steakknife/946f6ee331512a269145b293cbe898cc/raw/bzip2-1.0.6-install_docs-1.patch
wget https://gist.githubusercontent.com/steakknife/eceda09cae0cdb4900bcd9e479bab9be/raw/bzip2recover-CVE-2016-3189.patch
wget https://gist.githubusercontent.com/steakknife/42feaa223adb4dd7c5c85f288794973c/raw/bzip2-man-page-location.patch
wget https://gist.githubusercontent.com/steakknife/94f8aa4bfa79a3f896a660bf4e973f72/raw/bzip2-shared-make-install.patch
wget https://gist.githubusercontent.com/steakknife/4faee8a657db9402cbeb579279156e84/raw/bzip2-pkg-config.patch
patch -u < bzip2-1.0.6-install_docs-1.patch
patch -u < bzip2recover-CVE-2016-3189.patch
patch -u < bzip2-man-page-location.patch
patch -u < bzip2-shared-make-install.patch
patch -u < bzip2-pkg-config.patch
make
make install PREFIX=$myprefix
ml load bzip2/1.0.6
#+end_src
*** sqlite
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/sqlite/3.32.3
wget https://www.sqlite.org/2020/sqlite-autoconf-3320300.tar.gz
gzip -dc sqlite-autoconf-3320300.tar.gz | tar xvf -
cd sqlite-autoconf-3320300
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load sqlite/3.32.3
#+END_SRC
*** editline
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/editline/1.17.1
wget https://github.com/troglobit/editline/releases/download/1.17.1/editline-1.17.1.tar.gz
gzip -dc editline-1.17.1.tar.gz | tar xvf -
cd editline-1.17.1
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load editline/1.17.1
#+END_SRC
*** Miniconda
We don't need this very much, but it is still useful for some edge cases, mainly revolving around ~jupyter~ infrastructure.
#+BEGIN_SRC bash
cd $HOME
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
chmod +x Miniconda3-latest-Linux-x86_64.sh
./Miniconda3-latest-Linux-x86_64.sh
# Do not allow it to mess up the shell rc files
eval "$($HOME/miniconda3/bin/conda shell.zsh hook)"
#+END_SRC
Note that we will prefer the manual evaluation since it can be handled in the ~lmod~ file.
** Applications
Libraries and ~git~ aside, there are some tools we might want to have.
*** ag
The silver searcher, along with ~rg~ is very useful to have.
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/the_silver_searcher/2.2.0
wget https://geoff.greer.fm/ag/releases/the_silver_searcher-2.2.0.tar.gz
gzip -dc the_silver_searcher-2.2.0.tar.gz | tar xvf -
cd the_silver_searcher-2.2.0
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load the_silver_searcher/2.2.0
#+END_SRC
*** Neovim
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/nvim/0.5.0
wget https://github.com/neovim/neovim/releases/download/nightly/nvim.appimage
chmod +x nvim.appimage
./nvim.appimage --appimage-extract
mkdir -p $myprefix
mv squashfs-root/usr/* $myprefix
ml load nvim/0.5.0
#+END_SRC
*** Tmux
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/tmux/3.1b
wget https://github.com/tmux/tmux/releases/download/3.1b/tmux-3.1b-x86_64.AppImage
chmod +x tmux-3.1b-x86_64.AppImage
rm -rf squashfs-root
./tmux-3.1b-x86_64.AppImage --appimage-extract
mkdir -p $myprefix
mv squashfs-root/usr/bin squashfs-root/usr/lib squashfs-root/usr/share $myprefix
ml load tmux/3.1b
#+END_SRC
*** Zsh
More of an update than a requirement.
#+BEGIN_SRC bash
cd $hpcroot
myprefix=$HOME/.hpc/zsh/5.8
wget https://github.com/zsh-users/zsh/archive/zsh-5.8.tar.gz
gzip -dc zsh-5.8.tar.gz | tar xvf -
cd zsh-zsh-5.8
autoreconf -fiv
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load zsh/5.8
#+END_SRC
** Conclusion
Having composed a bunch of these, I will of course try to somehow get ~nix~ up and running so it can bootstrap itself and allow me to work in peace. I might also eventually create shell scripts to automate updating these, but hopefully I can set up ~nix~ and not re-create package manager logic in ~lua~.

[fn:reallythatbad] They were called ~hpc2013~ and ~hpc2010~ respectively
[fn:mustisay] I really like Lua, enough to embed it in [[https://dseams.info][d-SEAMS]]
* DONE A Tutorial Introduction to Nix :@programming:tools:nix:workflow:python:
CLOSED: [2020-08-18 Tue 16:18]
:PROPERTIES:
:EXPORT_FILE_NAME: ccon-tut-nix
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:EXPORT_BIBLIOGRAPHY: biblio/CarpentryCon2020.bib
:EXPORT_HUGO_PANDOC_CITATIONS: t
:END:
#+BEGIN_QUOTE
Brief introduction to a nix based project workflow.
#+END_QUOTE

** Background
For [[https://2020.carpentrycon.org/][CarpentryCon@Home 2020]], along with [[https://scholar.google.com/citations?user=mviv92EAAAAJ&hl=en][Amrita Goswami]], I am to prepare and deliver a workshop on "[[https://2020.carpentrycon.org/schedule/#session-10][Reproducible Environments with the Nix Packaging System]]". In particular, as a community of practice lesson, the focus is not on packaging (as is typical of most Nix tutorials) nor on the Nix expression language itself, but instead on the use of Nix as a replacement for virtual environments using ~mkShell~.
** Materials
This is a Carpentries style single page lesson on setting up and working with Nix for reproducible environments. It was concieved to be a complimentary resource to the content of [[https://github.com/HaoZeke/CarpentryCon2020_Nix][this repository]], namely:
- [[https://github.com/HaoZeke/CarpentryCon2020_Nix/blob/master/slides/nixPython/nixPython.pdf][Slides on Python packages with Nix]]
- [[https://rgoswami.me/posts/nix-r-devtools/][Nix with R and devtools]]
- [[https://rgoswami.me/posts/rethinking-r-nix/][Statistical Rethinking and Nix]]
- [[https://pad.carpentries.org/cchome-nix-packaging][An Etherpad]]
- [[https://www.youtube.com/watch?v=np6TPzME0aA][Session recording]]
** Ten seconds into Nix
A few words to keep in mind, in no particular order.
- Nix is based of good academic principles by @dolstraNixSafePolicyFree2004 and @dolstraNixOSPurelyFunctional2010
  + It has been used in large scientific projects for reproducibility (e.g. [[https://dseams.info][d-SEAMS]] of @goswamiDSEAMSDeferredStructural2020)
- The Nix expression language is a *domain specific language*
  + Turing completeness is not a goal or a requirement
- Can leverage binary caches
  + Not _always_ true, only when installed in ~/nix~
** Setup
For this particular tutorial, we will assume the standard Nix installation proceedure, that is, one where the installer has root access to create the initial ~/nix~ directory and set up the build users[fn:whynotsingle]. This follows directly from the [[https://nixos.org/nix/manual/#chap-installation][Nix Manual]]:
#+BEGIN_SRC bash
# You need root permissions for this!!!
sh <(curl -L https://nixos.org/nix/install) --daemon
#+END_SRC
At this point we will also install the canonnical first package, the ~hello~ package, which simply outputs a friendly greeting.
#+BEGIN_SRC bash
nix-env -i hello
#+END_SRC
Note that the basic package search operation is ~nix search~ and it gives outputs which look like:

#+DOWNLOADED: screenshot @ 2020-08-18 14:57:27
#+caption: The ~nix search emacs~ output
[[file:images/Setup/nixSearchSetup.png]]

Though this is not bad by any standard, we will try to get a more interactive management tool.
** Basic Helpers
The first few things to obtain are:
- [[https://github.com/madjar/nox][nox]] :: This is a better package management helper
- [[https://github.com/nmattia/niv][niv]] :: For pinning dependencies as discussed later
- [[https://github.com/target/lorri][lorri]] :: For working seamlessly with project environments

*** Exercise 1
#+BEGIN_QUOTE
Try installing these and use ~nox emacs~ to test the output
#+END_QUOTE

#+DOWNLOADED: screenshot @ 2020-08-18 15:06:17
#+caption: The ~nox emacs~ output
[[file:images/Basic_Helpers/basicHelpers.png]]

** More Dependable Dependencies
*** Standard Channels
Nix works by searching a repository (local or online) of package derivations. Indeed, we can pass ~nix-env~ any local fork of the main [[https://github.com/NixOS/nixpkgs][nixpkgs repo]] as well.

#+BEGIN_SRC bash
# don't run this, it is a large repo
git clone https://github.com/NixOS/nixpkgs.git $mynixdir
# make changes..
$EDITOR $nixpkgs/pkgs/applications/editors/emacs/default.nix
nix-env -i emacs -f $nixpkgs
#+END_SRC

- This might serve as a way to mass modify the ~nixpkgs~ in a pinch
  + However, we will almost *never* use this in practice
  + The overlay approach is much better
- It is also useful if we need to build local derivations
*** Pinning Dependencies
Compared to globally tracking the branches of ~nixpkgs~ or even local changes and forks, for project oriented workflows it is better to use ~niv~ which we obtained previously. In a nutshell, ~niv~ will generate a ~json~ file to keep track of dependencies and wraps it in a ~nix~ file we can subsequently import and use.
** Project Setup
We are now in a position to start working with a project oriented workflow.
#+BEGIN_SRC bash :exports both
# Make directories
mkdir myFirstNix
cd myFirstNix
# Setup
niv init
niv update nixpkgs -b nixpkgs-unstable
#+END_SRC
At this stage your project should have the following structure:
#+BEGIN_SRC bash :exports both
tree myFirstNix
#+END_SRC

#+RESULTS:
| myFirstNix |              |   |       |
| └──        | nix          |   |       |
| ├──        | sources.json |   |       |
| └──        | sources.nix  |   |       |
|            |              |   |       |
| 1          | directory,   | 2 | files |
We can now move on to the heart of this tutorial, the ~nix-shell~. In a nutshell, running ~nix-shell~ when there is a defined ~shell.nix~ will spawn a virtual environment with the nix packages requested.
*** lorri and direnv
Though we haven't as yet generated a ~shell.nix~ we should point out that writing one by hand will mean that we need to rebuild the enviroment when we make changes using ~nix-shell~ every time. A more elegant approach is to offload the rebuilding of the environment to ~lorri~ which also has a neat ~direnv~ integrration. Let's try that out.
#+BEGIN_SRC bash :exports both
cd myFirstNix
lorri init
#+END_SRC

#+RESULTS:
| Aug | 18 | 15:24:06.524 | INFO | wrote | file, | path: | ./shell.nix |
| Aug | 18 | 15:24:06.524 | INFO | wrote | file, | path: | ./.envrc    |
| Aug | 18 | 15:24:06.524 | INFO | done  |       |       |             |
At this point we should now have:
#+BEGIN_SRC bash :exports both
tree -a myFirstNix
#+END_SRC

#+RESULTS:
| myFirstNix |            |              |       |
| ├──        | .envrc     |              |       |
| ├──        | nix        |              |       |
| │          | ├──        | sources.json |       |
| │          | └──        | sources.nix  |       |
| └──        | shell.nix  |              |       |
|            |            |              |       |
| 1          | directory, | 4            | files |

We might want to take a quick look at what is being loaded into the environment and the ~shell.nix~ at this point.

#+caption: ~shell.nix~
#+BEGIN_SRC nix
let
  pkgs = import <nixpkgs> {};
in
pkgs.mkShell {
  buildInputs = [
    pkgs.hello
  ];
}
#+END_SRC

#+caption: ~.envrc~
#+BEGIN_SRC bash
eval "$(lorri direnv)"
#+END_SRC
The ~.envrc~ output is not very useful at a glance, however when we cd into the directory it is very verbose and explicit about what is being set up.

#+DOWNLOADED: screenshot @ 2020-08-18 15:31:31
#+caption: Sample output of the evaluation
[[file:images/Project_Setup/prjSetup.png]]

Note that in order to set ~lorri~ up, we will need to set up a daemon.
#+BEGIN_SRC bash
systemctl --user start lorri
direnv allow
#+END_SRC

*** Pinning with niv
Note that inspite of having set up ~niv~, we have not yet used the sources defined therein. We will now fix this, by modifying ~shell.nix~.
#+caption: ~shell.nix~ with ~niv~
#+BEGIN_SRC nix
let
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  inherit (pkgs.lib) optional optionals;
in
pkgs.mkShell {
  buildInputs = [
    pkgs.hello
  ];
}
#+END_SRC
** Purity and Environments
There are a couple of things to note about this setup.
- The default shell is ~bash~
- On occasion, depending on your Dotfiles you might have paths overriden in an annoying way
One workaround is to use nix shell with an argument:
#+BEGIN_SRC bash
nix-shell --run "bash"
#+END_SRC
- We can also pass ~--pure~ to the function, but at the cost of having to define many more dependencies for our shell
** mkShell
The ~mkShell~ function is the focus of our tutorial, and we will mostly work around passing in different environments and hooks. Let us start by defining a hook.
*** Shell Hooks
Often, we will want to set an environment variable in our shell in advance. We should not use ~direnv~ for this, and instead we will focus on the ~shellHook~ option. Syntactically, we note that this is of the form:
#+BEGIN_SRC nix
let
  hook = ''
  export myvar="Test"
  ''
in pkgs.mkShell {
  shellHook = hook;
}
#+END_SRC
Often we will describe variables in the let section in favor of cluttering the actual function call itself.
** Overriding Global Packages
For overriding global packages, it is best to leverage the ~config.nix~ (which is commonly in ~$HOME/.config/nixpkgs/config.nix~) file instead of the current environment, though it could be managed in a per-project setup as well. Consider the case where we need to disable tests for a particular packages, say ~libuv~.
#+caption: A sample ~config.nix~ file
#+BEGIN_SRC nix
{
  packageOverrides = pkgs:
    with pkgs; {
      libuv = libuv.overrideAttrs (oldAttrs: {
        doCheck = false;
        doInstallCheck = false;
      });
    };
}
#+END_SRC
** Python Dependencies
As R dependency management has been covered [[Nix-R and Devtools][in an earlier post]], we will focus on the management of ~python~ environments.
*** Generic Environments
We can define existing packages as follows (and can check for existence with ~nox~) using the ~let..in~ syntax.
#+caption: Shell with basic ~python~ environment
#+BEGIN_SRC nix
let
  # Niv
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  inherit (pkgs.lib) optional optionals;
  # Python
  pythonEnv = pkgs.python38.withPackages (ps: with ps;[
    numpy
    toolz
  ]);
in pkgs.mkShell {
  buildInputs = with pkgs; [
    pythonEnv

    black
    mypy

    libffi
    openssl
  ];
}
#+END_SRC
*** Project Local Pip
We can leverage a trick from [[https://churchman.nl/2019/01/22/using-nix-to-create-python-virtual-environments/][here]] to set a local directory for ~pip~ installations, which boils down to some path hacking.
#+BEGIN_SRC nix
let
  # Niv
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  inherit (pkgs.lib) optional optionals;
  # Python
  pythonEnv = pkgs.python38.withPackages (ps: with ps;[
    numpy
    toolz
  ]);
  hook = ''
     export PIP_PREFIX="$(pwd)/_build/pip_packages"
     export PYTHONPATH="$(pwd)/_build/pip_packages/lib/python3.8/site-packages:$PYTHONPATH"
     export PATH="$PIP_PREFIX/bin:$PATH"
     unset SOURCE_DATE_EPOCH
  '';
in pkgs.mkShell {
  buildInputs = with pkgs; [
    pythonEnv

    black
    mypy

    libffi
    openssl
  ];
  shellHook = hook;
}

#+END_SRC
*Note that* this is discouraged as we will lose the caching capabilities of nix.
** Non-Standard Python
For more control over the environment, we can define it in more detail with some overlays.
#+BEGIN_SRC nix
let
  python = pkgs.python38.override {
    packageOverrides = self: super: {
      pytest = super.pytest.overridePythonAttrs (old: rec {
        doCheck = false;
        doInstallCheck = false;
      });
    };
  };
  myPy = python.withPackages
    (p: with p; [ numpy pip pytest ]);
in pkgs.mkShell {
  buildInputs = with pkgs; [
    myPy
  ];
}
#+END_SRC
We have used both overriden packages and standard packages in the above formulation.
*** Building Packages
For cases where we are certain that no existing package is present (use ~nox~) we can also build them. Take ~f90wrap~ as an example, and we will use the Github version, rather than the PyPi version (the difference is in the source fetch function).
#+BEGIN_SRC nix
f90wrap = self.buildPythonPackage rec {
  pname = "f90wrap";
  version = "0.2.3";
  src = pkgs.fetchFromGitHub {
    owner = "jameskermode";
    repo = "f90wrap";
    rev = "master";
    sha256 = "0d06nal4xzg8vv6sjdbmg2n88a8h8df5ajam72445mhzk08yin23";
  };
  buildInputs = with pkgs; [ gfortran stdenv ];
  propagatedBuildInputs = with self; [
    setuptools
    setuptools-git
    wheel
    numpy
  ];
  preConfigure = ''
    export F90=${pkgs.gfortran}/bin/gfortran
  '';
  doCheck = false;
  doIstallCheck = false;
};
#+END_SRC
This is quite involved, _discuss_.
*** Setting Versions
We can finally generalize our ~shell.nix~ to default to ~python 3.8~ but also take a command through ~--argstr~:
#+BEGIN_SRC bash
nix-shell --argstr pythonVersion 36 --run "bash"
#+END_SRC
Where we need to simply define the option at the top of the file, with a default.
#+caption: Full ~shell.nix~
#+BEGIN_SRC nix
{ pythonVersion ? "38" }:
# Define
let
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  inherit (pkgs.lib) optional optionals;
  hook = ''
    # Python Stuff
     export PIP_PREFIX="$(pwd)/_build/pip_packages"
     export PYTHONPATH="$(pwd)/_build/pip_packages/lib/python3.8/site-packages:$PYTHONPATH"
     export PATH="$PIP_PREFIX/bin:$PATH"
     unset SOURCE_DATE_EPOCH
  '';
  # Apparently pip needs 1980 or above
  # https://github.com/ento/elm-doc/blob/master/shell.nix
  python = pkgs."python${pythonVersion}".override {
    packageOverrides = self: super: {
      pytest = super.pytest.overridePythonAttrs (old: rec {
        doCheck = false;
        doInstallCheck = false;
      });
      ase = super.ase.overridePythonAttrs (old: rec {
        doCheck = false;
        doInstallCheck = false;
      });
      f90wrap = self.buildPythonPackage rec {
        pname = "f90wrap";
        version = "0.2.3";
        src = pkgs.fetchFromGitHub {
          owner = "jameskermode";
          repo = "f90wrap";
          rev = "master";
          sha256 = "0d06nal4xzg8vv6sjdbmg2n88a8h8df5ajam72445mhzk08yin23";
        };
        buildInputs = with pkgs; [ gfortran stdenv ];
        propagatedBuildInputs = with self; [
          setuptools
          setuptools-git
          wheel
          numpy
        ];
        preConfigure = ''
          export F90=${pkgs.gfortran}/bin/gfortran
        '';
        doCheck = false;
        doInstallCheck = false;
      };
    };
  };
  myPy = python.withPackages
    (p: with p; [ ase ipython ipykernel scipy numpy f90wrap pip ]);
in pkgs.mkShell {
  buildInputs = with pkgs; [
    # Required for the shell
    zsh
    perl
    git
    direnv
    fzf
    ag
    fd

    # Building thigns
    gcc9
    gfortran
    openblas

    myPy
    # https://github.com/sveitser/i-am-emotion/blob/294971493a8822940a153ba1bf211bad3ae396e6/gpt2/shell.nix
  ];
  shellHook = hook;
}
#+END_SRC
This is enough to cover almost all use-cases for ~python~ environments.
** Build Helpers
Note that we can speed up some aspects of fetch with the prefetch commands:
#+BEGIN_SRC bash
nix-prefetch-git $giturl
nix-prefetch-url $url
#+END_SRC
In practice, some trial and error is easier.
** Supplementary Reading Material
Though these are in no means exhaustive, they may offer a slightly more advanced or different focus than the material covered here.
*** Core Content
- Manuals
  - [[https://nixos.org/nix/manual/][Nix]] and Nixpkgs
- [[https://nixos.wiki/][Nix Wiki]]
  - [[https://nixos.wiki/index.php?title=Cheatsheet&useskin=vector][Nix Cheatsheet]]
- [[https://github.com/NixOS/nixpkgs/blob/master/doc/languages-frameworks/][Language Sections]]
*** Learning Paths
- [[https://nixos.org/nixos/nix-pills/][Nix pills]]
- [[https://nixos.org/learn.html][Official tutorials]]
- [[https://nix.dev/][Nix dev]] has some nice opinionated tips
*** Personal Correspondence
Tyson Whitehead from Compute Canada was kind enough to bring the folllowing additional training materials:
- A wiki pertaining to usage of Nix on in [[https://git.computecanada.ca/nix/training/-/tree/ccwiki][an HPC setting]]
- SWC style workshop materials from [[https://git.computecanada.ca/nix/training/-/tree/20180618-tecc][TECC 2018]]
- [[https://git.computecanada.ca/nix/training/-/tree/20180115-sharcnet][SHARCNET live presentation materials]] from 2018
** Conclusions
The standard dive into Nix is based on building derivations and playing with language, which is in no means a bad one, just too long for the time allocated. The best way to get into Nix is to start using it for everything.

[fn:whynotsingle] For reasons pertaining to latency and ease-of-use, we will assume the multi-user installation
* DONE Nix Shells for Node Projects :@programming:tools:nix:workflow:node:
CLOSED: [2020-08-23 Sun 10:09]
:PROPERTIES:
:EXPORT_FILE_NAME: nix-shell-node
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:EXPORT_HUGO_PANDOC_CITATIONS: t
:END:
** Background
As a prelude to writing up the details of how this site is generated, I realized I should write up a ~nix~ oriented workflow for ~node~ packages.
** Tooling and Idea
The basic concepts are:
- Use ~npm~ to generate a ~package-lock.json~ file
- Use ~node2nix~ in a shell to generate a set of ~nix~ derivations
- Enter a shell environment with the ~nix~ inputs
- Profit

However, the nuances of this are a bit annoying at first.
** Packaging Requirements
We will use the standard ~npm~ installation method at first, but since we shouldn't keep installing and removing things, so we need a way to modify ~package.json~ without running ~npm~ and will therefore add ~add-dependency~.
#+BEGIN_SRC bash
npm install add-dependency
#+END_SRC
** Setting up Node2Nix
We will first clean the directory of what we do not need.

#+BEGIN_SRC bash
rm -rf default.nix node-env.nix node-packages.nix node_modules
#+END_SRC

Now we can enter a shell with ~node2nix~ and generate files for the node packages.

#+BEGIN_SRC bash
nix-shell -p 'nodePackages.node2nix'
node2nix -l package-lock.json
#+END_SRC

** A Nix Environment

We will use the standard setup described in [[A Tutorial Introduction to Nix][the tutorial post]]:
#+BEGIN_SRC bash
nix-env -i niv lorri
niv init
niv update nixpkgs -b nixpkgs-unstable
#+END_SRC

This is to be in conjunction with the following ~shell.nix~ file [fn:whythis].

#+BEGIN_SRC nix
{ sources ? import ./nix/sources.nix }:
let
  pkgs = import sources.nixpkgs { };
  nodeEnv = pkgs.callPackage ./node-env.nix { };
  nodePackages = pkgs.callPackage ./node-packages.nix {
    globalBuildInputs = with pkgs; [ zsh ];
    inherit nodeEnv;
  };
in nodePackages.shell
#+END_SRC

Note that we have overridden the ~nodePackages~ shell which is defined in the files created by ~node2nix~.

We can now enter the environment and setup ~node_modules~[fn:whatlorri].
#+BEGIN_SRC bash
nix-shell
ln -s $NODE_PATH node_modules
#+END_SRC

** Updates
Unfortunately, this setup is a little fragile to updates. We will need to exit and re-create the setup. Note that we are removing the lock file now as well.

#+BEGIN_SRC bash
# In the nix-shell
add-dependencies babel-loader @babel/core @babel/preset-env core-js @babel/plugin-transform-regenerator
# Do not run in nix-shell
rm -rf default.nix node-env.nix node-packages.nix node_modules package-lock.json
# Update in a line
nix-shell -p 'nodePackages.node2nix' --run 'node2nix package.json'
#+END_SRC

The single line update mechanism can be run in the ~nix-shell~ itself, making things marginally less painful.
** Conclusions
This has been a short introduction to working with the ~nix-shell~ ecosystem. It isn't as fast as working with the normal setup, and it is a pretty annoying workflow. Given that most CI setups have good support for caching ~npm~ dependencies, it doesn't seem worthwhile at the moment.

[fn:whatlorri] We can't use ~lorri~ yet since we need to selectively add and remove the symbolic link to ~node_modules~
[fn:whythis] There might be a better approach defined in [[https://github.com/svanderburg/node2nix/issues/175][this issue]] later

* DONE Niv and Mach-Nix for Nix Python :@programming:tools:nix:workflow:python:
CLOSED: [2020-08-26 Wed 05:42]
:PROPERTIES:
:EXPORT_FILE_NAME: mach-nix-niv-python
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
Short post on using ~mach-nix~ with ~niv~.
#+END_QUOTE
** Background
In previous posts, there was a [[A Tutorial Introduction to Nix][discussion on a ground up approach to adding packages]] which aren't on the core ~nixpkgs~ channels using GitHub or PyPi sources. However, this lacked a way to do so programmatically, and also a way to convert existing python projects.

** Python Dependency Management
This time, instead of the more pedagogical approach of building packages from PyPi or GitHub, we will use overlays and the excellent [[https://github.com/DavHau/mach-nix][mach-nix]]
 to speed up the process. We will continue to use [[https://github.com/nmattia/niv][niv]].
 #+BEGIN_SRC bash
niv init
niv update nixpkgs -b nixpkgs-unstable
 #+END_SRC

To leverage ~mach-nix~ we will simply need the following setup to work with ~niv~.

#+BEGIN_SRC nix
let
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  inherit (pkgs.lib) optional optionals;
  mach-nix = import (builtins.fetchGit {
    url = "https://github.com/DavHau/mach-nix/";
    ref = "refs/tags/3.1.1";
  }) {
    pkgs = pkgs;

    # optionally specify the python version
    # python = "python38";

    # optionally update pypi data revision from https://github.com/DavHau/pypi-deps-db
    # pypiDataRev = "some_revision";
    # pypiDataSha256 = "some_sha256";
  };
  customPython = mach-nix.mkPython {
    requirements = ''
      copier
      pytest
    '';
    providers = {
      _default = "nixpkgs,wheel,sdist";
      pytest = "nixpkgs";
    };
    pkgs = pkgs;
  };
in pkgs.mkShell { buildInputs = with pkgs; [ customPython ]; }
#+END_SRC

Note that we have essentially written out a ~requirements.txt~ and can actually pass a path there instead as well. The key point to make it work with ~niv~ is the ~pkgs~ parameter. To use the older method of overriding parts of the setup, we can use the ~overrides_pre~ hook as shown below:
#+BEGIN_SRC nix
let
  sources = import ./prjSource/nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  inherit (pkgs.lib) optional optionals;
  mach-nix = import (builtins.fetchGit {
    url = "https://github.com/DavHau/mach-nix/";
    ref = "refs/tags/3.1.1";
  }) {
    pkgs = pkgs;

    # optionally specify the python version
    # python = "python38";

    # optionally update pypi data revision from https://github.com/DavHau/pypi-deps-db
    # pypiDataRev = "some_revision";
    # pypiDataSha256 = "some_sha256";
  };
  customPython = mach-nix.mkPython {
    requirements = ''
      copier
      pytest
      f90wrap
    '';
    providers = {
      _default = "nixpkgs,wheel,sdist";
      pytest = "nixpkgs";
    };
    overrides_pre = [
      (pythonSelf: pythonSuper: {
        pytest = pythonSuper.pytest.overrideAttrs (oldAttrs: {
          doCheck = false;
          doInstallCheck = false;
        });
        f90wrap = pythonSelf.buildPythonPackage rec {
          pname = "f90wrap";
          version = "0.2.3";
          src = pkgs.fetchFromGitHub {
            owner = "jameskermode";
            repo = "f90wrap";
            rev = "master";
            sha256 = "0d06nal4xzg8vv6sjdbmg2n88a8h8df5ajam72445mhzk08yin23";
          };
          buildInputs = with pkgs; [ gfortran stdenv ];
          propagatedBuildInputs = with pythonSelf; [
            setuptools
            setuptools-git
            wheel
            numpy
          ];
          preConfigure = ''
            export F90=${pkgs.gfortran}/bin/gfortran
          '';
          doCheck = false;
          doIstallCheck = false;
        };
      })
    ];
    pkgs = pkgs;
  };
in pkgs.mkShell { buildInputs = with pkgs; [ customPython ]; }
#+END_SRC

We can also pull in overrides from ~poetry2nix~ with ~overrides_post~ as [[https://github.com/DavHau/mach-nix/blob/master/examples.md][described here]].
** Conclusion
With the completion of this final remaining hurdle, ~nix~ is now fully realized as a ~python~ management system. At this point the "only" thing remaining is to find an optimal way of leveraging ~nix~ for setting up re-usable data science and scientific computing projects.

* DONE Local Nix without Root :@programming:workflow:projects:hpc:nix:tools:
CLOSED: [2020-09-07 Mon 18:30]
:PROPERTIES:
:EXPORT_FILE_NAME: local-nix-no-root
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
Monkeying around with ~nix~ for HPC systems which have no root access and NFS filesystems.
#+END_QUOTE
** Background
Nix is not well known for being friendly to users without root access. This is typically made worse by the "exotic" filesystem attributes common to HPC networks (this also plagues [[https://github.com/andrewchambers/hermes/issues/75][hermes]]). An [[Provisioning Dotfiles on an HPC][earlier post]] details how and why ~proot~ failed.
The short pitch is simply:

#+DOWNLOADED: screenshot @ 2020-09-07 17:34:25
#+caption: Does your HPC look like this?
[[file:images/Background/2020-09-07_17-34-25_screenshot.png]]

#+DOWNLOADED: screenshot @ 2020-09-07 17:37:28
#+caption: It really is an HPC
[[file:images/Background/2020-09-07_17-37-28_screenshot.png]]


If your HPC doesn't look that swanky and you'd like it to, then read on. Note that there are all the obvious benefits of ~nix~ as well, but this is a more eye-catchy pitch.
** Setup
#+BEGIN_QUOTE
The basic concept is to install ~nix~ from source, with appropriate patches, and then mess around with paths until it is ready and willing to work with stores which are not ~/nix~ [fn:thateasy]
#+END_QUOTE
This concept is strongly influenced by the work [[https://github.com/jefdaj/nix-no-root][described in this repo]]. The premise is similar to my earlier post [[HPC Dotfiles and LMod][on HPC Dotfiles]]. For the purposes of this post, we will assume that all the packages in the [[HPC Dotfiles and LMod][previous post]]
exist. ~lmod~ is not required, feel free to use an alternative path management system, or even just ~$HOME/.local~ but if ~lmod~ is present, it is highly recommended [fn:whatalternatives]. We *will need* the following:

- Pinned set of nixpkgs :: We would like to be able to modify a lot of paths, which is normally a bad practice, but then we don't normally rebuild all packages either. Grab a copy of the ~nixpkgs~ by following the instructions below. Now is also the time to fork the repo if you'd like to keep track of your changes.
#+BEGIN_SRC bash
mkdir -p $HOME/Git/Github
cd $HOME/Git/Github
git clone https://github.com/NixOS/nixpkgs
#+END_SRC
- dotgit :: We use the older, bash version of [[https://github.com/kobus-v-schoor/dotgit/][the excellent dotgit]] since ~python~ is not always present in HPC environments.
#+BEGIN_SRC bash
git clone https://github.com/kobus-v-schoor/dotgit/
mkdir -p $HOME/.local/bin
cp dotgit/old/bin/bash_completion dotgit/old/bin/dotgit dotgit/old/bin/dotgit_headers dotgit/old/bin/fish_completion.fish $HOME/.local/bin/ -r
#+END_SRC
- lmod packages :: If you do not or cannot use modulefiles as [[HPC Dotfiles and LMod][described in the earlier post]], inspect the module-files being loaded and set paths accordingly.
#+BEGIN_SRC bash
cd $HOME/Git/Github
git clone https://github.com/HaoZeke/hzHPC_lmod
cd hzHPC_lmod
$HOME/.local/bin/dotgit restore hzhpc
#+END_SRC

Now we can start by obtaining the ~nix~ sources.
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/nix/nix-boot
nixdir=$HOME/.nix
nix_version=2.3.7
ml load gcc/9.2.0 flex bison
ml load boost
ml load editline
ml load brotli/1.0.1
ml load libseccomp/2.4.4
ml load bdwgc/8.0.4
ml load bzip2/1.0.8
ml load sqlite
ml load patch xz
wget http://nixos.org/releases/nix/nix-${nix_version}/nix-${nix_version}.tar.bz2
tar xfv nix-2.3.7.tar.bz2
cd nix-2.3.7
#+END_SRC

Before actually configuring and installing from source, we need some patches.
*** Patches
I suggest carefully typing out the patches, though leave a comment if you want a repo with these changes (if you must star something in the meantime, [[https://github.com/d-SEAMS/seams-core][star this]]).
- Start with [[https://github.com/NixOS/nix/pull/1584/files][this patch]]
#+begin_src bash
wget https://github.com/NixOS/nix/commit/8d3cb66d22f348341d7afa626acfa53b40584fdd.patch
git apply 8d3cb66d22f348341d7afa626acfa53b40584fdd.patch
#+end_src
- Also [[https://nixos.wiki/wiki/Nix_Installation_Guide][this one]]

Remove the following ~ifdef~ stuff from  ~src/libutil/compression.cc~, leaving only the contents of the ~else~ statement.
#+BEGIN_SRC c++
#ifdef HAVE_LZMA_MT
            lzma_mt mt_options = {};
            mt_options.flags = 0;
            mt_options.timeout = 300; // Using the same setting as the xz cmd line
            mt_options.preset = LZMA_PRESET_DEFAULT;
            mt_options.filters = NULL;
            mt_options.check = LZMA_CHECK_CRC64;
            mt_options.threads = lzma_cputhreads();
            mt_options.block_size = 0;
            if (mt_options.threads == 0)
                mt_options.threads = 1;
            // FIXME: maybe use lzma_stream_encoder_mt_memusage() to control the
            // number of threads.
            ret = lzma_stream_encoder_mt(&strm, &mt_options);
            done = true;
#else
            printMsg(lvlError, "warning: parallel XZ compression requested but not supported, falling back to single-threaded compression");
#endif
#+END_SRC

If there is trouble with the bzip2 library, set ~$HOME/.hpc/bzip2/1.0.8/include/bzlib.h~ in ~src/libutil/compression.cc~, but expand ~$HOME~.

Finally, you will need edit ~nixpkgs~.
#+BEGIN_SRC nix
# vim pkgs/os-specific/linux/busybox/default.nix
  debianName = "busybox_1.30.1-6";
  debianTarball = fetchzip {
    url = "http://deb.debian.org/debian/pool/main/b/busybox/${debianName}.debian.tar.xz";
    sha256 = "05n6mxc8n4zsli4dijrr2x5c9ggwi223i5za4n0xwhgd4lkhqymw";
  };
#+END_SRC
*** User Build
We can now complete the build.
#+BEGIN_SRC bash
ml load openssl curl
./configure  --enable-gc --prefix=$myprefix --with-store-dir=$nixdir/store --localstatedir=$nixdir/var --with-boost=$BOOST_ROOT --disable-seccomp-sandboxing --disable-doc-gen --with-sandbox-shell=/usr/bin/sh CPPFLAGS="-I$HOME/.hpc/bzip2/1.0.8/include" LDFLAGS="-L$HOME/.hpc/bzip2/1.0.8/lib -Wl,-R$HOME/.hpc/bzip2/1.0.8/lib"
make -j $(nproc)
make install
ml load nix/user # Hooray!
ml unload openssl curl
#+END_SRC

Now we still need to set a profile. Inspect  ~.hpc/nix/nix-boot/etc/profile.d/nix.sh~ and check the value of ~NIX_PROFILES~

#+BEGIN_SRC bash
chmod +x .hpc/nix/nix-boot/etc/profile.d/nix.sh
./.hpc/nix/nix-boot/etc/profile.d/nix.sh
# OR, and this is better
nix-env --switch-profile .nix/var/nix/profiles/default
mkdir -p  ~/.nix/var/nix/profiles
#+END_SRC

We also need to kill the sandbox for now, as also seen in the [[https://aur.archlinux.org/packages/nix/?O=10&PP=10][AUR package]] (and [[https://github.com/NixOS/nix/issues/3337#issuecomment-579363583][here]]).
#+begin_src conf
# ~/.config/nix/nix.conf
sandbox = false
substituters = https://cache.nixos.org https://all-hies.cachix.org
trusted-public-keys = cache.nixos.org-1:6NCHdD59X431o0gWypbMrAURkbJ16ZPMQFGspcDShjY= all-hies.cachix.org-1:JjrzAOEUsD9ZMt8fdFbzo3jNAyEWlPAwdVuHw4RD43k=
#+end_src
Now we can test this before moving forward:
#+begin_src bash
nix-channel --update
nix-shell -p hello
#+end_src
** Rebuilding Natively
The astute reader will have noticed that we glibly monkeyed around with the ~nix~ source in the previous section, but all will be made well since we can rebuild to use ~nix~ with itself. Do *replace the variable with the corresponding path*:
#+BEGIN_SRC nix
storeDir = "$HOME/.nix/store";
stateDir = "$HOME/.nix/var";
confDif = "$HOME/.nix/etc";
#+END_SRC

Essentially, the ~$HOME/.config/nixpkgs/config.nix~ should look like (incorporating both the patches and also the full directory we will be using):
#+begin_src nix
{
  packageOverrides = pkgs:
    with pkgs; {
      autogen = autogen.overrideAttrs (oldAttrs: {
        postInstall = ''
          mkdir -p $dev/bin
          mv $bin/bin/autoopts-config $dev/bin
          for f in $lib/lib/autogen/tpl-config.tlib $out/share/autogen/tpl-config.tlib; do
            sed -e "s|$dev/include|/no-such-autogen-include-path|" -i $f
            sed -e "s|$bin/bin|/no-such-autogen-bin-path|" -i $f
            sed -e "s|$lib/lib|/no-such-autogen-lib-path|" -i $f
          done
          # remove /tmp/** from RPATHs
          for f in "$bin"/bin/*; do
            local nrp="$(patchelf --print-rpath "$f" | sed -E 's@(:|^)/tmp/[^:]*:@\1@g')"
            patchelf --set-rpath "$nrp" "$f"
          done
        '' + stdenv.lib.optionalString (!stdenv.hostPlatform.isDarwin) ''
          # remove /build/** from RPATHs
          for f in "$bin"/bin/*; do
            local nrp="$(patchelf --print-rpath "$f" | sed -E 's@(:|^)/build/[^:]*:@\1@g')"
            patchelf --set-rpath "$nrp" "$f"
          done
        '';
      });
      nix = nix.overrideAttrs (oldAttrs: {
        storeDir = "/users/home/jdoe/.nix/store";
        stateDir = "/users/home/jdoe/.nix/var";
        confDif = "/users/home/jdoe/.nix/etc";
        doCheck = false;
        doInstallCheck = false;
        prePatch = ''
          substituteInPlace src/libstore/local-store.cc \
            --replace '(eaName == "security.selinux")' \
                      '(eaName == "security.selinux" || eaName == "system.nfs4_acl")'
          substituteInPlace src/libstore/gc.cc \
            --replace 'auto mapLines =' \
                      'continue; auto mapLines ='
          substituteInPlace src/libstore/sqlite.cc \
            --replace 'SQLITE_OPEN_READWRITE | SQLITE_OPEN_CREATE, 0) != SQLITE_OK)' \
                      'SQLITE_OPEN_READWRITE | SQLITE_OPEN_CREATE, "unix-dotfile") != SQLITE_OK)'
        '';
      });
    };
}
#+end_src

We can "speed up" our build by disabling all tests. Go to the copy of ~nixpkgs~ and run:

#+BEGIN_SRC bash
find pkgs  -type f -name 'default.nix' | xargs sed -i 's/doCheck = true/doCheck = false/'
#+END_SRC

#+BEGIN_SRC bash
mkdir -p $HOME/.nix/var/nix/profiles/
nix-env -i nix -f $HOME/Git/Github/nixpkgs -j$(nproc) --keep-going --show-trace -v --cores 4 2>&1 | tee nix-no-root.log
ml load nix/bootstrapped
#+END_SRC

This will still take a couple of hours at least. Around 3-4 hours. Try to set this up on a lazy weekend to evade sysadmins.

If ~curl 429~ rate limits are encountered for ~musl~ sources, the solution is to replace the source (put the following in a file, say ~no429.patch~):
#+begin_src nix
diff --git a/pkgs/os-specific/linux/musl/default.nix b/pkgs/os-specific/linux/musl/default.nix
index ae175a3..1a6f6c7 100644
--- a/pkgs/os-specific/linux/musl/default.nix
+++ b/pkgs/os-specific/linux/musl/default.nix
@@ -4,12 +4,12 @@
 }:
 let
   cdefs_h = fetchurl {
-    url = "http://git.alpinelinux.org/cgit/aports/plain/main/libc-dev/sys-cdefs.h";
+    url = "https://raw.githubusercontent.com/akadata/aports/master/main/libc-dev/sys-cdefs.h";
     sha256 = "16l3dqnfq0f20rzbkhc38v74nqcsh9n3f343bpczqq8b1rz6vfrh";
   };
   queue_h = fetchurl {
-    url = "http://git.alpinelinux.org/cgit/aports/plain/main/libc-dev/sys-queue.h";
-    sha256 = "12qm82id7zys92a1qh2l1qf2wqgq6jr4qlbjmqyfffz3s3nhfd61";
+    url = "https://raw.githubusercontent.com/akadata/aports/master/main/libc-dev/sys-queue.h";
+    sha256 = "049pd547ckrsky72s18a649mz660yph14wdrlw9gnbk903skdnz4";
   };
   tree_h = fetchurl {
     url = "http://git.alpinelinux.org/cgit/aports/plain/main/libc-dev/sys-tree.h";
#+end_src
This can be applied with ~git apply~.
** Usage
We have finally obtained a bootstrapped ~nix~ which is bound to our set of ~nixpkgs~. To ensure its use:
#+BEGIN_SRC bash
ml use $HOME/Modulefiles
ml purge
ml load nix/bootstrapped
ml save
#+END_SRC
*** Flakes and DevShells
Newer versions of ~nix~ depend on ~mdbook~ which is meant for generating the documentation. Unfortunately, the ~cargo256~ hashes are path dependent. A quick fix is to remove the dependency on ~mdbook~ and disable documentation generation with the following ugly patch:
#+begin_src nix
diff --git a/pkgs/tools/package-management/nix/default.nix b/pkgs/tools/package-management/nix/default.nix
index 7eda5ae..91bf1b8 100644
--- a/pkgs/tools/package-management/nix/default.nix
+++ b/pkgs/tools/package-management/nix/default.nix
@@ -14,7 +14,7 @@ common =
   , pkg-config, boehmgc, libsodium, brotli, boost, editline, nlohmann_json
   , autoreconfHook, autoconf-archive, bison, flex
   , jq, libarchive, libcpuid
-  , lowdown, mdbook
+  , lowdown
   # Used by tests
   , gtest
   , busybox-sandbox-shell
@@ -36,7 +36,7 @@ common =

       VERSION_SUFFIX = suffix;

-      outputs = [ "out" "dev" "man" "doc" ];
+      outputs = [ "out" "dev" ];

       nativeBuildInputs =
         [ pkg-config ]
@@ -45,7 +45,6 @@ common =
           [ autoreconfHook
             autoconf-archive
             bison flex
-            (lib.getBin lowdown) mdbook
             jq
            ];

@@ -119,8 +118,8 @@ common =
         [ "--with-store-dir=${storeDir}"
           "--localstatedir=${stateDir}"
           "--sysconfdir=${confDir}"
-          "--disable-init-state"
           "--enable-gc"
+          "--disable-doc-gen"
         ]
         ++ lib.optionals stdenv.isLinux [
           "--with-sandbox-shell=${sh}/bin/busybox"
@@ -136,7 +135,8 @@ common =

       installFlags = [ "sysconfdir=$(out)/etc" ];

-      doInstallCheck = true; # not cross
+      doInstallCheck = false; # not cross
+      doCheck = false;

       # socket path becomes too long otherwise
       preInstallCheck = lib.optionalString stdenv.isDarwin ''
@@ -160,7 +160,7 @@ common =
         license = lib.licenses.lgpl2Plus;
         maintainers = [ lib.maintainers.eelco ];
         platforms = lib.platforms.unix;
-        outputsToInstall = [ "out" "man" ];
+        outputsToInstall = [ "out" ];
       };

       passthru = {
#+end_src
We also need to update our ~config.nix~:
#+begin_src nix
nixUnstable = nixUnstable.overrideAttrs (oldAttrs: {
    storeDir = "/users/home/rog32/.nix/store";
    stateDir = "/users/home/rog32/.nix/var";
    confDif = "/users/home/rog32/.nix/etc";
    doCheck = false;
    doInstallCheck = false;
    prePatch = ''
      substituteInPlace src/libstore/local-store.cc \
        --replace '(eaName == "security.selinux")' \
                  '(eaName == "security.selinux" || eaName == "system.nfs4_acl")'
      substituteInPlace src/libstore/gc.cc \
        --replace 'auto mapLines =' \
                  'continue; auto mapLines ='
     '';
   });
#+end_src
Now we can finally get to the installation of a newer version. I prefer to live life on the edge:
#+begin_src bash
nix-env -iA nixUnstable -f $HOME/Git/Github/nixpkgs -j$(nproc) --keep-going --show-trace --cores 4 2>&1 | tee nix-install-base.log
#+end_src
We are now able activate flakes and other features like ~nix shell~ (note the space!).
#+begin_src conf
# ~/.config/nix/nix.conf
experimental-features = nix-command flakes
#+end_src
**** Bonus: Fixing Documentation
In order to get the original derivation working, we need to essentially modify the ~cargo256~ hashes. Thankfully the ~nix~ build log is rather verbose.
#+begin_src bash
installing
error: hash mismatch in fixed-output derivation '/users/home/jdoen/.nix/sto$
e/n71nkimlbazmq1vpyyavqcxzg9c86brs-mdbook-0.4.7-vendor.tar.gz.drv':
         specified: sha256-2kBJcImytsSd7Q0kj1bsP/NXxyy2Pr8gHb8iNf6h3/4=
            got:    sha256-4bYLrmyI7cPUes6DYREiIB9gDze0KO2jMP/jPzvWbwQ=
error: 1 dependencies of derivation '/users/home/jdoen/.nix/store/wr31pgva8a
zn9jvvpa4bshykv80xf5qi-mdbook-0.4.7.drv' failed to build
error: 1 dependencies of derivation '/users/home/jdoen/.nix/store/y8pkc0hhgz
rvxgrj7c00mmsy50plya6p-nix-2.4pre20210326_dd77f71.drv' failed to build
#+end_src
We need to modify ~pkgs/tools/text/mdbook/default.nix~ to update the hash; and then:
#+begin_src bash
nix-env -iA nixUnstable -f $HOME/Git/Github/nixpkgs -j$(nproc) --keep-going --show-trace --cores 4 2>&1 | tee nix-install-base.log
ml load nix/bootstrapped
nix-shell --help # Works
nix-shell -p hello # Also works
#+end_src
**** Channels
We would like to move away from having to constantly pass our cloned set of packages.
#+begin_src bash
nix-channel --add https://nixos.org/channels/nixpkgs-unstable nixpkgs
nix-channel --update
#+end_src
*** Basic Packages
Now we can get some basic stuff too.
#+BEGIN_SRC bash
nix-env -i tmux zsh lsof pv git -f $HOME/Git/Github/nixpkgs -j$(nproc) --keep-going --show-trace --cores 4 2>&1 | tee nix-install-base.log
#+END_SRC
*** Ruby Caveats
#+begin_quote
No longer relevant as of April 2020
#+end_quote

While installing packages which depend on ~ruby~, there will be permission errors inside the build folder. These can be "fixed" by setting very permissive controls on the *build-directory* in question. *Do not* set permissions directly on the ~.nix/store/$HASH~ folder, as doing so will make ~nix~ reject the build artifact.
#+BEGIN_SRC bash
# neovim depends on ruby
nix-env -i neovim -v -f $HOME/Git/Github/nixpkgs
#+END_SRC

A more elegant way to fix permissions involves a slightly more convoluted approach. We can note where the build is occurring (e.g. ~/tmp~) and run a ~watch~ command to fix permissions.

#+BEGIN_SRC bash
watch -n1 -x chmod 777 -R /tmp/nix-build-ruby-2.6.6.drv-0/source/lib/
#+END_SRC

Naturally this must be run in a separate window.
*** Dotfiles
Feel free to set up ~dotfiles~ ([[https://github.com/HaoZeke/Dotfiles][mine]], perhaps) to profit even further. We will consider the process of obtaining my set below.
Minimally, we will want to obtain ~tmux~ and ~zsh~.
#+BEGIN_SRC bash
nix-env -i tmux zsh -v -f $HOME/Git/Github/nixpkgs
#+END_SRC
Now we can set the ~dotfiles~ up.
#+BEGIN_SRC bash
git clone https://github.com/HaoZeke/Dotfiles
cd Dotfiles
$HOME/.local/bin/dotgit restore hzhpc
#+END_SRC
The final installation configures ~neovim~ and ~tmux~.
#+BEGIN_SRC bash
zsh
# Should install things with zinit
tmux
# CTRL+b --> SHIFT+I to install
nvim
#+END_SRC
*** Misc NFS
For issues concerning NFS lock files, consider simply moving the problematic file and let things sort themselves out. Consider:
#+BEGIN_SRC bash
nix-build
# something about a .nfs lockfile in some .nix/$HASH-pkg/.nfs0234234
mv .nix/$HASH-pkg/ .diePKGs/
nix-build # profit
#+END_SRC
The right way to deal with this is of course:
#+begin_src bash
nix-build
lsof +D .nix/$HASH-pkg/.nfs0234234
kill $whatever_blocks
nix-build # profit
#+end_src
** Conclusions
Though this is slow and seems like an inefficient use of cluster resources, the benefits of reproducible environments typically outweighs the cost. Also it is much more pleasant to have a proper package manager which can work with Dotfiles.

[fn:thateasy] Note that this will of course entail rebuilding everything from scratch, every time, which means no binary caches. Thus there is no reasonable defence for trying this out without access to a high powered limited access machine
[fn:whatalternatives] The rest of the post assumes we are on the same page and working towards the same end-goal, substitute and remix at will
* TODO Jupyter for HPC Environments :@programming:workflow:projects:hpc:tools:
** Background
Unfortunately, the closely coupled nature of Jupyter instances require both ~nodejs~ builds and ~python~ extensions. Furthermore there are no real tangible gains to running ~jupyterWith~ instances with ~nix~ on a per-project basis.
** Reproducible Environments
We will use ~conda~ along with ~nvm~ to manage the dependencies of our ~jupyter~ instance, and spin it up only when required as facilitated by ~tmux~.
*** Paths
Unfortunately it is impossible to set the file-system paths in a machine agnostic manner, since the configuration file cannot accept environment variables.
#+BEGIN_SRC bash
jupyter lab --generate-config
vim ~/.jupyter/jupyter_notebook_config.py
# Change c.NotebookApp.notebook_dir to a full path
#+END_SRC
** Automation

* TODO Introduce JuPyYod
#+BEGIN_QUOTE
The rationale behind my ~JuPyYod~ project for working with Python and Nix
#+END_QUOTE

** Background
A lot of my projects have a few common requirements, namely:
- A lot of common packages
- A ~python~ based workflow
  + Often this includes a ~jupyter~ component depending on my collaborators
From my own background, I typically *demand* a couple of things:
- Reproducibility
- Tests
- Coverage
- Docs
Typically these are at odds with the ad-hoc nature of the scripts.
** Tooling
For the rest of this post, I will focus on the components which come together in my latest project, ~JuPyYod~.
*** Nix
- Ensures reproducible code
  + Freely converts to ~docker~
- Has great Travis CI support
- Also useful for building at a later stage
*** Project Generators
For project generators, the options were:
- Cookiecutter :: This is [[https://github.com/audreyr/cookiecutter][well defined and super popular]], but having to write ~json~ by hand was really not a very appealing prospect
- *Copier* :: This is a newer template engine, but with an appealing set of features as [[https://copier.readthedocs.io/en/latest/comparisons/][compared here]]
*** Static Types
Most of my problems stem from ~python~ being sloppy to read compared to statically typed languages like Fortran or C++, so ~mypy~ integration goes a long way towards fixing that.
*** Runtime Type Checks
*** Linting and Formatting
We will use ~black~ ([[https://github.com/psf/black][site]]) and ~flake8~ ([[http://flake8.pycqa.org/en/latest/][site]]) for formatting and linting respectively.
** Inspirations
Some of the following configurations influenced this set-up to a large/small extent:
- [[https://sourcery.ai/blog/python-best-practices/][Sourcery's python project ideas]]

** JupyterLab
Unfortunately, notwithstanding efforts from Tweag, involving JupyterWith, the state of dealing with impure paths is still murky. So it makes sense to set up a user-level ~conda~ setup.
#+BEGIN_SRC bash
nix-env -i conda-shell
#+END_SRC

* TODO Rstudio with Nix :@programming:tools:nix:workflow:R:
:PROPERTIES:
:EXPORT_FILE_NAME: nix-rstudio
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
This post covers the setup of Rstudio with custom packages, along with Qt
patches.
#+END_QUOTE

** Background
After the post on using [[Nix with R and devtools][non-CRAN packages with Nix]], I naively assumed I had covered all possible use-cases involving Nix and R. Unfortunately, because GUI programs which aren't browsers or Emacs tend to fly underneath my personal radar, it wasn't until I was up to teach a [[https://nairps.github.io/2020-09-08-ggc-SBDH-online/][Data Carpentries lesson at Georgia Gwinnett College]] that I realized RStudio has additional messy moving parts.

In particular, the error inspiring this post is:
#+BEGIN_SRC bash
qt.glx: qglx_findConfig: Failed to finding matching FBConfig for QSurfaceFormat(version 2.0, options QFlags<QSurfaceFormat::FormatOption>(), depthBufferSize -1, redBufferSize 1, greenBufferSize 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile  QSurfaceFormat::NoProfile)
qt.glx: qglx_findConfig: Failed to finding matching FBConfig for QSurfaceFormat(version 2.0, options QFlags<QSurfaceFormat::FormatOption>(), depthBufferSize -1, redBufferSize 1, greenBufferSize 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile  QSurfaceFormat::NoProfile)
Could not initialize GLX
zsh: abort (core dumped)  rstudio
#+END_SRC

To put that in context, however, before attempting a fix, we will start at the beginning.

** RStudio and Nix
Similar to the environment created with ~rWrapper~ we will set up a global environment in our ~config.nix~.
#+BEGIN_SRC nix
{
  packageOverrides = super:
    let
      self = super.pkgs;
    in {
      rStudioEnv = super.rstudioWrapper.override {
        packages = with self.rPackages; [ tidyverse ];
      };
    };
}
#+END_SRC

The extensions to this are fairly standard, and follow the CRAN guidelines in [[Nix with R and devtools][the previous post]]. Installation works the same way too.
#+BEGIN_SRC bash
nix-env -f "<nixpkgs>" -iA rStudioEnv
#+END_SRC

At this point , when things seem to be rosy, we obtain the error discussed earlier.

#+BEGIN_SRC bash
rstudio
qt.glx: qglx_findConfig: Failed to finding matching FBConfig for QSurfaceFormat(version 2.0, options QFlags<QSurfaceFormat::FormatOption>(), depthBufferSize -1, redBufferSize 1, greenBufferSize 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile  QSurfaceFormat::NoProfile)
qt.glx: qglx_findConfig: Failed to finding matching FBConfig for QSurfaceFormat(version 2.0, options QFlags<QSurfaceFormat::FormatOption>(), depthBufferSize -1, redBufferSize 1, greenBufferSize 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile  QSurfaceFormat::NoProfile)
Could not initialize GLX
zsh: abort (core dumped)  rstudio
#+END_SRC

** Fixes
The crux of the error is that the drivers [[https://github.com/NixOS/nixpkgs/issues/9415#issuecomment-134051412][are always considered to be impure]]. The fix is [[https://github.com/NixOS/nixpkgs/issues/9415#issuecomment-139655485][described here]], and can be interpreted as follows.

- We need to figure out where the library is to begin with.

#+BEGIN_SRC bash
ldconfig -p | grep libGLX
#+END_SRC

** Conclusion
* DONE Documenting C++ with Doxygen and Sphinx - Exhale :@programming:documentation:workflow:cpp:
CLOSED: [2020-09-22 Tue 06:58]
:PROPERTIES:
:EXPORT_FILE_NAME: doc-cpp-dox-sph-exhale
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
This post outlines a basic workflow for C++ projects using Doxygen, Sphinx, and Exhale.
#+END_QUOTE
** Background
My project proposal for documenting [[https://github.com/symengine/symengine][Symengine]] was recently accepted for the Google Summer of Docs initiative. In the past I have been more than happy to document C++ code using *only* [[https://www.doxygen.nl/][Doxygen]] (with [[https://docs.dseams.info][pretty fantastic results]]), while keeping example usage separate ([[https://wiki.dseams.info][d-SEAMS wiki]]). Though this is still a feasible method, a monolithic multi-project setup might benefit from [[https://www.sphinx-doc.org/][Sphinx]], which is what will be covered.
*** Series
This post is the first in a series based on best C++ documentation practices for Sphinx+Doxygen workflows.
1. *Documenting C++ with Doxygen and Sphinx - Exhale* <-- You are here!
2. [[Publishing Doxygen and Sphinx with Nix and Rake][Publishing Doxygen and Sphinx with Nix and Rake]]
3. Documenting C++ with Doxygen and Sphinx - doxyrest (TBD)
4. Adding Tutorials to Sphinx Projects (TBD)
** Goals
A couple of goals informed this approach:
- We expect our documentation to link to the source files
- We expect a lot of Python developers to contribute
  + Hence Sphinx
- We would like to write ~.ipynb~ files into the docs
  + Another reason to use Sphinx (via [[https://myst-nb.readthedocs.io/en/latest/][MyST{NB}]])
** Folder Structure
#+BEGIN_SRC bash :eval never :exports both
tree -d $prj/ -L 2
#+END_SRC

#+RESULTS:
| .   |             |           |
| ├── | docs        |           |
| │   | ├──         | Doxygen   |
| │   | └──         | Sphinx    |
| ├── | nix         |           |
| │   | └──         | pkgs      |
| ├── | projects    |           |
| │   | └──         | symengine |
| └── | scripts     |           |
|     |             |           |
| 8   | directories |           |

Essentially we have a ~scripts~ directory to store basic build scripts, and two kinds of documentation folders.
** Basic Doxygen
The ~doxygen~ setup is beautifully simple:
#+BEGIN_SRC bash
cd docs/Doxygen
doxygen -g
# Easier to edit
mv Doxyfile Doxyfile.cfg
#+END_SRC
Since ~Doxyfile~ should be updated by subsequent versions of ~doxygen~, it is
best to separate the project settings. We will therefore modify some basic
settings in a separate file
#+BEGIN_SRC bash
touch Doxyfile-prj.cfg
vim Doxyfile-prj.cfg # or whatever
#+END_SRC
Edit the file to be (minimally):
#+BEGIN_SRC bash
@INCLUDE                = "./Doxyfile.cfg"
GENERATE_HTML           = NO
GENERATE_XML            = YES
XML_PROGRAMLISTING = NO

# Project Stuff
PROJECT_NAME           = "myProject"
PROJECT_BRIEF          = "Dev docs"
OUTPUT_DIRECTORY       = "./gen_docs"

# Inputs
INPUT                  = "./../../projects/symengine/symengine"
RECURSIVE              = NO
#+END_SRC
With this we will now be able to obtain the ~xml~ files for the rest of this setup.
** Exhale
For our first attempt, we will focus on the automation of Sphinx using the [[https://github.com/svenevs/exhale][exhale tool]].
#+BEGIN_SRC bash
# Basic setup
poetry init
poetry add exhale breathe
#+END_SRC
Now we can generate the basic Sphinx structure.
#+BEGIN_SRC bash
# Separate source and build
sphinx-quickstart --sep --makefile docs/Sphinx \
    --project "My Proj" \
    --author "Juurj" \
    --release "latest" \
    --language "en"
#+END_SRC
This allows us to generate the Sphinx documentation we require, with some changes to the ~docs/Sphinx/source/config.py~ file (lifted from the [[https://github.com/svenevs/exhale][exhale documentation]]):
#+BEGIN_SRC python
extensions = [
    'breathe',
    'exhale',
]

# -- Exhale configuration ---------------------------------------------------
# Setup the breathe extension
breathe_projects = {
    "My Proj": "./../../Doxygen/gen_docs/xml"
}
breathe_default_project = "My Proj"

 # Setup the exhale extension
exhale_args = {
    # These arguments are required
    "containmentFolder":     "./api",
    "rootFileName":          "library_root.rst",
    "rootFileTitle":         "Library API",
    "doxygenStripFromPath":  "..",
    # Suggested optional arguments
    "createTreeView":        True,
    # TIP: if using the sphinx-bootstrap-theme, you need
    # "treeViewIsBootstrap": True,
}

# Tell sphinx what the primary language being documented is.
primary_domain = 'cpp'

# Tell sphinx what the pygments highlight language should be.
highlight_language = 'cpp'
#+END_SRC
We also need to add the output to the ~index.rst~ use the following:
#+BEGIN_SRC rst
.. toctree::
   :maxdepth: 2
   :caption: Contents:

   api/library_root
#+END_SRC
At this point we are ready to manually build our documentation.
#+BEGIN_SRC bash
cd docs/Doxygen
doxygen Doxyfile-prj.cfg
cd ../Sphinx
make html
#+END_SRC
This is still pretty cumbersome though. We can view our documentation in a more pleasant manner with ~darkhttpd~.
#+BEGIN_SRC bash
darkhttpd docs/Sphinx/build/html
#+END_SRC
With this we now beautify the documentation (with the [[https://github.com/executablebooks/sphinx-book-theme][sphinx_book_theme]]):
#+BEGIN_SRC bash
poetry add sphinx-book-theme
#+END_SRC
We need to set the theme as well (in the Sphinx ~config.py~ file):
#+BEGIN_SRC python
html_theme = 'sphinx_book_theme'
#+END_SRC
This leads to some pretty documentation.

#+DOWNLOADED: screenshot @ 2020-09-22 06:41:11
#+caption: Generated documentation (Exhale)
[[file:images/Exhale/2020-09-22_06-41-11_screenshot.png]]


#+DOWNLOADED: screenshot @ 2020-09-22 06:44:48
#+caption: Exhale does a great job with file-based hierarchy.
[[file:images/Exhale/2020-09-22_06-44-48_screenshot.png]]

** Conclusions
At this point, we have a basic setup, which we can tweak with a bunch of themes, and/or different parsers, but this is still pretty rough around the edges. However, a caveat of this setup is that the actual contents of the source are not visible in the generated documentation. [[Publishing Doxygen and Sphinx with Nix and Rake][In the next post]], we will look at automating this setup for deploying with Travis.
* DONE Publishing Doxygen and Sphinx with Nix and Rake :@programming:documentation:workflow:nix:cpp:
CLOSED: [2020-09-22 Tue 10:30]
:PROPERTIES:
:EXPORT_FILE_NAME: pub-doc-cpp-dox-sph-nix
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
Automating documenation deployment with Travis, ~rake~ and ~nix~
#+END_QUOTE
** Background
In [[Documenting C++ with Doxygen and Sphinx - Exhale][the previous post]] we generated documentation using Doxygen with Exhale to handle Sphinx. Now we will clean up the earlier workflow with ~rake~ and ensure the environment is reproducible with ~nix~ while deploying to [[https://travis-ci.com/][Travis CI]].
*** Series
1. [[Documenting C++ with Doxygen and Sphinx - Exhale][Documenting C++ with Doxygen and Sphinx - Exhale]]
2. *Publishing Doxygen and Sphinx with Nix and Rake* <-- You are here
3. Documenting C++ with Doxygen and Sphinx - doxyrest
4. Adding Tutorials to Sphinx Projects
** Setup
A quick reminder of the setup we generated in the last post:

#+BEGIN_SRC bash :eval never :exports both
tree -d $prj/ -L 2
#+END_SRC

#+RESULTS:
| .   |             |           |
| ├── | docs        |           |
| │   | ├──         | Doxygen   |
| │   | └──         | Sphinx    |
| ├── | nix         |           |
| │   | └──         | pkgs      |
| ├── | projects    |           |
| │   | └──         | symengine |
| └── | scripts     |           |
|     |             |           |
| 8   | directories |           |
We had further setup files to enable documentation generation with a manual two stage process (handling ~doxygen~ and ~sphinx~ separately).
#+BEGIN_SRC bash
cd docs/Doxygen
doxygen Doxyfile-prj.cfg
cd ../Sphinx
make html
mv build/html ../../public
#+END_SRC
This might be extracted into a simple ~build.sh~ script, and then we might decide to have a ~clean.sh~ script and then we might try to replicate all the functionality of a good build system with scripts.

Thankfully, we will instead start with a ~build~ script defined as above to transition to ~nix~, before using an actual build tool for our dirty work.
** Adding Nix
It wouldn't make sense for me to not stick ~nix~ into this. I recall the dark days of setting up ~Dockerfiles~ to ensure reproducible environments on Travis.

At this point one might assume we will leverage the ~requirements.txt~ based workflow described earlier in [[Niv and Mach-Nix for Nix Python][Niv and Mach-Nix for Nix Python]]. While this would make sense, there are two barriers to its usage:

- It is slower than a ~poetry~ build, as dependency resolution is performed
- It does not play well with existing projects
  + Most ~python~ projects do not rely solely on ~requirements.txt~ [fn:whatthen]

*** Poetry2Nix
Recall that as ~sphinx~ is originally meant for and most often used for Python projects, we will need to consider the possibility (remote though it is) that there might be users who would like to test the documentation without setting up ~nix~.

Thus we will look to the [[https://github.com/nix-community/poetry2nix#mkPoetryEnv][poetry2nix project instead]]. We note the following:
- The ~poetry2nix~ setup is faster (as it consumes a ~lockfile~ instead of solving dependencies from ~requirements.txt~)
  + ~mach-nix~ however, is more flexible and can make use of the ~poetry2nix~ overrides
- In a strange chicken and egg problem, we will have to manually generate the lockfile, thereby creating an impure ~poetry~ project for every update, though the ~nix~ setup will not need it later
  + This is one of the major reasons to prefer ~mach-nix~ for newer projects
*** Shell Environment
We prep our sources in the usual way, by running ~niv init~ in the project root to generate the ~nix/~ folder and the sources therein. With all that in mind, the ~shell.nix~ file at this point is fairly standard, keeping the general ~niv~ setup in mind (described in a [[A Tutorial Introduction to Nix][previous Nix tutorial]]):

#+BEGIN_SRC nix
# -*- mode: nix-mode -*-
let
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  customPython = pkgs.poetry2nix.mkPoetryEnv { projectDir = ./.; };
in pkgs.mkShell {
  buildInputs = with pkgs; [ doxygen customPython rake darkhttpd ];
}
#+END_SRC

Where the most interesting aspect is that the ~projectDir~ is to be the location of the project root, though both ~poetrylock~ and ~pyproject~ variables are supported.
** Refactoring
We consider the problem of refactoring the ~build.sh~ script:
#+BEGIN_SRC bash
#!/usr/bin/env bash
cd docs/Doxygen
doxygen Doxyfile-prj.cfg
cd ../Sphinx
make html
mv build/html ../../public
#+END_SRC
Without resorting to methods such as ~nix-shell --run build.sh --pure~.
*** Nix Bash
Script in hand, we would like to be able to run it directly in the ~nix~ environment. We modify the script as follows:
#+BEGIN_SRC bash
#! /usr/bin/env nix-shell
#! nix-shell deps.nix -i bash

# Build Doxygen
cd docs/Doxygen
doxygen Doxyfile-syme.cfg

# Build Sphinx
cd ../Sphinx
make html
mv build/html ../../public

# Local Variables:
# mode: shell-script
# End:
#+END_SRC

This calls on a ~deps.nix~[fn:whatshebang] which we shall generate in a manner very reminiscent of the ~shell.nix~ [fn:whybother] as follows:

#+BEGIN_SRC nix
let
  sources = import ./../nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  customPython = pkgs.poetry2nix.mkPoetryEnv { projectDir = ./../.; };
in pkgs.runCommand "dummy" {
  buildInputs = with pkgs; [ doxygen customPython ];
} ""
#+END_SRC

Only the paths have changed, and instead of creating and returning a shell environment with ~mkShell~ we instead "run" a derivation instead. At this point we can run this simply as:

#+BEGIN_SRC bash
./scripts/build.sh
#+END_SRC

This is reasonably ready (as a first draft) for being incorporated into a continuous integration workflow.

** Travis CI
Seeing as Travis provides first class ~nix~ support, as well as excellent integration with GitHub, we will prefer it.
*** Settings
A minor but necessary evil is setting up a PAP ([[https://docs.github.com/apps/building-oauth-apps/scopes-for-oauth-apps/][personal access token]]) [[https://github.com/settings/tokens][from here]]. Depending on what repositories are being used, the scope should encompass ~repo~ permissions (minimally ~public_repo~), and ~admin:org~ permissions might be required.

Having obtained the token, we will need to navigate to the Settings section on the Travis web-UI and add the token as an environment variable, we might be partial to a name like ~GH_TOKEN~.

#+DOWNLOADED: screenshot @ 2020-09-22 09:55:21
#+caption: Settings at ~travis-ci.com/host/proj/settings~
[[file:images/Travis_CI/2020-09-22_09-55-21_screenshot.png]]

*** Build Configuration
We will leverage the following configuration:
#+BEGIN_SRC yaml
language: nix

before_install:
  - sudo mkdir -p /etc/nix
  - echo "substituters = https://cache.nixos.org/ file://$HOME/nix.store" | sudo tee -a /etc/nix/nix.conf > /dev/null
  - echo 'require-sigs = false' | sudo tee -a /etc/nix/nix.conf > /dev/null

before_script:
  - sudo mkdir -p /etc/nix && echo 'sandbox = true' | sudo tee /etc/nix/nix.conf

script:
  - scripts/build.sh

before_cache:
  - mkdir -p $HOME/nix.store
  - nix copy --to file://$HOME/nix.store -f shell.nix buildInputs

cache:
  nix: true
  directories:
    - $HOME/nix.store

deploy:
  provider: pages
  local_dir: ./public/
  skip_cleanup: true
  github_token: $GH_TOKEN # Set in the settings page of your repository, as a secure variable
  keep_history: true
  target_branch: master # Required for user pages
  on:
    branch: src
#+END_SRC

Where all the action is essentially in ~script~ and ~deploy~. Note however, that the ~before_cache~ step should change if there is a ~default.nix~ instead. We will in this case, consider the situation of having an organization or user page being the deploy target.

** Rake
Usable though the preceding setting is, it is still rather unwieldy in that:
- there are a bunch of artifacts which need to be cleaned manually
- it is fragile and tied to the folder names
We can fix this with any of the popular build systems, however here we will focus on the excellent ~rake~ [fn:teachmerake]. We shall commit to our course of action by removing ~make~.
#+BEGIN_SRC bash
cd docs/Sphinx
rm Makefile make.bat # other make cruft
#+END_SRC
*** Components
**** Variables
We will begin by requiring ~rake~ and setting basic variables.
#+BEGIN_SRC ruby
require 'rake'

CWD = File.expand_path(__dir__)
DOXYFILE = "Doxyfile-prj.cfg"
OUTDIR = File.join(CWD,"public")
SPHINXDIR = File.join(CWD,"docs/Sphinx")
#+END_SRC
This section should give a fairly clear idea of how the ~Rakefile~ itself is essentially pure ~ruby~ code. We are now beginning to have more holistic control of how our project is structured.
*** Tasks
The general form of a ~task~ is simply:
#+BEGIN_SRC ruby
desc "Blah blah"
task :name do
# Something
end
#+END_SRC

Some variations of this will be considered when appropriate.
**** Clean
A ~clean~ task is a good first task, being as it is almost trivial in all build systems.
#+BEGIN_SRC ruby
desc "Clean the generated content"
task :clean do
  rm_rf "public"
  rm_rf "docs/Doxygen/gen_docs"
  rm_rf "docs/Sphinx/build"
end
#+END_SRC
**** Serve
We will use the [[https://wiki.alpinelinux.org/wiki/Darkhttpd][lightweight darkhttpd server]] for our generated documentation.
#+BEGIN_SRC ruby
desc "Serve site with darkhttpd"
task :darkServe, [:port] do |task, args|
  args.with_defaults(:port => "1337")
  sh "darkhttpd #{OUTDIR} --port #{args.port}"
end
#+END_SRC
Note that we have leveraged the ~args~ system in this case, and also used the top-level ~OUTDIR~ variable.
**** Doxygen
Since the ~doxygen~ output is a pre-requisite, it makes sense to set it up early on.
#+BEGIN_SRC ruby
desc "Build doxygen"
task :mkDoxy do
  Dir.chdir(to = File.join(CWD,"docs/Doxygen"))
  system('doxygen', DOXYFILE)
end
#+END_SRC
**** Sphinx
This task will depend on having the ~doxygen~ output, so we will express this idiomatically by making the ~doxygen~ task run early on.
#+BEGIN_SRC ruby
desc "Build Sphinx"
task :mkSphinx, [:builder] => ["mkDoxy"] do |task, args|
  args.with_defaults(:builder => "html")
  Dir.chdir(to = File.join(CWD,"docs/Sphinx"))
  sh "poetry install"
  sh "poetry run sphinx-build source #{OUTDIR} -b #{args.builder}"
end
#+END_SRC
There are some subtleties here, notably:
- The task is meant to run *without* ~nix~
- We use the ~args~ setup as before
**** No Nix Meta
With this we can now set up a task to build the documentation without having ~nix~.
#+BEGIN_SRC ruby
desc "Build site without Nix"
task :noNixBuild => "mkSphinx" do
  Rake::Task["darkServe"].execute
end
#+END_SRC
The main take-away here is that we finally call the ~Rake~ library itself, but within the task, which means the dependency tree is respected and we get ~doxygen->sphinx->darkhttpd~ as required.
**** Nix Builder
For ~nix~ use we note that we are unable to enter the ~nix~ environment from within the ~Rakefile~ itself. We work around this by being more descriptive.
#+BEGIN_SRC ruby
desc "Build Nix Sphinx, use as nix-shell --run 'rake mkNixDoc' --pure"
task :mkNixDoc, [:builder] => "mkDoxy" do |task, args|
  args.with_defaults(:builder => "html")
  Dir.chdir(to = SPHINXDIR)
  sh "sphinx-build source #{OUTDIR} -b #{args.builder}"
end
#+END_SRC
*** Final Form
The final ~Rakefile~ shall be (with a default task defined):
#+BEGIN_SRC ruby
require 'rake'

# Variables
CWD = File.expand_path(__dir__)
DOXYFILE = "Doxyfile-prj.cfg"
OUTDIR = File.join(CWD,"public")
SPHINXDIR = File.join(CWD,"docs/Sphinx")

# Tasks
task :default => :darkServe

desc "Clean the generated content"
task :clean do
  rm_rf "public"
  rm_rf "docs/Doxygen/gen_docs"
  rm_rf "docs/Sphinx/build"
end

desc "Serve site with darkhttpd"
task :darkServe, [:port] do |task, args|
  args.with_defaults(:port => "1337")
  sh "darkhttpd #{OUTDIR} --port #{args.port}"
end

desc "Build Nix Sphinx, use as nix-shell --run 'rake mkNixDoc' --pure"
task :mkNixDoc, [:builder] => "mkDoxy" do |task, args|
  args.with_defaults(:builder => "html")
  Dir.chdir(to = SPHINXDIR)
  sh "sphinx-build source #{OUTDIR} -b #{args.builder}"
end

desc "Build site without Nix"
task :noNixBuild => "mkSphinx" do
  Rake::Task["darkServe"].execute
end

desc "Build doxygen"
task :mkDoxy do
  Dir.chdir(to = File.join(CWD,"docs/Doxygen"))
  system('doxygen', DOXYFILE)
end

desc "Build Sphinx"
task :mkSphinx, [:builder] => ["mkDoxyRest"] do |task, args|
  args.with_defaults(:builder => "html")
  Dir.chdir(to = File.join(CWD,"docs/Sphinx"))
  sh "poetry install"
  sh "poetry run sphinx-build source #{OUTDIR} -b #{args.builder}"
end
#+END_SRC
*** Travis
We are now in a position to fix our ~travis~ build configuration. Simply replace the old and fragile ~build.sh~ script section with the following:
#+BEGIN_SRC yaml
script:
  - nix-shell --run "rake mkNixDoc" --show-trace --verbose --pure
#+END_SRC
*** Direnv
As a bonus section, consider the addition of the following ~.envrc~ for those who keep multiple ~ruby~ versions:
#+BEGIN_SRC bash
eval "$(rbenv init -)"
rbenv shell 2.6.2
rake -T
#+END_SRC
Activate this with the usual ~direnv allow~. This has the added benefit of listing the defined tasks when ~cd~'ing into the project directory.
** Conclusions
A lot has happened on the tooling end, even though the documentation itself has not been updated further. We have managed to setup a robust environment which is both reproducible and also amenable to users who do not have ~nix~. We have also setup a build system, which can help us in many more ways as well (asset optimization through the ~rails~ pipeline). In the next post, we will return to the documentation itself for further tinkering.
[fn:whatthen] [[https://python-poetry.org/][Poetry]] and [[https://pipenv-fork.readthedocs.io/][Pipenv]] come to mind
[fn:whybother] In this instance, we could have simply called on ~shell.nix~ instead, but it illustrates a more general concept
[fn:teachmerake] [[https://avdi.codes/tag/rake/page/2/][Avdi's blog has a fantastic introduction]] to ~rake~ and ~Rakefiles~
[fn:whatshebang] Chris Warbo has a good [[http://chriswarbo.net/projects/nixos/nix_shell_shebangs.html][introduction to the nix shebang]]
* TODO Documenting C++ with Doxygen and Sphinx - doxyrest :@programming:documentation:workflow:nix:cpp:
#+BEGIN_QUOTE
Advanced documentation customization with doxyrest for C++ projects
#+END_QUOTE
** Background
*** Series
1. [[Documenting C++ with Doxygen and Sphinx - Exhale][Documenting C++ with Doxygen and Sphinx - Exhale]]
2. [[Publishing Doxygen and Sphinx with Nix and Rake][Publishing Doxygen and Sphinx with Nix and Rake]]
3. *Documenting C++ with Doxygen and Sphinx - doxyrest* <-- You are here
4. [[Adding Tutorials to Sphinx Projects][Adding Tutorials to Sphinx Projects]]
* TODO Adding Tutorials to Sphinx Projects :@programming:documentation:workflow:cpp:
#+BEGIN_QUOTE
Multi-source documentation coupled to tutorials, using ~.ipynb~ and other extensions
#+END_QUOTE
** Background
*** Series
1. [[Documenting C++ with Doxygen and Sphinx - Exhale][Documenting C++ with Doxygen and Sphinx - Exhale]]
2. [[Publishing Doxygen and Sphinx with Nix and Rake][Publishing Doxygen and Sphinx with Nix and Rake]]
3. [[Documenting C++ with Doxygen and Sphinx - doxyrest][Documenting C++ with Doxygen and Sphinx - doxyrest]]
4. *Adding Tutorials to Sphinx Projects* <-- You are here

* DONE Talk Supplements for PyCon India 2020 :@conferences:presentations:ramblings:nix:python:
CLOSED: [2020-10-02 Fri 23:56]
:PROPERTIES:
:EXPORT_FILE_NAME: pycon-in-2020-meta
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A meta-post on my talk at PyCon India 2020
#+END_QUOTE
** Background
I [[https://in.pycon.org/2020/][am to present]] at PyCon IN 2020. Some of the motivating reasons for having a post are:
- I would like to preserve questions
- I would like to collect the video, slides and other miscellaneous stuff in one location [fn:officialsite]
- It would be nice to have my own thoughts here afterwards

Details of this happy circumstance are reproduced below from the [[https://in.pycon.org/cfp/2020/proposals/reproducible-scalable-workflows-with-nix-papermill-and-renku~dNkxD/][CFP here]].
*** Details
- Title :: Reproducible Scalable Workflows with Nix, Papermill and Renku
**** Abstract
#+BEGIN_QUOTE
The provenance of Jupyter notebook interfaces can no longer be denied in the data-science and analysis community. In particular, fledgling and "fresh out of school" researchers and practitioners are used to using Jupyter notebooks for their initial analysis. As might be expected, these workflows are difficult to reproduce and also store. Caching efficiency and dependency re-use are almost always sub-optimal with virtual environments, compared to native installations, and the same issues (along with additional security concerns) plague docker setups as well. There are a set of Jupyter tools which have evolved to close this gap, like JupyText. However, the fundamental aspect of reproducing workflows on high performance computing clusters, of being able to compose programmatically, compilation rules which efficiently use underlying hardware with minimal user intervention is still not a solved problem. In this talk, I will discuss packaging Python applications and workflows in an end-to-end composable manner using the Nix ecosystem, which leverages a functional programming paradigm and then show how this allows for both user-friendly low-compute analysis, while being scalable on large clusters. To that end, the tools introduced will be:

The Nix programming language (emphasis on developer environments for python with mkShell)
Jupyter Python kernels (the Xeus kernel for Python debugging) and Jupytext
Papermill for parameterizing notebooks
Renku for tracing provenance
The goal is to have the audience familiarized with the best practices for reproducibility and analysis. The focus will be on scientific HPC applications, though any managed cluster can and will benefit from the practices described.
#+END_QUOTE
**** Other Content
A more in-depth introductory workshop on Nix itself given by me (and Amrita Goswami) at [[https://2020.carpentrycon.org/schedule/#session-10][CarpentryCon2020]] is here:
- [[https://github.com/HaoZeke/CarpentryCon2020_Nix][CarpentryCon2020 Materials]]
- [[https://rgoswami.me/posts/ccon-tut-nix/][A tutorial introduction to Nix and Python]]
** Slides
The slides are embedded below. The ~orgmode~ source [[https://github.com/HaoZeke/haozeke.github.io/blob/src/presentations/PyConIN2020/nixPyconIN.org][is here on the site's GH repo]].
#+BEGIN_EXPORT html
<script async class="speakerdeck-embed" data-id="6db471426fc24d6cbfb433f2464b8146" data-ratio="1.77966101694915" src="//speakerdeck.com/assets/embed.js"></script>
#+END_EXPORT
** Video

{{< youtube 2GX9TK4uNfU >}}

[fn:officialsite] One location I am going to be able to keep track of

* DONE Replacing Zoom with Open Broadcaster Software :@notes:workflow:tools:
CLOSED: [2020-10-03 Sat 17:05]
:PROPERTIES:
:EXPORT_FILE_NAME: rep-zoom-obs
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A post on local recordings
#+END_QUOTE
** Background
Since the advent of the COVID-19 situation, there has been an increase in the demand for recorded materials. Standard approaches involve Zoom, which is not only proprietary, but also quite a bit of a privacy nightmare. The last straw was the random placement of my speaker bauble head.

#+DOWNLOADED: screenshot @ 2020-10-03 16:51:07
#+caption: Zoom webcam placement
[[file:images/Background/2020-10-03_16-51-07_screenshot.png]]

At this point, given that I was to set up a pre-recorded video for [[Talk Supplements for PyCon India 2020][PyCon India 2020]], I decided to look into alternatives.
** Alternatives
The search for alternative screen recording systems isn't really a very new one. For group work (like [[https://wc3m.github.io/][W3cm]] [fn:whatthat]), I tend to prefer Skype, since it handles speaker galleries very well. Unfortunately, Skype has no capacity for recording single person calls, at least as yet. This is not the place for an extended debate on the pros and cons of Skype, or Google Meet (only records corporate accounts), or the rest. Instead, lets sum up all these issue with the simple understanding that, if *one person* wants to record a webcam connected to their local computer, along with the screen, it is insane to imagine that the only way to get this is by:
- Making an account somewhere (Zoom, Meet, Skype, anything)
- Giving a cloud service permission to record our screens
At the same time, a lot of standard tools for screen recording do not play nice with webcam recorders (like [[https://www.maartenbaert.be/simplescreenrecorder/][Simple Screen Recorder]] and [[https://help.gnome.org/users/cheese/stable/][Cheese]]).
** Open Broadcaster Software
The [[https://duckduckgo.com/?q=obs+studio&ia=web][OBS studio project]] is a godsend. It allows for simultaneously managing multiple streams, of both audio and video. Furthermore, since these are implemented as overlays, it is possible to fine-tune the positioning of each of these, which is something Zoom and friends lack.

#+DOWNLOADED: screenshot @ 2020-10-03 16:53:51
#+caption: Into the matrix
[[file:images/Open_Broadcaster_Software/2020-10-03_16-53-51_screenshot.png]]

The ability to resize the webcam is best shown in the figure below.

#+DOWNLOADED: screenshot @ 2020-10-03 16:58:26
#+caption: Almost Zoom, only better
[[file:images/Open_Broadcaster_Software/2020-10-03_16-58-26_screenshot.png]]

OBS also generates beautifully small videos and supports live-streaming.
*** Common Caveats
- The standard setting is set to work with hardware acceleration, which may not be present for many users
  + Use the settings to change this back to the software setting
** Conclusions
I cannot imagine going back to Zoom to record anything local. It is an added bonus that OBS is both cross-platform and FOSS. It is only incredible more people do not use it.

[fn:whatthat] Water, Chemicals and more with Computers for Chemistry, a computational chemistry course aimed at middle school students taught with my sister [[http://scholar.google.com/citations?user=mviv92EAAAAJ&hl=en][Amrita Goswami]]

* DONE Old Laptops as Secondary Monitors
CLOSED: [2020-10-23 Fri 23:22]
:PROPERTIES:
:EXPORT_FILE_NAME: laptop-as-second-screens
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
Dual screen workflows without screens across operating systems
#+end_quote
** Background
My X380 sadly has been having port issues. This meant that my M14 was no longer a viable option for my second screen needs.
** Outline
The general form of the solution works in one of two ways:
- VNC Viewer :: Where the (second-screen) laptop connects to a VNC server on the primary laptop
- Peripheral Shares :: Where the secondary laptop runs a server to enable proxying mouse and keyboard access from the primary laptop
** VNC and Windows
For laptops running Windows, I personally just set up [[https://tightvnc.com/][TightVNC]]. The standard settings work well enough for the peripheral share described below.
*** Comments
This is best used for working with Windows only stuff like Office.
** VNC and Linux
*** Peripheral Share
For the secondary laptop we need to run a server (~tigervnc~) without setting an external screen.
#+begin_src bash
x0vncserver -rfbauth ~/.vnc/passwd
#+end_src
Now on the main laptop, we will simply leverage ~x2vnc~ to extend into the secondary laptop.
#+begin_src bash
x2vnc $ip -west
#+end_src
Where we can get the IP (local) by checking with ~ifconfig~ on the secondary laptop.
**** Meta
This works best when combined with a networked file-system, since then you can interact with files in tandem. Otherwise, there is quite a bit of ~git~ based back and forth.
** VNC and Android
There are two parts to this solution. Note that, as Android devices don't run X11 systems in a meaningful way, the direct access method is through a paid application, a2vnc server lite, which also did not work well in my tests. We will therefore focus on setting a VNC viewer up to connect to the primary laptop.
*** Primary Settings
**** XRandR Setup
For the primary laptop, we will start by obtaining our present screens configuration.
#+BEGIN_SRC bash :exports both :results raw :eval never
xrandr | grep " connected"
#+END_SRC

#+BEGIN_SRC bash
eDP1 connected primary 1920x1080+1920+0 (normal left inverted right x axis y axis) 290mm x 170mm
#+END_SRC

Naturally your output will differ. We also need the resolution of the Android device. In my case, they are the same. At this point we are ready to figure out the mode-line.

#+BEGIN_SRC bash :results raw
gtf 1920 1080 60
#+END_SRC

#+BEGIN_SRC bash
  # 1920x1080 @ 60.00 Hz (GTF) hsync: 67.08 kHz; pclk: 172.80 MHz
  Modeline "1920x1080_60.00"  172.80  1920 2040 2248 2576  1080 1081 1084 1118  -HSync +Vsync
#+END_SRC

Let us now use this information to create a bunch of modelines.

#+BEGIN_SRC bash
xrandr --newmode "1920x1080_60.00"  172.80  1920 2040 2248 2576  1080 1081 1084 1118  -HSync +Vsync
#+END_SRC

Note that we can create more of these in the same manner. We can now move forward with making a virtual screen.

#+BEGIN_SRC bash
xrandr --addmode VIRTUAL1 1920x1080_60.00
#+END_SRC

We can now finally set up the output.

#+BEGIN_SRC bash
xrandr --output VIRTUAL1 --mode 1920x1080_60.00 --left-of eDP1
#+END_SRC

Note that it is better to use ~mons~ to work with our newly created virtual screen.

#+BEGIN_SRC bash
mons -e left
#+END_SRC

This is still a bit ugly, since the process needs to be repeated with each reboot.

**** VNC Setup
Now we need prepare our VNC. ~x11vnc~ is recommended at the moment.
#+BEGIN_SRC bash
x11vnc -vencrypt nodh:only-ssl -ssl SAVE -clip 1920x1080+0+0
#+END_SRC
*** Android Settings
For this section, I personally use [[https://play.google.com/store/apps/details?id=com.iiordanov.bVNC&hl=en&gl=US][bVNC Pro]]. The setup is pretty dead simple. A basic VNC connection is all that is required.
*** Comments
In practice, I use the x86 setup, with the secondary laptop acting as a viewer for a virtual screen, mostly because that way I can tune into multiple Zoom meetings (a bonus).
** Conclusion
The final setup is quite robust to changes. Future posts might go into setting up the kind of local networking tools to help move files, code and more between both machines, to improve on the peripheral share approach. Additionally, there are still some manual steps which can and should be automated. I'm not super pleased with the setup, it takes longer than a wireless screen. This post is complimented by the [[Multiple Monitors with Touchscreens][work and setup with touchscreens here]].

* TODO Haskell with Nix for Hacktoberfest :@programming:tools:nix:workflow:python:
:PROPERTIES:
:EXPORT_FILE_NAME: haskell-with-nix-hacktober
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
Short post on Haskell Nix, and Hacktoberfest
#+END_QUOTE
** Background
Full disclaimer. This post is not like my other ~nix~ posts, which are /ex-cathedra/ expositions. My working knowledge of Haskell, however, is essentially some non-production hacking of [[https://github.com/jaspervdj/hakyll][Hakyll]] and [[https://github.com/jgm/pandoc][Pandoc]]. That said, I thought I'd take advantage of the [[https://github.com/kowainik/learn4haskell#how-to-get-started][learn4haskell initiative]] of [[https://hacktoberfest.digitalocean.com/][Hacktoberfest]]. I truly do believe incentivized FOSS development is terrible [fn:fightme], though incentivized learning is not that bad, so I was pleasantly surprised to see this excellent initiative. Plus, I'd always wanted to get into Haskell. Naturally, working through something without a take is pointless, so I thought I'd work through the exercises with a bit of Nix.
** Setup
There are several methods of working with ~nix~ and Haskell, typically centered around ~cabal2nix~ and ~stack2nix~ (including [[https://docs.haskellstack.org/en/stable/README/][this official approach]]). I will implement and leverage the [[https://input-output-hk.github.io/haskell.nix/][haskell.nix approach]]. Additionally, I will leverage some [[https://www.youtube.com/watch?v=5p2Aq3bRuL0][concepts from Tsoding]] and [[https://github.com/hlissner/doom-emacs][doom-emacs]] as always.

*** Haskell-Nix
This section follows directly from the [[https://input-output-hk.github.io/haskell.nix/tutorials/getting-started/][haskell.nix documentation]], as adapted to the ~learn4haskell~ project.

#+begin_src bash
nix-env -iA cachix -f https://cachix.org/api/v1/install
cachix use iohk
cachix use ghcide-nix
#+end_src

#+RESULTS:
: Configured https://iohk.cachix.org binary cache in /home/haozeke/.config/nix/nix.conf

We will also setup the optional keys in ~nix.conf~. Assuming that someone is "following along" we will configure our *fork* of the ~learn4haskell~ project in the standard manner. In case you haven't done so already, and you have set up the very excellent [[https://github.com/github/hub][hub wrapper for git]].

#+begin_src bash
# wherever you keep git stuff
git clone https://github.com/kowainik/learn4haskell
cd learn4haskell
hub fork
#+end_src

Anyway now we can start by pinning dependencies, with some inspiration from [[https://github.com/Ptival/haskell-nix-template/][this template]].

#+begin_src bash :eval never
nix-env -i niv
niv init
niv add input-output-hk/haskell.nix
niv add cachix/ghcide-nix
#+end_src

With that out of the way, we will start working on a ~default.nix~ [fn:whyverbose].

#+begin_src nix :tangle ~/Git/Github/Haskell/learn4haskell/default.nix :comments link
let
  # Variables
  name = "learn4haskell";
  compiler-nix-name = "ghc884";
  # Niv
  sources = import ./nix/sources.nix;
  # Load haskellNix first
  haskellNix = import (fetchTarball { inherit (sources."haskell.nix") url sha256; }) {
      sourcesOverride = {
        hackageSrc = fetchTarball { inherit (sources."hackage.nix") url sha256; };
      };
  };
  # Override pkgs with the cached versions
  pkgs = import haskellNix.sources.nixpkgs haskellNix.nixpkgsArgs;
  inherit (pkgs.lib) optional optionals;
in pkgs.haskell-nix.project {
  inherit compiler-nix-name;
  src = pkgs.haskell-nix.haskellLib.cleanGit {
    inherit name;
    src = ./.;
  };
}
#+end_src

Now for a nice development environment in a ~shell.nix~ file.

#+begin_src nix :tangle ~/Git/Github/Haskell/learn4haskell/shell.nix :comments link
let
  hsPkgs = import ./default.nix { inherit pkgs; };
in hsPkgs.shellFor {
    # Include only the *local* packages of your project.
    packages = ps: with ps; [
      learn4haskell
    ];
    withHoogle = true;
    tools = { cabal = "3.2.0.0"; hlint = "2.2.11"; };
    buildInputs = with pkgs.haskellPackages;
      [ ghcid ];
    # Prevents cabal from choosing alternate plans, so that
    # *all* dependencies are provided by Nix.
    exactDeps = true;
}
#+end_src


[fn:fightme] [[https://joel.net/how-one-guy-ruined-hacktoberfest2020-drama][This is an extreme case]], but all "part-time" developers end up contributing features which rot over time
[fn:whyverbose] If the post seems overly verbose, it is because these are tangled directly

* DONE Talk Supplements for NixCon 2020 :@conferences:presentations:ramblings:nix:hpc:
CLOSED: [2020-10-17 Sat 10:40]
:PROPERTIES:
:EXPORT_FILE_NAME: nixcon-in-2020-meta
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A meta-post on my lightning talk at NixCon 2020
#+END_QUOTE
** Background
Much the same as the rationale behind my [[Talk Supplements for PyCon India 2020][meta-post on my talk at PyCon India 2020]], that is:
- I would like to preserve questions
- I would like to collect the video, slides and other miscellaneous stuff in one location [fn:officialsite]
- It would be nice to have my own thoughts here afterwards
*** Details
- Title :: Nix from the dark ages (without Root)
- Proposal :: [[https://cfp.nixcon.org/nixcon2020/talk/CUE78W/][See the cfp response here]]
**** Abstract
#+BEGIN_QUOTE
Short comments from the trenches of High Performance Clusters on working with Nix on kernel locked-in systems without proot support.
#+END_QUOTE
**** Linked Posts
- [[Local Nix without Root][Local Nix without Root]] :: Motivation and installation
  + [[Provisioning Dotfiles on an HPC][Provisioning Dotfiles on an HPC]] :: Looks into how standard approaches fail (~proot~)
  + [[HPC Dotfiles and LMod][HPC Dotfiles and LMod]] :: All the ugly manual install steps
**** Other Content
Anything on this site [[https://rgoswami.me/tags/nix/][tagged with Nix]]. Also an introduction to ~nix~ given by me (and Amrita Goswami) at [[https://2020.carpentrycon.org/schedule/#session-10][CarpentryCon2020]] is here:
- [[https://github.com/HaoZeke/CarpentryCon2020_Nix][CarpentryCon2020 Materials]]
- [[https://rgoswami.me/posts/ccon-tut-nix/][A tutorial introduction to Nix and Python]]
** Slides
- [[file:/revealjs/NixCon2020/darkNix.html][Best viewed here]] using a browser (in a new tab)
- A ~pdf~ copy of the slides are embedded below
- The ~orgmode~ source [[https://github.com/HaoZeke/haozeke.github.io/blob/src/presentations/NixCon2020/darkNix.org][is here on the site's GH repo]]
#+BEGIN_EXPORT html
<script async class="speakerdeck-embed" data-id="2cc4e5d0cca445be95e1e77827ea782c" data-ratio="1.37081659973226" src="//speakerdeck.com/assets/embed.js"></script>
#+END_EXPORT

[fn:officialsite] One location I am going to be able to keep track of

* TODO Streaming Music with Mopidy :@personal:tools:workflow:
:PROPERTIES:
:EXPORT_FILE_NAME: stream-music-mopidy
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
Short post on multi-device (android devices and laptops) streaming with Mopidy and Icecast.
#+END_QUOTE
** Background
Back home, backed by my ridiculously large local music library, I had an elegant, efficient MPD setup for streaming music throughout the house. Now that I'm constrained to two laptops (one superannuated) and two phones, in a drastically smaller space, I thought I'd do without any streaming setups at all. However, the Spotify desktop application is ridiculous. It is bloated and doesn't minimize well on ~i3~ and is just plain annoying.

** Tools
We will require, in broad strokes:
- A music daemon
  + Should be lightweight (MPD)
  + Will collate a bunch of sources and provide the ignorant end user with a unified interface (Mopidy)
- A streaming server
  + A good streaming server will minimally support multiple listeners (Icecast)
  + An excellent server will allow for synchronous output (Snapcast)
- A client for every OS
  + That's two clients for every OS, and 3 web interfaces [fn:oddname]
  + Cantata, Sonata, M.A.L.P., MPDroid, RompR, Iris (we'll see them show up later)

*** Mopidy
I'm still leery of ~python~ projects, though, given that mopidy supports Spotify (with a premium account), I didn't have much of a choice. The idea is to set up the configuration locally first.
#+begin_src bash
vim ~/.conf/mopidy/mopidy.conf
mopidy # Test things
#+end_src
**** Spotify
This setup essentially entails filling in the ~[spotify]~ block in the configuration file. The only thing to really keep in mind is that the client and secret values [[https://mopidy.com/ext/spotify/][are from here]].
**** Systemd
Once the configuration works as expected, transfer it to the [[https://docs.mopidy.com/en/latest/running/service/?highlight=systemctl#service-management-with-systemd][system configuration location]]. It is best to manually edit this, instead of over-writing this.
**** Comments
At this point, if all that was needed was control over a single PC, then we'd be done. There's no need fo anything more complicated than an MPD client for controlling the running ~mopidy~ server.
*** Icecast

[fn:oddname] An [[https://en.wikipedia.org/wiki/Two_Cars_in_Every_Garage_and_Three_Eyes_on_Every_Fish][old Simpsons pun]]

* DONE Anki Decks with Orgmode :@programming:workflow:projects:tools:emacs:orgmode:
CLOSED: [2020-10-27 Tue 01:05]
:PROPERTIES:
:EXPORT_FILE_NAME: anki-decks-orgmode
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
Setting up unicode math and ~orgmode~ for painless Anki deck building
#+end_quote

** Background
A recent [[https://news.ycombinator.com/item?id=24878171][Hacker News post]] reminded me of [[https://docs.ankiweb.net/#/getting-started][Anki]], and that brought back memories of
my Anki ~orgmode~ setup. I thought I'd re-create and immortalize it.

The standard way of working with Anki, is with a pretty awkward GUI. There are
changes to be made here, which make life a little easier, including the setup of
custom cards, but the inherent concerns of the WYSIWYG editor are basically
insurmountable.

#+DOWNLOADED: screenshot @ 2020-10-27 00:13:31
#+caption: Anki GUI
[[file:images/Background/2020-10-27_00-13-31_screenshot.png]]


The goal is to get this a better workflow than manual editing of Anki decks.
~orgmode~ is perfect for making cards, especially in the larger context of using
it for storing images and rich ~pdfs~.

#+DOWNLOADED: screenshot @ 2020-10-26 23:53:05
#+caption: A pleasant way to make anki decks
[[file:images/Background/2020-10-26_23-53-05_screenshot.png]]

** Methodology
To accomplish this, we basically need to have the following:
- [[https://github.com/louietan/anki-editor][anki-editor]]  :: This ~emacs~ plugin will facilitate the conversion from our ~orgmode~ files to the Anki markup
- [[https://github.com/FooSoft/anki-connect][anki-connect]] :: We need a server of sorts set up to allow us to push pull and get errors from the running Anki server, this is an Anki plugin
- [[https://ankiweb.net/shared/info/937148547][LaTeX process editor]] :: It wouldn't be much better than manually making cards in Anki if we couldn't leverage ~unicode~ characters, so we need to modify the internal Anki build process for TeX
*** Anki Editor
As with all ~emacs~ related setup snippets on this site, these should be modified and adapted as needed, especially for those not using [[https://github.com/hlissner/doom-emacs/][doom-emacs]]. 
#+BEGIN_SRC emacs-lisp :tangle no :eval never
(use-package anki-editor
  :after org-noter
  :config
  ; I like making decks
  (setq anki-editor-create-decks 't))
#+END_SRC
Also, my [[https://dotdoom.rgoswami.me/config.html#text-3][full configuration]] has additional non-essential quality of life keybindings amongst other things.
*** Anki Connect
~CTRL+Shift+A~ will bring up the addon settings, and Anki has to be restarted after installing the addons. [[https://github.com/FooSoft/anki-connect][Anki Connect]] itself does not need any further configuration, though the ~readme~ is very comprehensive.
*** TeX Setup
The [[https://ankiweb.net/shared/info/937148547][LaTeX process editor]] can be set in two stages, wherein we will first ensure that we can use ~xelatex~ and that we can generate an ~svg~.
#+begin_src json
{
    "svgCommands": [
        [
            "xelatex",
            "--no-pdf",
            "-interaction=nonstopmode",
            "tmp.tex"
        ],
        [
            "dvisvgm",
            "--no-fonts",
            "-Z",
            "2",
            "tmp.xdv",
            "-o",
            "tmp.svg"
        ]
    ]
}
#+end_src
The ~png~ settings can be modified in a similar manner if required, but it is better to generate ~svg~ files, which will set up in the cosmetics section. Note that we pass ~--no-pdf~ to get the ~xdv~ file which has replaced ~dvi~ files for ~xelatex~.
*** Cosmetics
The final aspect of this is to be configured with the GUI. The easiest option is to clone the Basic card type and customize that. ~CTRL+Shift+N~ should bring up the card editor. The relevant styles are[fn:wherevert] (from the ~Cards~ option):
#+begin_src css
.card {
 font-family: Literata;
 font-size: 26px;
 text-align: center;
 color: black;
 background-color: white;
}
img {
max-height:1000px;
height: auto;
width: auto;
}
img[src*="latex"] {
  vertical-align: middle;
}
#+end_src

Now we need setup our TeX headers as well, and enable the ~Create scalable
images with dvisvgm~ option. The header needs to have (minimally):

#+begin_src tex
\documentclass[12pt]{article}
\special{papersize=3in,5in}
\usepackage{geometry} 
\usepackage{unicode-math}
\usepackage{mathtools}
\pagestyle{empty}
\setlength{\parindent}{0in}
\begin{document}
#+end_src

While the ~footer~ is simply ~\end{document}~. With this, we have achieved
pretty formatting.

#+DOWNLOADED: screenshot @ 2020-10-26 23:53:25
#+caption: Pretty card formatting
[[file:images/Background/2020-10-26_23-53-25_screenshot.png]]
*** Font Locking
Inspired by [[https://yiufung.net/post/anki-org/][this post]], we will also use [[https://github.com/gongzhitaao/orgcss][orgcss]] to obtain some ~orgmode~ font-locking. We will add the following styles:
#+begin_src css
:not(pre) > code {
  padding: 2px 5px;
  margin: auto 1px;
  border: 1px solid #ddd;
  border-radius: 3px;
  background-clip: padding-box;
  color: #333;
  font-size: $code-size;
}

.org-src-container {
  border: 1px solid #ccc;
  box-shadow: 3px 3px 3px #eee;
  font-family: $monospace;
  font-size: $code-size;
  margin: 1em auto;
  padding: 0.1em 0.5em;
  position: relative;
}

.org-src-container > pre {
  overflow: auto;
}

.org-src-container > pre:before {
  display: block;
  position: absolute;
  background-color: #b3b3b3;
  top: 0;
  right: 0;
  padding: 0 0.5em;
  border-bottom-left-radius: 8px;
  border: 0;
  color: white;
  font-size: $code-size;
}

/* from http://demo.thi.ng/org-spec/ */

.org-src-container > pre.src-sh:before {
  content: "sh";
}
.org-src-container > pre.src-bash:before {
  content: "bash";
}
.org-src-container > pre.src-emacs-lisp:before {
  content: "Emacs Lisp";
}
.org-src-container > pre.src-R:before {
  content: "R";
}
.org-src-container > pre.src-org:before {
  content: "Org";
}
.org-src-container > pre.src-cpp:before {
  content: "C++";
}
.org-src-container > pre.src-c:before {
  content: "C";
}
.org-src-container > pre.src-html:before {
  content: "HTML";
}
.org-src-container > pre.src-js:before {
  content: "Javascript";
}
.org-src-container > pre.src-javascript:before {
  content: "Javascript";
}

// More languages from http://orgmode.org/worg/org-contrib/babel/languages.html

.org-src-container > pre.src-abc:before {
  content: "ABC";
}
.org-src-container > pre.src-asymptote:before {
  content: "Asymptote";
}
.org-src-container > pre.src-awk:before {
  content: "Awk";
}
.org-src-container > pre.src-C:before {
  content: "C";
}
.org-src-container > pre.src-calc:before {
  content: "Calc";
}
.org-src-container > pre.src-clojure:before {
  content: "Clojure";
}
.org-src-container > pre.src-comint:before {
  content: "comint";
}
.org-src-container > pre.src-css:before {
  content: "CSS";
}
.org-src-container > pre.src-D:before {
  content: "D";
}
.org-src-container > pre.src-ditaa:before {
  content: "Ditaa";
}
.org-src-container > pre.src-dot:before {
  content: "Dot";
}
.org-src-container > pre.src-ebnf:before {
  content: "ebnf";
}
.org-src-container > pre.src-forth:before {
  content: "Forth";
}
.org-src-container > pre.src-F90:before {
  content: "Fortran";
}
.org-src-container > pre.src-gnuplot:before {
  content: "Gnuplot";
}
.org-src-container > pre.src-haskell:before {
  content: "Haskell";
}
.org-src-container > pre.src-io:before {
  content: "Io";
}
.org-src-container > pre.src-java:before {
  content: "Java";
}
.org-src-container > pre.src-latex:before {
  content: "LaTeX";
}
.org-src-container > pre.src-ledger:before {
  content: "Ledger";
}
.org-src-container > pre.src-ly:before {
  content: "Lilypond";
}
.org-src-container > pre.src-lisp:before {
  content: "Lisp";
}
.org-src-container > pre.src-makefile:before {
  content: "Make";
}
.org-src-container > pre.src-matlab:before {
  content: "Matlab";
}
.org-src-container > pre.src-max:before {
  content: "Maxima";
}
.org-src-container > pre.src-mscgen:before {
  content: "Mscgen";
}
.org-src-container > pre.src-Caml:before {
  content: "Objective";
}
.org-src-container > pre.src-octave:before {
  content: "Octave";
}
.org-src-container > pre.src-org:before {
  content: "Org";
}
.org-src-container > pre.src-perl:before {
  content: "Perl";
}
.org-src-container > pre.src-picolisp:before {
  content: "Picolisp";
}
.org-src-container > pre.src-plantuml:before {
  content: "PlantUML";
}
.org-src-container > pre.src-python:before {
  content: "Python";
}
.org-src-container > pre.src-ruby:before {
  content: "Ruby";
}
.org-src-container > pre.src-sass:before {
  content: "Sass";
}
.org-src-container > pre.src-scala:before {
  content: "Scala";
}
.org-src-container > pre.src-scheme:before {
  content: "Scheme";
}
.org-src-container > pre.src-screen:before {
  content: "Screen";
}
.org-src-container > pre.src-sed:before {
  content: "Sed";
}
.org-src-container > pre.src-shell:before {
  content: "shell";
}
.org-src-container > pre.src-shen:before {
  content: "Shen";
}
.org-src-container > pre.src-sql:before {
  content: "SQL";
}
.org-src-container > pre.src-sqlite:before {
  content: "SQLite";
}
.org-src-container > pre.src-stan:before {
  content: "Stan";
}
.org-src-container > pre.src-vala:before {
  content: "Vala";
}
.org-src-container > pre.src-axiom:before {
  content: "Axiom";
}
.org-src-container > pre.src-browser:before {
  content: "HTML";
}
.org-src-container > pre.src-cypher:before {
  content: "Neo4j";
}
.org-src-container > pre.src-elixir:before {
  content: "Elixir";
}
.org-src-container > pre.src-request:before {
  content: "http";
}
.org-src-container > pre.src-ipython:before {
  content: "iPython";
}
.org-src-container > pre.src-kotlin:before {
  content: "Kotlin";
}
.org-src-container > pre.src-Flavored Erlang lfe:before {
  content: "Lisp";
}
.org-src-container > pre.src-mongo:before {
  content: "MongoDB";
}
.org-src-container > pre.src-prolog:before {
  content: "Prolog";
}
.org-src-container > pre.src-rec:before {
  content: "rec";
}
.org-src-container > pre.src-ML sml:before {
  content: "Standard";
}
.org-src-container > pre.src-Translate translate:before {
  content: "Google";
}
.org-src-container > pre.src-typescript:before {
  content: "Typescript";
}
.org-src-container > pre.src-rust:before {
  content: "Rust";
}
#+end_src
However, in the interests of sanity, we will leverage the [[https://ankiweb.net/shared/info/1972239816][Syntax Highlighting Anki plugin]] for managing the actual style-sheets instead of manual edits to each card type.
#+caption: A screencast from the plugin readme
https://raw.githubusercontent.com/ijgnd/syntax-highlighting/master/screenshots/demo_config_with_nm_toggle_addon.gif

At this stage, we have a card which can gracefully handle both XeLaTeX and code in an elegant manner. An example is presented in the next section.
** Usage
For the sample card[fn:lolwut] shown, the markup is dead simple.

#+include: "tmpCards/tCardbasic.org" src org

Essentially:
- Enable and load ~anki-editor~
  + Add local variable section to ensure we load ~anki-editor~. This is essentially via ~eval: (anki-editor-mode)~ in the Local variables block
- Fire up Anki
- Export at will, and continue adding more cards or non-card details to the ~orgmode~ file

The [[https://raw.githubusercontent.com/louietan/anki-editor/master/examples.org][Anki editor examples]] file is excellent and the [[https://github.com/louietan/anki-editor/issues/30][issue tracker]] also has a ton
of information.
*** Code

#+include: "tmpCards/testCard.org" src org

#+DOWNLOADED: screenshot @ 2020-10-28 13:31:43
#+caption: Code card with TeX
[[file:images/Methodology/2020-10-28_13-35-16_screenshot.png]]
*** More Content
- [[https://github.com/Anton-Latukha/Fundamental-Haskell][Fundamental Haskell]] :: An excellent example of how a multiple frontend learning repository can be, written with ~org-drill~[fn:whynotdrill]
- [[https://yiufung.net/post/anki-org/][Anki powerups with orgmode]] :: A post brought to my attention after I had published this, an excellent introduction with videos
** Conclusions
Some final comments:
- Screenshots and other images linked are automatically synced
- The TeX is best rendered on the PC first, so run through these at-least once
A missing link in this setup is the ability to use a touch screen and stylus to
write proofs or skip the TeX setup altogether, but that would require another
post altogether. Additionally, all the standard bells and whistles of having an
~orgmode~ document can be applied, including, crucially, the ability to have
long-form notes as well, a coherent approach to this can also be covered later.

[fn:lolwut] It _is_ a *gag* card, no judgement here
[fn:whynotdrill] ~org-drill~ doesn't support any kind of mobile synchronization
[fn:wherevert] The alignment trick is from [[https://clementc.github.io/blog/2018/08/15/anki_setup/][this post]]

* TODO The Decline of Linux Email :@personal:workflow:tools:email:
:PROPERTIES:
:EXPORT_FILE_NAME: declining-linux-email
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
A scathing indictment of Linux Email clients and a methodology to revert to command line oriented workflows.
#+end_quote
** Background
I have loved and used email for over 16 years now. Back in middle school, my dad would travel, and along with my mom and sister, we'd send emails. Of course, over the years, nothing in software has declined so much as email clients.

#+DOWNLOADED: screenshot @ 2020-11-23 08:11:55
#+caption: Ah yes, another casualty of Mozilla's epic life choices
[[file:images/Background/2020-11-23_08-11-55_screenshot.png]]


In brief order:

- [[https://www.thunderbird.net/en-US/][Thunderbird]] :: This was simply impossible to beat, back in the days of XUL. Infact, Mozilla and the entire "Foundation" should have a special circle reserved for them in hell for what they did to both Firefox (making it a chrome clone) and Thunderbird (stripping it of all the loving crafted extensions)[fn:noreally]. Even after the ridiculous new engine, the community bounced back to make a good set of extensions, some of which [[https://addons.thunderbird.net/en-US/thunderbird/collections/rgoswami/favorites/][are enumerated here]]. Most recently, once again version 78 has broken all addons and made my client unusable again. The *last straw*. Also, of late, Thunderbird needs a paid extension to work well with Exchange accounts.
- [[https://getmailspring.com/][Mailspring]] :: Always leery of Electron apps, this is a pathetically heavy, plodding mess. Constantly disconnecting and requiring re-authentication, while also being very coy about losing emails. Their "pro" feature set includes a privacy invasive method of figuring out when people read what you send. This has been championed a lot by the Tech media, but it is simply not a working mail client, and the themes are drab anyway. Not to mention the fact that it is *paid*, and *not FOSS*, and had an ugly history with Nylas mail.
- [[https://www.postbox-inc.com/][Postbox]] :: A fork from Thunderbird, without official Linux support, this [[https://appdb.winehq.org/objectManager.php?sClass=application&iId=9199][runs surprisingly well via Wine]]. That said, it is still annoying to not have a native client, and there are no good extensions.
- [[https://wiki.gnome.org/Apps/Geary][Geary]] :: The shining hope of the GNOME desktop. Good, but not extensible enough yet. Also lacks a lot of user-facing options, due to the ridiculous design paradigm of GNOME, which appears to be "what if we had no options, people like that right, that's why they use Linux!"


Perhaps most importantly, none of these clients are stable. They change at the whims of a shadowy development cabal which has no consideration for users. Many years ago, back when Thunderbird ditched XUL, ~mutt~ presented itself as an alternative. At the time, however, it was
** Conclusions
[fn:noreally] A full description would span several posts, and detracts from the main goal

* DONE Reclaiming Email with Astroid :@personal:rant:tools:email:workflow:
CLOSED: [2020-12-30 Wed 06:37]
:PROPERTIES:
:EXPORT_FILE_NAME: reclaim-email-astroid
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
Migrating Imap, Gmail and Exchange, mail accounts from GUI clients to [[https://astroidmail.github.io/][Astroid]]
#+end_quote
** Background
Initially, I had planned this post to start with a brief history of the decline of email clients for Linux. That quickly got out of hand, and was therefore spun out into a post of its own (TBD). To keep things brief. Thanks to the incredible ineptitude of the Thunderbird steering committee, I ended up requiring a new mail client. Having despaired of the GUI based bloat heavy approaches of most clients, I decided to go the old fashioned route and build one up in a modular manner.
*** Series
This post is part of a series on email.
1. The Decline of Linux Email (TBD)
2. *Reclaiming Email with Astroid* <-- You are here!
3. Emacs and Email (TBD)
** The Accounts
In no order of preference, for a variety of reasons, I have 23 distinct email accounts. However, these are actually broken down into a few basic types and associated mail fetching software.
- Generic IMAP :: Yahoo, Mail.ru and the rest are managed with [[https://isync.sourceforge.io/][isync]]
- Exchange :: A school account of mine uses Office 365, and will be handled with [[http://davmail.sourceforge.net/][davmail]] in conjunction with ~isync~
- Gmail :: There are a few of these, two personal, and two institutional, all of which are handled with [[http://lieer.gaute.vetsj.com/][lieer]]
** InSync
For general IMAP accounts (anything which isn't backed by ~gmail~ or ~outlook~) an ~mbsync~ approach works best. Exhange accounts need ~davmail~ and are described in a separate sub-section. Before we get to the ~poll.sh~ script and ~astroid~, it is a good idea to run each of these as we set them up, especially as the first run can take quite a long time (around an hour for 9205 messages).

#+begin_src bash
mbsync -V blah
#+end_src

*** General IMAP
For a standard IMAP account (anything which isn't Exchange or Gmail) the only thing we need to keep track of is a good way to obfuscate passwords. We will use ~pass~ for this [fn:whatkey].

#+begin_src bash
pass git init $GPG_KEY # Optional, use a secure private git repo
pass init $GPG_KEY
pass add mail/rgoswami.iitk
#+end_src

With this, we can configure a general IMAP account (an IITK webmail in this instance, but it could be anything).

#+begin_src tcl
IMAPAccount rgoswami.iitk
Host qasid.iitk.ac.in
User rgoswami
PassCmd "pass show mail/rgoswami.iitk"
AuthMechs LOGIN
SSLType IMAPS

IMAPStore rgoswami.iitk-remote
Account rgoswami.iitk

MaildirStore rgoswami.iitk-local
SubFolders Verbatim
Path /run/media/Storage/.mail/rgoswami_iitk/
INBOX /run/media/Storage/.mail/rgoswami_iitk/Inbox
Trash trash:///

Channel rgoswami.iitk
Master :rgoswami.iitk-remote:
Slave :rgoswami.iitk-local:
Patterns *
Create Both
SyncState *
#+end_src

*** Exchange Accounts
For Exchange accounts we need to setup ~davmail~. Thankfully the process is actually excessively simple for a single account. For each account, a ~.properties~ file needs to be generated. The password section is kept blank in this case, since we will authenticate with the ~O365Interactive~ protocol.

#+begin_src bash
# The default setup
davmail ~/.davmail.properties
#+end_src

Use the following properties in the ~.davemail.properties~ file to prevent timeout errors:
#+begin_src bash
davmail.folderSizeLimit=50
davmail.clientSoTimeout=0
davmail.enableKeepAlive=true
#+end_src

Further information is available at [[http://davmail.sourceforge.net/faq.html][the official docs]].

#+DOWNLOADED: screenshot @ 2020-12-29 21:54:39
#+caption: ~davmail~ setup
[[file:images/InSync/2020-12-29_21-54-39_screenshot.png]]

#+DOWNLOADED: screenshot @ 2020-12-29 21:54:20
#+caption: Interactive login workflow
[[file:images/InSync/2020-12-29_21-54-20_screenshot.png]]

Now we can configure the ~.mbsyncrc~ in a manner analogous to the standard IMAP setup, but with the port and host where ~davmail~ is running.
#+begin_src tcl
IMAPAccount rog32
Host localhost
User rog32@hi.is
Pass dummy
Port 1143
SSLType None
AuthMechs LOGIN

IMAPStore rog32-remote
Account rog32
MaildirStore rog32-local
SubFolders Verbatim
Path /run/media/Storage/.mail/rog32/
Inbox /run/media/Storage/.mail/rog32/Inbox

Channel rog32
Master :rog32-remote:
Slave :rog32-local:
Patterns *
Create Both
SyncState *
#+end_src

The ~SSLType~ and ~AuthMechs~ are important parameters. Note that the ~Pass~ is truly not important since we an OAuth token is stored in the ~properties~ file.

#+DOWNLOADED: screenshot @ 2020-12-29 23:51:10
#+caption: First run of a large inbox with ~davmail~ and ~isync~
[[file:images/InSync/2020-12-29_23-51-10_screenshot.png]]

** Gmaileer
The general setup ~gmi init blah@gmail.com~ works well for personal accounts. However, there was an additional step for the IEEE account.
*** IEEE
Some issues with tags led to the following changes in the ~.gmaileer.json~ file
#+begin_src js
{"replace_slash_with_dot": false, "account": "rgoswami@ieee.org", "timeout": 600, "drop_non_existing_label": false, "ignore_empty_history": false, "ignore_tags": ["TODO","new"], "ignore_remote_labels": ["CATEGORY_SOCIAL", "CATEGORY_FORUMS", "CATEGORY_PROMOTIONS", "CATEGORY_PERSONAL", "CATEGORY_UPDATES"], "remove_local_messages": true, "file_extension": ""}
#+end_src
Essentially just the addition of two new ~ignore_tags~.
** Notmuch
At this point, we have all mail synced into local directories, but we have no access to view or interact with them. We will start by setting up ~notmuch~ to search and index our mail. This is pretty basic for now.
#+begin_src toml
[database]
path=/run/media/Storage/.mail/
[user]
name=Person
primary_email=primary@domain.com
other_email=one@domain.com;two@domain.com
[new]
tags=new;unread;inbox;TODO;
ignore=/.*[.](json|lock|bak)$/
[search]
exclude_tags=deleted;spam;
[maildir]
synchronize_flags=true
#+end_src

There are a lot more options which may be configured, but this is enough to get started.
** Sending Email
At this stage we need a way to actually send email. A simple configuration can be setup in ~/.config/msmtp/config~ is given below (where we use ~pass~ again):
#+begin_src tcl
defaults
port 587
tls on
auth on
logfile ~/.config/msmtp/.msmtp.log
tls_trust_file /etc/ssl/certs/ca-certificates.crt

account r95g10
host smtp.gmail.com
from r95g10@gmail.com
user r95g10@gmail.com
tls_starttls on
tls on
auth on
port 587
passwordeval pass show mail/r95g10.gmail

account rog32
host localhost
from rog32@hi.is
user rog32@hi.is
tls_starttls off
tls off
auth plain
port 1025
password dummy
#+end_src

Note that the exchange account again uses a dummy password, and the real password will be prompted for on the first run. The permissions of the file above should be ~600~. It is prudent to test at-least the exchange section to login.

#+begin_src bash
echo "Hello world" | msmtp --account=rog32 r95g10@gmail.com
#+end_src
** Astroid
The bulk of this setup is [[https://github.com/astroidmail/astroid/wiki/Astroid-setup][by the numbers]], with the exception of the poll script. However, minimally configure ~astroid~ with the location of our ~notmuch~ configuration.
#+begin_src bash
astroid --new-config
vim ~/.config/astroid/config
#+end_src

The relevant sections are:
#+begin_src json
"notmuch_config": "\/home\/whoever\/.notmuch-config",
#+end_src

The accounts section is fairly self explanatory, but we will need to use the following ~sendmail~ line as well:

#+begin_src json
            "sendmail": "msmtp --read-envelope-from -i -t",
#+end_src


*** Nix Python Poll Script
Natively only bash appears to be supported. However, with ~nix~, we can use a reproducible python script with [[https://amoffat.github.io/sh][the sh library]] to call system functions instead.
#+begin_src python
from pathlib import Path
import sh

# For generic IMAP maildirs

ISYNC_LABELS = ["rgoswami.iitk", "rog32"]

for isync in ISYNC_LABELS:
    sh.mbsync("-V",isync,_bg=True)


# Gmaileer
GMAIL_IDENTIFIERS = ["gmail", "univ", "ieee"]

path = Path(r"/run/media/haozeke/Storage/.mail/")

for dirs in path.iterdir():
    if dirs.is_dir():
        for gmi in GMAIL_IDENTIFIERS:
            if gmi in dirs.name:
                print(f"Syncing {dirs.name}")
                sh.gmi("sync", _cwd=dirs, _fg=True)
#+end_src
This needs to be coupled with the standard ~nix~ shebang in the ~poll.sh~ file:
#+begin_src bash
#!/usr/bin/env nix-shell
#!nix-shell -i python3 -p "python38.withPackages(ps: [ ps.numpy ps.sh ])"
#+end_src
*** Navigation
For working with HTML emails, we need to highlight the notice about potentially sketchy HTML using (defaults) ~j~ or ~k~ and then hit ~enter~ or ~o~.

#+DOWNLOADED: screenshot @ 2020-12-30 01:07:09
#+caption: Unhelpful default
[[file:images/Astroid/2020-12-30_01-07-09_screenshot.png]]


#+DOWNLOADED: screenshot @ 2020-12-30 01:08:45
#+attr_html: :width 600
#+caption: After viewing the sketchy bit
[[file:images/Astroid/2020-12-30_01-08-45_screenshot.png]]

Note that since most email is actually sent with ~text/html~ it might make more sense to configure the ~thread_view~ in the configuration file.

#+begin_src json :hl_lines 3
    "thread_view": {
        "open_html_part_external": "false",
        "preferred_type": "html",
        "preferred_html_only": "false",
        "allow_remote_when_encrypted": "false",
        "open_external_link": "xdg-open",
        "default_save_directory": "~",
        "indent_messages": "false",
        "gravatar": {
            "enable": "true"
        },
        "mark_unread_delay": "0.5",
        "expand_flagged": "true"
    },
#+end_src

**** Deletion
There are again, two different approaches to deletion.

- Gmail :: For ~gmail~ accounts it is simple, just adding a ~trash~ tag will do the trick, so we can select multiple emails with ~t~ and then hit ~+~ to add ~trash~ and everything works out
- Other Accounts :: We can *delete mail* forever (from the server as well), by using a safe-tag and then using ~notmuch~

The generic non-~lieer~ method requires:

#+begin_src bash
notmuch search --output=files --format=text0 tag:killed | xargs -r0 rm
#+end_src

A ~pre-new~ hook will work.
# ** Systemd Integration
# Albert Weichselbraun has [[https://semanticlab.net/desktop/e-mail/linux/sysadmin/Managing-DavMail-with-systemd-and-preventing-service-timeouts-after-network-reconnects/][an excellent setup]] for working with ~systemd~ and ~davmail~ which we will extend to cover the entire workflow using dispatcher scripts.

**** Composition
Thankfully, ~astroid~ supports GPG encryption as well as markdown support. This makes for simple integration with any popular editor.


#+DOWNLOADED: screenshot @ 2020-12-30 06:27:38
#+caption: Composition with ~emacs~ and ~astroid~
[[file:images/Astroid/2020-12-30_06-27-38_screenshot.png]]


** Conclusions
This is enough to get started for a while, but it isn't yet at the stage where I can replace ~thunderbird~ unfortunately. However, there are several pain points to be addressed, which will be covered in a future post. Some of these are essentially related to network fluctuations, and the awkward deletion setup.

*** Update
- ~astroid~ appears to be having a [[https://github.com/astroidmail/astroid/issues/669][bit of a development crisis]]
  + The [[https://github.com/gauteh/ragnarok][sequel (ragnarok)]] unfortunately seems to have gone towards a [[https://github.com/gauteh/ragnarok/tree/master/astroid][browser frontend]], which is quite unacceptable to me
    - This makes it far too similar to (in principle) Mailspring (though with less problematic connectivity issues)
    - This means I won't be moving forward with the next set of posts regarding ~astroid~ and will move towards ~emacs~ again
- [[https://mehl.mx/][Max Mehl]] has a [[https://src.mehl.mx/mxmehl/mail-config/src/branch/master/astroid/scripts][good collection of scripts]] for ~astroid~

[fn:whatkey] The GPG key itself can be stored with ~keybase~
* DONE Remapping Keys with XKB and KLFC :@personal:workflow:tools:
CLOSED: [2020-12-05 Sat 22:05]
:PROPERTIES:
:EXPORT_FILE_NAME: remap-keys-xkb-klfc
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
An introduction to hacking keyboard layouts with X keyboard extension (XKB) and ~klfc~, focused on Colemak and vim bindings
#+end_quote
** Background
Inspite of maximizing ergonomic bindings for most common software (e.g. Vimium, doom-emacs), every operation with the arrow keys still trouble me. Here I will lay out my experiments transitioning to a stable, uniquely defined setup with the X keyboard extension.
*** Series
This post is part of a series on Colemak and keyboard management in general.
1. [[Switching to Colemak][Switching to Colemak]]
2. [[Refactoring Dotfiles For Colemak][Refactoring Dotfiles For Colemak]] 
3. *Remapping Keys Globally and Persistently with XKB* <-- You are here!
** Keyboard Basics
Some terms to keep in mind for this post are[fn:morespecifics]:
- Dead Keys :: These don't actually output anything, but modify the next key pressed. Like applying an umlaut on the subsequent letter.
- Lock Keys :: State modifiers which are toggled, like Caps Lock
- Compose Key :: A key which interprets a series of subsequent key strokes. A dead key on steroids.

Also the different levels ([[https://fsymbols.com/keyboard/linux/choosers/#:~:text=Shift%20is%20called%20the%20second,on%20your%20usual%20keyboard%20layout.][from here]]) are concisely defined in the following table.

#+caption: Levels for a keyboard
| *Level* | *Modifier*   | *Keys*                                          |
|---------+--------------+-------------------------------------------------|
|       1 | None         | Lowercase letters, numbers other symbols        |
|       2 | Shift        | Uppercase letters, symbols placed above numbers |
|       3 | AltGr        | Accented characters, symbols, some dead keys    |
|       4 | Shift+AltGr  | More dead keys and symbols                      |
|       5 | Extend       | User-defined                                    |
|       6 | Shift+Extend | User-defined                                    |
|---------+--------------+-------------------------------------------------|

** Modification Strategies
Common approaches to quick remapping of keys involves ~xmodmap~, which does not persist between reboots. Manually recreating or spinning off of XKB configuration files was also not very appealing.
*** KLFC
A more elegant approach is by using the excellent [[https://github.com/39aldo39/klfc/][klfc Haskell binary]]. To install this from source:
#+begin_src bash :eval never
git clone https://github.com/39aldo39/klfc
cd klfc
# Kanged from the AUR https://aur.archlinux.org/packages/klfc/
cabal v1-sandbox init
cabal v1-update
cabal v1-install --only-dependencies --ghc-options=-dynamic --force-reinstalls
cabal v1-configure --prefix=/usr --ghc-options=-dynamic
cabal v1-build
#+end_src

The output binary is in ~./dist/build/klfc/klfc~.

*Note that* the set of keys mapped by the ~json~ files are relative to the QWERTY layout, that is:

#+DOWNLOADED: screenshot @ 2020-12-01 20:38:58
[[file:images/Modification_Strategies/2020-12-01_20-38-58_screenshot.png]]

This means that we have to ensure that the keys are mapped relative to QWERTY as well, *not* relative to the modified base layout.
** Remapping
Some goals were:
- Programming (particularly in ~python~ and ~lisp~) put a lot of stress on the right hand pinky for Colemak users[fn:whycolemak]
- VIM keys should be global but toggled with a lock
My primary use case is currently my ThinkPad X380, which comes with a basic [[http://www.keyboard-layout-editor.com/#/layouts/ae55dc3c6b6c80904d0901d856247486][QWERTY contracted layout]] as shown in Fig. [[fig:qwerty]].

#+name: fig:qwerty
#+caption: Basic X380 QWERTY
file:images/keyboards/qwerty_x380.png
*** Colemak - Layers 1 and 2
The first mapping is a [[http://www.keyboard-layout-editor.com/#/gists/93c31b6e61608c19c26933b0a212b262][basic Colemak setup]] as shown in Fi. fig:colemak.

#+name: fig:colemak
#+caption: Basic X380 Colemak
[[file:images/keyboards/colemak_x380.jpg]]

It wouldn't make much sense to remap the first two layers. We can use the ~json~ from the examples of the ~klfc~ repository.
#+begin_src json
// Base Colemak layout
// https://colemak.com
{
  "fullName": "Colemak",
  "name": "colemak",
  "localeId": "00000409",
  "copyright": "Public Domain",
  "company": "2006-01-01 Shai Coleman",
  "version": "1.0",
  "shiftlevels": [ "None", "Shift" ],
  "singletonKeys": [
    [ "CapsLock", "Backspace" ]
  ],
  "keys": [
    { "pos": "~", "letters": [ "`", "~" ] },
    { "pos": "1", "letters": [ "1", "!" ] },
    { "pos": "2", "letters": [ "2", "@" ] },
    { "pos": "3", "letters": [ "3", "#" ] },
    { "pos": "4", "letters": [ "4", "$" ] },
    { "pos": "5", "letters": [ "5", "%" ] },
    { "pos": "6", "letters": [ "6", "^" ] },
    { "pos": "7", "letters": [ "7", "&" ] },
    { "pos": "8", "letters": [ "8", "*" ] },
    { "pos": "9", "letters": [ "9", "(" ] },
    { "pos": "0", "letters": [ "0", ")" ] },
    { "pos": "-", "letters": [ "-", "_" ] },
    { "pos": "+", "letters": [ "=", "+" ] },
    { "pos": "Q", "letters": [ "q", "Q" ] },
    { "pos": "W", "letters": [ "w", "W" ] },
    { "pos": "E", "letters": [ "f", "F" ] },
    { "pos": "R", "letters": [ "p", "P" ] },
    { "pos": "T", "letters": [ "g", "G" ] },
    { "pos": "Y", "letters": [ "j", "J" ] },
    { "pos": "U", "letters": [ "l", "L" ] },
    { "pos": "I", "letters": [ "u", "U" ] },
    { "pos": "O", "letters": [ "y", "Y" ] },
    { "pos": "P", "letters": [ ";", ":" ] },
    { "pos": "[", "letters": [ "[", "{" ] },
    { "pos": "]", "letters": [ "]", "}" ] },
    { "pos": "\\", "letters": [ "\\", "|" ] },
    { "pos": "A", "letters": [ "a", "A" ] },
    { "pos": "S", "letters": [ "r", "R" ] },
    { "pos": "D", "letters": [ "s", "S" ] },
    { "pos": "F", "letters": [ "t", "T" ] },
    { "pos": "G", "letters": [ "d", "D" ] },
    { "pos": "H", "letters": [ "h", "H" ] },
    { "pos": "J", "letters": [ "n", "N" ] },
    { "pos": "K", "letters": [ "e", "E" ] },
    { "pos": "L", "letters": [ "i", "I" ] },
    { "pos": ";", "letters": [ "o", "O" ] },
    { "pos": "'", "letters": [ "'", "\"" ] },
    { "pos": "Z", "letters": [ "z", "Z" ] },
    { "pos": "X", "letters": [ "x", "X" ] },
    { "pos": "C", "letters": [ "c", "C" ] },
    { "pos": "V", "letters": [ "v", "V" ] },
    { "pos": "B", "letters": [ "b", "B" ] },
    { "pos": "N", "letters": [ "k", "K" ] },
    { "pos": "M", "letters": [ "m", "M" ] },
    { "pos": ",", "letters": [ ",", "<" ] },
    { "pos": ".", "letters": [ ".", ">" ] },
    { "pos": "/", "letters": [ "/", "?" ] }
  ],
  "variants": [
    {
      "name": "mod-dh",
      "shiftlevels": [ "None", "Shift" ],
      "keys": [
        { "pos": "V", "letters": [ "d", "D" ] },
        { "pos": "B", "letters": [ "v", "V" ] },
        { "pos": "G", "letters": [ "g", "G" ] },
        { "pos": "T", "letters": [ "b", "B" ] },
        { "pos": "H", "letters": [ "k", "K" ] },
        { "pos": "N", "letters": [ "m", "M" ] },
        { "pos": "M", "letters": [ "h", "H" ] }
      ]
    }
  ]
}
#+end_src
*** VIM Extensions
The additions are primarily through the Extend Layer[fn:whyext] (Fig. [[fig:extlayer]]), with a Shift addition (Fig. [[fig:extshift]]) and more keys with AltGr (Fig. [[fig:altgrlayer]]).
As mentioned before, we have to continue mapping relative to QWERTY, so these mappings can be used by QWERTY users as well. We will essentially use the ~ISO_5~ shift key.

#+name: fig:extlayer
#+caption: Extend layer mapping
[[file:images/keyboards/extendLayer_hz.jpg]]

- Maps basic ~vim~ movement

#+name: fig:extshift
#+caption: Extend+Shift layer mapping
[[file:images/keyboards/extShift_hz.jpg]]

- Relatively empty, just has a bracket

#+name: fig:altgrlayer
#+caption: Extend+AltGr layer mapping
[[file:images/keyboards/extAltGr_hz.png]]

- Includes deletions

These are defined in a single ~json~ as shown.
#+begin_src json
{
  "filter": "no klc,keylayout",
  "singletonKeys": [
    [ "CapsLock", "Extend" ],
    [ "Alt_L", "AltGr" ]
  ],
  "shiftlevels": [ "Extend", "Shift+Extend", "AltGr+Extend" ],
  "keys": [
    { "pos": "H", "letters": [ "Left", "", "Backspace" ] },
    { "pos": "J", "letters": [ "Down" ] },
    { "pos": "K", "letters": [ "Up" ] },
    { "pos": "L", "letters": [ "Right", "", "Delete" ] },
    { "pos": ";", "letters": [ "Enter" ] },
    { "pos": "N", "letters": [ "(", "[", "{" ] },
    { "pos": "V", "letters": [ ")", "]" , "}"] }
  ]
}
#+end_src
- The key idea is to have the ~AltGr~ keys placed symmetrically
- One major issue is that ~Backspace~ has gone from a single stroke of ~CapsLock~ to a three-key-combo
  + This is the least intuitive, and might need to be changed
- The third level (~Extend+AltGr~) is more accessible than the second in this layout
*** Usage
To generate the files needed to load the new layout:
#+begin_src bash :eval never
klfc colemak.json extendVIM.json -o coleVIM
cd coleVIM/xkb
./run-session.sh    # to try them out
./install-system.sh && ./scripts/install-xcompose.sh # to install them
#+end_src

** Conclusions
The layout takes a bit of time to get used to, but it is a lot more transparent in the end compared to manually remapping to Colemak's NEIO instead of HJKL for movement. It is both persistent and easily extended, though it is likely that more needs to be done. Perhaps some metrics [fn:whatmetrics] might be collected as well.

[fn:whycolemak] Colemak, unlike Dvorak, prioritises finger rolls over alternating hands
[fn:morespecifics] For more details the [[https://en.wikipedia.org/wiki/Keyboard_layout][Wikipedia article on Keyboard Layouts]] is useful or this [[https://github.com/tmk/tmk_keyboard/blob/master/tmk_core/doc/keymap.md][file on the tmk keyboard]]
[fn:whyext] [[https://forum.colemak.com/topic/2014-extend-extra-extreme/][DreymaR's Extend mappings]] might be good for QWERTY people 
[fn:whatmetrics] The metric collection of [[https://github.com/mw8/white_keyboard_layout][Michael White]] or the [[http://mkweb.bcgsc.ca/carpalx/][CARPALX metrics]]

* DONE Private Github Actions without PAT :@notes:tools:github:workflow:
CLOSED: [2020-12-23 Wed 14:34]
:PROPERTIES:
:EXPORT_FILE_NAME: priv-gh-actions
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
A workflow for managing private submodules in a private repository without personal access tokens for Github actions
#+end_quote
** Background
Ever since Travis CI decided to drink [[https://blog.travis-ci.com/2020-11-02-travis-ci-new-billing][corporate kool-aid]], the search for a new CI has been brought to the fore again. [[https://github.com/features/actions][Github Actions]] seem uniquely suited for private repositories, since most CIs bill for private repos. However, the basic authentication setup for the [[https://github.com/actions/checkout][checkout action]] involves using one SSH key, effectively a personal access token, for both the [[https://github.com/actions/checkout/issues/116#issuecomment-708092052][main project and all submodules]]. This is untenable for anyone working with a team.
** Solution
The fix, as it were, is in two steps. We will first require a deploy key to be set for the private submodule, and then store the private portion in the private repo. We will begin with a concrete setup.
*** Setup
Consider a standard C++ build setup:
#+begin_src yaml
name: Fake secret project

on:
  push:
    branches: [master, development]
  pull_request:
    branches: [master]

jobs:
  build:
    runs-on: ${{ matrix.os }}

    strategy:
      max-parallel: 4
      matrix:
        os: [ubuntu-18.04]
        cpp-version: [g++-7, g++-9, clang++]

    steps:
      - uses: actions/checkout@v2
      - name: build
        env:
          CXX: ${{ matrix.cpp-version }}
        run: |
          mkdir build && cd build
          cmake -DCMAKE_CXX_COMPILER="$CXX" -DCMAKE_CXX_FLAGS="-std=c++11" ../
          make -j$(nproc)
      - name: run
        run: |
          ./super_secret
#+end_src
*** Key Generation
This section is standard. We will generate a deploy key essentially. Note that it isn't necessary to set a password, using one would only minimally improve security and bring in some annoying script modifications.
#+begin_src bash
# Anywhere safe
ssh-keygen -t rsa -b 4096 -C "Fake Deployment Key" -f 'priv_sub_a' -N ''
#+end_src
*** Private Submodule Repo
We will store the *public portion of the key* as a deploy key in the *private submodule repository*.
#+begin_src bash
# Copy contents
cat priv_sub_a.pub | xclip -selection clipboard
#+end_src

Note that, as shown in Fig. [[fig:privsub]] we do not need to give write access to this key.

#+name: fig:privsub
#+caption: Deploy key setup in the private submodule
#+DOWNLOADED: screenshot @ 2020-12-23 14:18:40
[[file:images/Solution/2020-12-23_14-18-40_screenshot.png]]

*** Private Project Repo
Now we will need the *private portion of the key* as a secret in the *private project repository* (see Fig. [[fig:privprj]]).
#+begin_src bash
# Copy contents of private key
cat priv_sub_a | xclip -selection clipboard
#+end_src

#+name: fig:privprj
#+DOWNLOADED: screenshot @ 2020-12-23 14:21:36
#+caption: Secret setup in the private project
[[file:images/Solution/2020-12-23_14-21-36_screenshot.png]]
*** Workflow Modifications
Now we will simply update our workflow [fn:givecredit]. We will simply add the following step:
#+begin_src yaml
      - name: get_subm
        env:
          SSHK: ${{ secrets.SUB_SSHK_A }}
        run: |
          mkdir -p $HOME/.ssh
          echo "$SSHK" > $HOME/.ssh/ssh.key
          chmod 600 $HOME/.ssh/ssh.key
          export GIT_SSH_COMMAND="ssh -i $HOME/.ssh/ssh.key"
          git submodule update --init --recursive
#+end_src
This will work for a single private submodule and multiple public submodules. For multiple private submodules, we would not initialize them recursively, but instead use a separate key for each.
#+begin_src yaml
      - name: get_subm_a
        env:
          SSHK: ${{ secrets.SUB_SSHK_A }}
        run: |
          mkdir -p $HOME/.ssh
          echo "$SSHK" > $HOME/.ssh/ssh.key
          chmod 600 $HOME/.ssh/ssh.key
          export GIT_SSH_COMMAND="ssh -i $HOME/.ssh/ssh.key"
          git submodule update --init -- <specific relative path to submodule A>
      - name: get_subm_b
        env:
          SSHK: ${{ secrets.SUB_SSHK_B }}
        run: |
          mkdir -p $HOME/.ssh
          echo "$SSHK" > $HOME/.ssh/ssh.key
          chmod 600 $HOME/.ssh/ssh.key
          export GIT_SSH_COMMAND="ssh -i $HOME/.ssh/ssh.key"
          git submodule update --init -- <specific relative path to submodule B>
#+end_src
Note that it is not possible to use the same SSH key in multiple submodule repositories, as each deploy key can only be associated with one repository.
*** Complete Workflow
Putting the above steps together, for the case of a single private submodule and multiple public submodules, we have:
#+begin_src yaml
name: Fake secret project

on:
  push:
    branches: [master, development]
  pull_request:
    branches: [master]

jobs:
  build:
    runs-on: ${{ matrix.os }}

    strategy:
      max-parallel: 4
      matrix:
        os: [ubuntu-18.04]
        cpp-version: [g++-7, g++-9, clang++]

    steps:
      - uses: actions/checkout@v2
      - name: get_subm
        env:
          SSHK: ${{ secrets.SUB_SSHK_A }}
        run: |
          mkdir -p $HOME/.ssh
          echo "$SSHK" > $HOME/.ssh/ssh.key
          chmod 600 $HOME/.ssh/ssh.key
          export GIT_SSH_COMMAND="ssh -i $HOME/.ssh/ssh.key"
          git submodule update --init --recursive
      - name: build
        env:
          CXX: ${{ matrix.cpp-version }}
        run: |
          mkdir build && cd build
          cmake -DCMAKE_CXX_COMPILER="$CXX" -DCMAKE_CXX_FLAGS="-std=c++11" ../
          make -j$(nproc)
      - name: run
        run: |
          ./super_secret
#+end_src
** Conclusions
We have demonstrated a minimally invasive setup for working with a private submodule, which is trivially extensible to multiple such submodules. With this, it appears that GH actions might a viable option (as opposed to say, [[https://app.wercker.com/][Wercker]]), at least for private teams. A more full comparative post might be warranted at a later date.

 [fn:givecredit] [[https://github.com/jwsi/submodule-checkout][submodule-checkout]] uses a similar concept but unfortunately does not extend to multiple submodules and the submodules are checked out as ~root~

* DONE Project Specific Expressions from Nixpkgs for Sphinx documentation :@programming:documentation:tools:nix:workflow:
CLOSED: [2020-12-22 Tue 05:09]
:PROPERTIES:
:EXPORT_FILE_NAME: nix-prj-spec-doc
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
Short post on making minimal changes to derivations in ~nixpkgs~ at a project level using ~callPackage()~ along with GH-Actions for deployment of ~sphinx~ documentation.
#+END_QUOTE
** Background
As part of my work on the [[https://symengine.org][Symengine documentation]][fn:whopaid],  I had originally thought of leveraging ~nix~ for reproducible builds for each of the language bindings with GH-Actions. There exists a derivation in the upstream package repository, but it was outdated ([[https://github.com/NixOS/nixpkgs/blob/66e44425c6dfecbea68a5d6dc221ccd56561d4f1/pkgs/development/libraries/symengine/default.nix][v6.0.0]] instead of v6.0.1) [fn:whocares]. Normally this would simply require a PR to be fixed; but since documenting different language bindings requires specific flags and tests were largely not an issue, I needed to make a project level derivation for each repository.
** Project Layout
The standard ~niv~ setup, as described in the [[A Tutorial Introduction to Nix][longer tutorial post]] will suffice.
#+begin_src bash
niv init -b master
#+end_src
Since the bindings were for ~python~, ~mach-nix~ was leveraged for the ~shell.nix~; reasons for which [[Niv and Mach-Nix for Nix Python][have been outlined elsewhere]]. This led to the following expression:
#+begin_src nix :hl_lines 4
let
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  nsymengine = pkgs.callPackage ./nix/nsymengine.nix { };
  mach-nix = import (builtins.fetchGit {
    url = "https://github.com/DavHau/mach-nix/";
    ref = "refs/tags/3.1.1";
  }) {
    pkgs = pkgs;
  };
  customPython = mach-nix.mkPython rec {
    requirements = builtins.readFile ./requirements.txt;
  };
in pkgs.mkShell {
  buildInputs = with pkgs; [
    customPython
    cmake
    nsymengine
    bashInteractive
    sage
    which
  ];
}
#+end_src

The highlighted line calls ~nsymengine.nix~ which is derived from the upstream expression. Since ~callPackage~ is being used from the ~niv~ pinned sources, the expression is completely project-specific and reproducible.
*** Upstream
For completion; the upstream expression is reproduced. We will actually grab this directly with ~show-derivation~:
#+begin_src bash
nix show-derivation -f "<nixpkgs>" symengine > nix/nsymengine.nix
#+end_src
Which gives us:
#+begin_src nix
{ lib, stdenv
, fetchFromGitHub
, cmake
, gmp
, flint
, mpfr
, libmpc
}:

stdenv.mkDerivation rec {
  pname = "symengine";
  version = "0.6.0";

  src = fetchFromGitHub {
    owner = "symengine";
    repo = "symengine";
    rev = "v${version}";
    sha256 = "129iv9maabmb42ylfdv0l0g94mcbf3y4q3np175008rcqdr8z6h1";
  };

  nativeBuildInputs = [ cmake ];

  buildInputs = [ gmp flint mpfr libmpc ];

  cmakeFlags = [
    "-DWITH_FLINT=ON"
    "-DINTEGER_CLASS=flint"
    "-DWITH_SYMENGINE_THREAD_SAFE=yes"
    "-DWITH_MPC=yes"
    "-DBUILD_FOR_DISTRIBUTION=yes"
  ];

  doCheck = true;

  checkPhase = ''
    ctest
  '';

  meta = with lib; {
    description = "A fast symbolic manipulation library";
    homepage = "https://github.com/symengine/symengine";
    platforms = platforms.unix ++ platforms.windows;
    license = licenses.bsd3;
    maintainers = [ maintainers.costrouc ];
  };

}
#+end_src
*** Repo-native
The truncated expression meant to be bundled with the repo is reproduced below.
#+begin_src nix
{ lib, stdenv
, fetchFromGitHub
, cmake
, gmp
, flint
, mpfr
, libmpc
}:

stdenv.mkDerivation rec {
  pname = "symengine";
  name = "symengine";
  version = "44eb47e3bbfa7e06718f2f65f3f41a0a9d133b70"; # From symengine-version.txt

  src = fetchFromGitHub {
    owner = "symengine";
    repo = "symengine";
    rev = "${version}";
    sha256 = "137cxk3x8vmr4p5x0knzjplir0slw0gmwhzi277si944i33781hd";
  };

  nativeBuildInputs = [ cmake ];

  buildInputs = [ gmp flint mpfr libmpc ];

  cmakeFlags = [
    "-DWITH_FLINT=ON"
    "-DINTEGER_CLASS=flint"
    "-DWITH_SYMENGINE_THREAD_SAFE=yes"
    "-DWITH_MPC=yes"
    "-DBUILD_TESTS=no"
    "-DBUILD_FOR_DISTRIBUTION=yes"
  ];

  doCheck = false;

}

# Derived from the upstream expression : https://github.com/r-ryantm/nixpkgs/blob/34730e0640710636b15338f20836165f29b3df86/pkgs/development/libraries/symengine/default.nix
#+end_src
Some project-specific notes:
-  [[https://github.com/CrossNox/m2r2/][m2r2]] was used to inject the project ~README.md~ into the [[https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html][Sphinx reStructuredText]] index file
- To prevent ~symengine~ from picking up the system ~python~, a pure shell was required ~nix-shell --pure --run bash~
- ~symengine-version.txt~ was already being used by the CI setup for cloning the right commit, and though a text file for hash storage is inelegant, it is a valid approach without ~nix~
** GH-Actions and Deployment
Having obtained an environment; the penultimate step involved a trivial shell script which uses the fantastic ~nix-shell~:
#+begin_src bash
#!/usr/bin/env nix-shell
#!nix-shell ../shell.nix -i bash

python setup.py build_ext --inplace
sphinx-build docs/ genDocs/
#+end_src
Finally an action definition in ~.github/workflows/mkdocs.yaml~:
#+begin_src yaml
on:
  push:
    branches:
      - main
      - master

name: Make Sphinx API Docs

jobs:
  mkdocs:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - uses: cachix/install-nix-action@v12
        with:
          nix_path: nixpkgs=channel:nixos-unstable

      - name: Build
        run: ./bin/docbuilder.sh

      - name: Deploy
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./genDocs
#+end_src
** Conclusion
In the end, to ensure easy long-term maintenance by the other maintainers, it was decided that the ~nix~ derivations should be trashed in favor of ~conda~ madness. Though most changes should be contributed upstream, the efficient re-use of upstream expressions is still viable for certain projects, especially those related to documentation.
[fn:whopaid] Sponsored by the Google Season of Docs initiative
[fn:whocares] Part of the documentation design worked out involved the [[https://abseil.io/about/philosophy#we-recommend-that-you-choose-to-live-at-head][live at head concept]] (also described in this [[https://cppcon2017.sched.com/event/Bhci/c-as-a-live-at-head-language][CPPCon17 talk]])
* DONE My Life in E-ink :@personal:workflow:tools:ramblings:
CLOSED: [2021-02-20 Sat 01:41]
:PROPERTIES:
:EXPORT_FILE_NAME: my-life-in-eink
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
Collection of odds and ends relating to e-readers including personal reminisces
#+end_quote
** Background
Reading has been a huge part of my life. The written word has had arguably more of an impact on my life than anything I have experienced in person. As a kid back in early 2000's; this meant a lot of library trips and saving for paperbacks. I also caught the first wave of the e-ink revolution. Nothing beats a real book, in terms of textures and scents; but e-ink devices and the fantastic tools outlined here should make reading digital books much more palpable [fn:watelse].

I have been reading on my [[https://en.wikipedia.org/wiki/Kobo_Aura_HD][Kobo Aura HD]] for almost a decade now, ever since its release. This means my setup is about as stable as its going to get in the near future. As good a time as any to collect my thoughts [fn:whypost]. The focus is on e-ink devices and auxiliary tools; not on all digital content; so there are no mentions of syncing or reading on the go (with a phone) or of monitors which are good for reading on.
*** The Content
In general; my e-ink reading habits can be broadly broken into the following categories:
- Light Reading :: Practically this includes [[https://www.goodreads.com/user/show/33462912-rohit-goswami][anything I review on Goodreads]]; these are not often re-read; nor read very deeply; since they are read for pleasure. They are however, rarely deleted
- Required Reading :: Anything which typically requires me to take notes or practice / write out proofs; these are most often considered to be either coursework (for someone somewhere) or research monographs. These are typically large (in size) and unwieldy (in that they often lack TOCs) and are read multiple times; with a focus on highlights and notes
- Active Research :: These are the most ephemeral of my reading habits; and also the most numerous; I do not typically store these on my e-reader; and rarely need to make notes on the reader [fn:whynotes]. These are often tiny; but require special work due to the metadata involved

| Content Type     | Software Stack   | Deletion Rate |
| Light Reading    | Calibre          | Rare          |
| Required Reading | Calibre          | Never         |
| Active Research  | Calibre + Zotero | Frequent      |

Though I am a huge proponent of RSS feeds (with [[https://gitlab.com/news-flash/news_flash_gtk][Newsflash]]) and read online content voraciously with both a [[https://app.getpocket.com/][Pocket]] and [[https://www.diigo.com/user/rgoswami][Diigo]] subscription[fn:whytwo]; I sincerely do not believe blog stuff or anything tailored for the web should have a presence on an e-ink device; so there shall be no mention of those parts of my reading habits[fn:whatelsebloggy].
** Hardware
My primary e-reader is still my [[https://www.kobo.com/koboaurahd?___store=au&style=onestore][Kobo Aura HD]] (complete with a snazzy [[https://www.amazon.com/Cover-Up-eReader-Natural-Cover-Function/dp/B00DZJ5VM0][hemp sleep-cover]]), and has been my go to for almost a decade now since its release. Recently I have augmented my workflow with the [[https://remarkable.com/store/remarkable-2][reMarkable 2]]; though I have yet to break it in very well; mostly because I tend to gravitate towards typing out my thoughts [fn:whytype] instead of writing.

The Kobo Aura HD is still the pinnacle of reading technology to me; mostly because the firmware is easy to bypass; and there is a vibrant community of developers on the [[https://www.mobileread.com/forums/showthread.php?t=210800][MobileRead Forums]]. Display and spec aside; the biggest reason for never replacing it has been been the simple fact that most modern e-readers no longer support SD cards; and much of my workflow depends on storing insane amounts of material offline [fn:alsobattery].


#+DOWNLOADED: screenshot @ 2021-02-20 01:39:20
#+caption: Primary reading device with Koreader
[[file:images/Hardware/2021-02-20_01-39-20_screenshot.png]]


Personally, I never use Nickel (the default Kobo interface), and it would probably choke trying to scan my 200 GB of content; so I haven't updated the firmware in forever. My interactions are almost always in [[https://koreader.rocks][Koreader]]; and my launching poison of choice is the now no longer developed [[https://www.mobileread.com/forums/showthread.php?t=293804][Kobo start menu]] [fn:notrecco].
** Software
Broadly speaking; the main parts of the software pipeline from digital book to brain are simply the syncing mechanism and the UI/UX/OS of the device in question; though it is often best to consider pre-processing books for devices too. These are covered in the order used.
*** k2pdfopt
The thought of reflowing text for an optimal reading experience, especially given the slightly limited processing power of my primary reading device is an enticing prospect. [[https://www.willus.com/k2pdfopt/][K2pdfopt or the Kindle 2 PDF Optimizer]] is as criminally underrated as it is fantastic. An approach which works well for my device involves setting up [[https://github.com/HaoZeke/Dotfiles/blob/master/dotfiles/common/.local/bin/fileHelpers/docK2pdf][a simple shell script]] (part [[https://github.com/HaoZeke/Dotfiles][of my Dotfiles]]) for optimizing files on the fly before sending them through ~calibre~.
#+begin_src bash
#!/usr/bin/env bash
# Get a filename
case "$#" in
0)
	echo "No arguments, so enter the filename, WITH the extension"
	read -p 'Document: ' docfile
	;;
1)
	echo "OK, using the filename"
	docfile="$1"
	;;
*)
	echo "Illegal number of parameters"
	exit
	;;
esac
# Get basename
basename="${docfile%.*}"
ext="${docfile##*\.}"
echo "Basename ${basename} with $ext from $docfile"
echo "Making a local store for the outputs"
mkdir -p "$HOME/auraHDopt"

case "$ext" in
"djvu")
	echo "Converting djvu to pdf via ps and running k2pdfopt"
	djvups "${basename}.djvu" "${basename}.ps"
	ps2pdf "${basename}.ps" "${basename}.pdf"
	# The newline is for simulating the Enter key
	echo | k2pdfopt "${basename}.pdf" -wrap -hy -ws -0.2 -dev kbhd -x
	echo "Cleaning up"
	mv "${basename}_k2opt.pdf" "$HOME/auraHDopt"
	rm -rf "${basename}.{ps,pdf}"
	;;
"pdf")
	echo "Converting pdf with gs and running k2pdfopt"
	gs -sDEVICE=pdfwrite -dCompatibilityLevel=1.4 -dPDFSETTINGS=/screen \
		-dNOPAUSE -dQUIET -dBATCH -sOutputFile="${basename}gs.pdf" "${basename}.pdf"
	echo | k2pdfopt "${basename}gs.pdf" -wrap -hy -ws -0.2 -dev kbhd -x
	echo "Cleaning up"
	rm "${basename}gs.pdf" -rf
	mv "${basename}gs_k2opt.pdf" "$HOME/auraHDopt"
	;;
*)
	echo "Illegal file type"
	exit
	;;
esac

#+end_src
The outputs can also be further processed with an [[https://github.com/HaoZeke/Dotfiles/blob/master/dotfiles/common/.local/bin/fileHelpers/isOcr][OCR (Optical Character Recognition) script]] if required, and then edited in [[https://code-industry.net/masterpdfeditor/][Master PDF Editor]] or something similar to add the table of contents interactively as well.
#+begin_src bash
#!/bin/bash
# Use as  find . -type f -name "*.pdf" -exec isOcr '{}' \;

# Shamelessly kanged from here:
# https://stackoverflow.com/questions/7997399/bash-script-to-check-pdfs-are-ocrd
# Only searches for text on the first 5 pages
# Modified to have red text. Also to possibly ocr the thing.

# -*- mode: shell-script-mode -*-

MYFONTS=$(pdffonts -l 15 "$1" | tail -n +3 | cut -d' ' -f1 | sort | uniq)
if [ "$MYFONTS" = '' ] || [ "$MYFONTS" = '[none]' ]; then
    echo "$(tput setaf 1)NOT OCR'ed: $1"
    if [[ -x "$(which ocrmypdf)" ]]; then
        echo "$(tput setaf 4)"
        echo "Converting to ${1%.*}_ocr.pdf with ocrmypdf"
        echo "$(tput setaf 7)"
        ocrmypdf --deskew --clean --rotate-pages \
            --jobs 4 -v --output-type pdfa "$1" "${1%.*}_ocr.pdf"
    elif [[ -x "$(which pypdfocr)" ]]; then
        echo "$(tput setaf 2) Looking for config files at $XDG_CONFIG_HOME/pypdfocr/config.yml"
        echo "$(tput setaf 3)"
        if [[ -e $XDG_CONFIG_HOME/pypdfocr/config.yml ]]; then
            echo "Using configuration settings"
            echo "$(tput setaf 4)"
            pypdfocr -c $XDG_CONFIG_HOME/pypdfocr/config.yml "$1"
        else
            echo "Using default settings"
            echo "$(tput setaf 4)"
            pypdfocr "$1"
        fi
        echo "$(tput setaf 2) You might want to get pypdfocr"
    fi
else
    echo "$1 is OCR'ed."
fi
#+end_src
The end result is:
- A directory with perfectly ~pdf~ files re-flowed text
  + Possibly OCR'ed for string searches
TOC editing is still rather janky; but this is also because the OCR process is still rather spotty.
*** Calibre
[[https://calibre-ebook.com/][Calibre]] is an excellent library software, and there are very few alternatives which offer all the salient features:
- Syncing :: Apart from working well with a plethora of official devices, Koreader is also pretty well supported, and mounting folders allows for easy management of a secret library (e.g. ~.Library~) on an SD card to prevent Nickel from reading and choking on large libraries
- Multiple Libraries :: I personally keep one for fiction, one for non-fiction, and one (transiently populated) one for papers
- Good metadata collection :: Nothing beats rich metadata, and with third party plugins, all the best content providers can be leveraged for blurbs; plus most purchased books come with metadata which ~calibre~ can read
It isn't perfect, there are far better [[https://opds.io/][OPDS (Open Publication Distribution System)]] servers like the fantastic [[https://github.com/seblucas/cops][COPS (Calibre OPDS)]] project, and there have been [[https://anarc.at/software/desktop/calibre/][some security concerns in the past]], but it is really usable and is [[https://github.com/kovidgoyal/calibre][under active development]]; plus it has a [[https://kovidgoyal.net/][fun developer]]. I also personally find the file conversion lacking, compared to ~k2pdfopt~, but as a library management system it is really good.
*** Zotero Sync
Calibre provides a handy [[https://www.mobileread.com/forums/showthread.php?p=3339191#poststop][ZMI (Zotero Metadata Importer) plugin]] which allows for exported papers to be imported into ~calibre~ and from then into the e-reader as expected. Combined with the folder mounts facilitated by ~calibre~ this allows for a painless way to ensure a quick export; optimize; sync; read and delete workflow.
*** Koreader
Koreader is probably the best thing to happen to e-ink devices since sliced bread. It replaces the need to use any cables with an e-reader; since newer versions have a nice SSH server, and can also update itself. Since this is mostly used as is; and all the information required is on the [[https://github.com/koreader/koreader/wiki][Github Wiki]], there's not much else to say here.

It is probably worth noting that the in-built re-flow options do tend to cause major artifacts on older hardware, and is best avoided. Almost equivalently, and at a far lower cost in terms of performance, page contents can be fit to width and zoomed in automatically, which is almost as good as working with ~k2pdfopt~ in some special cases.
** Conclusions
Given my unfortunate separation from my library back home; it is likely that my e-ink devices will continue to be my primary source of reading material. Plus the long retarded color e-ink market finally seems to be moving out of its stupor [fn:sayswho]. The only possible addendum to this methodology would probably involve integrating ~orgmode~ and the reMarkable 2 sometime. E-ink is here to stay. This setup would probably need revisions involving ~rclone~ or ~syncthing~ if I ever gave up and opted for a "modern" device without an SD slot.

[fn:sayswho] [[https://www.youtube.com/watch?v=OlnzrxaZViU][YT video on color and e-ink]], [[https://www.youtube.com/watch?v=_6K53j1k1vY][YT video on recent color e-ink tech]] and [[https://www.theverge.com/2021/2/15/22284923/pocketbook-inkpad-color-kaleido-e-reader-available-now-price][Verge release notes]] for the [[https://pocketbook.ch/en-ch/news/pocketbook-inkpad-color-news-ch][PocketBook InkPad Color]]
[fn:whypost] On an unrelated note, this represents a stylistic departure from older normal posts, with much more of a rambling narrative
[fn:whynotes] These are annotated [[An Orgmode Note Workflow][in Emacs with org-mode]] when necessary
[fn:whytype] For which I have been perfecting a rather [[Switching to Colemak][personalized workflow with Colemak]]
[fn:whatelsebloggy] The same can be said of Wikipedia content and tweets; spending time on the written word does not automatically worth putting on an e-reader
[fn:alsobattery] Also why the battery lasts for days without any strain
[fn:whytwo] Diigo is great for saving storing sites forever and Pocket is just cheaper Instapaper
[fn:notrecco] [[https://pgaskin.net/NickelMenu/][NickelMenu]] seems to be recommended for newer devices
[fn:watelse] They're also more convenient than holding a torch in bed with a paperback
* TODO Nix For Static Sites
* TODO Improving Astroid :@personal:rant:tools:email:workflow:
:PROPERTIES:
:EXPORT_FILE_NAME: improving-astroid
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
Adding encryption and scripts for better mail management
#+end_quote
** Background
** Conclusions
* TODO Grant Proposals - II :@personal:ramblings:thoughts:academia:
:PROPERTIES:
:EXPORT_FILE_NAME: grant-proposals-two
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:END:
#+BEGIN_QUOTE
A short post on post-grant application paperwork
#+END_QUOTE
** Background
A while ago, [[Grant Proposals - I][I wrote about my experience applying]] for the Rannís [[https://en.rannis.is/funding/research/icelandic-research-fund/][Icelandic Research Fund]]. I mentioned there that I should not go into great detail without knowing if my process was correct. I have since [[https://en.rannis.is/news/allocation-from-the-icelandic-research-fund-for-the-financial-year-2021-1][been awarded]] the three-year grant for my proposal on "Magnetic interactions of itinerant electrons modeled using Bayesian machine learning". It would make sense then, to go into parts of the process in more depth.
** The Process

** Conclusion
I might decide to keep a running tab of grant related posts and activities.

* DONE SymEngine and the Season of Docs :@personal:ramblings:thoughts:
CLOSED: [2021-03-25 Thu 21:18]
:PROPERTIES:
:EXPORT_FILE_NAME: symengine-gsod20
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:EXPORT_BIBLIOGRAPHY: biblio/refs.bib
:EXPORT_HUGO_PANDOC_CITATIONS: t
:EXPORT_HUGO_CUSTOM_FRONT_MATTER+: :link-citations true
:END:
#+BEGIN_QUOTE
An overview of documentation complexity and an analysis of incentives.
#+END_QUOTE
** Background
As mentioned [[https://rgoswami.me/search/?q=symengine][a few times before]], last year [[https://developers.google.com/season-of-docs/docs/2020/participants/project-sympy-rohitgoswami?hl=en][a proposal]] of mine to improve the documentation for the [[https://github.com/symengine/][SymEngine]] organization was accepted under the Google Season of Docs 2020 initiative. This is a more personal and expanded discussion on the [[https://github.com/symengine/symengine/wiki/GSoD-2020-Report-Rohit-Goswami:-SymEngine][report submitted on the SymEngine Wiki]] regarding the goals and completion metrics.

#+DOWNLOADED: screenshot @ 2021-03-25 20:26:32
#+caption: Promotional image Google seemed to strongly suggest
[[file:images/Background/2021-03-25_20-26-32_screenshot.png]]

** Documentation and Me
I have been a huge proponent of documentation throughout my decade long dabbling in FOSS projects. Quite bluntly, my memory is not great, and writing things down in a manner tailored to my own needs helps me reduce the amount of time it takes to do similar tasks. In many cases, the problem is not even the lack of existing documentation; it is just good to have a representation which encourages the minimum cognitive load for me.
** SymEngine and Me
The SymEngine repository will turn 10 next year. There are few, if any, contenders in the field of C++ symbolic mathematics libraries; the [[http://viennamath.sourceforge.net/][Vienna math]] project seems to have stalled and does not support matrices.

Symbolic math is one of those fields I feel should be used a lot more in the applied sciences, but somehow gets overlooked. Part of this is undoubtedly because of Mathematica, Maple and other proprietary players, which stifle innovation and FOSS development, but also the documentation and technical debt incurred is pretty large.
** Meeting the Mentors
Originally, the meeting with my mentoring group was rather anxiety inducing; this was mostly due to my own unfamiliarity with symbolic algebra. Over time though, the meetings quickly became something I started to look forward to; and we were able to collectively come up with a unique; yet sustainable documentation workflow. I was also able to gain insights into the logic underlying the code which would have ordinarily taken longer to process on my own. I also ended up going over some references like @cohencomputer2003 and @cohencomputer2002.
** Timelines and Deliverable Assets
One of the early issues I faced was restricting myself to documentation; with such a vibrant and fast-growing project, there were several avenues I wanted to explore and enhance, not all of which were technically under the ambit of documentation. This was an impulse which I eventually had to ignore; though the team were kind enough to strongly suggest submitting PRs in other directions after the documentation project concluded.
*** Doxygen Alternatives
Initially I had hoped to have an all-Sphinx documentation site, with all language bindings parsed into Sphinx. For this I explored [[https://github.com/svenevs/exhale][exhale]], as well as the more flexible [[https://github.com/vovkos/doxyrest][doxyrest]]. None of these could match the flexibility or rich output of Doxygen for C++; so eventually I did the more rational thing and developed a nice theme for Doxygen instead, [[https://github.com/HaoZeke/doxyYoda][doxyYoda]].
**** Base Doxygen
#+DOWNLOADED: screenshot @ 2021-03-25 20:46:50
#+caption: Base Doxygen
[[file:images/Timelines_and_Deliverable_Assets/2021-03-25_20-46-50_screenshot.png]]

- Is ugly

Other than that, there were no standards for consistent documentation originally.
**** Exhale Documentation

#+DOWNLOADED: screenshot @ 2021-03-25 20:48:42
#+caption: Exhale example
[[file:images/Timelines_and_Deliverable_Assets/2021-03-25_20-48-42_screenshot.png]]

- Cannot include source code

That is a deal breaker, since the algorithms are often described step by step.
**** Doxyrest Documentation

#+DOWNLOADED: screenshot @ 2021-03-25 20:48:59
#+caption: Doxyrest
[[file:images/Timelines_and_Deliverable_Assets/2021-03-25_20-48-59_screenshot.png]]

- Includes more structure than exhale
- Can be extended to other source languages
- Has a rather complicated setup
**** Doxygen with DoxyYoda
#+DOWNLOADED: screenshot @ 2021-03-25 20:47:09
#+caption: Doxygen with doxyYoda
[[file:images/Timelines_and_Deliverable_Assets/2021-03-25_20-47-09_screenshot.png]]

- All the goodness of Doxygen
  - Includes hierarchies
- Also is now pretty
*** Tooling Decisions
This is more of a technically specification; but after rooting around, it was decided to throw consistency of design to the wind and use native tools for each language binding as shown in Table [[tbl:langtool]].

#+name: tbl:langtool
#+caption: Language bindings and tools
| Language         | Package                  |
| [[https://symengine.org/symengine.R][R]]                | [[https://pkgdown.r-lib.org/][pkgdown]]                  |
| [[https://symengine.org/symengine.py][Python]]           | [[https://www.sphinx-doc.org/][Sphinx]]                   |
| [[https://symengine.org/symengine][C++]]              | [[https://www.doxygen.nl/index.html][Doxygen]] + [[https://github.com/HaoZeke/doxyYoda][doxyYoda]]       |
| [[https://symengine.org/SymEngine.jl/][Julia]]            | [[https://juliadocs.github.io/Documenter.jl/stable/man/guide/][Documenter.jl]]            |
| [[https://symengine.org/#api-documentation][Notebooks / MyST]] | [[https://www.sphinx-doc.org/][Sphinx]] + [[https://myst-nb.readthedocs.io/en/latest/][myst]] + [[https://jupytext.readthedocs.io/en/latest/install.html][jupytext]] |

*** Notebooks and Documentation
*I do not like* Jupyter Notebooks. This is because, to me, a rabid ~org-mode~ fanatic [fn:soyousay], it seems pretty odd that they are essentially monstrous ~json~ nightmares instead of plain text. Thankfully [[https://jupytext.readthedocs.io/][jupytext]] and [[https://myst-nb.readthedocs.io/en/latest/][MyST-NB]] solve this problem admirably. There are still some use-cases for having pure notebooks, especially since GitHub now renders them; so a CI (Github Actions) generates a "notebooks" branch for the entrenched as well.
*** Cell-Tags and Disappointment
One of the major disappointments faced during the project had to do with the way cell tags work; or rather, do not work. It was planned to have notebooks executed with ~papermill~ and switch between development and stable packages based on cell-tags; however, cell-tags apparently cannot be used for meta-injection; which means they are absolutely useless. A workaround of course might be using macro expansions; however, ~papermill~ did not really seem up to the challenge of injecting non-python variables either so this was abandoned.
** Future Plans
Personally, I will not be participating in further rounds in the foreseeable future, mostly because I intend to continue working on SymEngine until it is compliant with the standards I championed, and the addition of more projects to the portfolio I consider to be part of my moral responsibilities is not a good idea right now.
** Conclusions
It is difficult to gauge the effect of financial incentives on the quality of code. That analysis and possible rant I'll save for another post. Projects like SymEngine attract very good proposals during high incentive development bursts like the Google Summer of Code and Season of Documentation; but these contributions tend to rot over time; which makes them of dubious use. That said, my own experience involving SymEngine was personally enriching, and I look forward to working on the project further. I believe the most useful aspect of the program is the ability to meet with the rest of the development team regularly, none of the other projects I work on have regular meetings, and it is a model I intend to carry forward in some of my projects.

[fn:whydoc] I have a very bad memory, and writing things down means that I am less likely to repeat tasks
[fn:soyousay] This site is [[https://github.com/HaoZeke/haozeke.github.io/][written in org-mode]], my ~doom-emacs~ configuration is also [[https://dotdoom.rgoswami.me][extensive and online]]

* TODO LFortran Intrinsics with C :@programming:workflow:projects:tools:
* DONE Free Software and Me :@personal:rant:
CLOSED: [2021-03-27 Sat 21:28]
:PROPERTIES:
:EXPORT_FILE_NAME: fsf-me-stallman
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments false
:END:
#+BEGIN_QUOTE
Thoughts about the FSF, toxicity, and Stallman
#+END_QUOTE
First a disclaimer. Comments are *not enabled* for this post. This is very personal, and it is in my online space so I reserve the right to not talk about this. This is not a conversation. It is a protracted comment on recent, disappointing, news about the "tech community". It is unlike anything I have normally published; and is hopefully the last time I will publish something like this. In general there is always something to be outraged about; and that is what Twitter is for. Most of these things are not relevant enough to me personally to write a post about.

I do not write about my personal life and I try not to have one [fn:seriously]. [[https://www.covid.is/english][COVID-19]], [[https://www.ruv.is/frett/2021/03/03/over-14000-earthquakes][earthquakes]] and even [[https://www.ruv.is/frett/2021/03/20/live-feed-from-iceland-volcano][a volcano]]; have all been treated with utter and complete indifference. None of those things have defined any part of me; and will never do so. The FSF and [[https://www.redhat.com/en/blog/red-hat-statement-about-richard-stallmans-return-free-software-foundation-board][its flip flopping]] with respect to Richard Stallman; does affect me in a way. It embarrasses me. I cannot imagine why people need to hero worship someone who has contributed nothing to the community for many years now, other than propagation of a toxic subculture. Even Emacs, which he "wrote" several decades ago; is being championed more by the fantastic [[https://github.com/hlissner][Henrik Lissner]] and his [[https://github.com/hlissner/doom-emacs][doom-emacs project]]; proof again that the "holy wars" were pointless and toxic[fn:moreleader].

I got into programming over ten years ago; when I was in the tenth grade I tried to submit something silly to a mailing list. I was (rightly) shot down. That annoyed me; and I clawed my way through reams of books and eventually turned out to be a passable programmer in my own right. I joke about the way I was "guided" into open source now when I teach more inclusive introductory workshops; but in some sense the sting of that initial set of interactions has never really dulled. Nonetheless, programming remains the most open of disciplines [fn:reallyprog], which many wrongly ascribe to Richard Stallman.

I am not here to vilify Richard Stallman; I have never met the guy. For those interested, and from people with more of a stake in the matter [[https://gcc.gnu.org/pipermail/gcc/2021-March/235091.html][Nathan Sidwell's take]] is pretty solid [fn:whystake]. I simply do not like that I can no longer vouch for my many GPL'd projects anymore. Many will point out (have pointed out actually), that Stallman was always there; and this late stage cancel culture means nothing or is pointless posturing. I reject that stance. I have not always been a stellar human being in my personal interactions; but I strive to both make amends (personally) and also to maintain an image of a better human being. Me, online, is simply a projection of the person I wish to be. This mindset grew from a time back when it was common to have multiple online personas, before the line between one's online identity and real identity was not so blurry. Over time; I hope to be the sort of person who can be both erudite and welcoming; inspite of the fact that it is difficult to not throw diva tantrums (which are often socially acceptable as well). This is also a stance (i.e. iterative improvement) which Stallman clearly does not share. This lack of willingness to change with the times is enough of a reason for this post.

My own data consisting of me and other people I have ranted to in person does not meaningfully represent a community [fn:wherecomm]; but it felt necessary to take a stand. Active rejection of the herd mentality which leads to elevating one person above the rest; and goes on to assert "proportional" justice does not involve removing such a toxic person from leadership roles is important to me. I would like to continue to be proud of my work (such as it is) and of my projects (such as they are) without having to tacitly agree with the views of Stallman or the FSF. I would prefer to wipe the names of all "icons" and "founders" from the record and deal only in terms of the concrete work they have generated. This would prevent sheeple from hero-worshiping and idolizing people (this goes for "good" and "bad" people). In my opinion; Stallman should be removed for failing to be a decent human being. So many stories are told about how "unrelenting" and "passionate" this person is about free software. It is sad then; that he feels the need to be listed so prominently. Society as a whole should reject the sort of repugnant behavior he has displayed; and he should not let that prevent him from contributing to FOSS. That there is no path to anonymous contributions to the FSF is a whole other discussion.

That's the end of this rant.

Thankfully, my stance has been shared by sections of society which actually do contribute code and manage open source projects.

- [[https://fedoramagazine.org/fedora-council-statement-on-richard-stallman-rejoining-fsf-board/][Fedora Council Statement]]
- [[https://www.redhat.com/en/blog/red-hat-statement-about-richard-stallmans-return-free-software-foundation-board][Red Hat statement]]
- [[https://gcc.gnu.org/pipermail/gcc/2021-March/235245.html][GCC Steering Committee statement]]

[fn:seriously] Thankfully this bothers no one, me least of all.
[fn:reallyprog] I dare you to find open resources which teach you chemical engineering.
[fn:wherecomm] There is a [[https://rms-open-letter.github.io/][letter against his reinstatement]]; and a [[https://rms-support-letter.github.io/][grovelling one in support]] of his reinstatement.
[fn:moreleader] This [[https://news.ycombinator.com/item?id=26535789][HN post describes other leadership failures]] under Stallman
[fn:whystake] He, unlike the "supporters" is an active contributor to projects which sheeple believe exist only because of Stallman

* DONE Talk Supplements for IOP's C++ Workshop :@conferences:presentations:ramblings:python:cpp:
CLOSED: [2021-04-14 Wed 01:17]
:PROPERTIES:
:EXPORT_FILE_NAME: iop-cpp-2021-meta
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A meta-post on the workshop I held for the IOP student community on Intermediate C++
#+END_QUOTE
** Background
Much the same as the rationale behind my other presentation [[https://rgoswami.me/tags/presentations/][meta-posts]], that is:
- I would like to preserve questions
- I would like to collect the video, slides and other miscellaneous stuff in one location [fn:officialsite]
- It would be nice to have my own thoughts here afterwards
*** Details
Blurb verbatim from [[https://www.iopconferences.org/iop/frontend/reg/thome.csp?pageID=1032921&eventID=1664][the workshop announcement]].
#+begin_quote
In the 2nd C++ workshop, we will explore the build process and its automation using CMake; before discussing continuous integration for testing on multiple operating systems.  We will then round out the workshop by augmenting the particle simulater class with Python bindings using the C++ pybind11 library for visualizing results using matplotlib​, plotnine​ and other libraries.

This workshop will include:

The build process and its automation using CMake; why bind C++ to other languages
Using C++ in python with bindings and using the pybind11 library
Visualising results using matplotlib, plotnine and other libraries
Delivered by Rohit Goswami, doctoral researcher at the University of Iceland, Reykjavik. Participation in the workshop will not require any pre-installed software. Everything demonstrated will be available for students to edit and run online.
#+end_quote
- Github Repository :: [[https://github.com/HaoZeke/2021_April_IOP_IntroductionToCpp_Part2][Here]]. This demonstrates the refactoring of a particle simulator.
**** Other Content
Anything on this site [[https://rgoswami.me/tags/nix/][tagged with Nix]] or [[https://rgoswami.me/tags/cpp/][tagged with C++]]. Also an introduction to ~nix~ given by me (and Amrita Goswami) at [[https://2020.carpentrycon.org/schedule/#session-10][CarpentryCon2020]] is here:
- [[https://github.com/HaoZeke/CarpentryCon2020_Nix][CarpentryCon2020 Materials]]
- [[https://rgoswami.me/posts/ccon-tut-nix/][A tutorial introduction to Nix and Python]]
** Slides
- [[file:/revealjs/iop_april_2021/introcpp2.html][Best viewed here]] using a browser (in a new tab)
- A ~pdf~ copy of the slides are embedded below
- The ~orgmode~ source [[https://raw.githubusercontent.com/HaoZeke/2021_April_IOP_IntroductionToCpp_Part2/main/docs/pres/iopRevealjs.org][is here on the site's GH repo]]
#+BEGIN_EXPORT html
<script async class="speakerdeck-embed" data-id="9e85011c15ff4d07beca4faf84e49039" data-ratio="1.37081659973226" src="//speakerdeck.com/assets/embed.js"></script>
#+END_EXPORT
** Video

{{< youtube jyA13tw3VKg >}}
** Thoughts
- Using the SHA commit IDs worked really well
  + Perfect for demonstrating refactors
- GitHub Discussions might be a good idea later too
- Using Powerpoint in the background to get subtitles was fantastic too
  + Might expand these

[fn:officialsite] One location I am going to be able to keep track of

* TODO HPC Configuration 2021 :@programming:workflow:projects:hpc:
:PROPERTIES:
:EXPORT_FILE_NAME: hpc-conf-2021
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc tre :comments true
:END:

#+begin_quote
A post on configuring a user account for heavy interactive HPC usage with Spack, Nix and self-hosted applications.
#+end_quote

** Background
I have in the past written a few posts about configuring my HPC setup. Since the [[https://www.rhnet.is/][Research and University Network of Iceland hf. (RHnet)]] recently recieved funding for a new machine, called Elja [fn:whyname], I decided to jot down my notes from my setup as part of the user interaction team. Without abusing the ability to bend the ear of the system administrators, I decided to document a user-facing workflow. All the commands here assume an interactive session (example in ~slurm~):
#+begin_src bash
# 26 should be the number of processors on a node
srun -n 26 --pty /bin/bash
#+end_src
** Rust and Utilities
~rustup~ is a fantastic way to get ~cargo~ up and running and immediately allows for the installation of many relevant command line utilities.

#+begin_src bash
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
#+end_src

The ones I use mostly are enumerated in Table [[tbl:rustalternates]].

#+name: tbl:rustalternates
#+caption: Language bindings and tools
| Utility  | Rust Alternative | Rationale                            |
| ~cat~    | [[https://github.com/sharkdp/bat][bat]]              | File reader with syntax highlighting |
| ~top~    | [[https://github.com/clementtsang/bottom][bottom]]           | Process management                   |
| ~ls~     | [[https://github.com/ogham/exa][exa]]              | Listing things with colors           |
| ~ps~     | [[https://github.com/dalance/procs][procs]]            | Better looking process lists         |
| ~grep~   | [[https://github.com/BurntSushi/ripgrep][ripgrep]]          | Search recursively                   |
| ~find~   | [[https://github.com/sharkdp/fd][fd]]               | Find things                          |
| ~sed~    | [[https://github.com/chmln/sd][sd]]               | Search and replace things            |
| ~time~   | [[https://github.com/sharkdp/hyperfine][hyperfine]]        | Better benchmarking with stats       |
| ~man~    | [[https://github.com/dbrgn/tealdeer][tealdeer]]         | A fast ~tldr~ page viewer            |
| ~fzf~    | [[https://github.com/lotabout/skim][skim]]             | A fuzy finder                        |
| --       | [[https://github.com/denisidoro/navi][navi]]             | Personalized cheatsheets             |
| ~cd / z~ | [[https://github.com/ajeetdsouza/zoxide][zoxide]]           | Directory jumping aid                |

These are trivial to obtain:
#+begin_src bash
cargo install bottom ripgrep hyperfine navi skim
# The following need some system libraries (OpenSSL) and shall be dealt with later
# bat exa fd tealdeer starship sd procs zoxide
#+end_src
*** MicroMamba
We will set up a ~micromamba~ installation to get ~git~ and other base utilities.
#+begin_src bash
mkdir -p ~/.local/bin
export PATH=$HOME/.local/bin:$PATH
wget -qO- https://micromamba.snakepit.net/api/micromamba/linux-64/latest | tar -xvj bin/micromamba
mv bin/micromamba ~/.local/bin
rm -rf bin
micromamba shell init -s bash -p $HOME/micromamba
. ~/.bashrc
#+end_src
Now we can use ~mamba~ to get a temporary interactive environment.
#+begin_src bash
# Spack requirements
PKGS="git patch curl make tar gzip unzip bzip2 xz zstd subversion lsof m4"
mkdir ~/.mamba_venvs
micromamba create -p ~/.mamba_venvs/intBase $PKGS
micromamba activate ~/.mamba_venvs/intBase
#+end_src
**** Configuration
A simple ~.condarc~ will suffice for the above.
#+begin_src yaml
channels:
  - conda-forge
  - defaults
channel_priority: disabled
#+end_src
*** GitHub Client
We can grab the [[https://github.com/cli/cli/][GitHub client binary]] for interacting with GitHub for now. Configuring and using this spares the necessity of explaining SSH authentication.
#+begin_src bash
ghcliVer=1.9.2
DST=~/.local
wget https://github.com/cli/cli/releases/download/v${ghcliVer}/gh_${ghcliVer}_linux_amd64.tar.gz
tar xf "gh_${ghcliVer}_linux_amd64.tar.gz"
mkdir -p ~/.local/
cd "gh_${ghcliVer}_linux_amd64"
# Merges directories
# From: https://unix.stackexchange.com/a/603209/246530
find -type d -exec mkdir -vp "$DST"/{} \; -or -exec mv -nv {} "$DST"/{} \;
cd ../
rm -rf "gh_${ghcliVer}_linux_amd64" "gh_${ghcliVer}_linux_amd64.tar.gz"
#+end_src
Now we can set this up.
#+begin_src bash
gh auth login # Follow the instructions
#+end_src
The rest of the post assumes ~gh~ is present.
** Dotfiles
Some of this post is simplified by using [[https://github.com/HaoZeke/Dotfiles][my Dotfiles]] with ~bombadil~:
#+begin_src bash
cargo install toml-bombadil
mkdir -p $HOME/Git/Github
cd ~/Git/Github
gh repo clone HaoZeke/Dotfiles
cd Dotfiles
git checkout bombadil
bombadil install -c bombadil.toml
#+end_src
We will need to make some directories since ~bombadil~ does not currently handle this. Make changes to the variables as required and then link them.
#+begin_src bash
mkdir -p ~/.config/nvim ~/.config/alacritty/ \
    ~/.spack/
bombadil link -p hpc elja # Combining profiles (variables)
#+end_src
** Spack
For setting multi compiler workflows on the HPC, [[https://spack.io/][spack]] is a great tool. It is also well documented and easy to add to. Rather than setting up a hierarchical [[https://lmod.readthedocs.io/en/latest/010_user.html][Lmod]] structure, we shall use a [[https://spack.readthedocs.io/en/latest/environments.html][spack environment]]. Although we have already a ~micromamba~ environment, ~spack~ configurations are more suited to the HPC workflow; since they are more easily customized.
#+begin_src bash
# Install
cd $HOME
gh repo clone spack/spack
#+end_src
Now we can start by getting a new compiler.
#+begin_src bash
. ~/spack/share/spack/setup-env.sh
spack install gcc@9.3.0
spack compiler add $(spack location -i gcc@9.3.0)
spack install gcc@10.2.0 %gcc@9.3.0
spack compiler add $(spack location -i gcc@10.2.0)
spacktivate interactive # After setting the environment
#+end_src
*** Environments
This step is not required if one uses the ~dotfiles~.
#+begin_src yaml
# $SPACK_HOME//var/spack/environments/interactive/spack.yaml
spack:
  definitions:
    - compilers: ['gcc@10.2.0']
    - packages: [
      'zsh', 'tmux', 'git',
      'openssl', 'gmp', 'mpfr',
      'mpc', 'perl', 'boost', 'subversion',
      'zlib', 'cmake', 'bison', 'bdw-gc',
      'flex', 'pcre', 'jq', 'patch',
      'bzip2', 'sqlite', 'editline',
      'ncurses', 'texinfo', 'brotli'
      ]
  specs:
    - $compilers
    - 'subversion%gcc@9.3.0'
    - matrix:
       - [$packages]
       - [$%compilers]
  view: true
#+end_src
Note that ~subversion~ depends on ~serf~ which currently does not build with ~gcc@10.x.x~.
#+begin_src bash
spacktivate interactive
spack install
#+end_src
*** Cleaning Modules
This step is not required if one uses the ~dotfiles~.
#+begin_src yaml
modules:
  enable::
    - lmod
  lmod:
    core_compilers:
      - 'gcc@10.2.0'
    hierarchy:
      - mpi
    hash_length: 0
    whitelist:
      - gcc
    blacklist:
      - '%gcc@10.2.0'
    all:
      filter:
        environment_blacklist:
          - "C_INCLUDE_PATH"
          - "CPLUS_INCLUDE_PATH"
          - "LIBRARY_PATH"
      environment:
        set:
          '{name}_ROOT': '{prefix}'
    openmpi:
      environment:
        set:
          SLURM_MPI_TYPE: pmi2
          OMPI_MCA_btl_openib_warn_default_gid_prefix: '0'
    projections:
      all:          '{name}/{version}'
      ^lapack:      '{name}/{version}-{^lapack.name}'
#+end_src
We can then rebuild the setup.
#+begin_src bash
spack module lmod refresh --delete-tree -y
#+end_src

** Nix
*** Portable
[[https://github.com/DavHau/nix-portable][nix-portable]] is one of the best projects for working with local ~nix~ installations. Where it works, it works very simply:
#+begin_src bash
wget https://github.com/DavHau/nix-portable/releases/download/v006/nix-portable
chmod +x nix-portable
mv nix-portable ~/.local/bin
#+end_src
*** Mamba
#+begin_src bash
# Spack requirements
PKGS="git patch curl make tar gzip bzip2 xz zstd brotli openssl sqlite bdw-gc nlohmann_json editline libxml2 libxslt libseccomp bison flex lsof libarchive automake pkg-config"
micromamba create -p ~/.mamba_venvs/nixIns $PKGS
micromamba activate ~/.mamba_venvs/nixIns
#+end_src
It is best to have a clean, maximal ~boost~.
#+begin_src bash
git clone --recursive https://github.com/boostorg/boost.git
cd boost
git checkout tags/boost-1.73.0
./bootstrap.sh
./b2 headers
./b2
./b2 install --prefix=$HOME/.hpc/boost/boost-1.73.0
ml load boost/boost-1.73.0
#+end_src
Now we can try to compile a patched older ~nix~.
#+begin_src bash
export myprefix=$HOME/.hpc/nix/nix-boot
export nixdir=$HOME/.nix
./configure  --enable-gc --prefix=$myprefix \
--with-store-dir=$nixdir/store --localstatedir=$nixdir/var \
\
--disable-seccomp-sandboxing --disable-doc-gen \
CPPFLAGS="-I$HOME/.mamba_venvs/nixIns/include" LDFLAGS="-L$HOME/.mamba_venvs/nixIns/lib -Wl,-R$HOME/.mamba_venvs/nixIns/lib"
make -j $(nproc)
make install
#+end_src
*** Spack
When the existing methods don't work, we can fall back to a more rustic method. We need some additional packages, which are not part of ~spack~ yet.
#+begin_src bash
spack create https://github.com/anrieff/libcpuid/releases/download/v0.5.1/libcpuid-0.5.1.tar.gz
spack install libcpuid
#+end_src

#+begin_src bash
mkdir -m 0755 ~/.nix
spack install nix storedir=$nixdir/store statedir=$nixdir/var doc=False sandboxing=False nfs=True
spack load nix
# Basic setup
mkdir -p  ~/.nix/var/nix/profiles
nix-env --switch-profile .nix/var/nix/profiles/default
#+end_src

Now we can install this natively with ~spack~.

#+begin_src bash
mkdir -p $HOME/Git/Github
gh repo clone nixos/nixpkgs
cd nixpkgs
export NIX_PATH=$(pwd)
export nixdir=$HOME/.nix
export PATH=$(spack location -i nix)/bin/:$PATH:$nixdir/bin
export INCLUDE=$(spack location -i nix)/include/:$INCLUDE
export LD_LIBRARY_PATH=$(spack location -i nix)/lib:$LD_LIBRARY_PATH
export PKG_CONFIG_PATH=$(spack location -i nix)/lib/pkgconfig/:$PKG_CONFIG_PATH
nix-env -i nix -f $HOME/Git/Github/nixpkgs -j$(nproc) --keep-going --show-trace -v --cores $(nproc) 2>&1 | tee nix-no-root.log
#+end_src

#+BEGIN_SRC bash
export BOOST_ROOT=$(spack location -i boost)
./configure --enable-gc \
 --prefix=$myprefix \
 --with-store-dir=$nixdir/store \
 --localstatedir=$nixdir/var \
 --with-boost=$BOOST_ROOT \
 --disable-seccomp-sandboxing \
 --disable-doc-gen \
 --includedir="$(spack location -i bdw-gc)" \
 --includedir="$(spack location -i bzip2)" \
 --includedir="$(spack location -i nlohmann-json)" \
 --libdir="$(spack location -i bzip2)" \
 --libdir="$(spack location -i bdw-gc)" \
 PKG_CONFIG=$(spack location -i pkg-config)/bin/pkg-config \
 OPENSSL_CFLAGS=-I$(spack location -i openssl)/include \
 OPENSSL_LIBS=-L$(spack location -i openssl)/lib \
 CPPFLAGS="-I$(spack location -i nlohmann-json)/include"
make -j $(nproc)
make install
ml load nix/user # Hooray!
#+END_SRC
** Self hosted binaries
*** Code Server
The first of the selfhosted set is the [[https://github.com/cdr/code-server/][browser based IDE]] inspired by Visual Studio Code.
#+begin_src bash
mkdir -p ~/.local/lib ~/.local/bin
curl -fL https://github.com/cdr/code-server/releases/download/v3.10.2/code-server-3.10.2-linux-amd64.tar.gz \
  | tar -C ~/.local/lib -xz
mv ~/.local/lib/code-server-3.10.2-linux-amd64 ~/.local/lib/code-server-3.10.2
ln -s ~/.local/lib/code-server-3.10.2/bin/code-server ~/.local/bin/code-server
PATH="~/.local/bin:$PATH"
#+end_src

This has the advantage of not needing any port forwarding (with ~--link~):
#+begin_src bash
code-server --link
#+end_src

[fn:whyname] A step in the right direction in terms of female representation amongst the university servers

* TODO Nix Shells for Ruby Projects :@programming:tools:nix:workflow:ruby:
:PROPERTIES:
:EXPORT_FILE_NAME: nix-shell-ruby
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:EXPORT_HUGO_PANDOC_CITATIONS: t
:END:
#+begin_quote
Using ~bundix~ for managing ~ruby~ projects, with a concrete ~jekyll~ example
#+end_quote
** Background
In the same spirit as my similar post on [[Nix Shells for Node Projects][Nix Shells for Node Projects]]. As a prelude to writing up the details of how this site is generated, I realized I should write up a ~nix~ oriented workflow for ~ruby~ packages managed with ~bundle~. This site (which is in ~hugo~), I also manage many beautiful sites built with ~jekyll~. This should serve as a useful reminder.
** Tooling and Idea
The basic concepts are:
- Use ~bundle~ to generate a ~Gemfile.lock~ file
- Use ~bundix~ in a shell to generate a set of ~nix~ derivations
- Enter a shell environment with the ~nix~ inputs
- Test the workflow
- Complete writing a derivation for the output
** Setting up Bundix
We will need to generate a ~gemset.nix~ from our existing ~Gemfile~:
#+begin_src bash
# Get a lock from Gemfile
bundle lock
# Generate gemset
nix-shell -p bundix --run "bundix -l"
# With flakes
nix shell nixpkgs#bundix -c bundix -l
#+end_src
** A Nix Environment
We will use the standard setup described in [[A Tutorial Introduction to Nix][the tutorial post]]:
#+BEGIN_SRC bash
nix-env -i niv lorri
niv init
niv update nixpkgs -b nixpkgs-unstable
#+END_SRC

This is to be in conjunction with the following ~shell.nix~ file [fn:whythis].
#+BEGIN_SRC nix
let
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  stdenv = pkgs.stdenv;
  myGems = pkgs.bundlerEnv {
    name = "gems-for-some-project";
    gemdir = ./.;
  };
in pkgs.mkShell {
  buildInputs = with pkgs; [
    # Shell
    bashInteractive
    direnv
    openssl
    # Build
    git
    curl
    # Ruby
    myGems
    (lowPrio myGems.wrappedRuby) # lowPrio packages will be used last
  ];
  GIT_SSL_CAINFO = "${pkgs.cacert}/etc/ssl/certs/ca-bundle.crt";
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${pkgs.glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

The rationale behind using ~lowPrio~ is described in the [[https://github.com/NixOS/nixpkgs/blob/master/doc/languages-frameworks/ruby.section.md][relevant wiki section]]:
#+begin_quote
One common issue that you might have is that you have Ruby 2.6, but also bundler in your gemset. That leads to a conflict for ~/bin/bundle~ and ~/bin/bundler~. You can resolve this by wrapping either your Ruby or your gems in a lowPrio call.
#+end_quote

We can now enter the environment and setup ~node_modules~[fn:whatlorri].
#+BEGIN_SRC bash
nix-shell
ln -s $NODE_PATH node_modules
#+END_SRC

** Conclusions
This has been a short introduction to working with the ~nix-shell~ ecosystem. It isn't as fast as working with the normal setup, and it is a pretty annoying workflow. Given that most CI setups have good support for caching ~npm~ dependencies, it doesn't seem worthwhile at the moment.

[fn:whatlorri] We can't use ~lorri~ yet since we need to selectively add and remove the symbolic link to ~node_modules~
[fn:whythis] There might be a better approach defined in [[https://github.com/svanderburg/node2nix/issues/175][this issue]] later

* DONE Dotfiles from dotgit to bombadil :@personal:ramblings:tools:workflow:
CLOSED: [2021-05-02 Sun 23:12]
:PROPERTIES:
:EXPORT_FILE_NAME: dotfiles-dotgit-bombadil
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
Discussion on dotfile management, a meandering path to my current setup from ~dotgit~ to ~bombadil~.
#+end_quote
** Background
No one gets very far working with stock one-size fits all tools in any discipline but it is especially true of working with computers. The right set of ~dotfiles~ have been compared to priming spells for invocation later, and this is probably true. More than anything else, ~dotfiles~ offer familiarity where there is none, be it from ~cowsay~ or a fancy shell prompt [fn:saywhatshell].
#+begin_src bash
$ cowsay Welcome home $USER
  ___________________________
< Welcome home rohitgoswami >
 ---------------------------
        \   ^__^
         \  (oo)\_______
            (__)\       )\/\
                ||----w |
                ||     ||
#+end_src

#+RESULTS:

*** Why Now?
#+DOWNLOADED: screenshot @ 2021-04-29 20:34:18
#+name: fig:fighomelocal
#+caption: Home is where the ~dotfiles~ are
[[file:images/Background/2021-04-29_20-34-18_screenshot.png]]

Well more prosaically, I have recently had to partially retire my beloved [[https://www.lenovo.com/us/en/laptops/thinkpad/thinkpad-x/ThinkPad-X380-Yoga/p/22TP2TXX380][ThinkPad X380 Yoga]] [fn:whatwhypart] for a new [[https://support.apple.com/kb/SP809?locale=en_US][MacBook Pro 2019 (Intel)]] [fn:whyapple]. This is the single largest test of my ~Dotfile~ management system, since I now have configurations which are no longer scoped to just a Linux distribution (e.g. ArchLinux and CentOS 6), but are fundamentally not interchangeable.

Complicating matters further, ~dotgit~ sunset the ~bash~ version I have grown to love in favor of a new ~python~ version ([[https://dotgit.readthedocs.io/en/latest/migration_v1.html#reasons-for-rewriting][explained here]]). I also have to manage profiles on more HPC systems than before. The time seemed ripe for a re-haul.

I'll try to justify the Figure [[fig:fighomelocal]] as I dissect and rebuild my ~Dotfiles~ as I switch from ~dotgit~ to ~bombadil~.
** Desiderata
What makes a good ~dotfile~ management system? Some things which are common to most/all good systems:

- *targets* :: These are configurations which are scoped to either machines or operating systems; e.g. ~archlinux~, ~colemak~ etc.
- *profiles* :: The concept of a profile is essentially a set of targets used together; e.g. ~mylaptop~, ~hzhpc~
- *symlinks* :: Most management systems use symlinks to modularly swap configurations in and out; ~ln -sf ....~
- *secrets* :: Commonly implemented (with varying levels of help) in the form of a ~gpg~ encrypted file/files

All of these features are exemplefied by the fantastic ~dotgit~ and no doubt [[https://dotgit.readthedocs.io/en/latest/][its python iteration]] is just as brilliant. However, I am wary of using ~python~ for my ~dotfile~ management, since I tend to use transient ~virtualenvs~ a lot and detest having a system ~python~ for anything.

Over the years, I've come to also value:

- *simplicity* :: Especially true of installation proceedures, I simply need to get started quickly too often
- *template expansion* :: A rare feature to need, but one I've been addicted to since ~lazybones~, ~pandoc~ and even ~orgmode~ brought a million snippets

On a probably technically unrelated note, I have recently been splurging on the "modern" (prettier) ~rust~ versions of standard command line tools; so I normally have ~cargo~ everywhere.
** Starting out with Bombadil
Since [[https://github.com/oknozor/toml-bombadil][toml-bombadil]] is:
- Written in Rust
  -  installs with ~cargo~ as a single binary
- Supports encryption
- Supports profiles
- Has template expansion

It was the obvious choice.
#+begin_src bash
# Get Cargo
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
# Get toml-bombadil
cargo install toml-bombadil
#+end_src

For the rest of this post I'll assume everyone is working with a set of ~dots~ [[https://github.com/HaoZeke/Dotfiles/tree/bombadil][like my own]].
#+begin_src bash
# Get my set
export mydots="$HOME/Git/Github/Dotfiles"
mkdir -p $mydots
git clone git@github.com/HaoZeke/Dotfiles $mydots
cd $mydots
git checkout bombadil
bombadil install -c "$mydots/bombadil.toml"
#+end_src

Now we can start defining profiles in our ~toml~ file and link them.
#+begin_src bash
# Get colemak and mac profiles
bombail link -p macos colemac
#+end_src

This isn't really meant to be a tutorial about ~bombadil-toml~, but it might include some pointers. The remainder of the post will focus on the layout and logic of my own usage.
** Shells
Shells form the core of most computing environments, and typically we would like to have basic support for at least ~bash~ and one additional shell. The secondary shell (~zsh~ for me) is the most preferred, but often might not be avaliable [fn:shellwhysec].
*** Current Logic
Since many shells are somewhat compatible with each other (especially within the [[https://wiki.archlinux.org/title/Command-line_shell][POSIX family]]); my current setup looked a bit like:
#+begin_src bash
. ~/.shellrc # With standard agnostic commands
#+end_src

Which in turn loaded a bunch of conditionally symlinked files from profiles.
#+begin_src bash
# Platform
#############

if [ -f ~/.shellPlatform ]; then
	. ~/.shellPlatform
fi

# Specifics
#############

if [ -f ~/.shellSpecifics ]; then
	. ~/.shellSpecifics
fi

# Wayland
#############

if [ -f ~/.waylandEnv ]; then
	. ~/.waylandEnv
fi

# XKB
######

if [ -f ~/.xkbEnv ]; then
	. ~/.xkbEnv
fi

# Nix
######

if [ -f ~/.nixEnv ]; then
	. ~/.nixEnv
fi
#+end_src

*** Current Approach
The problem with the older approach is that it isn't always clear where different divisions should be drawn and it isn't really flexible enough to add things arbitrarily. Basically, here there were only a few entry points. A more rational method is to emulate the work of ~init.d~ [fn:whereinit] scripts; where a set of files in a directory are all loaded if they exist [fn:adaptionnote].

This allows the previous setup to be instead refactored into the following folder setup (under ~$HOME/.config/shellrc~):
- ~.login.d~ which has machine specific files
  - ~Spack~ and ~Lmod~ modulefiles on various HPC nodes
  - Also just things which normally run only once per login (like system diagnostics)
  - MacOS considers every terminal to be a login shell for some odd reason
- ~.shell.d~ which contains POSIX compliant snippets
  - This is by far the longest section
- ~.bash.d~ for snippets which need ~bashisms~
  - Array operations
- ~.zsh.d~ for snippets which are specific to ~zsh~
  - Plugin management

This then flows very nicely into a smaller set of core ~rc~ files for scripts.
#+begin_src bash
# .bashrc / .zshrc
## Bashism (only for .bashrc)
if [[ $- != *i* ]]; then
	# shell is non-interactive. Do nothing and return
	return
fi
## Zshism (only for .zshrc)
if [[ -o interactive ]]; then
	# non-interactive, return
	return
fi

export shellHome=$HOME/.config/shellrc

# Load all files from the shell.d directory
if [ -d $shellHome/shell.d ]; then
	for file in $shellHome/shell.d/*.sh; do
		source $file
	done
fi

# Load all files from the bashrc.d directory
if [ -d $shellHome/bash.d ]; then
	for file in $shellHome/bash.d/*.bash; do
		source $file
	done
fi
#+end_src

Similarly, we can define our ~zshrc~. For profiles (~.zlogin~ and ~.bash_profile~), we source the ~rc~ files along with the ~login.d~ scripts.
*** Practicalities
This method isn't really very exciting at the offset. Each target has a series of scripts which are loaded in order.
#+begin_src bash
tree .
.
├── bash
│   └── bashrc
├── posix
│   ├── 00_warnings.sh
│   ├── 01_alias_def.sh
│   ├── 02_func_def.sh
│   ├── 03_exports.sh
│   ├── 04_sources.sh
│   ├── 05_paths.sh
│   └── 06_prog_conf.sh
├── tmux
└── zsh

4 directories, 8 files
#+end_src
This is correspondingly linked via the following snippet in ~bombadil.toml~.
#+begin_src toml
# Shells #
# POSIX
posix_warn = { source = "common/shell/posix/00_warnings.sh", target = ".config/shellrc/shell.d/00_warnings.sh" }
posix_alias = { source = "common/shell/posix/01_alias_def.sh", target = ".config/shellrc/shell.d/01_alias_def.sh" }
posix_func = { source = "common/shell/posix/02_func_def.sh", target = ".config/shellrc/shell.d/02_func_def.sh" }
posix_exports = { source = "common/shell/posix/03_exports.sh", target = ".config/shellrc/shell.d/03_exports.sh" }
posix_sources = { source = "common/shell/posix/04_sources.sh", target = ".config/shellrc/shell.d/04_sources.sh" }
posix_paths = { source = "common/shell/posix/05_paths.sh", target = ".config/shellrc/shell.d/05_paths.sh" }
posix_prog_conf = { source = "common/shell/posix/06_prog_conf.sh", target = ".config/shellrc/shell.d/06_prog_conf.sh" }
# Bash
bashrc =  { source = "common/shell/bash/bashrc", target = ".bashrc" }
#+end_src
The same concept (folder structure) is used for specific machines as well (e.g. ~archlinux~).
#+begin_src toml
[profiles.archlinux.dots]
archlinux_warn = { source = "archlinux/shell/posix/07_00_warnings.sh", target = ".config/shellrc/shell.d/07_00_warnings.sh" }
archlinux_alias = { source = "archlinux/shell/posix/07_01_alias_def.sh", target = ".config/shellrc/shell.d/07_01_alias_def.sh" }
archlinux_func = { source = "archlinux/shell/posix/07_02_func_def.sh", target = ".config/shellrc/shell.d/07_02_func_def.sh" }
archlinux_exports = { source = "archlinux/shell/posix/07_03_exports.sh", target = ".config/shellrc/shell.d/07_03_exports.sh" }
archlinux_sources = { source = "archlinux/shell/posix/07_04_sources.sh", target = ".config/shellrc/shell.d/07_04_sources.sh" }
archlinux_paths = { source = "archlinux/shell/posix/07_05_paths.sh", target = ".config/shellrc/shell.d/07_05_paths.sh" }
archlinux_prog_conf = { source = "archlinux/shell/posix/07_06_prog_conf.sh", target = ".config/shellrc/shell.d/07_06_prog_conf.sh" }
#+end_src
Note the way in which the files are saved to ensure the correct loading order. For ~zsh~ the list is a little different:
#+begin_src toml
zshrc = { source = "common/shell/zshrc.zsh", target = ".zshrc" }
zshenv = { source = "common/shell/zsh/zshenv.zsh", target = ".zshenv" }
zsh_keys = { source = "common/shell/zsh/01_keys.zsh", target = ".config/shellrc/zsh.d/01_keys.zsh" }
zsh_func = { source = "common/shell/zsh/02_func_def.zsh", target = ".config/shellrc/zsh.d/02_func_def.zsh" }
zsh_plugins = { source = "common/shell/zsh/03_plugins.zsh", target = ".config/shellrc/zsh.d/03_plugins.zsh" }
zsh_sources = { source = "common/shell/zsh/04_sources.zsh", target = ".config/shellrc/zsh.d/04_sources.zsh" }
zsh_theme = { source = "common/shell/zsh/04a_theme.zsh", target = ".config/shellrc/zsh.d/04a_theme.zsh" }
#+end_src
** Text Editors
A long time ago I switched from VIM and Sublime to Emacs. I still retain in my ~dotfiles~ enough syntactical sugar and targets to make ~vim~ and Sublime easier to use; mostly using [[https://github.com/junegunn/vim-plug][vim-plug]]. Emacs has a rather complicated set of configurations which are independently managed via ~doom-emacs~ and have their own [[https://dotdoom.rgoswami.me][self documenting site]].
** Conclusions
Though the ~shell.d~ setup is still overly verbose and not as flexible as I had initially hoped, this is still a few steps ahead of my previous setup. No part of this focused on the configurations themselves (more interesting examples, of [[Refactoring Dotfiles For Colemak][switching to Colemak are here]]) Will be fleshing this out more with auxilliary posts on my configuration (browsers, etc.) and its management perhaps.
* TODO Unifying UX with Starship
In the absence of administrative controls, it makes sense to have our customizations work at the lowest common abstraction. Thankfully, projects like [[https://starship.rs/guide/#%F0%9F%9A%80-installation][starship]] have stepped up to provide a unified experience across shells. It will form the core of the new setup.
* TODO Browser Wars
Though most browsers (Firefox and Chrome come to mind) have plenty of cloud synchronization services. There are still things which read off local configurations. For me, Firefox is better mainly due to [[https://addons.mozilla.org/en-US/firefox/addon/tree-style-tab/][tree style tabs]] and [[https://github.com/tridactyl/tridactyl/][Tridactyl]] which are superior to Chrome options, but Chrome supports translations *a lot more* than Firefox, so I do end up using both [fn:whatlang].

| *Feature*          | *Chrome*           | *Firefox*                 | *Comments*                                         |
| Tree Tabs          | [[https://chrome.google.com/webstore/detail/tabs-outliner/eggkanocgddhmamlbiijnphhppkpkmkl][Tabs Outliner]]      | [[https://addons.mozilla.org/en-US/firefox/addon/tree-style-tab/][Tree style tabs (TST)]] [★] | Painfully underdeveloped and unfree in Chrome      |
| New Tab Vocabulary | [[https://chrome.google.com/webstore/detail/magoosh-vocabulary/oooelhhaglnggehlocjjmgngfknfclak][Magoosh Vocabulary]] | [[https://addons.mozilla.org/en-US/firefox/addon/new-tab-vocab/?utm_source=addons.mozilla.org&utm_medium=referral&utm_content=search][New Tab Vocab]]  [★]        | Both good, but cleaner in FF (has a [[https://github.com/ezekg/chrome-new-tab-vocab/tree/e0d6d45b9eecb7bbb32a045b6f55d423da9a3724][Chrome port]])   |
| VIM Navigation     | [[https://chrome.google.com/webstore/detail/vimium/dbepggeogbaibhgnhhndojpepiihcmeb][Vimium]]             | [[https://github.com/tridactyl/tridactyl][Tridactyl]]  [★]            | Both usable, but Tridactyl is closer to Vimperator |

Naturally, this is not meant to discuss each and every one of my addon choices, those are found [[Chrome Extensions][here for Chrome]]; and [[https://addons.mozilla.org/en-US/firefox/collections/13625553/rgff/?page=1&collection_sort=-added][here for Firefox]]. From a ~dotfiles~ perspective, addon configuration files like ~.tridactylrc~ are unified in a ~browsers~ target while the standard account sync is leveraged to keep track of installations (profile symlinking is no longer recommended as a viable method).

** Appendix
*** Chrome Extensions
Generated [[https://chrome.google.com/webstore/detail/export-links-of-all-exten/cmeckkgeamghjhkepejgjockldoblhcb/related][with this extension]].
**** Enabled
   :PROPERTIES:
   :CUSTOM_ID: enabled
   :CLASS: enabled
   :END:
1.  [[https://chrome.google.com/webstore/detail/cmedhionkhpnakcndndgjdbohmhepckk][Adblock
    for Youtube™]]
3.  [[https://chrome.google.com/webstore/detail/jfnlkegibnoaagfdabjkchhocdhnoofk][Arxiv
    Vanity Plugin]]
4.  [[https://chrome.google.com/webstore/detail/hkhggnncdpfibdhinjiegagmopldibha][Checker
    Plus for Google Calendar™]]
5.  [[https://chrome.google.com/webstore/detail/jlhmfgmfgeifomenelglieieghnjghma][Cisco
    Webex Extension]]
6.  [[https://chrome.google.com/webstore/detail/aohghmighlieiainnegkcijnfilokake][Docs]]
7.  [[https://chrome.google.com/webstore/detail/fngmhnnpilhplaeedifhccceomclgfbg][EditThisCookie]]
8.  [[https://chrome.google.com/webstore/detail/kmcfomidfpdkfieipokbalgegidffkal][Enpass
    extension (requires desktop app)]]
9.  [[https://chrome.google.com/webstore/detail/cmeckkgeamghjhkepejgjockldoblhcb][Export
    links of all extensions]]
10. [[https://chrome.google.com/webstore/detail/kjacjjdnoddnpbbcjilcajfhhbdhkpgk][Forest:
    stay focused, be present]]
11. [[https://chrome.google.com/webstore/detail/pjkljhegncpnkpknbcohdijeoejaedia][Gmail]]
12. [[https://chrome.google.com/webstore/detail/ghbmnnjooekpmoecnnnilnnbdlolhkhi][Google
    Docs Offline]]
13. [[https://chrome.google.com/webstore/detail/apdfllckaahabafndbhieahigkjlhalf][Google
    Drive]]
14. [[https://chrome.google.com/webstore/detail/cmplbmppmfboedgkkelpkfgaakabpicn][Gridman -
    CSS Grid inspector. Ultra Fast!]]
15. [[https://chrome.google.com/webstore/detail/cchaceegbflphbdpfocjalgjhjoahiia][HN
    Special - An addition to Hacker News]]
16. [[https://chrome.google.com/webstore/detail/bjfhmglciegochdpefhhlphglcehbmek][Hypothesis -
    Web & PDF Annotation]]
17. [[https://chrome.google.com/webstore/detail/kglhbbefdnlheedjiejgomgmfplipfeb][Jitsi
    Meetings]]
18. [[https://chrome.google.com/webstore/detail/aghfnjkcakhmadgdomlmlhhaocbkloab][Just
    Black]]
19. [[https://chrome.google.com/webstore/detail/pncfaijgefbolbapchmpkfkjpgjelclh][Last.fm
    Scrobbler YouTube Canary]]
20. [[https://chrome.google.com/webstore/detail/ddoflfjcbemgfgpgbnlmaedfkpkfffbm][Librarian
    for arXiv | Fermat's Library]]
21. [[https://chrome.google.com/webstore/detail/liecbddmkiiihnedobmlmillhodjkdmb][Loom
    for Chrome]]
22. [[https://chrome.google.com/webstore/detail/oocalimimngaihdkbihfgmpkcpnmlaoa][Netflix
    Party is now Teleparty]]
23. [[https://chrome.google.com/webstore/detail/bpconcjcammlapcogcnnelfmaeghhagj][Nimbus
    Screenshot & Screen Video Recorder]]
24. [[https://chrome.google.com/webstore/detail/lcilaoigfgceebdljpanjenhmnoijmal][Octopatcher]]
25. [[https://chrome.google.com/webstore/detail/chphlpgkkbolifaimnlloiipkdnihall][OneTab]]
26. [[https://chrome.google.com/webstore/detail/kkkjlfejijcjgjllecmnejhogpbcigdc][Org
    Capture]]
27. [[https://chrome.google.com/webstore/detail/jplfopfpninjjpilebidfeboioagafjf][Pulse
    SMS]]
28. [[https://chrome.google.com/webstore/detail/ldgfbffkinooeloadekpmfoklnobpien][Raindrop.io]]
29. [[https://chrome.google.com/webstore/detail/bfhkfdnddlhfippjbflipboognpdpoeh][Read
    on reMarkable]]
30. [[https://chrome.google.com/webstore/detail/odpdkefpnfejbfnmdilmfhephfffmfoh][Readlang
    Web Reader]]
31. [[https://chrome.google.com/webstore/detail/bdakmnplckeopfghnlpocafcepegjeap][RescueTime
    for Chrome and Chrome OS]]
32. [[https://chrome.google.com/webstore/detail/niloccemoadcdkdjlinkgdfekeahmflj][Save
    to Pocket]]
33. [[https://chrome.google.com/webstore/detail/lkhjgdkpibcepflmlgahofcmeagjmecc][Scener
    -- Virtual Movie Theater]]
34. [[https://chrome.google.com/webstore/detail/mmeijimgabbpbgpdklnllpncmdofkcpn][Screencastify -
    Screen Video Recorder]]
35. [[https://chrome.google.com/webstore/detail/edacconmaakjimmfgnblocblbcdcpbko][Session
    Buddy]]
36. [[https://chrome.google.com/webstore/detail/felcaaldnbdncclmgdcncolpebgiejap][Sheets]]
37. [[https://chrome.google.com/webstore/detail/aapocclcgogkmnckokdopfmhonfmgoek][Slides]]
38. [[https://chrome.google.com/webstore/detail/ipikiaejjblmdopojhpejjmbedhlibno][SwiftRead -
    read faster, learn faster]]
39. [[https://chrome.google.com/webstore/detail/eggkanocgddhmamlbiijnphhppkpkmkl][Tabs
    Outliner]]
40. [[https://chrome.google.com/webstore/detail/cbimabofgmfdkicghcadidpemeenbffn][TeX
    All the Things]]
41. [[https://chrome.google.com/webstore/detail/plhnjpnbkmdmooideifhkonobdkgbbof][Time
    Zone Converter - Savvy Time]]
42. [[https://chrome.google.com/webstore/detail/hddnkoipeenegfoeaoibdmnaalmgkpip][Toby
    for Chrome]]
43. [[https://chrome.google.com/webstore/detail/twoseven-extension/cjdnfmjmdligcpfcekfmenlhiopehjkd][TwoSeven Extension]]
44. [[https://chrome.google.com/webstore/detail/jgeocpdicgmkeemopbanhokmhcgcflmi][Twitter]]
45. [[https://chrome.google.com/webstore/detail/djkbihbnjhkjahbhjaadbepppbpoedaa][Typio
    Form Recovery]]
46. [[https://chrome.google.com/webstore/detail/cjpalhdlnbpafiamejdnhcphjbkeiagm][uBlock
    Origin]]
47. [[https://chrome.google.com/webstore/detail/dbepggeogbaibhgnhhndojpepiihcmeb][Vimium]]
48. [[https://chrome.google.com/webstore/detail/bppamachkoflopbagkdoflbgfjflfnfl][WebRTC
    Leak Shield]]
49. [[https://chrome.google.com/webstore/detail/hnmpcagpplmpfojmgmnngilcnanddlhb][Windscribe -
    Free Proxy and Ad Blocker]]
50. [[https://chrome.google.com/webstore/detail/fiombgjlkfpdpkbhfioofeeinbehmajg][Word
    Online]]
51. [[https://chrome.google.com/webstore/detail/blpcfgokakmgnkcojhhkbfbldkacnbeo][YouTube]]
52. [[https://chrome.google.com/webstore/detail/ekhagklcjbdpajgpjgmbionohlpdbjgc][Zotero
    Connector]]

**** Disabled
   :PROPERTIES:
   :CUSTOM_ID: disabled
   :CLASS: disabled
   :END:
1. [[https://chrome.google.com/webstore/detail/pnhplgjpclknigjpccbcnmicgcieojbh][Diigo
   Web Collector - Capture and Annotate]]
2. [[https://chrome.google.com/webstore/detail/aapbdbdomjkkjkaonfhkkikfgjllcleb][Google
   Translate]]
3. [[https://chrome.google.com/webstore/detail/oooelhhaglnggehlocjjmgngfknfclak][Magoosh
   Vocabulary]]
4. [[https://chrome.google.com/webstore/detail/nbdfpcokndmapcollfpjdpjlabnibjdi][Saka]]
5. [[https://chrome.google.com/webstore/detail/hhhpdkekipnbloiiiiaokibebpdpakdp][Saka
   Key]]
6. [[https://chrome.google.com/webstore/detail/iomokcfebnfiflpgcpcijfkfmafgkjgh][Wakelet]]

[fn:whatlang] I don't speak Icelandic
[fn:adaptionnote] A little bit of googling around showed that [[https://chr4.org/posts/2014-09-10-conf-dot-d-like-directories-for-zsh-slash-bash-dotfiles/][chris or chr4]] had worked the idea trough to its logical conclusion before, so I essentially adapted it
[fn:whereinit] Perhaps better known in the context of ~udev.d~ [[https://wiki.archlinux.org/title/Udev][startup scripts]]
[fn:shellwhysec] Even more true if one wants something ~elvish~ (details [[https://elv.sh/][here]]) or [[https://wiki.archlinux.org/title/Command-line_shell][other shells]]
[fn:saywhatshell] ~bash~ defaults are particularly egregious
[fn:whatwhypart] Water damage means I can't type on it anymore, works as a great tablet now
[fn:whyapple] A gift, and therefore cherished inspite of the vagrancies of Apple ~clang~
* TODO Keybindings and MacOS
* TODO Comments: Graphcomment to Remarkbox :@notes:tools:rationale:webdev:
* TODO Fortran and Doxygen
#+begin_quote
An exploration of ~fortran~ and ~doxygen~
#+end_quote
** Background
Fortran has a perfectly capable documentation tool, the [[https://github.com/Fortran-FOSS-Programmers/ford][FORtran Documenter (FORD)]], however, I have a slight personal preference for [[https://doxygen.org][Doxygen]], especially since I am the author of [[https://github.com/HaoZeke/doxyYoda][doxyYoda]], a nice ~doxygen~ theme, it seemed to be a good time to revisit Doxygen's FORtran support.

Note that the ~doxygen~ documentation makes no mention of FORtran other than to mention that it is supported.
*** Licensing
Both ~ford~ and ~doxygen~ are licensed under the GPL (v3 and v2 respectively), however, they include the following disclaimers on the project pages:
#+begin_quote
Documents produced by FORD are derivative works derived from the input used in their production; they are not affected by this license.

Documents produced by doxygen are derivative works derived from the input used in their production; they are not affected by this license.
#+end_quote
** Modules
~@file~ does not seem to be supported.

* TODO Page and Tree Bundles with ox-hugo :@notes:tools:rationale:webdev:
:PROPERTIES:
:EXPORT_FILE_NAME: page-tree-ox-hugo-bundles
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
A discussion and demonstration of the restructuring of my ox-hugo site to use page and tree bundles
#+end_quote
** Background
I have been running this site of mine for over a year now, and in that time certain collections of articles have arisen. This marks an additional layer of existing categorization. I use ~tags~ and ~categories~ at the moment; but the concept of a collection allows for an introduction followed by a set of articles. This logical structure is facilitated in ~hugo~ with the usage of tree and leaf bundles.

* DONE Fortran, GSoC21 and Me :@personal:history:ramblings:workflow:fortran:
CLOSED: [2021-05-21 Fri 05:17]
:PROPERTIES:
:EXPORT_FILE_NAME: fortran-gsoc-me
:EXPORT_HUGO_PANDOC_CITATIONS: t
:EXPORT_BIBLIOGRAPHY: biblio/refs.bib
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true :link-citations true
:END:
#+begin_quote
Reminisces and prophecies brought upon by [[https://lfortran.org/][LFortran]] and [[https://summerofcode.withgoogle.com/projects/#5790375215628288][GSoC 2021]]
#+end_quote
** Background
#+begin_quote
I know not what the language of the future will look like, but I know it will be called FORTRAN... <br>
–- Charles Anthony Richard Hoare, circa 1982 <br>
-- Daan Frenkel, 2020
#+end_quote

This post is a little belated, given that the GSoC announcements were a few days ago. Therein lies the future, and much digital ink, sweat and blood shall be spilled towards accomplishing the goals outlined in my [[https://summerofcode.withgoogle.com/projects/#5790375215628288][accepted project proposal]]. However, interestingly, it was an offhand comment at a [[https://www.cecam.org/lectures-categories/cecam-marvel-classics][CECAM MARVEL Classics webinar]] on free energy of solids [fn:whatcomp] which triggered this.
** Before Fortran..
Formula translation, or FORTRAN was not my first programming language. I grew up with the Logo turtle, and progressed naturally into C for coursework (as was the tradition back during my schooldays) and C++ for monkeying around with my Xperia devices (mostly Cyanogenmod stuff, some LineageOS). I shifted rather awkwardly in and out of Java (also a popular mainstay in different schools) and decided at some point during high school to pick up a high level language. I choose Ruby, because it was cuter than Python (still is!) and I had just been bitten by the web-design bug (Jekyll!!! Ruby on Rails!).

I was not unaware of Fortran, but I had little to no inclination to learn it, at the time, in my youth I was only interested in systems programming, web-design and other practical, visual activities.
** Towards Fortran
As I grew older, my interest in crass physical things waned and my interest in the cool, calculating and sly discipline of simulation; fortune telling with confidence and code kept pace.

I *almost* came into contact with Fortran when I mistakenly stumbled onto [[http://www.quantum-espresso.org/][Quantum Espresso]] in the course of my first summer undergraduate research project under Prof. Rajarshi Chakrabarti at IIT Bombay back in 2014....

Almost, but not quite, for I was hunting models for soft matter (eventually found them too @theeyancheriTranslationalRotationalDynamics2020 ), and [[https://espressomd.org/wordpress/about/features/][EsPReSSO MD]] *is not* Quantum Espresso.
** First Encounter
I finally stumbled upon FORTRAN in all its free form ~f90~ glory in the form of [[https://bigdft.org/][BigDFT]] during my stint in the 2015 Students-Undergraduate Research Graduate Excellence (SURGE) program of IIT Kanpur. A mere six years ago.

By then I had been programming, as perhaps many do, largely on the largess of the internet. It was my source of information, and even seeded some of my opinions. I immediately set out to discover as much about Fortran as I could. Back then I failed miserably. For the first time, I was confronted only with ridiculously ugly ~F77~ code examples, and it seemed abundantly evident that most questions and projects were clearly related to coursework (poorly taught coursework no less).

The only ray of sunshine in a mess of poor content was Ondřej Čertík's [[https://www.fortran90.org/][fortran90.org]]. I knew better than to write off Fortran though, I knew even then its pedigree (impossible to configure [[https://github.com/lammps/lammps][LAMMPS]] without touching upon [[http://www.netlib.org/blas/][BLAS]] and [[http://performance.netlib.org/lapack/][LAPACK]]...) [fn:whyelse].

I was not a chemist, and my dealings with elegant EsPReSSo and LAMMPS in no way prepared me for the horrific mess which was (and is) the codes of ab-initio chemistry. I was however, introduced to @redwineUpgradingFortran901995 .
** Fortran the stable
During my third year, I attempted to submit a project written in Julia. I spent the next couple of months convincing the instructor of the fact that Julia was a legitimate language and not a figment of my imagination. Little wonder then, when the time came to submit an undergraduate thesis, much of the computations were was inspired by the venerable Fortran codes of @kayodeFortranProgramsChemical1995 .
** Ice, Fire and Fortran
The fortunate circumstances behind my position here in Iceland, are best left to other posts. One of my tasks, was the migration of existing code for the Single Center Multipole Expansion water model @wikfeldtTransferableH2OInteraction2013 from Fortran (free form, but not modern) to ~C++~ or at-least to write ~ISO_C_BINDING~ wrappers for incorporation into existing ~C++~ libraries (with an eye towards LAMMPS integration). At the time, given my projects in ~C++~ [@goswamiDSEAMSDeferredStructural2020] and passable familiarity with Fortran, this seemed like an optimal use of time.

However, the force-field was and remains to be heavily under development @jonssonTransferablePotentialFunction2020, @jonssonPolarizableEmbeddingTransferable2019, which made anything less than a hard fork unfeasible. Still on the books however, *another Fortran project* to look forward to!
** Quantum Chemistry and Fortran
With collaborators at SURFSara I was involved in the port from MATLAB to C++, Gaussian Process Regression models @koistinenNudgedElasticBand2019, @koistinenNudgedElasticBand2017, @koistinenMinimumModeSaddle2020, @koistinenAlgorithmsFindingSaddle2019  for accelerating saddle searches and coupling said methodologies to EON @chillEONSoftwareLong2014 for long time scales. This was under my brilliant advisor's [[https://www.reaxpro.eu/][ReaxPro project]] who also guided and encouraged me towards applying for a fellowship. As befitting a multiscale modeling project, this lead to an NDA and tweaks towards efficient workflows using potentials from the [[https://www.scm.com/][Amsterdam Modeling Suite]], which naturally, is *written in Fortran*.
** Tensors, IPAM and Fortran
Much [[https://fortran-lang.discourse.group/t/why-abandon-fortran-for-linear-algebra/1191/6?u=rgoswami][has been said]] about [[https://www.nwchem-sw.org][NWChem]]'s [[https://www.hpcwire.com/2021/04/29/nwchemex-computational-chemistry-code-for-the-exascale-era/][pivot towards]] ~C++~. However, this has quite a bit to do with [[https://github.com/pnnl/HiParTI][HiParTI]] (a tensor library) being in ~C++~. To throw away Fortran simply because Tensors are as yet easier to implement in other languages is treating a symptom, not fixing the cause. In any case, over the course of an [[http://www.ipam.ucla.edu/programs/long-programs/tensor-methods-and-emerging-applications-to-the-physical-and-data-sciences/?tab=participant-list][IPAM fellowship]] (supported by my advisor) I was able to gain a broad overview of the underlying numerical complaints driving the schism; and there are already fantastic libraries which can be leveraged for multilinear algebra. Fortran is still one of the only language to have standardized parallel programming paradigms, and in the long run there is no way that won't count in its favor.
** Fortran Lang and LFortran
Around the same time frame, I was lucky enough to be involved in the documentation for [[https://symengine.org][SymEngine]], and from there I was poised to witness the rapid rejuvenation and development of the Fortran ecosystem spearheaded in no small part by Ondřej Čertík (a fantastic person, mentor, and scientist; conversant in both algorithmic topics and quantum chemistry). ~LFortran~ is perhaps the most path-breaking of all the existing drives to fight the perceived obsolesce of Fortran. By bringing interactivity to the masses, the natural beauty of Fortran (recently reaffirmed by [[https://milancurcic.com/book/][Milan Curic's book]]) can come to the fore.

The Google Summer of Code project blends both my deep rooted interest in language semantics and flexible implementations; while also whetting my academic appetite by being so closely tied to the topics I love. Working on ~dftatom~ (and related quantum chemistry codes) while gaining familiarity with the LLVM based LFortran is fantastic. Being able to work closely with the [[https://fortran-lang.org/newsletter/2021/05/18/Welcome-GSoC-students/][rest of the team behind Fortran lang]] to bring about an "age of Fortran, as promised twenty years ago" is one of the most exciting premises I can imagine.
** Conclusions
The picture painted here, like all nostalgic reminisces, is colored and deeply biased. My love for modern ~C++~ and elegant ~R~, my forays into other languages, none of these are mentioned. The sheer *joy* of using modern Fortran, of projects with ~fpm~; is lost in translation. It is true, that my work, has more often involved production level ~C++~ over Fortran, however, given everything, I can be certain of one thing:
#+begin_quote
The future is Fortran!
(Or at-least involves a lot of Fortran)
#+end_quote

[fn:whyelse] Also a mentor and inspiration of mine (within my family) used Fortran for graduate research...
[fn:whatcomp] I am after-all, actually secretly a computational chemist, whatever my degrees and blog posts might imply..
* DONE Wrapping Up Code in Place 2021 :@notes:ideas:teaching:cs106a:
CLOSED: [2021-05-21 Fri 02:29]
:PROPERTIES:
:EXPORT_FILE_NAME: cip2021-fin
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
Short recollections of the second iteration of CIP; the course with the largest teaching (and voluntary) staff bringing the joy of programming to thousands
#+end_quote
** Background
Last year in 2020, as a section leader for the Stanford Code in Place initiative (which generated a series of posts), I was able to hone my teaching skills and bask in the reflected glory of many talented students who were in my Code in Place section. This year I was honored to return not only to lead a section, but also to act as a teaching mentor to a group of first time section leaders.

#+DOWNLOADED: screenshot @ 2021-05-21 01:48:21
#+name: fig:sectionleaders
#+caption: 1,100 wonderful teachers across the world...
[[file:images/Background/2021-05-21_01-48-21_screenshot.png]]

There are still the student projects to look forward to, and like last year, there shall be some workshops, which I might also keep track of. This is then a slightly premature post, however, I just signed off on my final session (optional presentation aside) and had a chat with some returning staff, so now is a good nostalgic moment to capture.

Once again (Fig. [[fig:sectionleaders]]) I had the dubious pleasure of being the only member of the teaching team (and student body) located in Iceland. This year we did also have a student on a fishing boat in Antarctica, so that was fascinating. There was a pretty fantastic age distribution (Fig. [[fig:agedist][fig:agedist]]) as well.


#+DOWNLOADED: screenshot @ 2021-05-21 02:22:16
#+name: fig:agedist
#+caption: Learner distribution
[[file:images/Background/2021-05-21_02-22-16_screenshot.png]]

** Teaching Mentorship
I was assigned a fantastic co-mentor, who turned out to be from the same undergraduate college I was at! (not during the same time period of course). Mentorship (training and delivery) was a lot of fun. Me and my co-mentor met a few times before and after to work on projects of mutual interest as well. All the section leaders had stars in their eyes and were super interactive. The sessions were packed to the gills and our Ed forum saw our shared camaraderie grow throughout the 5 week long course. I was also able to reconnect with some of the other stalwart section leaders who returned as teaching mentors.

- This [[https://docs.google.com/document/d/13RZzvY_9WTR_sjo_Y4oBNchsAWAv_z6kSJ9395snANU/preview][CC-BY-NC-SA 4.0 licensed document]] sums up the goals instilled in the new section leaders.
- I also often plug Greg Wilson's [[http://teachtogether.tech/en/index.html][Teaching Tech Together]]
** Differences
- As section leaders, we were allowed to nominate up to three students who would join the program
- No video recordings of the section were allowed
  - A contentious decision at first, which may have contributed to an apparent dip in student participation
  - Students gave permission for anonymized recordings of the OhYay sessions to allow for data mining
- Instead of asking the section leaders to manage video platforms, the team set up OhYay instances (managed by Julie)
  - The OhYay instance for the teacher lounge was particularly enjoyable

#+DOWNLOADED: screenshot @ 2021-05-21 02:05:43
#+caption: Scenes from the teacher lounge and watch party
[[file:images/Differences/2021-05-21_02-06-31_screenshot.png]]
[[file:images/Differences/2021-05-21_02-05-43_screenshot.png]]
** Tools
This year, coming at the tail end of an intensive year of online teaching experiences, helped me zero in on a series of tools I used throughout:
- [[https://dateful.com/eventlink][Starts at (now Dateful)]] for timezone agnostic links
- [[https://www.strawpoll.com/][Straw Poll]] for engagement
- [[https://jamboard.google.com/][Jamboard]] for persistent communal scribbles
- [[http://hackmd.io/][HackMD]] for collaorative notes and pseudocode
- [[https://www.jitbit.com/screensharing][JitBit]] for sharing screen
  - Didn't get around to needing this, there was an early bug in OhYay which was ironed out
** Surprises
- This year's set seemed to go by very quickly. Perhaps this can be partially attributed to the fact that the material was no longer new to me; however I did take on additional responsibilities. I can only conjecture that last year Zoom meetings were a novelty.
- There was a slightly grimmer tone throughout some of the forums, but the team (Chris, Mehran, Kylie, Julie, Brahm and everyone else) were as upbeat and inspiring as ever
** Conclusions
Last year's Code in place mattered a lot to me. It was absolutely fantastic to see and interact with such a vibrant community again; there were some odd hiccups, including some in-sensitivities and incivilities, but nothing major. This year in fact, the program meant even more to me; my mother, who has long been (like the rest of my family) an inspiration and friend, joined and was absolutely incredibly dedicated, hardworking and creative throughout.

I cannot speak to the future, but I know I shall probably once again have the CIP calendar sit unused till the next year, hoping to be called back to partake in the joys of teaching motivated students with brilliant colleagues.

* DONE Biblatex to Bibtex for Sphinx :@notes:tools:workflow:tex:documentation:
CLOSED: [2021-05-23 Sun 05:37]
:PROPERTIES:
:EXPORT_FILE_NAME: zotero-biblatex-bibtex
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
*** Background
#+begin_quote
Transitioning from ~biblatex~ to ~bibtex~ with ~biber~ for ~sphinx~
#+end_quote

I recently started a [[https://ipam21.netlify.app/][set of notes]] using ~jupyter-book~. However, in the process I ran into a horrific bibliography related SNAFU. ~sphinx~ in its infinite wisdom only accepts a rather odd subset of ~bibtex~.

I have been happily exporting my giant bibliography with [[https://www.zotero.org/][Zotero]] (and [[https://retorque.re/zotero-better-bibtex/][better bibtex]]) exporting my references as ~biblatex~ while ~sphinx~ started choking dreadfully. This post describes attempts to reconcile the ~biblatex~ sources without manual intervention.
*** Input
The input file was an innocuous looking tiny neat ~biblatex~ file:
#+begin_src bibtex
@online{grasedyckParameterdependentSmootherMultigrid2020,
  title = {A Parameter-Dependent Smoother for the Multigrid Method},
  author = {Grasedyck, Lars and Klever, Maren and Löbbert, Christian and Werthmann, Tim A.},
  date = {2020-08-03},
  url = {http://arxiv.org/abs/2008.00927},
  urldate = {2021-05-22},
  archiveprefix = {arXiv},
  eprint = {2008.00927},
  eprinttype = {arxiv},
  keywords = {65N55; 15A69,Mathematics - Numerical Analysis},
  primaryclass = {cs, math}
}

@article{grasedyckDistributedHierarchicalSVD2018,
  title = {Distributed Hierarchical {{SVD}} in the {{Hierarchical Tucker}} Format},
  author = {Grasedyck, Lars and Löbbert, Christian},
  date = {2018},
  journaltitle = {Numerical Linear Algebra with Applications},
  volume = {25},
  pages = {e2174},
  issn = {1099-1506},
  doi = {10.1002/nla.2174},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nla.2174},
  urldate = {2021-05-22},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nla.2174},
  keywords = {Hierarchical Tucker,HT,multigrid,parallel algorithms,SVD,tensor arithmetic},
  langid = {english},
  number = {6}
}

@article{grasedyckHierarchicalSingularValue2010,
  title = {Hierarchical {{Singular Value Decomposition}} of {{Tensors}}},
  author = {Grasedyck, Lars},
  date = {2010-01-01},
  journaltitle = {SIAM Journal on Matrix Analysis and Applications},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  volume = {31},
  pages = {2029--2054},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0895-4798},
  doi = {10.1137/090764189},
  url = {https://epubs.siam.org/doi/abs/10.1137/090764189},
  urldate = {2021-05-22},
  number = {4}
}
#+end_src
*** Manual cleaning
The first attempt was with ~bibutils~.
#+begin_src bash
biblatex2xml references.bib | xml2bib | tee  refs.bib
#+end_src
~sphinx~ does not support ~electronic~ and demands ~misc~ instead.
#+begin_src bash
biblatex2xml references.bib | xml2bib | sed -e "s/Electronic{/misc{/g" | tee  refs.bib
#+end_src

This brought us to:

#+begin_src bibtex
@misc{grasedyckParameterdependentSmootherMultigrid2020,
author="Grasedyck, Lars
and Klever, Maren
and L{\"o}bbert, Christian
and Werthmann, Tim A.",
title="A Parameter-Dependent Smoother for the Multigrid Method",
year="2020",
month="Aug",
day="03",
archivePrefix="arXiv",
eprint="2008.00927",
url="http://arxiv.org/abs/2008.00927"
}

@Article{grasedyckDistributedHierarchicalSVD2018,
author="Grasedyck, Lars
and L{\"o}bbert, Christian",
title="Distributed Hierarchical SVD in the Hierarchical Tucker Format",
journal="Numerical Linear Algebra with Applications",
year="2018",
volume="25",
number="6",
pages="e2174",
issn="1099-1506",
doi="10.1002/nla.2174",
url="https://onlinelibrary.wiley.com/doi/abs/10.1002/nla.2174",
url="https://doi.org/10.1002/nla.2174"
}

@Article{grasedyckHierarchicalSingularValue2010,
author="Grasedyck, Lars",
title="Hierarchical Singular Value Decomposition of Tensors",
journal="SIAM Journal on Matrix Analysis and Applications",
year="2010",
month="Jan",
day="01",
volume="31",
number="4",
pages="2029--2054",
issn="0895-4798",
doi="10.1137/090764189",
url="https://epubs.siam.org/doi/abs/10.1137/090764189",
url="https://doi.org/10.1137/090764189"
}
#+end_src

Unfortunately this also generated multiple ~url~ lines, which choke ~sphinx~ of course. Additionally, the workflow is rather fickle e.g. not starting the file with a blank line causes the first reference to be silently skipped.
*** Biber
The key concept here is that ~biber~ can take a configuration file to process the ~bib~ file and is also able to copy the formatted file over with ~tool~. So our solution will boil down to writing a configuration file and then using it.

First we get the location of the ~configuration~ (also check the ~version~).
#+begin_src bash
biber --tool-config
biber --version
#+end_src

#+RESULTS:
| /tmp/par-726f686974676f7377616d69/cache-bc6986241d41958a757fc1df8a704e6d22b2f953/inc/lib/Biber/biber-tool.conf |          |      |
| biber                                                                                                          | version: | 2.16 |

Then we inspect it and extract relevant entries to a configuration file of our own, with some modifications.
- We need to reverse each entry as [[https://tex.stackexchange.com/a/114803/130845][described here]]
  - However in practice we can get away with a minimal mapping and some command line options..
- We would also like to be mindful of the mapping from ~online~ to ~misc~
#+begin_src xml
<?xml version="1.0" encoding= "utf-8"?>
<config>
  <sourcemap>
    <maps datatype="bibtex">
      <!-- Easy type conversions -->
      <map>
        <map_step map_type_source="report" map_type_target="techreport"/>
        <map_step map_type_source="online" map_type_target="misc"/>
      </map>
    </maps>
  </sourcemap>
</config>
#+end_src
At this stage, we are partially done, but the resulting file is rather ugly, and *most importantly* dates have not yet been transformed. This [[https://tex.stackexchange.com/a/299338/130845][SE answer]] provided a hint, as did [[https://gist.githubusercontent.com/mkouhia/f00fea7fc8d4effd9dfd/raw/500e9dbc6aa43a47e39c45ba230738ff4544709f/biblatex-to-bibtex.conf][this gist]]. Effectively, we need to overwrite part of the map.
#+begin_src xml
      <!-- Date to year, month -->
      <map>
        <map_step map_field_source="date"
        map_field_target="year" />
      </map>
      <map>
        <map_step map_field_source="year"
        map_match="(\d{4}|\d{2})-(\d{1,2})-(\d{1,2})"
        map_final="1" />
        <map_step map_field_source="year"
        map_match="(\d{4}|\d{2})-(\d{1,2})-(\d{1,2})"
        map_replace="$1" />
        <map_step map_field_set="month" map_origfieldval="1" />
        <map_step map_field_source="month"
        map_match="(\d{4}|\d{2})-(\d{1,2})-(\d{1,2})"
        map_replace="$2" />
      </map>
      <map>
        <map_step map_field_source="year"
        map_match="(\d{4}|\d{2})-(\d{1,2})" map_final="1" />
        <map_step map_field_source="year"
        map_match="(\d{4}|\d{2})-(\d{1,2})" map_replace="$1" />
        <map_step map_field_set="month" map_origfieldval="1" />
        <map_step map_field_source="month"
        map_match="(\d{4}|\d{2})-(\d{1,2})" map_replace="$2" />
      </map>
#+end_src

Now we can also add some pretty printing (before the ~sourcemap~):
#+begin_src xml
  <output_fieldcase>lower</output_fieldcase>
  <output_resolve>1</output_resolve>
  <output_safechars>1</output_safechars>
  <output_format>bibtex</output_format>
#+end_src

Putting it all together:
#+begin_src xml
<?xml version="1.0" encoding="utf-8"?>
<!-- Got the date from https://gist.githubusercontent.com/mkouhia/f00fea7fc8d4effd9dfd/raw/500e9dbc6aa43a47e39c45ba230738ff4544709f/biblatex-to-bibtex.conf -->
<config>
  <output_fieldcase>lower</output_fieldcase>
  <output_resolve>1</output_resolve>
  <output_safechars>1</output_safechars>
  <output_format>bibtex</output_format>
  <sourcemap>
    <maps datatype="bibtex">
      <!-- Easy type conversions -->
      <map>
        <map_step map_type_source="report" map_type_target="techreport"/>
        <map_step map_type_source="online" map_type_target="misc"/>
      </map>
      <!-- Date to year, month -->
      <map>
        <map_step map_field_source="date"
        map_field_target="year" />
      </map>
      <map>
        <map_step map_field_source="year"
        map_match="(\d{4}|\d{2})-(\d{1,2})-(\d{1,2})"
        map_final="1" />
        <map_step map_field_source="year"
        map_match="(\d{4}|\d{2})-(\d{1,2})-(\d{1,2})"
        map_replace="$1" />
        <map_step map_field_set="month" map_origfieldval="1" />
        <map_step map_field_source="month"
        map_match="(\d{4}|\d{2})-(\d{1,2})-(\d{1,2})"
        map_replace="$2" />
      </map>
      <map>
        <map_step map_field_source="year"
        map_match="(\d{4}|\d{2})-(\d{1,2})" map_final="1" />
        <map_step map_field_source="year"
        map_match="(\d{4}|\d{2})-(\d{1,2})" map_replace="$1" />
        <map_step map_field_set="month" map_origfieldval="1" />
        <map_step map_field_source="month"
        map_match="(\d{4}|\d{2})-(\d{1,2})" map_replace="$2" />
      </map>
    </maps>
  </sourcemap>
</config>
#+end_src
However, we have not yet mapped ~journaltitle~ and ~location~. These are now easier to set on the command line so we shall do so. Also note that in order to have the ~date~ logic work, we will need the ~--output-legacy-date~ option as well.
#+begin_src bash
# Runs
biber --tool --configfile=biberConf.xml references.bib --output-file refsTmp.bib --output-legacy-date --output-field-replace=location:address,journaltitle:journal
#+end_src
This gives a ~sphinx~ compatible file:
#+begin_src bibtex
@misc{grasedyckParameterdependentSmootherMultigrid2020,
  author = {Grasedyck, Lars and Klever, Maren and L\"{o}bbert, Christian and Werthmann, Tim A.},
  url = {http://arxiv.org/abs/2008.00927},
  eprint = {2008.00927},
  eprinttype = {arxiv},
  keywords = {65N55; 15A69,Mathematics - Numerical Analysis},
  month = {08},
  title = {A Parameter-Dependent Smoother for the Multigrid Method},
  urldate = {2021-05-22},
  year = {2020},
}

@article{grasedyckDistributedHierarchicalSVD2018,
  author = {Grasedyck, Lars and L\"{o}bbert, Christian},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nla.2174},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nla.2174},
  doi = {10.1002/nla.2174},
  issn = {1099-1506},
  journal = {Numerical Linear Algebra with Applications},
  keywords = {Hierarchical Tucker,HT,multigrid,parallel algorithms,SVD,tensor arithmetic},
  langid = {english},
  number = {6},
  pages = {e2174},
  title = {Distributed Hierarchical {{SVD}} in the {{Hierarchical Tucker}} Format},
  urldate = {2021-05-22},
  volume = {25},
  year = {2018},
}

@article{grasedyckHierarchicalSingularValue2010,
  author = {Grasedyck, Lars},
  publisher = {Society for Industrial and Applied Mathematics},
  url = {https://epubs.siam.org/doi/abs/10.1137/090764189},
  doi = {10.1137/090764189},
  issn = {0895-4798},
  journal = {SIAM Journal on Matrix Analysis and Applications},
  month = {01},
  number = {4},
  pages = {2029--2054},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title = {Hierarchical {{Singular Value Decomposition}} of {{Tensors}}},
  urldate = {2021-05-22},
  volume = {31},
  year = {2010},
}
#+end_src
*** Conclusions
This was a rather annoying excursion into the unpleasant underbelly of ~bibtex~ and ~biblatex~. The ~biber~ solution is actually rather elegant, but most definitely not immediately obvious.
* TODO Hugo and ox-hugo
** Starting Hugo
We will initialize a default site:
#+begin_src bash
hugo new site orgsite
#+end_src

* DONE Talk Supplements for Code In Place 2021 SL Workshop :@conferences:presentations:ramblings:python:
CLOSED: [2021-05-29 Sat 07:07]
:PROPERTIES:
:EXPORT_FILE_NAME: cip-2021-slw-meta
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A meta-post on the workshop I held for the Section Leaders of Code in Place 2021 entitled "Wrangling Pythons with Nix for Reproducible Purity"
#+END_QUOTE
** Background
Much the same as the rationale behind my other presentation [[https://rgoswami.me/tags/presentations/][meta-posts]], that is:
- I would like to preserve questions
- I would like to collect the video, slides and other miscellaneous stuff in one location [fn:officialsite]
- It would be nice to have my own thoughts here afterwards
*** Details
Blurb verbatim from the [[https://docs.google.com/spreadsheets/d/1IEtF-fU9d57ddFJYkSgT9ABQwqF81M3dLS4TfwZ9Q24/edit#gid=0][spreadsheet]].
#+begin_quote
Docker. Virtual Machines. Pipenv. Virtualenv. The struggle to isolate development environments and pin package versions (both system and project) has been a defining feature of modern programming. Python packaging is a nightmare in and of itself, moving from eggs to wheels, collecting system requirements with distutils (PEP 518)... The horror never ends. Indeed every language level packaging system still falls short of collecting system dependencies (MKL, LAPACK etc). This workshop will introduce the Nix ecosystem for working with Python (and system dependencies) in a lightweight, reproducible manner. We shall work through generating pure shells for development, and clean builds for PyPI. We'll see how to package arbitrary Python code and what level of nix expression language proficiency is required for the same (spoiler: not much!).
#+end_quote
**** Other Content
Anything on this site [[https://rgoswami.me/tags/nix/][tagged with Nix]] or [[https://rgoswami.me/tags/python][tagged with Python]]. Also an introduction to ~nix~ given by me (and Amrita Goswami) at [[https://2020.carpentrycon.org/schedule/#session-10][CarpentryCon2020]] is here:
- [[https://github.com/HaoZeke/CarpentryCon2020_Nix][CarpentryCon2020 Materials]]
- [[https://rgoswami.me/posts/ccon-tut-nix/][A tutorial introduction to Nix and Python]]
** Slides
- [[file:/revealjs/cip2021_nixpy/wrangpynix.html][Best viewed here]] using a browser (in a new tab)
- A ~pdf~ copy of the slides are embedded below
- The ~orgmode~ source [[https://raw.githubusercontent.com/HaoZeke/2021_April_IOP_IntroductionToCpp_Part2/main/docs/pres/iopRevealjs.org][is here on the site's GH repo]]
#+BEGIN_EXPORT html
<script async class="speakerdeck-embed" data-id="4f19dd9e758448aeb44f20cd286afe9a" data-ratio="1.37081659973226" src="//speakerdeck.com/assets/embed.js"></script>
#+END_EXPORT
** Thoughts
I gave the workshop twice and it was rather well attended both times. The speaker presentation room in Oh Yay was very enjoyable and improved the interactivity a lot.

[fn:officialsite] One location I am going to be able to keep track of
* TODO Niv and Overlays
#+begin_quote
Pinning overlays without flakes using ~ bombadil~ for Darwin
#+end_quote
** Background
~emacs~ on Big Sur, both the Macport version and the standard version flicker a lot. This is my attempt to add a patchy fix. Much of this is a straightforward adaptation of [[https://github.com/twlz0ne/nix-gccemacs-darwin][this repository]] and [[https://dnr.im/tech/nix-overlays/][this blog post]].
** Setting Overlays
** Building Emacs
We will obtain the sources using ~niv~.
#+begin_src bash
niv init -b master # nixpkgs
niv add emacs-mirror/emacs -b feature/native-comp-macos-fixes -n emacs-nativecomp # emacs fixes
niv add nix-community/emacs-overlay -b master -n emacs-overlay # upstream builder
#+end_src

* DONE GSoC21 W1: LFortran Kickoff :@notes:gsoc21:fortran:rambling:
CLOSED: [2021-06-12 Sat 00:57]
:PROPERTIES:
:EXPORT_FILE_NAME: gsoc21-w1
:EXPORT_HUGO_PANDOC_CITATIONS: t
:EXPORT_BIBLIOGRAPHY: biblio/refs.bib
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true :link-citations true
:END:
#+begin_quote
Charting paths towards concrete ~lfortran~ usage
#+end_quote
** Background
As mentioned in [[Fortran, GSoC21 and Me][an earlier post]], I have had the immense pleasure of continuing
development of the disruptive ~lfortran~ compiler under the aegis of the [[https://fortran-lang.org/][Fortran
Lang organization]], financed by the [[https://summerofcode.withgoogle.com/projects/#5790375215628288][Google Summer of Code]] and mentored ably by
[[https://ondrejcertik.com/][Ondrej Certik]]. A rather interesting consequence of this is that we are strongly
encouraged to write a precis of our activities each week. This is mine, given
that the clock starting winding down ([[https://developers.google.com/open-source/gsoc/timeline][full timeline]]) last Monday.

I will workshop a few styles of getting maximal information into the post while
ensuring the bulk of the technical details and other related notes makes its way
into the appropriate channels, that is, into the ~lfortran~ [[https://gitlab.com/lfortran/lfortran][project repository]]
and [[https://docs.lfortran.org/][documentation]].
*** Series
This post is the first in a series based around my weekly GSoC21 project check-ins.
1. *GSoC21 W1: LFortran Kickoff* <-- You are here!
2. [[GSoC21 W2: LFortran Unraveling][GSoC21 W2: LFortran Unraveling]]
** Logistics
- I met with Ondrej on Thursday, though our standing arrangement is for Tuesday
  + Meeting earlier in the week helps reduce chances of veering off course
  + Though an equitable distribution of work is assumed, weekends tend to be
    more conductive to bursts of effort
- We decided to release progress reports every Friday night for community oversight and approval
  + Slight discontinuity here, in that the weekend remains, but the idea is to take community suggestions per week as well
- We discussed the work completed by my fellow GSoC student developers over the community bonding period
- Discussed the current [[https://gitlab.com/lfortran/lfortran/-/issues/313][minimum viable project issue]] relating to SNAP [fn:whatsnap], the [[https://github.com/lanl/SNAP][SN (Discrete Ordinates) Application Proxy]] from LANL
  + Also looked into starting work on [[https://gitlab.com/lfortran/lfortran/-/issues/319][the issue]] relating to ~dftatom~ which forms the crux of my project
- Determined approaches towards the final goal
  + Discussed the possibility of making a first pass in the ~C++~ backend instead of the ~LLVM~ one
    - Con :: Is not very fast
    - Pro :: Is very easy
- Notably discussed adding generic types, e.g. ~type(*)~ to *internal* routines for the compiler
  + Saves a lot of boilerplate code
  + Could start with a simple ~fypp~ based approach (similar to ~fpm~)
  + Should be fine as long as users do not start using them

As per my initial project plan, the goal for the week (in progress) is the
*Enumeration of language features used in the dftatoms project and cross
correlating it with implemented features in LFortran*. To this, we decided that
as the AST works well enough, it would be better to follow in the footsteps of
the SNAP issue and determine the dependency tree, before working through it
steadily.
** Setting up LFortran and dftatom
One approach to doing this was naturally, to compile everything by hand,
painstakingly using the logical dependency graph which comes with the
repository. Note that for completion (and to add unnecessary gravitas to this
post), we include *once* the setup details for ~lfortran~ and ~dftatom~ which
have been included verbatim from the documentation [fn:nixissue].
**** LFortran
On Linux systems, I use ~nix~ (naturally). On a Mac, ~nix~ has unresolved issues
upstream, so it is easier to use ~micromamba~ ([[https://mamba.readthedocs.io/en/latest/micromamba.html][here]]) or some other ~anaconda~ helper.
#+begin_src bash
# Linux
git clone git@gitlab.com:lfortran/lfortran
cd lfortran
nix-shell ci/shell.nix
./build0.sh
./build1.sh
export PATH=$(pwd)/inst/bin/:$PATH
./test_lfortran_cmdline
./run_tests.py
# MacOS
micromamba create -p ./tmp llvmdev=11.0.1 bison=3.4 re2c python cmake make toml -c conda-forge
micromamba activate tmp
./build0.sh
cmake -DCMAKE_BUILD_TYPE=Debug -DWITH_LLVM=yes -DCMAKE_INSTALL_PREFIX=`pwd`/inst .
make -j$(nproc)
ctest
./run_tests.py
#+end_src
**** dftatom
Another post can go over why ~dftatom~ was chosen as a benchmark code, but to
keep things short, it is well written, has a great ~cython~ wrapper, is fast,
and is useful @certikDftatomRobustGeneral2013. Rather than write a ~nix~
shell for it though, I opted to get started with the ~anaconda~ set of
instructions to test with.
#+begin_src bash
git clone git@github.com:certik/dftatom
cd dftatom
micromamba create -p ./tmp cython numpy python=3.8 pytest hypothesis -c conda-forge
cmake -DCMAKE_INSTALL_PREFIX=$CONDA_PREFIX -DCMAKE_PREFIX_PATH=$PREFIX -DWITH_PYTHON=yes .
make
make install
PYTHONPATH=. python examples/atom_U.py
#+end_src
Later in the week (over the weekend) I might add some quality of life commits like a ~nix~ environment.
** Testing the AST
The logic behind the AST test is that if we can parse each file with ~lfortran~
and write it back out into the ~f90~ files which can then be compiled by
~gfortran~ and then subsequently pass our tests; then the AST is roughly
alright.

Naturally this still [[file:][strips comments]] and therefore ~openmp~ directives, of which
~dftatom~ thankfully has none.

Now it so happens that this is fairly trivial with ~lfortran~ and ~bash~.
#+begin_src bash
cd $DFTATM/src # src under dftatom repo
find . -name "*.f90" -exec lfortran fmt -i {} \;
# Rebuild and test with gfortran
#+end_src

This worked well. No glaring errors yet. Which brings us to the ASR representation [fn:whatastasr].
** Testing the ASR
To start testing the ASR with ~lfortran -c fname.f90 -o fname.o~ and variations thereof, a good starting point is to set up a dependency graph. A logical one is provided in the repository itself as seen in Fig. [[dftatomgit]].

#+name: dftatomgit
#+caption: ~dftatom~ logical layout
file:images/dftatom_depgraph_git.png

However, this is not exactly the order in which things are built. Thankfully,
the fantastic ~fortdepend~ tool [[https://github.com/ZedThree/fort_depend.py][from Peter Hill]] can speed things up
considerably.

#+begin_src bash
# in the conda environment
pip install fortdepend
cd $DFTATM/src
fortdepend -o Makefile.dep
#+end_src

This can and was then manually inspected and marshalled by order of dependencies
into [[https://gitlab.com/lfortran/lfortran/-/issues/319][the issue]]. Here we will conclude with the visual details instead of dumping
the entire dependency tree.

#+name: dftatomgit
#+caption: ~dftatom~ compilation dependency graph
file:images/dftatom_depgraph_fdep.svg

** Conclusions
That leads us to the end of this week's effectively mid-week update. For the
remainder of the week, until the meeting with Ondrej I shall work through parts
of the dependency graph and write some more about ~dftatom~ and the ~lfortran~
terminology which will be used for the remainder of the project.


[fn:nixissue] Part of which [[https://gitlab.com/lfortran/lfortran/-/merge_requests/694][I added earlier]] and [[https://gitlab.com/lfortran/lfortran/-/merge_requests/975][fixed a typo]] in now
[fn:whatsnap] *Not* the Spectral Neighbor Analysis Potential implemented in LAMMPS
[fn:whatastasr] Should also add a post on standard terminology which will come up a lot

* DONE GSoC21 W2: LFortran Unraveling :@notes:gsoc21:fortran:rambling:
CLOSED: [2021-06-18 Fri 18:59]
:PROPERTIES:
:EXPORT_FILE_NAME: gsoc21-w2
:EXPORT_HUGO_PANDOC_CITATIONS: t
:EXPORT_BIBLIOGRAPHY: biblio/refs.bib
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true :link-citations true
:END:
#+begin_quote
Delving into language standards and back-ends for ~lfortran~
#+end_quote
** Background
As discussed in a previous post in this series, I have been spending roughly
half of each working day with LFortran as part of the [[https://summerofcode.withgoogle.com/projects/#5790375215628288][2021 Google Summer of Code]]
under the [[https://fortran-lang.org/][fortran-lang organization]], mentored by [[https://ondrejcertik.com/][Ondrej Certik]].
*** Series
This post is the second in a series based around my weekly GSoC21 project check-ins.
1. [[GSoC21 W1: LFortran Kickoff][GSoC21 W1: LFortran Kickoff]]
2. *GSoC21 W2: LFortran Unraveling* <-- You are here!
** Logistics
Some of the meeting points are to be expanded on below.
- Met with Ondrej on Tuesday, as discussed previously
  + Talked about language server implementations
    - Looked into ~rtags~ and generating a [[https://clang.llvm.org/docs/JSONCompilationDatabase.html][compilation-database]]
    - Discussed how the C++ concept of having file based units makes this simpler than the Fortran form, which recognizes no file based program units
- Talked about the status of the different back-ends
- Discussed [[https://mlir.llvm.org/][LLVM]] and [[https://mlir.llvm.org/][MLIR]], in the context of Flang (the ~f18~ compiler)
  + Also briefly touched upon ~legacy-flang~ and historical issues
- Discussed standardization of the mod-files
  + Decided this is not a good idea, because a lot of build systems expect Fortran compilers to generate ~.mod~ files, even if they are completely incompatible
  + The standard does not specify what should be contained in a ~mod~ file
  + Every ~mod~ file of every compiler is unique and needs to be handled separately
  + The goal (of this project too) is to handle at the very least conversion of ~gfortran~ module files to ~lfortran~ module files
** Overview
For the second week of my project on getting ~lfortran~ to compile ~dftatom~ and
in general form a usable compiler ecosystem to facilitate greater adoption in
the community, I opted to take a bit of a step back and delve into the
historical evolution of both the Fortran standard itself @lyonUsingAnsFortran1980 and also the compiler
ecosystem. This was also in no small part due to the fact that I ended up moving
during this week, which forced me to spend a lot of time cleaning and playing
with adult LEGO [fn:whatlego]. This naturally left me with plenty of time to
both participate [fn:silently] in the [[https://www.youtube.com/channel/UCTYRAlVmMCGGcrMkKxQLurw][monthly Fortran-lang call]] (which was rather
explosive [fn:whatexplosive]) and also contemplate the overall ecosystem of the
standards committee and compilers. Many more issues were set up and will be
completed over the weekend.
*** Merge Requests
- [[https://gitlab.com/lfortran/lfortran/-/merge_requests/985][Minor installation bug (985)]] :: Fixed a small issue with the ~cmake~ files
*** Assigned Issues
- [[https://gitlab.com/lfortran/lfortran/-/issues/357][Handling ~kind~ in the ASR (357)]] :: A straightforward correction to make the ASR handle more common, but non-standard use cases
- [[https://gitlab.com/lfortran/lfortran/-/issues/355][GFortran Module v15 support (355)]] :: Currently only supports v14
- [[https://gitlab.com/lfortran/lfortran/-/issues/354][Runtime Math and the CPP back-end (354)]] :: Might take a little longer, does not compile at the moment
*** Additional Tasks
I intend to write more about the Fortran standard and go through more of the
compiler code I can get my hands on, since that should give me a better handle
on the different way the standards have been implemented (and augmented!). As
part of this, I'll probably handle some of [[https://gitlab.com/lfortran/lfortran/-/issues/350][issue 350]]; regarding the main
~lfortran.org~ site.
** LFortran and Back-ends
*** LLVM
This is the default, and the most performance oriented. It is however, slightly
more complicated to extend due to the intricacies of the LLVM syntax.
*** C++
This is a newer back-end, perfect for rapid prototyping; and can be selected by
passing the ~--backend=cpp~ flag to ~lfortran~ (e.g. with ~FFLAGS~). This is
easier to work with in that many features boil down to ~stdlib~ calls. However,
this back-end requires KOKKOS. It also has uglier error handling as shown in
Fig. [[fig:llvmcpp]]. Since this is more attractive as a candidate for the math
intrinsics as a first approximation, it is of considerable interest to me.

It is however, still fairly straightforward to work with.
#+begin_src bash
cd $LFROOT # LFortran root
export LFORTRAN_KOKKOS_DIR=$LFROOT/ext/kokkos
mkdir extsrc && cd extsrc
gh repo clone kokkos/kokkos
cd kokkos && mkdir build && cd build
cmake -DCMAKE_INSTALL_PREFIX=$LFORTRAN_KOKKOS_DIR -DKokkos_ARCH_HSW=On ..
make -j$(nproc)
make install
cd $LFORTRAN_KOKKOS_DIR # Need to make a symlink
ln -sf lib64/ lib
cd ../../examples/project1
FC=lfortran FFLAGS="--backend=cpp" cmake .
make
./project1 # Profit
#+end_src

#+DOWNLOADED: screenshot @ 2021-06-16 20:01:03
#+name: fig:llvmcpp
#+caption: Contrasting backends
[[file:images/C++/2021-06-16_20-01-03_screenshot.png]]
** Inspecting intermediates
The module files cannot currently be read or queried from ~lfortran~, however a
representation of what gets compressed into these files can be obtained with the
~--show-asr~ flag. Additionally, a very ~python~ inspired ~--show-stacktrace~ is
implemented to help narrow down areas which need to be augmented.
*** gfortran
#+begin_src bash
cd $DFTATM/src
gfortran -c types.f90
# Read gfortran module
zcat types.mod
# Output
GFORTRAN module version '15' created from types.f90
(() () () () () () () () () () () () () () () () () () () () () () () ()
() () ())

()

()

()

()

()

(2 'dp' 'types' '' 1 ((PARAMETER UNKNOWN-INTENT UNKNOWN-PROC UNKNOWN
IMPLICIT-SAVE 0 0) () (INTEGER 4 0 0 0 INTEGER ()) 0 0 () (CONSTANT (
INTEGER 4 0 0 0 INTEGER ()) 0 '8' ()) () 0 () () () 0 0)
)

('dp' 0 2)
#+end_src
*** lfortran
#+begin_src bash
cd $DFTATM/src
lfortran --show-asr -c types.f90
# Output
(TranslationUnit (SymbolTable 1 {lfortran_intrinsic_kind: (Module (SymbolTable 4 {dkind: (Function (SymbolTable 5 {r: (Variable 5 r ReturnVar () Default (Integer 4 []) Source Public), x: (Variable 5 x In () Default (Real 8 []) Source Public)}) dkind [(Var 5 x)] [(= (Var 5 r) (ConstantInteger 8 (Integer 4 [])))] (Var 5 r) Intrinsic Public Implementation), kind: (Function (SymbolTable 6 {r: (Variable 6 r ReturnVar () Default (Integer 4 []) Source Public), x: (Variable 6 x In () Default (Logical 4 []) Source Public)}) kind [(Var 6 x)] [(= (Var 6 r) (ConstantInteger 4 (Integer 4 [])))] (Var 6 r) Intrinsic Public Implementation), lkind: (Function (SymbolTable 7 {r: (Variable 7 r ReturnVar () Default (Integer 4 []) Source Public), x: (Variable 7 x In () Default (Logical 4 []) Source Public)}) lkind [(Var 7 x)] [(= (Var 7 r) (ConstantInteger 4 (Integer 4 [])))] (Var 7 r) Intrinsic Public Implementation), selected_int_kind: (Function (SymbolTable 8 {R: (Variable 8 R In () Default (Integer 4 []) Source Public), res: (Variable 8 res ReturnVar () Default (Integer 4 []) Source Public)}) selected_int_kind [(Var 8 R)] [(If (Compare (Var 8 R) Lt (ConstantInteger 10 (Integer 4 [])) (Logical 4 [])) [(= (Var 8 res) (ConstantInteger 4 (Integer 4 [])))] [(= (Var 8 res) (ConstantInteger 8 (Integer 4 [])))])] (Var 8 res) Intrinsic Public Implementation), selected_real_kind: (Function (SymbolTable 9 {R: (Variable 9 R In () Default (Integer 4 []) Source Public), res: (Variable 9 res ReturnVar () Default (Integer 4 []) Source Public)}) selected_real_kind [(Var 9 R)] [(If (Compare (Var 9 R) Lt (ConstantInteger 7 (Integer 4 [])) (Logical 4 [])) [(= (Var 9 res) (ConstantInteger 4 (Integer 4 [])))] [(= (Var 9 res) (ConstantInteger 8 (Integer 4 [])))])] (Var 9 res) Intrinsic Public Implementation), skind: (Function (SymbolTable 10 {r: (Variable 10 r ReturnVar () Default (Integer 4 []) Source Public), x: (Variable 10 x In () Default (Real 4 []) Source Public)}) skind [(Var 10 x)] [(= (Var 10 r) (ConstantInteger 4 (Integer 4 [])))] (Var 10 r) Intrinsic Public Implementation)}) lfortran_intrinsic_kind [] .true.), types: (Module (SymbolTable 2 {dp: (Variable 2 dp Local (FunctionCall 2 kind () [(ConstantReal "0.d0" (Real 4 []))] [] (Integer 4 [])) Parameter (Integer 4 []) Source Public), kind: (ExternalSymbol 2 kind 4 kind lfortran_intrinsic_kind kind Private)}) types [lfortran_intrinsic_kind] .false.)}) [])
#+end_src
Note that, ~lfortran~ is expected to eventually interoperate with existing ~mod~ files from compilers like ~gfortran~.
#+begin_src bash
lfortran mod --show-asr types.mod
# Output
Traceback (most recent call last):
  File "/build/glibc-2.32/csu/../sysdeps/x86_64/start.S", line 120, in _start()
  Binary file "/nix/store/hp8wcylqr14hrrpqap4wdrwzq092wfln-glibc-2.32-37/lib/libc.so.6", in __libc_start_main()
  File "$HOME/Git/Github/Fortran/mylf/src/bin/lfortran.cpp", line 1076, in ??
    asr = LFortran::mod_to_asr(al, arg_mod_file);
  File "$HOME/Git/Github/Fortran/mylf/src/lfortran/mod_to_asr.cpp", line 361, in LFortran::mod_to_asr(Allocator&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)
    return parse_gfortran_mod_file(al, s);
LFortranException: Only GFortran module version 14 is implemented so far
#+end_src
Which is the source of an issue assigned to me.
** Conclusions
As I cross my fingers and hope my tenuously tethered wifi is equal to the task of
uploading this post; I can only hope that the installation of my WiFi on Monday
goes smoothly enough to be restored to full working capacity for the next week.
Until then, I shall trawl the code-bases and standards for an inkling of what
makes compiler design the art form as described in @cooperEngineeringCompiler2011a.

[fn:whatlego] I am convinced the concept behind IKEA and its Icelandic counterpart Rumfatalagerinn is that some people enjoy assembling their furniture the same way kids enjoy LEGO
[fn:whatexplosive] There were some interesting allegations of the standards committee being far too conservative, though many of the concerns reflect on the dearth of community efforts compared to other languages; Fortran is not after-all coupled to a compiler unlike modern languages like Rust and Julia
[fn:silently] Owing to the lack of a stable internet I was reduced to text-only participation
* DONE GSoC21 W3: Kind, Characters, and Standards :@notes:gsoc21:fortran:rambling:
CLOSED: [2021-06-26 Sat 00:28]
:PROPERTIES:
:EXPORT_FILE_NAME: gsoc21-w3
:EXPORT_HUGO_PANDOC_CITATIONS: t
:EXPORT_BIBLIOGRAPHY: biblio/GSoC21.bib
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true :link-citations true
:END:
#+begin_quote
Standard practice pragmatic approaches to ~kind~ for ~dftatom~
#+end_quote
** Background
Serialized update for the [[https://summerofcode.withgoogle.com/projects/#5790375215628288][2021 Google Summer of Code]] under the [[https://fortran-lang.org/][fortran-lang
organization]], mentored by [[https://ondrejcertik.com/][Ondrej Certik]].
*** Series
This post is the second in a series based around my weekly GSoC21 project check-ins.
1. [[GSoC21 W1: LFortran Kickoff][GSoC21 W1: LFortran Kickoff]]
2. [[GSoC21 W2: LFortran Unraveling][GSoC21 W2: LFortran Unraveling]] <-- You are here!
3. *GSoC21 W3: Kind and Characters* <-- You are here!
** Logistics
- Met with Ondrej on Tuesday
  + Went over my ~kind~ implementation
  + Merged older approved MRs
  + Worked on generating tests
- Talked about the test methodology in general
  + Most of the tests are better off in their integration form (discussed below)
  + Some aspects of the passes may be tested using the ~doctest~ setup
- Set an additional time to discuss the implementation of *assumed length* character declarations
  + These are not actually used in any ~dftatom~ routines but they are very common for utility functions
- Met with Ondrej on Thursday
  + Discussed repercussions of backends
    - Better, more explicit ASR rules can stem from not relying on the CPP backend
- Talked about the number of passes (SRC->AST->ASR->LLVM)
- Started working on getting the right thing happen when faced with ~character(len=*)~
** Overview
This week also saw an increase in community activities on the [[https://fortran-lang.discourse.group/][Fortran discourse]],
since the J3 meeting is now underway and user polling [fn:whatpoll] is in full swing.

*** New Merge Requests
- [[https://gitlab.com/lfortran/lfortran/-/merge_requests/997/diffs][Implement more kind() (997)]] :: Added tests and code to the AST->ASR pass for ~kind~ calls relevant to ~dftatom~
- [[https://gitlab.com/lfortran/lfortran/-/merge_requests/1000][Draft: Implement assumed length (1000)]] :: More of a trailer for the next week; also happens to be the 1000th MR (which is neat)
*** Freshly Assigned Issues
- [[https://gitlab.com/lfortran/lfortran/-/issues/373][Some more Kind considerations (373)]] :: More of a speculative issue about standards compliance, led to the more concrete and general [[https://gitlab.com/lfortran/lfortran/-/issues/375][375]]
*** Additional Tasks
Still unofficially planning to take a stab at [[https://gitlab.com/lfortran/lfortran/-/issues/350][issue 350]]; regarding the main ~lfortran.org~ site.
** Kinds
The standard allows for any constant to be used in a ~kind~ function call. Some commonly seen variants in the wild are:

#+begin_src fortran
integer, parameter :: dp=kind(0.d0), &             ! double precision
                      hp=selected_real_kind(15), & ! high precision
                      qp=selected_real_kind(32), & ! quadruple precision
                      sp = kind(0.)                ! single precision
#+end_src

Most commonly; ~kind(0.d0)~ is seen in the wild. Currently the ~lfortran~ ASR defines: ~ConstantInteger, ConstantReal, ConstantComplex, ConstantLogical~ of which only ~ConstantLogical~ is implemented.

The idea is that ~0.~ is single precision; while ~1._dp~, ~1.d0~ or ~0.d0~ are double precision. Essentially then the solution presented itself naturally; to check if ~d~ is present; assign double precision if true, otherwise stick to single precision. At the moment, ~lfortran~ considers either ~4~ for single or ~8~ for double.

More generally, it might be useful to make the single and double precision values managed by a pre-processor. For the other kinds of precision, some more thought is required.

At this stage, it compiles with the LLVM backend.
#+begin_src bash
lfortran -v -c --show-asr --show-stacktrace lapack.f90
lfortran -v -c --backend=llvm --show-stacktrace lapack.f90
#+end_src
** Tests
While implementing I used the simplest of debugging concepts, that of making manual changes and writing out results. However, no compiler can survive without rigorous unit tests, and ~lfortran~ is no different.
*** Fortran Integration Tests
So called because these are run by a ~python~ driver and store verbatim the ~stdout~ in files. Note that this form of testing, though convenient, does require the user to be certain of the test before writing it; due to the update process it is possible to store the wrong result and consistently get it wrong.
#+begin_src bash
./run_tests.py -u # Update
#+end_src
*** Unit Tests for Passes
These are not favored, as any change in any of the passes causes a large amount of test metadata to be invalidated. There are some tests of this nature, and the framework used is ~doctest~.
** Towards Assumed Lengths
The first step towards implementing assumed lengths was to isolate the problem,
which was done through the age-old comment recompile and test methodology. The offending function is a rather innocuous looking helper function:
#+begin_src fortran
function upcase(s) result(t)
! Returns string 's' in uppercase
character(*), intent(in) :: s
character(len(s)) :: t
integer :: i, diff
t = s; diff = ichar('A')-ichar('a')
do i = 1, len(t)
    if (ichar(t(i:i)) >= ichar('a') .and. ichar(t(i:i)) <= ichar('z')) then
        ! if lowercase, make uppercase
        t(i:i) = char(ichar(t(i:i)) + diff)
    end if
end do
end function
#+end_src

Having zeroed in on a prototypical failure point within ~dftatom~, in the ~utils.f90~ file, the next steps are documented in the relevant issue and PR.

The AST representation of the code is :
#+begin_src bash
(TranslationUnit [(Module utils [] [(ImplicitNone [])] [(Declaration () [(SimpleAttribute AttrPrivate)] []) (Declaration () [(SimpleAttribute AttrPublic)] [(upcase [] [] () None ())])] [(Function upcase [(s)] [] t () [] [] [] [(Declaration (AttrType TypeCharacter [(len () Star)] ()) [(AttrIntent In)] [(s [] [] () None ())]) (Declaration (AttrType TypeCharacter [(() 20 Value)] ()) [] [(t [] [] () None ())])] [] [])])])
#+end_src
No simplifications can be expected to occur from the AST representation to the ASR for this situation so the first order of business is to let the ASR accept the AST representation and pass it through to the backend.

Early on, I considered managing this particular feature in the ~cpp~ backend, but Ondrej pointed out that the ease-of-use features and guarantees of the ~cpp~ compiler would lead to a more sloppy ASR implementation.
** Conclusions
This third week led me down the rabbit hole with regards to the standard,
and a statement comes to mind from @clermanModernFortranStyle2012:
#+begin_quote
The standard is the contract between the compiler writer and the application developer.
#+end_quote
For most of my life I've been the latter, but now, the intricate legalese of the
standard haunts much of my day.

This attempt at standards level rigor in particular, for ~kind~ ([[https://gitlab.com/lfortran/lfortran/-/issues/373][#373]]) lead to the discussion on the best way to work with constant expressions at compile time ([[https://gitlab.com/lfortran/lfortran/-/issues/375][#375]]).

My vaccination with the single shot Janssen vaccine and subsequent side effects,
have had a deliberating effect. However, the additional meeting with Ondrej more
than made up for it in terms of project productivity.

I also satiated mostly my interest in the historical development of "automatic
coding" or compiling as we would now call it in the context of Fortran largely
due to @cipraBest20thCentury, @backusHistoryFortranII1998; and its use at the
[[https://link.springer.com/chapter/10.1007%2F978-3-642-03757-3_16][University of Iceland]] [fn:forumhist]. It is very liberating to be able to keep a
public record of my reading habits and fancies, however 20 hours of text and
code consumed along with ruminative thoughts is rather too much for a single
weekly post, might need to eventually step it up. Other interesting things which
just haven't made the cut for this post include finally going through the
~gfortran~ implementation in some more detail and looking into the linking and
loading process via @levineLinkersLoaders2000.

The primary goal for next week shall remain implementing assumed length character arrays and then continuing with the laundry list of ~dftatom~ tests.

[fn:whatpoll] For example, this [[https://fortran-lang.discourse.group/t/poll-fortran-202x-conditional-expressions-syntax/1425/31][poll on conditional expressions]]
[fn:forumhist] The community discourse has [[https://fortran-lang.discourse.group/t/intel-releases-oneapi-toolkit-free-fortran-2018/471/14?u=rgoswami][historical records]] too
* TODO Compiler Design
True, from some perspective, it is a race to the end goal of flipping some
switches at the hardware level (a massive over simplification) but it is in the
implementation, that poetry can be formulated. Compiler design is like no other
project I have undertaken. The standard, despite its forbidding length, leaves
even more unsaid than said. Compilers and their authors, which in this
happenstance include in some form, me and my fellow GSoC contributors, have the
unique opportunity to find elegant patterns within the constraints of the
standard. Like writing a haiku or solving a puzzle, elegance is a core tenet of
the process. It is not merely enough to brute force a feature into compliance
with the standard, it must be finessed and provide transparent insights into the
mind of the programmer. The standards themselves have a symbiotic relationship
to the compilers, with the committee actively considering extensions introduced
by compilers for inclusion into future standards.

- GFortran Module version 15 support
- Extend the handling of constant integers to handle more than just logical
- Flang was first a summer project, then in 2017 the so called legacy flag was published, which around the same time Ondrej decided to start LFortran
- Currently LFortran shadows things which is part of a deprecated workflow
* TODO A Tourist's Guide to the Fortran Standard :@programming:standards:fortran:
:PROPERTIES:
:EXPORT_FILE_NAME: tourist-fortran
:EXPORT_HUGO_PANDOC_CITATIONS: t
:EXPORT_BIBLIOGRAPHY: biblio/refs.bib
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true :link-citations true
:END:
#+begin_quote
A meandering path through the history of Fortran
#+end_quote
** Background
* DONE Talk Supplements for IOP CAPS'21 Webdev Workshop :@conferences:presentations:ramblings:webdev:
CLOSED: [2021-06-26 Sat 10:05]
:PROPERTIES:
:EXPORT_FILE_NAME: iop-caps-webdev-2021-meta
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A meta-post on the workshop I held for the IOP CAPS'21 student conference on Web Development for Physicists
#+END_QUOTE
** Background
Much the same as the rationale behind my other presentation [[https://rgoswami.me/tags/presentations/][meta-posts]], that is:
- I would like to preserve questions
- I would like to collect the video, slides and other miscellaneous stuff in one location [fn:officialsite]
- It would be nice to have my own thoughts here afterwards
*** Details
- Workshop listing on [[https://www.iopconferences.org/CAPS21][conference site]]
Blurb:
#+begin_quote
Social media might be here to stay, however, it is a poor medium to share expert content. In particular, short sound bites aside, two main cornerstones of scientific work in the 21st century, programming and mathematics, are ill suited to most commercial platforms. We will journey into the depths of web development, skimming for the most part for the components needed to develop a well designed platform from which non-peer reviewed content can be disseminated, and expertise can be proven. We will also discuss the types of content and how to decide between multiple possible content outlets.
#+end_quote

** Slides
- [[file:/revealjs/iop_caps21/caps21webdev.html][Best viewed here]] using a browser (in a new tab)
- A ~pdf~ copy of the slides are embedded below
- The ~orgmode~ source is here on the [[https://github.com/HaoZeke/haozeke.github.io/tree/src/presentations/iopWEBDEV2021][site's GH repo]]
#+BEGIN_EXPORT html
<script async class="speakerdeck-embed" data-id="ff53a01f882847d49f04f2818adddff6" data-ratio="1.37081659973226" src="//speakerdeck.com/assets/embed.js"></script>
#+END_EXPORT
** Video

{{< youtube _YmYbkGA2oc >}}

[fn:officialsite] One location I am going to be able to keep track of
