#+author: Rohit Goswami

#+hugo_base_dir: ../
#+hugo_front_matter_format: yaml
#+hugo_front_matter_key_replace: description>summary
#+bibliography: biblio/refs.bib

#+seq_todo: TODO DRAFT DONE
#+seq_todo: TEST__TODO | TEST__DONE

#+property: header-args :eval never-export

#+startup: logdone indent overview inlineimages

* DONE About
:PROPERTIES:
:EXPORT_HUGO_SECTION: /
:EXPORT_FILE_NAME: about
:EXPORT_DATE: 1995-08-10
:END:

#+begin_description
A short historical recollection of some *thoughts* and _stuff_.
#+end_description

*Hi.*

I'm [[https://orcid.org/0000-0002-2393-8056][Rohit Goswami]], better known across the web as ~HaoZeke~. I'm not the
first of my name, which is why instead of ~rgoswami~, I occasionally use ~rg0swami~ when I need to be
identified by something closer to my name.

The actual username is a throwback to back when people *liked* being anonymous (and with multiple personalities)
online, so that ought to give an idea of how old I am. A curriculum vitae is
[[https://github.com/HaoZeke/CV/blob/master/RG_Latest-cv.pdf][available here]].

It is difficult to keep this section short and not let it spill
into an unstructured memoir. For a while I considered trying to consolidate my
online presences but that turned out to be completely impossible without a
series of posts and avatars[fn:notrefs]. I did however eventually set up a
sporadically updated [[Collection of WebLinks][collection of web-links]] involving me. There is also, a separate list of quotes, or other things I have something to say about.
** Intangible Positions
This is a set of things which are primarily online and/or voluntary in a
non-academic sense.
- I administer and design a bunch of websites, mostly verified [[https://keybase.io/HaoZeke][on Keybase]]
- I am a certified Software Carpentries [[https://carpentries.org/instructors/#HaoZeke][instructor]]
- I [[https://carpentries.org/maintainers/#HaoZeke][officially maintain]], for the Software Carpentries, the lesson on R ([[https://github.com/swcarpentry/r-novice-inflammation][r-novice-inflammation]])
- I [[https://aur.archlinux.org/packages/?SeB=m&K=HaoZeke][also maintain]] some packages on the AUR (ArchLinux User Repository)
- I hone coursework development and teaching [[https://www.univ.ai/team/rohit-goswami][with univ.ai]]
- I [[https://forum.xda-developers.com/xperia-z5/orig-development/cm-14-1-lineageos-t3536846][maintain(ed)]] the official LineageOS image for the Xperia Z5 Dual
** Historical Places
What follows is a more informal set of places I am or have been associated with or are of significance to
me[fn:growingUp].
*** ReykjavÃ­k
- I [[https://english.hi.is/staff/rog32][am associated]] with the [[https://notendur.hi.is/hj/researchgroup.html][reputed Jonsson group]] of the [[http://raunvisindastofnun.hi.is/the_science_institute][Science Institute]] at the
  [[https://english.hi.is/school_of_engineering_and_natural_sciences][University of Iceland]], where I benefit from the
  guidance of the erudite and inspiring [[https://notendur.hi.is/hj/indexE.html][Prof. Hannes Jonsson]]
- My doctoral committee is here, which includes the very excellent inputs of
  [[https://english.hi.is/staff/elvarorn][Dr. Elvar Jonsson]]
- I have also benefited from sitting in on some formal coursework here, which
  has been a fascinatingly useful experience
*** Kanpur
- I [[https://femtolab.science/people/rohit][retain a close association]] with the fantastic [[https://femtolab.science/][Femtolab]] at [[http://home.iitk.ac.in/~dgoswami/][IIT Kanpur]] under
  [[https://femtolab.science/people/dgoswami][Prof. Debabrata Goswami]], who has provided constant guidance throughout my career
- I am the co-lead developer of the FOSS scientific [[https://dseams.info][d-SEAMS software suite]] for
  [[https://wiki.dseams.info/#citation][graph theoretic approaches to structure determination]] of molecular dynamics
  simulations, along with my exceptional co-lead [[https://www.researchgate.net/profile/Amrita_Goswami2][Amrita Goswami]] of the CNS Lab
  under Prof. Jayant K. Singh at IITK
- I worked with the Nair group as part of the [[http://surge.iitk.ac.in/AnnualReport/report2017.pdf][Summer Undergraduate in Research Excellence]] (SURGE) program, also at IITK
- Harcourt Butler Technical Institute (HBTI) Kanpur, or the [[http://hbtu.ac.in/][Harcourt Butler Technological University]], as it is now called, was where I trained to be a chemical engineer
*** Bombay
- I [[https://rajarshichakrabarti.wixsite.com/rajarshichakrabarti/team][spent a formative summer]] under [[https://rajarshichakrabarti.wixsite.com/rajarshichakrabarti][Prof. Rajarshi Chakrabarti]] of the IIT Bombay
  Chemistry department, who has been instrumental in developing my interests
- I also spent some time discussing experiments with [[https://www.che.iitb.ac.in/online/faculty/rajdip-bandyopadhyaya][Prof. Rajdip Bandyopadhyaya]]
  of the IIT Bombay Chemical Engineering department during an industrial
  internship in fragnance compounding at the R&D department of KEVA Ltd. under
  [[https://in.linkedin.com/in/debojit-chakrabarty-b9a2262][Dr. Debojit Chakrabarty]]
*** Bangalore
- At IISc, I had the good fortune to meet Prof. Hannes Jonsson at a summer
  workshop [[https://chemeng.iisc.ac.in/rare-events/index.html][on Rare events]]
- At the [[http://bangaloreinternationalcentre.org/][BIC]], I undertook formal machine learning and artificial intelligence
  training under Harvard's [[https://www.extension.harvard.edu/faculty-directory/rahul-dave][Dr. Rahul Dave]] and [[https://iacs.seas.harvard.edu/people/pavlos-protopapas][Dr. Pavlos Protopapas]] as part of the [[https://univ.ai][univ.ai]]
  summer course
*** Chennai
- I spent a very fruitful summer on quantum tomography under [[https://www.imsc.res.in/~sibasish/qis.html][Prof. Sibashish Ghosh]] at the [[https://www.imsc.res.in/][Institute for Mathematical Sciences]] (IMSc Chennai)
** Avatars
I thought it might be of use to list a few of my more official visages. This is
mostly to ensure people do not confuse me with a Sasquatch[fn:notpersonal].
These mugshots are exactly that, mugshots for profile icons[fn:mountaintapir].

#+caption: A collage of mugshots, shuffled and not ordered by date to confuse people trying to kill me
[[file:images/avatarCollage.jpg]]

** Donations
If you've gotten this far, you might also want to check out the
following[fn:patreon]:
- [[https://www.patreon.com/rgoswami][Patreon]]
- [[https://liberapay.com/rohit][Librepay]]

[fn:patreon] There won't ever be any content behind paywalls though
[fn:growingUp] I grew up on the verdant and beautiful [[https://www.tifr.res.in/][TIFR Mumbai]] campus, and
completed high school and undergraduate stuff while playing with peacocks and things on
the [[https://www.iitk.ac.in][IIT Kanpur]] campus
[fn:notrefs] I didn't think it would be necessary, but just in case it isn't
clear, people listed here are not necessarily all references or anything, this is
a personal list of people associated with each city, not a cover letter
[fn:notpersonal] This is not a replacement for [[https://www.instagram.com/rg0swami/][an Instagram feed]] or a [[https://www.facebook.com/rg0swami][Facebook
wall]], or even a [[https://www.researchgate.net/profile/Rohit_Goswami2][ResearchGate]] or [[https://publons.com/researcher/2911170/rohit-goswami/][Publons]] or [[https://orcid.org/0000-0002-2393-8056][ORCID]] page; all of which I do sporadically remember I have
[fn:mountaintapir] Made with the [[https://github.com/tttppp/mountain_tapir][Mountain Tapir Collage Maker]]
* TODO Collection of Quotes
From Statistical Rethinking:
#+begin_quote
An engineer, a physicist, and a statistician go hunting with a bow and arrow. The physicist underestimates ("let us ignore windspeed") and his arrow falls short. The engineer adds a fudge factor and his shot goes wide. The statistician averages the two and claims they have bagged the animal.
#+end_quote
Illustrates the usage of stats.

From some talk by Cederic Villani:
#+begin_quote
A mathematician and a statistician see two people go into a building, one comes out. The statistician asserts there is a 50-50 chance of someone coming out, and the mathematician says if one person goes in, no one comes out.
#+end_quote
Illustrates the concept of rigor.

* DONE Collection of WebLinks :@personal:ramblings:
CLOSED: [1995-08-10]
:PROPERTIES:
:EXPORT_FILE_NAME: rg-collection-weblinks
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments false
:END:
#+BEGIN_QUOTE
An attempt to re-claim and verify my digital presence.
#+END_QUOTE
** Background
I mentioned [[About][on my about page]], that it is nigh impossible to keep track of every
digital trace there is of me. That said, it is really not even a countable
infinite set yet, so it is a good idea to get started before it gets much worse.
This is minimally curated, and will only be sporadically updated, so take
everything here with a grain of salt. I honestly have no idea why anyone who is
not me would like to see this, other than to prove one of these with respect to
the rest[fn:whereswaldo].
** Profiles
*** Professional
- [[https://english.hi.is/staff/rog32][University of Iceland]]
- [[https://femtolab.science/people/rohit][FemtoLab]]
- [[https://g.co/kgs/Y9fpDC][Google Knowledge Panel]]
*** Voluntary
- The Carpentries
  - [[https://carpentries.org/instructors#HaoZeke][Instructor]], [[https://carpentries.org/maintainers#HaoZeke][Maintainer]], [[https://2020.carpentrycon.org/task-force/][CarpentyrCon2020 Committee Task Force]], [[https://2020.carpentrycon.org/schedule/#session-10][CarpentryCon2020 Speaker]]
- [[https://forum.igdore.org/t/rohit-goswami/656][IGDORE]]
- [[https://www.univ.ai/teams/rohit-goswami][Univ.ai]]
- TeX Users Group
  - [[http://tug.org/tug2020/][TUG 2020 Conference Committee member]]
  - TUG Zulip Admin
- [[https://developers.google.com/season-of-docs/docs/participants/project-sympy-rohitgoswami][Google Summer of Docs 2020 Symengine]]
*** Academic
- [[https://publons.com/researcher/2911170/rohit-goswami/][Publons]]
- [[https://peerj.com/rgoswami/][PeerJ]]
- [[http://scholar.google.com/citations?user=36gIdJMAAAAJ&hl=en][Google Scholar]]
- [[https://orcid.org/0000-0002-2393-8056][OrCiD]]
- [[https://loop.frontiersin.org/people/829611/overview][Loop]]
- [[https://osf.io/c47v3/][OSF]]
*** Societies
- [[https://engage.aiche.org/network/community-directory/profile?UserKey=8f135c8b-747d-4f79-95fd-84975458e3bd][American Institute of Chemical Engineers]]
- [[https://ieeexplore.ieee.org/author/37086865956][IEEE]]
*** Communities
- [[https://figshare.com/authors/Rohit_Goswami/5453063][Figshare]]
*** Misc
- [[https://github.com/HaoZeke][Github]]
- [[https://gitlab.com/HaoZeke][Gitlab]]
- [[https://www.goodreads.com/user/show/33462912-rohit-goswami][Goodreads]]
- [[https://keybase.io/HaoZeke][Keybase]]
** Pages and Articles
*** By Me
- Everything on [[https://keybase.io/HaoZeke][any of my many websites]]
- [[https://www.aiche.org/community/sites/committees/young-professionals/blog/fundamental-research-chemical-engineering-undergrad][Write-up on research with a ChemE undergraduate degree]] for the American Institute of Chemical Engineers (AIChE) Young Professionals Committee (YPC)
- Hackernoon article on [[https://hackernoon.com/locking-and-encrypting-apps-with-encfs-c1484e77f479][Locking and Encrypting Apps with Encfs]]
- The Water, Chemicals and more with Computers for Chemistry (WC3m) [[https://wc3m.github.io/][course website]]
- A collaborative post on [[https://carpentries.org/blog/2020/08/r-4-migration/][migrating to R 4.0 for The Carpentries]]
**** Tech Conferences
These are tech talks, for academic presentations, [[https://github.com/haozeke/cv][my CV is a better guide]].
- Reproducible Scalable Workflows with Nix, Papermill and Renku at [[https://in.pycon.org/2020/][PyCon India 2020]]
- Reproducible Environments with the Nix Packaging System at [[https://2020.carpentrycon.org/schedule/#session-10][CarpentryCon 2020]]
**** Lightning Talks
- Nix from the dark ages (without Root) at [[https://2020.nixcon.org/][NixCon 2020]]
*** Mentioning Me
**** Lists
- [[https://sites.google.com/view/2020crt/home?authuser=0][2020 Categorifications in Representation Theory]] ([[https://sites.google.com/view/2020crt/registered-participants?authuser=0][participant]])
- [[https://web-eur.cvent.com/event/16fae83c-5b72-4690-a368-829688f0d14b/summary][Heilbronn Annual Conference 2020]] ([[https://web-eur.cvent.com/event/16fae83c-5b72-4690-a368-829688f0d14b/websitePage:1faf08a0-dcdd-4a2c-b4b8-d9d445202377?RefId=HIMR][participant]])
- [[https://www.cirm-math.fr/Listes/liste_pre_verif.php?id_renc=2255&num_semaine=0][CIRM - Jean-Morlet Chair - Quasi-Monte Carlo Methods and Applications]] ([[https://www.chairejeanmorlet.com/2255.html][info]]) [participant]
- [[https://www.cirm-math.fr/Listes/liste_pre_verif.php?id_renc=2146&num_semaine=0][CIRM Mathematical Methods of Modern Statistics 2]] ([[https://www.cirm-math.com/cirm-virtual-event-2146.html][info]]) [participant]
- Kavli IPMU conference on "[[https://indico.ipmu.jp/event/314/overview][The McKay correspondence, mutation and related topics]]" ([[https://indico.ipmu.jp/event/314/registrations/participants][participant]])
- [[https://chemeng.iisc.ac.in/rare-events/gallery.html][Gallery of RARE 2019]] at IISc
- [[https://gitlab.com/openresearchlabs/probabilistic_data_analysis_2020/-/blob/master/session7.md][Probablistic Data Analysis]] (University of Turku)
- [[https://twitter.com/aiidateam/status/1281323480290660352][AiiDA Virtual Tutotrial 2020]]
- [[https://tcevents.chem.uzh.ch/event/12/contributions/45/author/73][FortranCon 2020 Author]]
- [[https://github.com/stan-dev/math/releases/tag/v3.3.0][Stan Math 3.3.0 contributor]]
- CECAM Participant for:
  - [[https://www.cecam.org/workshop-details/26#participant_tab][CECAM-DE-SMSM: (Machine) learning how to coarse-grain]]
  - [[https://www.cecam.org/workshop-details/28#participant_tab][CECAM-DE-SMSM: ESPResSo and Python: Versatile Tools for Soft Matter Research]]
**** Teaching
- [[https://wc3m.github.io/][Water, Chemicals and more with Computers for Chemistry (WC3m)]] :: July 28-August 28, 2020 (co-lead)
- [[https://coderefinery.github.io/2020-05-25-online/][2020 MegaCodeRefinery]] (helper)
***** Carpentries
- [[https://flatironinstitute.github.io/sciware-swc-2020-09-git/][Sciware: Git and GitHub at the Flatiron Institute]] :: September 24 and October 1, 2020 (instructor)
- [[https://nairps.github.io/2020-09-08-ggc-SBDH-online/][Data Carpentry Workshop for Social Sciences Georgia Gwinnett College]] :: September 8-11, 2020 (instructor)
- [[https://smcclatchy.github.io/2020-06-22-biotechPartners-pm/][Data Carpentry Ecology for Biotech Partners]] :: June 22-July 2, 2020 (instructor)
- [[https://sadilar.github.io/2020-06-29-SA-ONLINE/][Online Data Carpentry Workshop SADiLaR, South Africa]] :: 29 June - 3 July, 2020 (instructor)
**** Quotes
- Quoted in a [[https://www.stanforddaily.com/2020/06/08/code-in-place-makes-cs-accessible-to-thousands-worldwide/][Stanford Daily Article on CS106A Code in Place]]
- [[https://fortran-lang.org/newsletter/2020/06/01/Fortran-Newsletter-June-2020/][Fortran Monthly Newsletter (June 2020)]]
- Emacs News
  - [[https://sachachua.com/blog/2020/05/2020-05-11-emacs-news/][2020-06-22]] :: for [[Temporary LaTeX Documents with Orgmode][Temporary LaTeX Documents with Orgmode]]
  - [[https://sachachua.com/blog/2020/05/2020-05-11-emacs-news/][2020-06-15]] :: for [[Emacs for Nix-R][Emacs for Nix-R]]
  - [[https://sachachua.com/blog/2020/05/2020-05-11-emacs-news/][2020-05-11]] :: for [[An Orgmode Note Workflow][An Orgmode Note Workflow]]
  - [[https://sachachua.com/blog/2020/05/2020-05-04-emacs-news/][2020-05-04]] :: for [[Pandoc to Orgmode with Babel][Pandoc to Orgmode with Babel]]
  - [[https://sachachua.com/blog/2020/04/2020-04-27-emacs-news/][2020-04-27]] :: for [[Using Mathematica with Orgmode][Using Mathematica with Orgmode]]
  - [[https://sachachua.com/blog/2020/04/2020-04-13-emacs-news/][2020-04-13]] :: for [[https://dotdoom.rgoswami.me][my dotDoom doom-emacs configuration]]
  - [[https://sachachua.com/blog/2020/04/2020-04-06-emacs-news/][2020-04-06]] :: for [[Replacing Jupyter with Orgmode][Replacing Jupyter with Orgmode]]
** Videos
*** Of Me
- Everything [[https://www.youtube.com/channel/UC9f_UGqNsa60kmNGj_WkLPw?][on my YouTube channel]]
- [[https://www.youtube.com/watch?v=Q1St1VT43sc&feature=youtu.be][Discussion session]] for the CS196A Code in Place AMA with the students
- Panelist for the CarpentryCon 2020 session on [[https://pad.carpentries.org/cchome-teaching-online][lessons learnt from remote teaching]]
- Student Presentation at UI on [[https://hi.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=6cda865b-c58a-4a79-89a3-ab9300d6ede6][Scrum for Software Quality Management]]
- CarpentryCon 2020 [[https://www.youtube.com/watch?v=np6TPzME0aA][session recording]] on "[[https://2020.carpentrycon.org/schedule/#session-10][Reproducible Environments with the Nix Packaging System]]"
*** Including Me
This category involves recordings where I asked questions, and therefore
technically involve me in a sense.
- I appear [[https://www.linkedin.com/posts/univ-ai_neuralnetwork-regularisation-ucla-activity-6689748037511782400-gMSx][in the audience of this clip]] on neural networks and regularization
- Code in Place AMA with [[https://www.youtube.com/watch?v=J7S_SJ2Adi4&list=PLcil-m27rjnZ2f85ao8_EauguYhzkzSQA&index=5&t=0s][Stanford CS Lecturers]]
- I [[http://www.youtube.com/watch?v=-Y_OYhoHOb8][appear (audibly)]] to ask a question for the TUG2020 closing seminar by John MacFarlane
**** HPC Carpentry
- [[https://pad.carpentries.org/hpccarpentry-tour][A guided tour]]
**** Fortran Maintainers Monthly Calls
- [[https://www.youtube.com/watch?v=i-gRNGRzugc][June 2020]]
**** IAS TML Lecture Questions
I've been sitting in on these for a while thanks to [[https://www.math.ias.edu/~ke.li/][Ke Li]], but this section lists some of the lectures I asked a question in
  - "[[https://video.ias.edu/tml/2020/0609-AleksanderMadry][What Do Models Learn?]]" by Aleksander MÄ…dry
  - "[[https://youtu.be/QTnjqdxG99c?t=3879][Langevin Dynamics in Machine Learning]]" by Michael Jordan
  - "[[https://youtu.be/Wx8J-Kw3fTA?t=4814][Graph Nets: The Next Generation]]" by Max Welling
  - "[[https://youtu.be/IbKWTF4MzMY?t=3787][Priors for Semantic Variables]]" by Yoshua Bengio
  - "A Blueprint of Standardized and Composable Machine Learning" by Eric Xing

[fn:whereswaldo] If you do think you have seen me somewhere not on this list, drop me an email
* DONE Search
:PROPERTIES:
:EXPORT_HUGO_SECTION: /
:EXPORT_FILE_NAME: search
:END:
#+begin_src yaml :front_matter_extra t
layout: "search"
outputs:
  - html
  - json
sitemap:
  priority: 0.1
#+end_src
#+begin_description
Search in full-text, the entire contents of the site.
#+end_description
* DONE Categories
:PROPERTIES:
:EXPORT_HUGO_SECTION: /categories/
:EXPORT_FILE_NAME: _index.md
:END:
#+begin_src yaml :front_matter_extra t
mainlist: True
#+end_src
* DONE Tags
:PROPERTIES:
:EXPORT_HUGO_SECTION: /tags/
:EXPORT_FILE_NAME: _index.md
:END:
#+begin_src yaml :front_matter_extra t
mainlist: True
#+end_src
* DONE Site Rationale :@personal:ramblings:explanations:
:PROPERTIES:
:EXPORT_FILE_NAME: rationale
:EXPORT_DATE: 2020-02-11 23:28
:END:
** Why this site exists
I have a lot of online presences. I have been around (or at-least, lurking) for
over ten years. Almost as long as I have been programming. Anyway, I have a
penchant lately for using ~emacs~ and honestly there isn't very good support for
~org-mode~ files. There are options recently with ~gatsby~ as well, but this
seemed kinda neat.
** What 'this' is
- This site is [[http://gohugo.io/][built by Hugo]]
- The posts are [[https://ox-hugo.scripter.co/][generated with ox-hugo]]
- The theme is based of this [[https://github.com/rhazdon/hugo-theme-hello-friend-ng][excellent one]] by Djordje Atlialp, which in turn is based off of this [[https://github.com/panr/hugo-theme-hello-friend][theme by panr]]
    - My modifications [[https://github.com/HaoZeke/hugo-theme-hello-friend-ng-hz][are here]]
** What is here
- Mostly random thoughts I don't mind people knowing
- Some tech stuff which isn't coherent enough to be put in any form with
  references
- Emacs specific workflows which I might want to write about more than [[https://dotdoom.rgoswami.me/][short
  notes on the config]]
** What isn't here
- Some collections should and will go to my [[https://grimoire.science][grimoire]]
- My [[https://dotdoom.rgoswami.me/][doom-emacs configuration]]
- Academic stuff is better tracked on [[https://publons.com/researcher/2911170/rohit-goswami/][Publons]] or [[https://scholar.google.co.in/citations?user=36gIdJMAAAAJ&hl=en][Google Scholar]] or my pages
  hosted by my favorite [[https://femtolab.science/people/rohit][IITK group]] or [[https://www.hi.is/starfsfolk/rog32][UI group]]
* DONE Taming Github Notifications :@notes:tools:github:workflow:
:PROPERTIES:
:EXPORT_FILE_NAME: ghNotif
:EXPORT_DATE: 2020-02-12 11:36
:END:
** Background
As a member of several large organizations, I get a lot of github notifications.
Not all of these are of relevance to me. This is especially true of
~psuedo-monorepo~ style repositories like the [[https://github.com/openjournals/joss-reviews][JOSS review system]] and
*especially* the [[https://github.com/exercism/v3/][exercism community]].

- I recently (re-)joined the [[https://exercism.io/][exercism community]] as a maintainer for the C++
  lessons after having been a (sporadic) teacher
- This was largely in response to a community call to action as the group needed
  new blood to usher in *v3* of the exercism project

Anyway, I have since found that at the small cost of possibly much of my public
repo data, I can manage my notifications better with [[https://octobox.io/][Octobox]]

** Octobox
- It appears to be free for now
- It syncs on demand (useful)
- I can search things quite easily
- They have a neat logo
- There appear to be many features I probably won't use

It looks like this:

#+caption: Octobox Stock Photo
[[file:images/octoboxSample.png]]
* DONE Poetry and Direnv :@programming:tools:direnv:workflow:python:
:PROPERTIES:
:EXPORT_FILE_NAME: poetry-direnv
:EXPORT_DATE: 2020-02-13 21:36
:END:
** Background
- I end up writing about using [[https://python-poetry.org/][poetry]] a lot
- I almost always [[https://direnv.net/][use direnv]] in real life too
- I don't keep writing mini scripts in my ~.envrc~

Honestly there's nothing here anyone using the [[https://github.com/direnv/direnv/wiki/Python][direnv wiki]] will find surprising,
but then it is still neat to link back to.

** Setting Up Poetry
This essentially works by simply modifying the global ~.direnvrc~ which
essentially gets sourced by every local ~.envrc~ anyway.
#+BEGIN_SRC sh
vim $HOME/.direnvrc
#+END_SRC
So what we put in there is the following snippet derived from other snippets [[https://github.com/direnv/direnv/wiki/Python][on
the wiki]], and is actually now there too.

#+BEGIN_SRC bash
# PUT this here
layout_poetry() {
  if [[ ! -f pyproject.toml ]]; then
    log_error 'No pyproject.toml found.  Use `poetry new` or `poetry init` to create one first.'
    exit 2
  fi

  local VENV=$(dirname $(poetry run which python))
  export VIRTUAL_ENV=$(echo "$VENV" | rev | cut -d'/' -f2- | rev)
  export POETRY_ACTIVE=1
  PATH_add "$VENV"
}
#+END_SRC

Now we can just make ~.envrc~ files with ~layout_poetry~ and everything will
/just workâ„¢/.

* DONE Replacing Jupyter with Orgmode :@programming:tools:emacs:workflow:orgmode:
:PROPERTIES:
:EXPORT_FILE_NAME: jupyter-orgmode
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:EXPORT_DATE: 2020-02-13 22:36
:END:
** Background
- I dislike Jupyter notebooks (and [[https://jupyter.org/][JupyterHub]]) a lot
- [[https://tkf.github.io/emacs-ipython-notebook/][EIN]] is really not much of a solution either

In the past I have written some posts on [[https://grimoire.science/latex-and-jupyterhub/][TeX with JupyterHub]] and discussed ways
to use virtual [[https://grimoire.science/python-and-jupyterhub/][Python with JupyterHub]] in a more reasonable manner.

However, I personally found that EIN was a huge pain to work with, and I mostly
ended up working with the web-interface anyway.

It is a bit redundant to do so, given that at-least for my purposes, the end
result was a LaTeX document. Breaking down the rest of my requirements went a
bit like this:

- What exports well to TeX? :: *Org*, Markdown, anything which goes into pandoc
- What displays code really well? :: LaTeX, Markdown, *Org*
- What allows easy visualization of code snippets? :: Rmarkdown, RStudio,
  JupyterHub, *Org* with babel

Clearly, [[https://orgmode.org/manual/][orgmode]] is the common denominator, and ergo, a perfect JupyterHub alternative.
** Setup
Throughout this post I will assume the following structure:
#+BEGIN_SRC bash :exports both
tree tmp
mkdir -p tmp/images
touch tmp/myFakeJupyter.org
#+END_SRC

#+RESULTS:
| tmp |                   |   |      |
| â”œâ”€â”€ | images            |   |      |
| â””â”€â”€ | myFakeJupyter.org |   |      |
| 1   | directory,        | 1 | file |

As is evident, we have a folder ~tmp~ which will have all the things we need for
dealing with our setup.

*** Virtual Python
Without waxing too eloquent on the whole reason behind doing this, since I will
rant about virtual python management systems elsewhere, here I will simply
describe my preferred method, which is [[https://python-poetry.org/][using poetry]].

#+BEGIN_SRC bash
# In a folder above tmp
poetry init
poetry add numpy matplotlib scipy pandas
#+END_SRC

The next part is optional, but a good idea if you figure out [[https://direnv.net/][using direnv]] and
have configured ~layout_poetry~ as [[https://rgoswami.me/posts/poetry-direnv][described here]]:
#+BEGIN_SRC bash
# Same place as the poetry files
echo "layout_poetry()" >> .envrc
#+END_SRC

*Note:*
- We can nest an arbitrary number of the ~tmp~ structures under a single place
  we define the poetry setup
- I prefer using ~direnv~ to ensure that I never forget to hook into the right environment
** Orgmode
This is not an introduction to org, however in particular, there are some basic
settings to keep in mind to make sure the set-up works as expected.

*** Indentation
Python is notoriously weird about whitespace, so we will ensure that our export
process does not mangle whitespace and offend the python interpreter. We will
have the following line at the top of our ~orgmode~ file:

#+BEGIN_SRC orgmode :tangle tmp/myFakeJupyter.org :exports code
# -*- org-src-preserve-indentation: t; org-edit-src-content: 0; -*-
#+END_SRC

*Note:*
- this post is actually generating the file being discussed here by
[[https://orgmode.org/manual/Extracting-Source-Code.html][tangling the file]]
- You can get the [[https://github.com/HaoZeke/haozeke.github.io/blob/src/content-org/tmp/myFakeJupyter.org][whole file here]]
*** TeX Settings
These are also basically optional, but at the very least you will need the
following:

#+BEGIN_SRC orgmode :tangle tmp/myFakeJupyter.org
#+author: Rohit Goswami
#+title: Whatever
#+subtitle: Wittier line about whatever
#+date: \today
#+OPTIONS: toc:nil
#+END_SRC

I actually use a lot of math using the ~TeX~ input mode in Emacs, so I like the
following settings for math:

#+BEGIN_SRC orgmode :tangle tmp/myFakeJupyter.org
# For math display
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{unicode-math}
#+END_SRC

There are a bunch of other settings which may be used, but these are the bare
minimum, more on that would be in a snippet anyway.

*Note:*
- rendering math in the ~orgmode~ file in this manner requires that we
 use ~XeTeX~ to compile the final file
*** Org-Python
We essentially need to ensure that:
- Babel uses our virtual python
- The same session is used for each block

We will get our poetry python pretty easily:
#+BEGIN_SRC bash
which python
#+END_SRC

#+RESULTS:
: /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python

Now we will use this as a common ~header-arg~ passed into the property drawer to
make sure we don't need to set them in every code block.

We can use the following structure in our file:

#+BEGIN_SRC orgmode :tangle tmp/myFakeJupyter.org :exports code
\* Python Stuff
  :PROPERTIES:
  :header-args:    :python /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python :session One :results output :exports both
  :END:
Now we can simply work with code as we normally would
\#+BEGIN_SRC python
print("Hello World")
\#+END_SRC
#+END_SRC

*Note:*
- For some reason, this property needs to be set on *every* heading (as of Feb 13 2020)
- In the actual file you will want to remove extraneous Â \ symbols:
  - \* â†’ *
  - \#+BEGIN_SRC â†’ #+BEGIN_SRC
  - \#+END_SRC â†’ #+END_SRC
*** Python Images and Orgmode
To view images in ~orgmode~ as we would in a JupyterLab notebook, we will use a
slight trick.
- We will ensure that the code block returns a file object with the arguments
- The code block should end with a print statement to actually generate the file
  name

 So we want a code block like this:

#+begin_example
#+BEGIN_SRC python :results output file :exports both
import matplotlib.pyplot as plt
from sklearn.datasets.samples_generator import make_circles
X, y = make_circles(100, factor=.1, noise=.1)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plt.xlabel('x1')
plt.ylabel('x2')
plt.savefig('images/plotCircles.png', dpi = 300)
print('images/plotCircles.png') # return filename to org-mode
#+end_src
#+end_example

Which would give the following when executed:

#+begin_example
#+RESULTS:
[[file:images/plotCircles.png]]
#+end_example

Since that looks pretty ugly, this will actually look like this:

#+BEGIN_SRC python :results output file :exports both
import matplotlib.pyplot as plt
from sklearn.datasets.samples_generator import make_circles
X, y = make_circles(100, factor=.1, noise=.1)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plt.xlabel('x1')
plt.ylabel('x2')
plt.savefig('images/plotCircles.png', dpi = 300)
print('images/plotCircles.png') # return filename to org-mode
#+end_src

[[file:tmp/images/plotCircles.png]]

*** Bonus
A better way to simulate standard ~jupyter~ workflows is to just specify the
properties once at the beginning.

#+BEGIN_SRC orgmode
#+PROPERTY: header-args:python :python /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python :session One :results output :exports both
#+END_SRC

This setup circumvents having to set the properties per sub-tree, though for
very large projects, it is useful to use different processes.
** Conclusions
- The last step is of course to export the file as to a ~TeX~ file and then
  compile that with something like ~latexmk -pdfxe -shell-escape file.tex~

There are a million and one variations of this of course, but this is enough to
get started.

The whole file is also [[https://github.com/HaoZeke/haozeke.github.io/blob/src/content-org/tmp/myFakeJupyter.org][reproduced here]].

** Comments
The older commenting system was implemented with [[https://utteranc.es][utteranc.es]] as seen below.
@@html:<script src="https://utteranc.es/client.js" repo="haozeke/haozeke.github.io" issue-term="pathname" theme="photon-dark" label="Utterance ðŸ’¬" crossorigin="anonymous" async></script>@@

* TODO Orgmode and Hugo :@programming:tools:emacs:webdev:hugo:
:PROPERTIES:
:EXPORT_FILE_NAME: hugo-orgmode
:END:
** Background
- This is about the site you are reading
- It is also a partial rant
- It has a lot to do with web development in general
* DONE Switching to Colemak :@personal:workflow:explanations:
:PROPERTIES:
:EXPORT_FILE_NAME: colemak-switch
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:EXPORT_DATE: 2020-02-29 14:06
:EXPORT_HUGO_AUTO_SET_LASTMOD: t
:END:
#+BEGIN_QUOTE
Thoughts on and rationale behind leaving QWERTY and touch typing in general. Followed [[Refactoring Dotfiles For Colemak][by this post]] on refactoring my Dotfiles.
#+END_QUOTE

** Background
I just realized that it has been over two years since I switched from QWERTY to
Colemak but somehow never managed to write about it. It was a major change in my
life, and it took forever to get acclimatized to. I do not think I'll ever again be
in a position to make such a change in my life again, but it was definitely
worth it.
** Touch Typing
My interest in touch typing in I decided to digitize my notes for posterity, during the
last two years of my undergraduate studies back in Harcourt Butler Technical
Institute (HBTI) Kanpur, India. in one of my many instances of yak shaving, I
realized I could probably consume and annotate a lot more content by typing
faster. Given that at that stage I was already a fast talker, it seemed like a
natural extension. There was probably an element of nostalgia involved as well.
That and the end of a bachelors involves the thesis, which generally involves a
lot of typing.

There were (and are) some fantastic resources for learning to touch type
nowadays, I personally used:
- [[https://www.typing.com/][Typing.com]] :: This is short, but a pretty good basic setup. The numbering and
  special characters are a bit much to take in at the level of practice you get
  by completing all the previous exercises, but eventually they make for a good workout.
- [[https://www.typingclub.com/][TypingClub]] :: This is what I ended up working my way through. It is
  comprehensive, beautiful, and fun.

Also, later, I ended up using [[https://www.keybr.com/][keybr]] a lot, simply because typing gibberish is a
good way of practicing, and it is independent of the keyboard layout.

Just to foreshadow things, the enemy facing me at this point was the layout
itself[fn:img] .

https://www.keyboard-design.com/kb-images/qwerty-kla.jpg

** Alternate layouts
Having finally broken into the giddy regimes of 150+ wpm, I was ecstatic, and
decided to start working my way through some longer reports. However, I quickly
realized I was unable to type for more than a couple of minutes without getting
terribly cramped. Once it got to the point of having to visit a physiotherapist,
I had to call it quits. At that stage, relearning the entire touch typing
corpus, given that I already was used to QWERTY, seemed pretty bleak.

It took forever, and I ended up applying my choices to my phone keyboard as
well, which presumably helped me in terms of increasing familiarity, had the
unintended effect of making me seem distant to people I was close to, since my
verbose texts suddenly devolved to painful one-liners.

The alternative layouts I tried were:

- [[https://www.dvorak-keyboard.com/][DVORAK]] :: At the time, TypingClub only supported QWERTY and DVORAK, so it was
  pretty natural for me to try it out. There are also some [[https://www.dvzine.org/][very nice comics
  about it]]. I remember that it was pretty neat, with
  a good even distribution, until I tried coding. The placement of the
  semicolons make it impossible to use while programming. I would still say it
  makes for a comfortable layout, as long as special characters are not required.

https://www.keyboard-design.com/kb-images/dvorak-kla.jpg

- [[http://mkweb.bcgsc.ca/carpalx][CarpalX]] :: I experimented with the entire carpalx family, but I was unable to get
  used to it. I liked QFMLWY best. I do recommend reading the training methodology, especially if
  anyone is interested in numerical optimization in general. More importantly,
  though it was relatively easy to set up on my devices and operating systems,
  the fact that it wasn't natively supported meant a lot of grief whenever I
  inevitably had to use a public computer.

https://www.keyboard-design.com/kb-images/qgmlwy-kla.jpg

- Colemak :: Eventually I decided to go with [[https://colemak.com/][Colemak]], especially since it is
  widely available. Nothing is easier than ~setxkbmap us -variant colemak -option grp:alt_shift_toggle~ on public machines and it's easy on Windows as
  well. Colemak seems like a good compromise. I personally have not been able to
  reach the same speeds I managed with QWERTY, even after a year, but then
  again, I can be a lot more consistent, and it hurts less. Nowadays, Colemak
  has made its way onto most typing sites as well, including TypingClub

https://www.keyboard-design.com/kb-images/colemak-kla.jpg

*** What about VIM?
- DVORAK makes it impossible, so do most other layouts, but there are some
  tutorials purporting to help use vim movement with DVORAK
- Colemak isn't any better, but the fact of the matter is that once you know VIM
  on QWERTY, and have separately internalized colemak or something else, hitting
  keys is just hitting keys

+All that said, I still occasionally simply remap HJKL (QWERTY movement) to HNEI (Colemak analog) when it is feasible.+
*update:* I actually ended up refactoring my entire Dotfiles to use more Colemak native bindings, as described [[Refactoring Dotfiles For Colemak][in this post]].
** Conclusion
Changing layouts was a real struggle. Watching my WPM drop back to lower than
hunt and peck styles was pretty humiliating, especially since the reports kept
coming in, and more than once I switched to QWERTY. However, since then, I have
managed to stay on course. I guess if I think about it, it boils down to a few
scattered thoughts:
- Typing is kinda like running a marathon, knowing how it is done and doing it
  are two different things
- Tell *everyone*, so people can listen to you lament your reduced speed and not
  hate you for replying slowly
- Practice everyday, because, well, it works out in the long run, even when you
  plateau
- Alternate shifts! That's really something which should show up more in
  tutorials, especially for listicles, not changing the shifts will really hurt
- Try and get a mechanical keyboard (like the [[https://www.annepro.net/][Anne Pro 2]] or the [[https://www.coolermaster.com/catalog/peripheral/keyboards/masterkeys-pro-l-white/][Coolermaster Masterkeys]]), they're fun and easy to change layouts on

** Comments
The older commenting system was implemented with [[https://utteranc.es][utteranc.es]] as seen below.
@@html:<script src="https://utteranc.es/client.js" repo="haozeke/haozeke.github.io" issue-term="pathname" theme="photon-dark" label="Utterance ðŸ’¬" crossorigin="anonymous" async></script>@@

[fn:img] The images are [[https://www.keyboard-design.com/best-keyboard-layouts.html][from here]], where there's also an effort based metric
used to score keyboard layouts.
* TODO Replacing Rstudio with Emacs :@programming:tools:emacs:workflow:R:
:PROPERTIES:
:EXPORT_FILE_NAME: rstudio-emacs
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:EXPORT_DATE: 2020-02-15 04:38
:END:
** Background
RStudio is one of the best IDEs around, in that it is essentially a text editor
and terminal with some pretty printing and object viewing functionality. It is
really great, but it is also relatively resource intensive. It turns out that
thanks to Emacs ESS, it is possible to circumvent Rstudio completely in favor of
an Emacs-native workflow.
* TODO Role models and colleges
* TODO My current courses
* TODO Rude college admissions
* DONE Pandora and Proxychains :@personal:tools:workflow:
:PROPERTIES:
:EXPORT_FILE_NAME: pandora-proxychains
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :comments true
:EXPORT_DATE: 2020-02-15 05:28
:END:
** Background
- Pandora doesn't work outside the states
- I keep forgetting how to set-up ~proxychains~
** Proxychains
Technically this article [[https://github.com/rofl0r/proxychains-ng][expects proxychains-ng]], which seems to be the more
up-to-date fork of the original ~proxychains~.

1. Install ~proxychains-ng~
   #+BEGIN_SRC bash
# I am on archlinux..
sudo pacman -S proxychains-ng
   #+END_SRC
2. Copy the configuration to the ~$HOME~ directory
   #+BEGIN_SRC bash
cp /etc/proxychains.conf .
   #+END_SRC
3. Edit said configuration to add some US-based proxy

In my particular case, I don't keep the tor section enabled.
#+BEGIN_SRC bash :exports both :results raw
tail $HOME/proxychains.conf
#+END_SRC

#+RESULTS:
#+begin_example
#
#       proxy types: http, socks4, socks5
#        ( auth types supported: "basic"-http  "user/pass"-socks )
#
[ProxyList]
# add proxy here ...
# meanwile
# defaults set to "tor"
# socks4 	127.0.0.1 9050
#+end_example

I actually use [[https://windscribe.com][Windscribe]] for my VPN needs, and they have a neat [[https://windscribe.com/getconfig/socks][SOCKS5 proxy
setup]]. This works out to a line like ~socks5 $IP $PORT $USERNAME $PASS~ being
added. The default generator gives you a pretty server name, but to get the IP
I use ~ping $SERVER~ and put that in the ~conf~ file.
** Pandora
I use the excellent ~pianobar~ frontend.
1. Get [[https://github.com/PromyLOPh/pianobar][pianobar]]
   #+BEGIN_SRC bash
sudo pacman -S pianobar
   #+END_SRC
2. Use it with ~proxychains~
   #+BEGIN_SRC bash
proxychains pianobar
   #+END_SRC
3. Profit

I also like setting up some defaults to make life easier:
#+BEGIN_SRC bash
mkdir -p ~/.config/pianobar
vim ~/.config/pianobar/config
#+END_SRC
I normally set the following (inspired by the [[https://wiki.archlinux.org/index.php/Pianobar][ArchWiki]]):
#+BEGIN_SRC conf
audio_quality = {high, medium, low}
autostart_station = $ID
password = "$PASS"
user = "$emailID"
#+END_SRC

The ~autostart_station ID~ can be obtained by inspecting the terminal output
during an initial run. I usually set it to the QuickMix station.
* Bojack Horseman :@personal:thoughts:random:review:TV:
:PROPERTIES:
:EXPORT_FILE_NAME: bojack-horseman
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :comments false
:EXPORT_DATE: 2020-02-27 22:28
:END:
** Background
For a while I was worried about writing about a TV show here. I thought it might
be frivolous, or worse, might outweigh the other kinds of articles I would like
to write. However, like most things, that which is ignored just grows, so it is
easier to just write and forget about it.
** The Show
Much has been said about how Bojack Horseman is one of the best shows ever, and
they're all correct. For that matter I won't be going into the details of how
every episode ties together a tapestry of lives in a meaningful way, or any of
that. The show was amazingly poignant. The characters felt real. Which actually
leads me to the real issue.
** The End
The end of Bojack was *good*. It was the way it was meant to be. For a
slice-of-life show, it is a natural conclusion. It isn't necessary that any
catharsis occurs or that the characters change or become better or all that
jazz. It isn't about giving the viewers closure. It is simply about a window
onto the lives of (fictional) characters being shut. To that end, I disliked
attempts to bring closure in the show itself.

One of the main reasons why I felt strongly enough to write this, is simply
because when I looked around, the prevailing opinion was that the main character
should have been killed off, _for his sins_. This strikes me as a very flippant
attitude to take. It reeks of people trying to make the show a cautionary tale,
which is frankly speaking a weird approach to take towards any fictional story.
The idea that the character should be redeemed also seemed equally weak, for
much the same reasons.

The fact that the characters are hypocrites, and that none of them are as good
or bad as they make themselves out to be is one of the best parts of the show.

** Conclusion
That's actually all I have to say about this. I thought of adding relevant memes
or listing episodes or name dropping sites, but this isn't buzzfeed. The show is
incredible, and there are far better ways of proving that. Bust out your
favorite search engine + streaming content provider / digital piracy eye-patch
and give it a whirl. The only thing I'd suggest is watching everything in order,
it's just that kind of show.

* TODO The Morpho Language :@programming:review:
:PROPERTIES:
:EXPORT_FILE_NAME: morpho-lang
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:END:
* TODO Towards DOOM-Emacs :@programming:workflow:review:
:PROPERTIES:
:EXPORT_FILE_NAME: towards-doom-emacs
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments false
:END:
** Background
[[https://dotdoom.rgoswami.me/][My doom-emacs configuration]] gets a rather insane number of views every month.
Statistically, it accounts for 90% of the traffic to [[https://grimoire.science][my other site]], and that is
essentially around three times time traffic on the rest of my presences,
combined. I followed a pretty standard path to finally reach doom-emacs.
However, before delving into it, I thought I'd discuss the chronological aspects
of my road to doom. In a nutshell it was just:

Word â†’ Notepad++ â†’ Sublime Text 3 â†’ VIM â†’ Emacs (Spacemacs) â†’ Emacs (doom-emacs)
* DONE Provisioning Dotfiles on an HPC :@programming:workflow:projects:hpc:
:PROPERTIES:
:EXPORT_FILE_NAME: prov-dots
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:EXPORT_DATE: 2020-03-16 00:06
:END:
** Background
[[https://github.com/HaoZeke/Dotfiles][My dotfiles]] turned 4 years old a few months ago (since 9th Jan 2017) and remains one of my most
frequently updated projects for obvious reasons. Going through the changes
reminds me of a whole of posts I never got around to writing.

Anyway, recently I gained access to another HPC cluster, with a standard configuration
(bash, old CentOS) and decided to track my provisioning steps. This is really a
very streamlined experience by now, since I've used the same setup across scores
of machines. This is actually also a generic intro to configuring user setups on
HPC (high performance cluster) machines, if one is inclined to read it in that
manner. To that end, sections of this post involve restrictions relating to user
privileges which aren't normally part of most Dotfile setups.
*** Aside
- Dotfiles define most people who maintain them
- No two sets are ever exactly alike
- They fall somewhere between winging it for each machine and using something
  like [[https://www.habitat.sh/learn/][Chef]] or [[https://www.ansible.com/][Ansible]]
- Tracking dotfiles is really close to having a sort of out-of-context journal

Before I settled on using [[https://github.com/kobus-v-schoor/dotgit][the fabulous dotgit]], I considered several
alternatives, most notably [[https://www.gnu.org/software/stow/][GNU stow]].
** Preliminaries
It is important to note the environment into which I had to get my
setup.
*** SSH Setup
- The very first thing to do is to use a new ~ssh-key~
#+BEGIN_SRC bash
export myKey="someName"
ssh-keygen -f $HOME/.ssh/$myKey
# I normally don't set a password
ssh-add $HOME/.ssh/$myKey
ssh-copy-id $myHPC
# myHPC being an IP address
#+END_SRC
I more often than not tend to back this up with a cutesy alias, also because I
do not always get my username of choice on these machines. So in
~$HOME/.ssh/config~ I use:
#+BEGIN_SRC conf
Host myHPC
 Hostname 127.0.0.1
 User somethingIgot
 IdentityFile ~/.ssh/myKey
#+END_SRC
*** Harvesting Information
- I normally use [[https://github.com/dylanaraps/neofetch][neofetch]] on new machines
#+BEGIN_SRC bash
mkdir -p $HOME/Git/Github
cd $HOME/Git/Github
git clone https://github.com/dylanaraps/neofetch.git
cd neofetch
./neofetch
#+END_SRC

#+caption: Neofetch Output
[[file:images/sampleHPC.png]]

Where the top has been tastefully truncated. Just for context, the latest ~bash~
as of this writing is ~v5.0.16~ so, that's not too bad, given that ~neofetch~
works for ~bash~ â‰¥ 3.2

** TODO Circumventing User Restrictions with Nix
- A post in and of itself would be required to explain why and how users are
  normally restricted from activities in cluster nodes
- Here, we leverage the [[https://nixos.org/nix/manual/#chap-installation][nix-package management system]] to circumvent these
- User installation of ~nix~ is sadly non-trivial, so this might be of some use[fn:nixUsr]
*** Testing nix-user-chroot
1. We will first check namespace support
#+BEGIN_SRC bash
# Errored out
unshare --user --pid echo YES
# Worked!
zgrep CONFIG_USER_NS /boot/config-$(uname -r)
# CONFIG_USER_NS=y
#+END_SRC

Thankfully we have support for namespaces, so we can continue with ~nix-user-chroot~.

2. Since we definitely do not have ~rustup~ or ~rustc~ on the HPC, we will use [[https://github.com/nix-community/nix-user-chroot/releases][a
   prebuilt binary]] of ~nix-user-chroot~

#+BEGIN_SRC bash
cd $HOME && wget -O nix-user-chroot  https://github.com/nix-community/nix-user-chroot/releases/download/1.0.2/nix-user-chroot-bin-1.0.2-x86_64-unknown-linux-musl
#+END_SRC

3. Similar to [[https://nixos.wiki/wiki/Nix_Installation_Guide#Installing_without_root_permissions][the wiki example]], we will use ~$HOME/.nix~

#+BEGIN_SRC bash
cd ~/
chmod +x nix-user-chroot
mkdir -m 0755 ~/.nix
./nix-user-chroot ~/.nix bash -c 'curl https://nixos.org/nix/install | sh'
#+END_SRC

- Only, this *doesn't work*

Turns out that since ~unshare~ is too old, ~nix-user-chroot~ won't work either.

*** Using PRoot
PRoot is pretty neat in general, they even have a [[https://proot-me.github.io/][nice website describing it]].
0. Set a folder up for local installations (this is normally done by my
   Dotfiles, but we might as well have one here too)
#+BEGIN_SRC bash
mkdir -p $HOME/.local/bin
export PATH=$PATH:$HOME/.local/bin
#+END_SRC
1. Get a binary from the [[https://gitlab.com/proot/proot/-/jobs][GitLab artifacts]]
#+BEGIN_SRC bash
cd $HOME
mkdir tmp
cd tmp
wget -O artifacts.zip https://gitlab.com/proot/proot/-/jobs/452350181/artifacts/download
unzip artifacts.zip
mv dist/proot $HOME/.local/bin
#+END_SRC
2. Bind and install ~nix~
#+BEGIN_SRC bash
mkdir ~/.nix
export PROOT_NO_SECCOMP=1
proot -b ~/.nix:/nix
export PROOT_NO_SECCOMP=1
curl https://nixos.org/nix/install | sh
#+END_SRC

If you're very unlucky, like I was, you may be greeted by a lovely little error
message along the lines of:

#+begin_example
/nix/store/ddmmzn4ggz1f66lwxjy64n89864yj9w9-nix-2.3.3/bin/nix-store: /opt/ohpc/pub/compiler/gcc/5.4.0/lib64/libstdc++.so.6: version `GLIBCXX_3.4.22' not found (required by /nix/store/c0b76xh2za9r9r4b0g3iv4x2lkw1zzcn-aws-sdk-cpp-1.7.90/lib/libaws-cpp-sdk-core.so)
#+end_example

Which basically is as bad as it sounds. At this stage, we need a newer compiler
to even get ~nix~ up and running, but can't without getting an OS update. This
chicken and egg situation calls for the drastic measure of leveraging ~brew~
first[fn:brewStuff].

#+BEGIN_SRC bash
sh -c "$(curl -fsSL https://raw.githubusercontent.com/Linuxbrew/install/master/install.sh)"
#+END_SRC

Note that nothing in this section suggests the best way is not to lobby your
sys-admin to install ~nix~ system-wide in multi-user mode.
** Giving Up with Linuxbrew
- Somewhere around this point, [[https://docs.brew.sh/Homebrew-on-Linux][linuxbrew]] is a good idea
- More on this later
** Shell Stuff
~zsh~ is my shell of choice, and is what my ~Dotfiles~ expect and work best with.
- I did end up making a quick change to update the ~dotfiles~ with a target
  which includes a snippet to transition to ~zsh~ from the default ~bash~ shell
** Dotfiles
The actual installation steps basically tracks [[https://github.com/HaoZeke/Dotfiles][the readme instructions]].

#+BEGIN_SRC bash
git clone https://github.com/kobus-v-schoor/dotgit.git
mkdir -p ~/.bin
cp -r dotgit/bin/dotgit* ~/.bin
cat dotgit/bin/bash_completion >> ~/.bash_completion
rm -rf dotgit
# echo 'export PATH="$PATH:$HOME/.bin"' >> ~/.bashrc
echo 'export PATH="$PATH:$HOME/.bin"' >> ~/.zshrc
#+END_SRC

[fn:nixUsr] Much of this section is directly adapted from [[https://nixos.wiki/wiki/Nix_Installation_Guide#Installing_without_root_permissions][the NixOS wiki]]
[fn:brewStuff] This used to be called linuxbrew, but the [[https://docs.brew.sh/Homebrew-on-Linux][new site]] makes it clear
that it's all one ~brew~ now.
* DONE Shorter Posts :@notes:tools:rationale:workflow:ideas:
:PROPERTIES:
:EXPORT_FILE_NAME: shortpost
:EXPORT_DATE: 2020-03-16 00:16
:END:
** Background
Sometime this year, I realized that I no longer have access to a lot of my older
communication. This included, a lot of resources I enjoyed and shared with the
people who were around me at that point in time. To counter this, I have decided
to opt for shorter posts, even if they don't always include the same level of
detail I would prefer to provide.

*** Alternatives
- I have an automated system based around IFTTT combined with Twitter, Diigo,
  and even Pocket
- However, that doesn't really tell me much, and trawling through a massive glut
  of data is often pointless as well
- There's always Twitter, but I don't really care to hear the views of others
  when I want to revisit my own ideas
** Conclusions
- I will be making shorter posts here, like the random one on [[https://rgoswami.me/posts/ghnotif/][octobox]]
* DONE D3 for Git :@notes:tools:rationale:workflow:ideas:
:PROPERTIES:
:EXPORT_FILE_NAME: d3git
:EXPORT_DATE: 2020-03-16 00:17
:END:
** Background
- I have had a lot of discussions regarding the teaching of ~git~
- This is mostly as a part of [[https://static.carpentries.org/maintainers/#HaoZeke][the SoftwareCarpentries]], or in view of my
  [[https://www.univ.ai/teams/rohit-goswami][involvement with univ.ai]], or simply in every public space I am associated with
- Without getting into my views, I just wanted to keep this resource in mind
** The site
- Learning ~git~ is a highly contentious thing
- People seem to be fond of GUI tools, especially since on non *nix systems, it
  seems that there is a lot of debate surrounding obtaining the ~git~ utility in
  the first place

One of the best ways of understanding (without installing stuff) the mental
models required for working with ~git~ is [[https://onlywei.github.io/explain-git-with-d3/#checkout][this site]]

#+caption: A screenshot of the site
[[file:images/d3git.png]]

- However, as is clear, this is not exactly a replacement for a good old command-line.

- It does make for a good resource for teaching with slides, or for generating
  other static visualizations, where live coding is not an option
* DONE Trees and Bags :@notes:theory:statistics:math:
:PROPERTIES:
:EXPORT_FILE_NAME: trees-and-bags
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:EXPORT_HUGO_CUSTOM_FRONT_MATTER+: :link-citations true
:EXPORT_HUGO_PANDOC_CITATIONS: t
:EXPORT_DATE: 2020-03-26 00:28
:END:
# :EXPORT_HUGO_CUSTOM_FRONT_MATTER+: :nocite '(@hastieElementsStatisticalLearning2009)

#+BEGIN_QUOTE
  Explain why using bagging for prediction trees generally improves
  predictions over regular prediction trees.
#+END_QUOTE


** Introduction

Bagging (or Bootstrap Aggregation) is one of the most commonly used
ensemble method for improving the prediction of trees. We will broadly
follow a historical development trend to understand the process. That
is, we will begin by considering the Bootstrap method. This in turn
requires knowledge of the Jacknife method, which is understandable from
a simple bias variance perspective. Finally we will close out the
discussion by considering the utility and trade-offs of the Bagging
technique, and will draw attention to the fact that the Bagging method
was contrasted to another popular ensemble method, namely the Random
Forest method, in the previous section.

Before delving into the mathematics, recall that the approach taken by
bagging is given as per @cichoszDataMiningAlgorithms2015 to be:

- create base models with *bootstrap* samples of the training set
- combine models by unweighted voting (for classification) or by
  averaging (for regression)

The reason for covering the Jacknife method is to develop an intuition
relating to the sampling of data described in the following table:

| Data-set   Size  per   sample | Estimator         |
| Reduces                       | Jacknife          |
| Remains    the   same         | Bootstrap         |
| Increases                     | data-augmentation |

** Bias Variance Trade-offs

We will recall, for this discussion, the bias variance trade off which
is the basis of our model accuracy estimates (for regression) as per the
formulation of @jamesIntroductionStatisticalLearning2013.

\begin{equation}
E(yâ‚€-\hat{f}(xâ‚€))Â²=\mathrm{Var}(\hat{f}(xâ‚€))+[\mathrm{Bias(\hat{f(xâ‚€)})}]Â²+\mathrm{Var}(Îµ)
\end{equation}

Where:

- $E(y_{0}-\hat{f}(x_{0}))Â²$ is the expected test MSE, or the average
  test MSE if $f$ is estimated with a large number of training sets and
  tested at each $xâ‚€$
- The variance is the amount by which our approximation $\hat{f}$ will
  change if estimated by a different training set, or the *flexibility*
  error
- The bias is the (reducible) *approximation* error, caused by not
  fitting to the training set exactly
- $\mathrm{Var}(Îµ)$ is the *irreducible* error

We will also keep in mind, going forward the following requirements of a
good estimator:

- Low variance AND low bias
- Typically, the variance increases while the bias decreases as we use
  more flexible methods (i.e.Â methods which fit the training set
  better[fn:smooth])

Also for the rest of this section, we will need to recall from
@hastieElementsStatisticalLearning2009, that the bias is given by:

\begin{equation}
[E(\hat{f_{k}}(xâ‚€)-f(xâ‚€)]Â²
\end{equation}

Where the expectation averages over the randomness in the training data.

To keep things in perspective, recall from
@hastieElementsStatisticalLearning2009:

#+CAPTION: Test and training error as a function of model complexity
[[file:images/biasVar.png]]

** Jacknife Estimates
    :PROPERTIES:
    :CUSTOM_ID: jacknife-estimates
    :END:

We will model our discussion on the work of
@efronJackknifeBootstrapOther1982. Note that:

- The $\hat{Î¸}$ symbol is an estimate of the true quantity $Î¸$
- This is defined by the estimate being $\hat{Î¸}=Î¸(\hat{F})$
- $\hat{F}$ is the empirical probability distribution, defined by mass
  $1/n$ at $xáµ¢ âˆ€ iâˆˆI$, i is from 1 to n

The points above establishes our bias to be given by
$E_FÎ¸(\hat{F})-Î¸(F)$ such that $E_F$ is the expectation under xâ‚â‹¯xâ‚™~F.

To derive the Jacknife estimate $(\tilde{Î¸})$ we will simply
sequentially delete points xáµ¢ (changing $\hat{F}$), and recompute our
estimate $\hat{Î¸}$, which then simplifies to:

\begin{equation}
\tilde{Î¸}\equiv n\hat{Î¸}-(\frac{n-1}{n})âˆ‘_{i=1}â¿\hat{Î¸}
\end{equation}

In essence, the Jacknife estimate is obtained by making repeated
estimates on increasingly smaller data-sets. This intuition lets us
imagine a method which actually makes estimates on larger data-sets
(which is the motivation for data augmentation) or, perhaps not so
intuitively, on estimates on data-sets of the same size.

** Bootstrap Estimates

Continuing with the same notation, we will note that the bootstrap is
obtained by draw random data-sets with replacement from the training
data, where each sample is the same size as the original; as noted by @hastieElementsStatisticalLearning2009.

We will consider the bootstrap estimate for the standard deviation of
the $\hat{Î¸}$ operator, which is denoted by $Ïƒ(F,n,\hat{\theta})=Ïƒ(F)$

The bootstrap is simple the standard deviation at the approximate F,
i.e., at $F=\hat{F}$:

\begin{equation}
\hat{\mathrm{SD}}=\sigma(\hat{F})
\end{equation}

Since we generally have no closed form analytical form for $Ïƒ(F)$ we
must use a Monte Carlo algorithm:

1. Fit a non parametric maximum likelihood estimate (MLE) of F,
   i.e.Â $\hat{F}$
2. Draw a sample from $\hat{F}$ and calculate the estimate of $\hat{Î¸}$
   on that sample, say, $\hat{Î¸}^*$
3. Repeat 2 to get multiple (say B) replications of $\hat{Î¸}^*$

Now we know that as $Bâ†’âˆž$ then our estimate would match $Ïƒ(\hat{F})$
perfectly, however, since that itself is an estimate of the value we are
actually interested in, in practice there is no real point using a very
high B value.

Note that in actual practice we simply use the given training data with
repetition and do not actually use an MLE of the approximate true
distribution to generate samples. This causes the bootstrap estimate to
be unreasonably good, since there is always significant overlap between
the training and test samples during the model fit. This is why cross
validation demands non-overlapping data partitions.

*** Connecting Estimates

The somewhat surprising result can be proved when $\hat{Î¸}=Î¸(\hat{F}$ is
a quadratic functional, namely:

\begin{equation}\hat{\mathrm{Bias}}_{boot}=\frac{n-1}{n} \hat{\mathrm{Bias}}_{jack}\end{equation}

In practice however, we will simply recall that the Jacknife tends to
overestimate, and the Bootstrap tends to underestimation.

** Bagging

Bagging, is motivated by using the bootstrap methodology to improve the
estimate or prediction directly, instead of using it as a method to
asses the accuracy of an estimate. It is a representative of the
so-called parallel ensemble methods where the base learners are
generated in parallel. As such, the motivation is to reduce the error by
exploiting the independence of base learners (true for mathematically
exact bootstrap samples, but not really true in practice).

Mathematically the formulation of @hastieElementsStatisticalLearning2009
establishes a connection between the Bayesian understanding of the
bootstrap mean as a posterior average, however, here we will use a more
heuristic approach.

We have noted above that the bagging process simply involves looking at
different samples in differing orders. This has some stark repercussions
for tree-based methods, since the trees are grown with a /greedy/
approach.

- Bootstrap samples may cause different trees to be produced
- This causes a reduction in the *variance*, especially when not too
  many samples are considered
- Averaging, reduces variance while leaving bias unchanged

Practically, these separate trees being averaged allows for varying
importance values of the variables to be calculated.

In particular, following @hastieElementsStatisticalLearning2009, it is
possible to see that the MSE tends to decrease by bagging.

\begin{align}
 E_P[Y-\hat{f}^*(x)]Â² & = & E_P[Y-f*{ag}(x)+f^*_{ag}(x)-\hat{f}^*(x)]Â² \\
& = & E_P[Y-f^*_{ag}(x)]Â²+E_P[\hat{f}^*(x)-f^*_{ag}(x)]Â² â‰¥ E_P[Y-f^*_{ag}(x)]Â²
\end{align}

Where:

- The training observations are independently drawn from a distribution
  $P$
- $f_{ag}(x)=E_P\hat{f}^*(x)$ is the ideal aggregate estimator

For the formulation above, we assume that $f_{ag}$ is a true bagging
estimate, which draws samples from the actual population. The upper
bound is obtained from the variance of the $\hat{f}^*(x)$ around the
mean, $f_{ag}$

Practically, we should note the following:

- The regression trees are deep
- The greedy algorithm growing the trees cause them to be unstable
  (sensitive to changes in input data)
- Each tree has a high variance, and low bias
- Averaging these trees reduces the variance

Missing from the discussion above is how exactly the training and test
sets are used in a bagging algorithm, as well as an estimate for the
error for each base learner. This has been reported in the code above as
the OOB error, or out of bag error. We have, as noted by
@zhouEnsembleMethodsFoundations2012 and @breimanBaggingPredictors1996
the following considerations.

- Given $m$ training samples, the probability that the iáµ—Ê° sample is
  selected 0,1,2... times is approximately Poisson distributed with
  $Î»=1$
- The probability of the iáµ—Ê° example will occur at least once is then
  $1-(1/e)â‰ˆ0.632$
- This means for each base learner, there are around $36.8$ % original
  training samples which have not been used in its training process

The goodness can thus be estimated using these OOB error, which is
simply an estimate of the error of the base tree on the OOB samples.

As a final note, random forests are conceptually easily understood by combining
bagging with subspace sampling, which is why in most cases and packages, we used
bagging as a special case of random forests, i.e.Â when no subspace sampling is
performed, random forests algorithms perform bagging.


[fn:smooth] This is mostly true for reasonably smooth true functions
* TODO d-SEAMS got published
* TODO Why I don't cure cancer
With the coronavirus pandemic going on, some of the louder rabble of the
academic community (as evinced by Twitter) have been calling for the shut down
of non-essential work. The real reason why it doesn't matter if there's a
pandemic going on is simply because the work keeps you up anyway. Working on
projects you love is like carrying a pandemic around with you all the time. It
is impossible to let go of in the first place. Understandably not everyone works
like this, and there are as many reasons to be on a project as there are people
probably.
* TODO Machine Learning is not the future
- I dislike machine learning in terms of scientific achievement
- Competitions are no way to bring a field forward
- State of the art on a month to month basis is a very poor way of understanding
  any field
- The ability to provide direct industrial applications is probably why this is
  so popular

This kind of behavior would be pretty unthinkable in other fields. The push to
clear a benchmark simply discards the basic ideas behind learning a subject in
the first place.
* TODO The net is not for socializing
- I used to go online to be an idea, an embodiment of an idea
Nowadays we bring our selves to the internet and I don't think that is as
liberating as the older format.
* DONE Analytics: Google to Goat :@notes:tools:rationale:workflow:ideas:
CLOSED: [2020-04-09 Thu 17:17]
:PROPERTIES:
:EXPORT_FILE_NAME: goat-google
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments false
:EXPORT_DATE: 2020-04-09 11:17
:END:
** Background
Like a lot of my tech based rants, this was brought on by a recent [[https://news.ycombinator.com/item?id=22813168][Hacker News
post]]. I won't go into why the product listed there is a hollow faux FOSS
rip-off. I won't discuss how the 'free' analytics option, like many others are
just hobby projects taking pot shots at other projects. Or how insanely
overpriced most alternatives are.

I will however discuss why and how I transitioned to using the awesome Goat
Counter.
** Google Analytics
I would like to point out that it is OK to start out with Google Analytics. It
is easy, and free, and scales well. There are reasons not to, but it is a good
starting point.

*** Pros
- Google Analytics is free, truly free
- The metrics are very detailed
- It is easy to set up
*** Cons
- Privacy concerns
- Blocked by people
- Easy to obsess over metrics

** Goat Counter
As with most Hacker News posts, the article itself was nothing compared to the
excellent comment thread. It was there that I came across people praising [[https://www.goatcounter.com/][Goat Counter]].

*** Pros
- Is open sourced ([[https://github.com/zgoat/goatcounter][here on Github]])
- Super lightweight
- Anonymous statistics
- Easy to share
*** Cons
- Has an upper limit on free accounts (10k a month)
- I am not very fond of Go
** Conclusions
I might eventually go back to GA, if I go over the 10k page view limit. Then
again, I might not. It might be more like, I only care about the first 10k
people who make it to my site.

**UPDATE:** This site has since shifted to Clicky, for [[Analytics II: Goat to Clicky][reasons outlined here]]
* TODO Fediverse Thoughts
** Background
I recently decided to take a half day off. Naturally I began looking into things
I've never seen before. I then ran into the delightful fediverse again.
* DONE On-boarding for Code in Place :@notes:ideas:teaching:cs106a:
CLOSED: [2020-04-10 Fri 16:01]
:PROPERTIES:
:EXPORT_FILE_NAME: scp-onboarding
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:END:
** Background
A few weeks ago, I ended up recording a video for the [[https://compedu.stanford.edu/codeinplace/announcement/][Stanford CS106A: Code in Place]]
initiative (which can be [[https://youtu.be/J0ULMEtM00w][found here]]). I heard back a while ago, and am now to lead a section for the
course!

I'll probably be making a series of short posts as this process continues.
** On-Boarding
This was very reminiscent of the [[http://carpentries.github.io/instructor-training/][Carpentries instructor training]], which makes
sense, given how well thought out that experience was.

We started out with a pre-presentation where people were able to just spitball
and connect, which is pretty neat.

One of the interesting parts of this, was the idea of *interactive recorded
lectures*, where the professors will be watching lectures with the students. The
entire slide deck [[https://docs.google.com/presentation/d/12DFKzJWYunNbVMdJ3PbMlS3ZutSoPlMnyzXpFcKyHJc/edit#slide=id.p2][is here]].

The other great idea for this kind of long course was the idea of having a Tea
room and a Teachers lounge where people can just tune in to chat.
*** Caveats
A couple of things which keep cropping up for online teaching in general are the
following:
- Zoom does not have persistent chats, so an auxiliary tool like an [[https://board.net][Etherpad]] is great
* DONE Small Section On-boarding :@notes:teaching:cs106a:
CLOSED: [2020-04-14 Tue 02:48]
:PROPERTIES:
:EXPORT_FILE_NAME: scp-smallgrp
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:END:
** Background
As I mentioned in my [[https://rgoswami.me/posts/scp-onboarding/][last post]], I'm leading a section for [[https://compedu.stanford.edu/codeinplace/announcement/][Stanford CS106A: Code
in Place]]. I did also mention I'd try to keep a set of short notes on the
process. So there[fn:videos].
** The Training
Given the overwhelming number of students, and section leaders, the small groups
are for fostering a community of teachers.

# Arun Kulshrestha is the section leader. He graduated a while ago from Stanford
# and was a section leader too.

- [ ] Consider allowing for daisy chaining during introductions
- [ ] Discussions are the primary take-away
- [ ] Only the instructor should be coding during the session

*** Core components
- Clarity
- Content
- Atmosphere
- Section management
- Correctness
*** Sectional Details
- Check in at the start
- Notice the space
- Check in regularly
- Avoid negative phrases
- Establish norms and the general culture
*** Zoom Norms
- Have people introduce themselves
- Mute people when they aren't talking
- Raise hands
- Try to use icebreakers which respect privacy
*** Materials
Here's some of the stuff which, being as it was open-sourced, I suppose is OK to
put here[fn:help].
- [[https://docs.google.com/document/d/1PPei3a5yORmKW1KusD4kearBUzZZM8DYCG7X0NY1oaM/preview][Section Leader Training]]
- [[https://docs.google.com/document/d/1VTnPA7dMwqpoE_Dl-jWL32g99P_ey4g-NmF7OEzhqR8/preview][Section Leaders' Guide to Virtual Sections]]
- [[https://docs.google.com/document/d/1lHdnwAB17iLyvASZbWrIZz4PVy9zMmHjxGBGPwXNDs4/preview#heading=h.7dq0u3orjv9z][Some Zoom Icebreakers]]
[fn:help] If you know otherwise, let me know in the comments
[fn:videos] As you may know, the official playlist [[https://www.youtube.com/channel/UCWw34Ie0yNe96myEZ5RLHhg][is here]]
* DONE CS106A Section Meeting I :@notes:teaching:cs106a:
CLOSED: [2020-04-17 Fri 22:54]
:PROPERTIES:
:EXPORT_FILE_NAME: scp-smallgrp-meet1
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:END:
** Background
As I mentioned [[https://rgoswami.me/posts/scp-onboarding/][earlier]], I'm leading a section for [[https://compedu.stanford.edu/codeinplace/announcement/][Stanford CS106A: Code
in Place]]. I did also mention I'd try to keep a set of short notes on the
process. I finally had my first section meeting!
** Preparation
I went through the following:
- Sent out a welcome message
- Detailed the workflow
- Set up a HackMD instance
- Set up some slides in ~beamer~[fn:whyslides]

However, after that, I was still concerned since I didn't get much of a response on the ice-breakers for EdStem. Thankfully, everyone showed up.
** Teaching
- I had a fabulous session, and we went through a variety of concepts.
- Didn't spend much time on icebreakers, but did get a feel for where the students stand on the functional vs imperative programming paradigms
- Possibly because of working through two different approaches, the 40 minute long session went on for two hours and fifteen minutes.
- Some students had more of a background than the others, thankfully computational thinking is not normally taught very well

** Conclusion

- The notes are [[https://hackmd.io/tqd-a5SlSbK_NAtnSdqEPA][visible here]], and the session was [[https://youtu.be/rLak1v4k4o0][recorded here]][fn:whatnow]
- It was fun, and I hope the students enjoyed it as much as I did.
- I will probably expand this in terms of the concepts covered, to give the students more of an overview of what was covered

[fn:whyslides] Even though most of the session was supposed to be live, it was still helpful to show I was interested enough to set up slides
[fn:whatnow] As always, advice is much appreciated (and moderated)
* DONE CS106A Small Group Training :@notes:teaching:cs106a:
CLOSED: [2020-04-22 Wed 07:01]
:PROPERTIES:
:EXPORT_FILE_NAME: scp-smallgrp-trainig
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments false
:END:
** Background
As I mentioned [[https://rgoswami.me/posts/scp-onboarding/][earlier]], I'm leading a section for [[https://compedu.stanford.edu/codeinplace/announcement/][Stanford CS106A: Code
in Place]]. This post relates to the notes and thoughts garnered during the small group training session[fn:whenpost].
** Reflections
*** Demographics
Redacted. Did not use breakout meetings due to privacy issues.
*** Engagement and Participation
- Some people were more active (skewed responses)
- Some of the more rudimentary questions might have been suppressed
*** Highlighted Moments
- Covering multiple perspectives
- Different mental models
*** Challenges and Transformations
- Technical debt was an issue
- Lack of engagement
- Went on for too long
For me in particular:
#+BEGIN_QUOTE
It took over two hours, and though most people stayed on, not everyone was engaged.
#+END_QUOTE
** Scenarios
These are to be dealt with as per the [[https://docs.google.com/document/d/13RZzvY_9WTR_sjo_Y4oBNchsAWAv_z6kSJ9395snANU/preview][guidelines here]]. Since different groups covered different scenarios, not all of these have answers here.
*** Ensuring Engagement
#+BEGIN_QUOTE
You have some students who didn't participate at all in the section. What do you do?
#+END_QUOTE
*** Effective Communication
#+BEGIN_QUOTE
What might not be effective about the policy, â€œStudents should just tell me if I say something that offends themâ€?
#+END_QUOTE
*** Sharing Experiences
#+BEGIN_QUOTE
You just finished your section and are staying behind to answer questions from your students. A couple students asked what itâ€™s like studying/working in an engineering/tech field.

What things might you want to keep in mind when answering their questions?
#+END_QUOTE
*** Time Management
#+BEGIN_QUOTE
Section went way over time due to lots of questions being asked by students. What are some time management strategies you can use moving forward?
#+END_QUOTE
*** Homework Assists
#+BEGIN_QUOTE
A sectionee posts in your Ed group, â€œI am a little bit frustrated because I don't really know where to start on the first assignment. A little hint would be very helpful.â€ How do you respond?
#+END_QUOTE
*** Debugging
#+BEGIN_QUOTE
A  sectionee shows you the following buggy code for printing all the elements in a list:

my_lst = ['apple', 'banana', 'carrot']
i = 0
while len(my_lst) > 0:
  print(my_lst[i])
  i = i + 1

They explain that the code works (it prints all the elements in the right order) but then throws a weird error: â€œIndexError: list index out of range.â€  How would you help them find their bug?
#+END_QUOTE
*** Quitting
#+BEGIN_QUOTE
You have a student who is already discouraged by how difficult the first assignment is and has told you they donâ€™t feel cut out for CS.  What do you say to them?
#+END_QUOTE
1. Provide encouragement
2. Give examples of hardship faced
3. Be positive and make sure they donâ€™t feel worse, even if they do follow through and quit
4. â€œYouâ€™re not the firstâ€
5. Takes a lot of time. Doesnâ€™t happen overnight
6. Ask them why they donâ€™t feel cut out and try to solve that problem

*** Looking up issues
#+BEGIN_QUOTE
Why might it be problematic to say something like, â€œItâ€™s easy to download X or look up the answer to Yâ€? Why might those statements not be true?
#+END_QUOTE
1. Difficulty in backgrounds (language barriers)
2. They might not be able to understand stackoverflow.com until they learn more CS
3. They might not know where to look online (lack of domain expertise)
4. Dependencies (for downloads)
5. Makes them feel bad if they donâ€™t end up finding it easy

[fn:whenpost] This post was created on the day of training, 21-04-20, but will be posted later
* DONE Using Mathematica with Orgmode :@programming:tools:emacs:workflow:orgmode:
CLOSED: [2020-04-26 Sun 20:01]
:PROPERTIES:
:EXPORT_FILE_NAME: org-mathematica
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
** Background
I have been wanting to find a workflow which allows me to bypass writing a lot of TeX by hand for a while now. To that end I looked into using a computer algebra system (CAS). Naturally, my first choice was the [[http://maxima.sourceforge.net/][FOSS Maxima]] (also because it uses Lisp under the hood). However, for all the reasons [[http://thingwy.blogspot.com/2015/07/maxima-versus-mathematica-should-i-go.html][listed here]], relating to its accuracy, which have not been fixed even though the post was over 5 years ago, I ended up having to go with the closed source [[https://www.wolfram.com/mathematica/][Mathematica]].
** Packages
Support for Mathematica in modern orgmode is mainly through the use of [[https://github.com/emacsmirror/org/blob/master/contrib/lisp/ob-mathematica.el][ob-mathematica]], which is the official org-babel extension (from ~contrib~) for working with Mathematica. However, ~ob-mathematica~ relies on the now-defunct ~mma~ package for font-locking, which is less than ideal. Thankfully, there exists the excellent [[https://github.com/kawabata/wolfram-mode][wolfram-mode]] package which happens to be in MELPA as well. Finally, since the default return type of a ~mathematica~ block is an input-string meant to be used in another ~mathematica~ block, which is not useful when we work with ~org-babel~, we will use the excellent ~mash.pl~ utility [[http://ai.eecs.umich.edu/people/dreeves/mash/][from here]], as suggested by the ~ob-mathematica~ package to sanitize our output and set a unifying path.

So to recap, use your favorite manager to get:
- [x] ~ob-mathematica~ (in contrib)
- [x] ~wolfram-mode~ ([[https://melpa.org/#/wolfram-mode][MELPA]])
- [x] ~mash.pl~ ([[http://ai.eecs.umich.edu/people/dreeves/mash][from here]])[fn:aboutmash]

After obtaining the packages, the configuration is then simply[fn:fullconf]:

#+begin_src emacs-lisp
;; Load mathematica from contrib
(org-babel-do-load-languages 'org-babel-load-languages
                             (append org-babel-load-languages
                                     '((mathematica . t))
                                     ))
;; Sanitize output and deal with paths
(setq org-babel-mathematica-command "~/.local/bin/mash")
;; Font-locking
(add-to-list 'org-src-lang-modes '("mathematica" . wolfram))
;; For wolfram-mode
(setq mathematica-command-line "~/.local/bin/mash")
#+end_src

** Results
*** LaTeX
Now we are in a position to simply evaluate content with font-locking. We will test our set up with an example lifted from the ~ob-mathematica~ [[https://github.com/analyticd/wy-els/blob/master/ob-mathematica.el][source-code]].

#+NAME: example-table
#+caption: A table
  | 1 | 4 |
  | 2 | 4 |
  | 3 | 6 |
  | 4 | 8 |
  | 7 | 0 |

#+BEGIN_SRC mathematica :var x=example-table :results latex
(1+Transpose@x) // TeXForm
#+END_SRC

#+RESULTS:
#+begin_export latex
\left(
\begin{array}{ccccc}
 2 & 3 & 4 & 5 & 8 \\
 5 & 5 & 7 & 9 & 1 \\
\end{array}
\right)
#+end_export

Where our header-line (with ~#+begin_src~) is:
#+BEGIN_SRC orgmode
mathematica :var x=example-table :results latex
#+END_SRC

*** Sanity Checks

We can also test the example from the [[http://thingwy.blogspot.com/2015/07/maxima-versus-mathematica-should-i-go.html][blog post]] earlier to test basic mathematical sanity.

#+BEGIN_SRC mathematica :results raw
Limit[Log[b - a + I eta], eta -> 0, Direction -> -1,Assumptions -> {a > 0, b > 0, a > b}]
TeXForm[Limit[Log[b - a + I eta], eta -> 0, Direction -> 1,Assumptions -> {a > 0, b > 0, a > b}]]
#+END_SRC

#+RESULTS:
$(I*Pi + Log[a - b])*\log (a-b)-i \pi$

*** Inline Math

Note that we can now also write fractions, integrals and other cumbersome TeX objects a lot faster with this syntax, like src_mathematica[:exports none :results raw]{Integrate[x^2,x] // TeXForm} $\frac{x^3}{3}$. Where we are using the following snippet:

#+BEGIN_SRC orgmode
src_mathematica[:exports none :results raw]{Integrate[x^2,x] // TeXForm}
#+END_SRC

*** Plots

For plots, the standard ~orgmode~ rules apply, that is, we have to export to a file and return the name through our code snippet. Consider:
#+BEGIN_SRC mathematica :results file
p=Plot[Sin[x], {x, 0, 6 Pi},Frame->True];
Export["images/sine.png",p];
Print["images/sine.png"]
#+END_SRC

#+RESULTS:
#+caption: An exported Mathematica image
[[file:images/sine.png]]

Where we have used ~mathematica :results file~ as our header line.

** Comments
The older commenting system was implemented with [[https://utteranc.es][utteranc.es]] as seen below.
@@html:<script src="https://utteranc.es/client.js" repo="haozeke/haozeke.github.io" issue-term="pathname" theme="photon-dark" label="Utterance ðŸ’¬" crossorigin="anonymous" async></script>@@

[fn:aboutmash] As noted in the comments, it is nicer to rename ~mash.pl~ to ~mash~
[fn:whydoom] The dark side has cookies
[fn:fullconf] For reference, my whole config [[https://dotdoom.rgoswami.me][is here]]
* TODO Thoughts on Chemical Engineering :@notes:ramblings:explanations:
:PROPERTIES:
:EXPORT_FILE_NAME: cheme-thoughts
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments false
:END:
** Background
** Mass and Energy Balances
From the first lecture on the topic, I came away with an implicit understanding, that though reactor design might be the heart of a chemical engineering plant, the soul would be mass and energy balances.
* DONE An Orgmode Note Workflow :@programming:tools:emacs:workflow:orgmode:
CLOSED: [2020-05-10 Sun 15:01]
:PROPERTIES:
:EXPORT_FILE_NAME: org-note-workflow
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
** Background
One of the main reasons to use ~orgmode~ is definitely to get a better note
taking workflow. Closely related to blogging or writing, the ideal note workflow
is one which lets you keep a bunch of throwaway ideas and also somehow have
access to them in a coherent manner. This will be a long post, and it is a
work-in-progress, so, keep that in mind. Since this is mainly me[fn:bojackme]
work-shopping my technique, the philosophy will come in a later post probably.
This workflow is documented more sparsely in my [[https://dotdoom.rgoswami.me/config.html#text-3][config file here]], in the
~noteYoda~ section[fn:whoyoda]. Some parts of this post also include mini video
clips for clarity[fn:howrec].

The entire workflow will end up being something like this[fn:notrepro]:

{{< youtube UWB6ZABRVq0 >}}

** Concept
While working through ideas, it actually was more useful to describe the
workflow I want, and then implement it, instead of relying on the canned
approaches of each package. So the basics of the ideology are listed below.
*** Reference Management
Reference management is one of the main reasons to consider a plain-text setup, and mine is no different. The options most commonly seen are:
- Mendeley :: This is a great option, and the most mobile friendly of the bunch. Sadly, the price tiers aren't very friendly so I have to give it a hard pass.
- Jabref :: This is fun, but really more of a per-project management system, but it works well for that. The fact that it is Java based was a major issue for me.
- Zotero :: This is what I personally use and recommend. More on that in a later post.
*** Notes
The idea is to be able to create notes for all kinds of content. Specifically,
papers or books, along with webpages. This then requires a separate system for
each which is described by:
- Search Engine :: The search engine is key, both in terms of accessibility and scalability. It is assumed that there will be many notes, and that they will have a wide variety of content. The search interface must then simply allow us to narrow down our candidates in a meaningful manner.
- Contextual Representation :: This aspect of the workflow deals with representations, which should transcend the usage of tags or categories. In particular, it would be nice to be able to visualize the flow of ideas, each represented by a note.
- Backlinks :: In particular, by backlinks at this point we are referring to the ability to link to a ~pdf~ or a website with a unique key such that notes can be added or removed at will.
- Storage :: Not actually part of the workflow in the same way, since it will be handled at the system level, it is worth nothing, that in this workflow Zotero is used to export a master *bib* file and keeps it updated, while the notes themselves are version controlled[fn:nodrop].
The concepts above will be handled by the following packages.

| Concept   | Package                      | Note                                                   |
|-----------+------------------------------+--------------------------------------------------------|
| Search    | deft                         | Has a great interface                                  |
| Context   | org-roam                     | Allows the export of graphiz mindmaps                  |
| Backlinks | org-roam, org-ref, org-noter | Covers websites, bibliographies, and pdfs respectively |

A key component in this workflow is actually facilitated by the fabulous
~org-roam-bibtex~ [[https://github.com/org-roam/org-roam-bibtex][or ORB]]. The basic idea is to ensure meaningful templates which
interpolate smoothly with ~org-roam~, ~org-ref~, ~helm-bibtex~, and
~org-capture~.
*** Basic Variables
Given the packages we will be using, some variable settings are in order, namely:
#+BEGIN_SRC emacs-lisp
(setq
   org_notes (concat (getenv "HOME") "/Git/Gitlab/Mine/Notes/")
   zot_bib (concat (getenv "HOME") "/GDrive/zotLib.bib")
   org-directory org_notes
   deft-directory org_notes
   org-roam-directory org_notes
   )
#+END_SRC
** Search
For the search setup, the ~doom-emacs~ ~deft~ setup, by adding ~+deft~ in my
~init.el~, worked out of the box for me. For those who do not use
~doom~[fn:whynot], the following should suffice:
#+BEGIN_SRC emacs-lisp
(use-package deft
  :commands deft
  :init
  (setq deft-default-extension "org"
        ;; de-couples filename and note title:
        deft-use-filename-as-title nil
        deft-use-filter-string-for-filename t
        ;; disable auto-save
        deft-auto-save-interval -1.0
        ;; converts the filter string into a readable file-name using kebab-case:
        deft-file-naming-rules
        '((noslash . "-")
          (nospace . "-")
          (case-fn . downcase)))
  :config
  (add-to-list 'deft-extensions "tex")
  )
#+END_SRC
For more about the ~doom-emacs~ defaults, check [[https://github.com/hlissner/doom-emacs/search?q=deft&unscoped_q=deft][the Github repo]]. The other
aspect of interacting with the notes is via the ~org-roam~ interface and will be
covered below.
** Bibliography
Since I will be using ~org-ref~, it makes no sense to load or work with the
~+biblio~ module at the moment. Thus this section is actually ~doom~ agnostic.
The basic tools of bibliographic management from the ~emacs~ end are the
venerable ~helm-bibtex~ ([[https://github.com/tmalsburg/helm-bibtex][repo here]]) and ~org-ref~ ([[https://github.com/jkitchin/org-ref/][repo here]]). In order to make
this guide complete, I will also describe the [[https://www.zotero.org/][Zotero]] settings I have.
*** Zotero
Without getting too deep into the weeds here, the basic requirements are:
- [x] [[https://zotero.org][Zotero]]
- [x] The [[https://retorque.re/zotero-better-bibtex/][better bibtex extension]]
The idea is to then have one top level ~.bib~ file in some handy location which
you will set up to sync automatically. To make life easier, there is a tiny
recording of the next steps.

{{< youtube iDRpIo7mcKE >}}

** Helm-Bibtex
This [[https://github.com/tmalsburg/helm-bibtex][venerable package]] is really good at interfacing with a variety of
externally formatted bibliographic managers.
#+BEGIN_SRC emacs-lisp
(setq
 bibtex-completion-notes-path "/home/haozeke/Git/Gitlab/Mine/Notes/"
 bibtex-completion-bibliography "/home/haozeke/GDrive/zotLib.bib"
 bibtex-completion-pdf-field "file"
 bibtex-completion-notes-template-multiple-files
 (concat
  "#+TITLE: ${title}\n"
  "#+ROAM_KEY: cite:${=key=}\n"
  "* TODO Notes\n"
  ":PROPERTIES:\n"
  ":Custom_ID: ${=key=}\n"
  ":NOTER_DOCUMENT: %(orb-process-file-field \"${=key=}\")\n"
  ":AUTHOR: ${author-abbrev}\n"
  ":JOURNAL: ${journaltitle}\n"
  ":DATE: ${date}\n"
  ":YEAR: ${year}\n"
  ":DOI: ${doi}\n"
  ":URL: ${url}\n"
  ":END:\n\n"
  )
 )
#+END_SRC
~doom-emacs~ users like me might want to wrap the above in a nice ~after!
org-ref~ expression, but it doesn't really matter.
*** Explanation
To break-down aspects of the configuration snippet above:
- The template includes the ~orb-process-file-field~ function to allow selecting the ~pdf~ to be used with ~org-noter~
- The ~file~ field is specified to work with the ~.bib~ file generated by Zotero
- ~helm-bibtex~ allows for any of the keys in a ~.bib~ file to be used in a template, and an overly expressive one is more useful
- The ~ROAM_KEY~ is defined to ensure that cite backlinks work correctly with ~org-roam~
- As I prefer to have one notes file per ~pdf~, I have only configured the ~bibtex-completion-notes-template-multiple-files~ variable
** Org-Ref
As discussed above, this just makes citations much more meaningful in ~orgmode~.
#+BEGIN_SRC emacs-lisp
(use-package org-ref
    :config
    (setq
         org-ref-completion-library 'org-ref-ivy-cite
         org-ref-get-pdf-filename-function 'org-ref-get-pdf-filename-helm-bibtex
         org-ref-default-bibliography (list "/home/haozeke/GDrive/zotLib.bib")
         org-ref-bibliography-notes "/home/haozeke/Git/Gitlab/Mine/Notes/bibnotes.org"
         org-ref-note-title-format "* TODO %y - %t\n :PROPERTIES:\n  :Custom_ID: %k\n  :NOTER_DOCUMENT: %F\n :ROAM_KEY: cite:%k\n  :AUTHOR: %9a\n  :JOURNAL: %j\n  :YEAR: %y\n  :VOLUME: %v\n  :PAGES: %p\n  :DOI: %D\n  :URL: %U\n :END:\n\n"
         org-ref-notes-directory "/home/haozeke/Git/Gitlab/Mine/Notes/"
         org-ref-notes-function 'orb-edit-notes
    ))
#+END_SRC
An essential aspect of this configuration is just that most of heavy lifting in
terms of the notes are palmed off to ~helm-bibtex~.
*** Explanation
To break-down aspects of the configuration snippet above:
- The ~org-ref-get-pdf-filename-function~ simply uses the ~helm-bibtex~ settings to find the ~pdf~
- The default bibliography and notes directory are set to the same location as all the ~org-roam~ files, to encourage a flat hierarchy
- The ~org-ref-notes-function~ simply ensures that, like the ~helm-bibtex~ settings, I expect one file per ~pdf~, and that I would like to use my ~org-roam~ template instead of the ~org-ref~ or ~helm-bibtex~ one
Note that for some reason,
the format specifiers for ~org-ref~ are *not* the keys in ~.bib~ but are
instead, the following[fn:wherereforg]:
#+BEGIN_SRC bash
In the format, the following percent escapes will be expanded.
%l   The BibTeX label of the citation.
%a   List of author names, see also `reftex-cite-punctuation'.
%2a  Like %a, but abbreviate more than 2 authors like Jones et al.
%A   First author name only.
%e   Works like %a, but on list of editor names.  (%2e and %E work as well)
It is also possible to access all other BibTeX database fields:
%b booktitle     %c chapter        %d edition    %h howpublished
%i institution   %j journal        %k key        %m month
%n number        %o organization   %p pages      %P first page
%r address       %s school         %u publisher  %t title
%v volume        %y year
%B booktitle, abbreviated          %T title, abbreviated
%U url
%D doi
%S series        %N note
%f pdf filename
%F absolute pdf filename
Usually, only %l is needed.  The other stuff is mainly for the echo area
display, and for (setq reftex-comment-citations t).
%< as a special operator kills punctuation and space around it after the
string has been formatted.
A pair of square brackets indicates an optional argument, and RefTeX
will prompt for the values of these arguments.
#+END_SRC
** Indexing Notes
This part of the workflow builds on the concepts best known as the [[https://www.zettelkasten.de/][Zettelkasten method]]. More details about the philosophy behind ~org-roam~ is [[https://www.orgroam.com/][here]].
*** Org-Roam
The first part of this interface is essentially just the ~doom-emacs~
configuration, adapted for those who don't believe in the dark side below.
#+BEGIN_SRC emacs-lisp
(use-package org-roam
  :hook (org-load . org-roam-mode)
  :commands (org-roam-buffer-toggle-display
             org-roam-find-file
             org-roam-graph
             org-roam-insert
             org-roam-switch-to-buffer
             org-roam-dailies-date
             org-roam-dailies-today
             org-roam-dailies-tomorrow
             org-roam-dailies-yesterday)
  :preface
  ;; Set this to nil so we can later detect whether the user has set a custom
  ;; directory for it, and default to `org-directory' if they haven't.
  (defvar org-roam-directory nil)
  :init
  :config
  (setq org-roam-directory (expand-file-name (or org-roam-directory "roam")
                                             org-directory)
        org-roam-verbose nil  ; https://youtu.be/fn4jIlFwuLU
        org-roam-buffer-no-delete-other-windows t ; make org-roam buffer sticky
        org-roam-completion-system 'default
)

  ;; Normally, the org-roam buffer doesn't open until you explicitly call
  ;; `org-roam'. If `+org-roam-open-buffer-on-find-file' is non-nil, the
  ;; org-roam buffer will be opened for you when you use `org-roam-find-file'
  ;; (but not `find-file', to limit the scope of this behavior).
  (add-hook 'find-file-hook
    (defun +org-roam-open-buffer-maybe-h ()
      (and +org-roam-open-buffer-on-find-file
           (memq 'org-roam-buffer--update-maybe post-command-hook)
           (not (window-parameter nil 'window-side)) ; don't proc for popups
           (not (eq 'visible (org-roam-buffer--visibility)))
           (with-current-buffer (window-buffer)
             (org-roam-buffer--get-create)))))

  ;; Hide the mode line in the org-roam buffer, since it serves no purpose. This
  ;; makes it easier to distinguish among other org buffers.
  (add-hook 'org-roam-buffer-prepare-hook #'hide-mode-line-mode))


;; Since the org module lazy loads org-protocol (waits until an org URL is
;; detected), we can safely chain `org-roam-protocol' to it.
(use-package org-roam-protocol
  :after org-protocol)


(use-package company-org-roam
  :after org-roam
  :config
  (set-company-backend! 'org-mode '(company-org-roam company-yasnippet company-dabbrev)))
#+END_SRC
Once again, for more details, check the [[https://github.com/hlissner/doom-emacs/search?q=roam&unscoped_q=roam][Github repo]].
*** Org-Roam-Bibtex
The configuration required is:
#+BEGIN_SRC emacs-lisp
 (use-package org-roam-bibtex
  :after (org-roam)
  :hook (org-roam-mode . org-roam-bibtex-mode)
  :config
  (setq org-roam-bibtex-preformat-keywords
   '("=key=" "title" "url" "file" "author-or-editor" "keywords"))
  (setq orb-templates
        '(("r" "ref" plain (function org-roam-capture--get-point)
           ""
           :file-name "${slug}"
           :head "#+TITLE: ${=key=}: ${title}\n#+ROAM_KEY: ${ref}

- tags ::
- keywords :: ${keywords}

\n* ${title}\n  :PROPERTIES:\n  :Custom_ID: ${=key=}\n  :URL: ${url}\n  :AUTHOR: ${author-or-editor}\n  :NOTER_DOCUMENT: %(orb-process-file-field \"${=key=}\")\n  :NOTER_PAGE: \n  :END:\n\n"

           :unnarrowed t))))
#+END_SRC
Where most of the configuration is essentially the template again. Like ~helm-bibtex~, [[https://github.com/org-roam/org-roam-bibtex][ORB]] allows taking arbitrary keys from the ~.bib~ file.
** Org Noter
The final aspect of a ~pdf~ workflow is simply ensuring that every ~pdf~ is
associated with notes. The philosophy of ~org-noter~ is [[https://github.com/weirdNox/org-noter][best described here]].
Only minor tweaks should be required to get this working with ~interleave~ as
well.
#+BEGIN_SRC emacs-lisp
(use-package org-noter
  :after (:any org pdf-view)
  :config
  (setq
   ;; The WM can handle splits
   org-noter-notes-window-location 'other-frame
   ;; Please stop opening frames
   org-noter-always-create-frame nil
   ;; I want to see the whole file
   org-noter-hide-other nil
   ;; Everything is relative to the main notes file
   org-noter-notes-search-path (list org_notes)
   )
  )
#+END_SRC
Evidently, from my configuration, it appears that I decided to use [[https://github.com/weirdNox/org-noter][org-noter]] over
the more commonly described [[https://github.com/rudolfochrist/interleave][interleave]] because it has better support for working
with multiple documents linked to one file.
** Org-Protocol
I will only cover the bare minimum relating to the use of ~org-capture~ here,
because eventually I intend to handle a lot more cases with [[https://github.com/abo-abo/orca][orca]]. Note that this
part of the workflow has more to do with using ~org-roam~ with websites than
~pdf~ files.
*** Templates
This might get complicated but I am only trying to get the bare minimum for
~org-protocol~ right now.
#+BEGIN_SRC emacs-lisp
;; Actually start using templates
(after! org-capture
  ;; Firefox and Chrome
  (add-to-list 'org-capture-templates
               '("P" "Protocol" entry ; key, name, type
                 (file+headline +org-capture-notes-file "Inbox") ; target
                 "* %^{Title}\nSource: %u, %c\n #+BEGIN_QUOTE\n%i\n#+END_QUOTE\n\n\n%?"
                 :prepend t ; properties
                 :kill-buffer t))
  (add-to-list 'org-capture-templates
               '("L" "Protocol Link" entry
                 (file+headline +org-capture-notes-file "Inbox")
                 "* %? [[%:link][%(transform-square-brackets-to-round-ones \"%:description\")]]\n"
                 :prepend t
                 :kill-buffer t))
)
#+END_SRC
** Conclusions
At this point, many might argue that since by the end, only one template is
called, defining the rest were pointless. They would be right, however, this is
just how my configuration evolved. Feel free to cannibalize this for your
personal benefit. Eventually I plan to expand this into something with
~org-journal~ as well, but not right now.

** Comments
The older commenting system was implemented with [[https://utteranc.es][utteranc.es]] as seen below.
@@html:<script src="https://utteranc.es/client.js" repo="haozeke/haozeke.github.io" issue-term="pathname" theme="photon-dark" label="Utterance ðŸ’¬" crossorigin="anonymous" async></script>@@

[fn:notrepro] The video uses ~org-ref-notes-function-many-files~ as the ~org-ref-notes-function~ so the template looks a little different
[fn:nodrop] For some strange reason a lot of online posts suggested Dropbox for syncing notes, which makes no sense to me, it is always better to have version control and ignore rules
[fn:wherereforg] Where these are from the [[https://github.com/jkitchin/org-ref/blob/875371a63430544e446db7a76b44b33c7e20a8bd/org-ref-utils.el][org-ref documentation]]
[fn:howrec] Recorded with [[https://www.maartenbaert.be/simplescreenrecorder/][SimpleScreenRecorder]], cut with [[https://github.com/mifi/lossless-cut][LosslessCut]], uploaded to [[https://www.youtube.com/][YouTube]], and embedded with a [[https://gohugo.io/extras/shortcodes/#youtube][Hugo shortcode]]
[fn:bojackme] Rohit Goswami that is, from the [[https://rgoswami.me][landing page]]; obviously
[fn:whoyoda] This is a reference to my fantastic pet, named Yoda
[fn:whynot] Therefore clearly proving that the cookies of the dark side have no power in the holy text editor war
* DONE Pandoc to Orgmode with Babel :@programming:tools:emacs:workflow:orgmode:
CLOSED: [2020-05-02 Sat 16:39]
:PROPERTIES:
:EXPORT_FILE_NAME: org-pandoc-babel
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
** Background
One of the best things about writing in ~orgmode~ is that we can embed and
execute arbitrary code snippets. However, not all languages have an exporter,
for obvious reasons. Somewhat surprisingly, there is no way to call [[https://pandoc.org/MANUAL.html][pandoc]] on embedded snippets, which feels like a waste, especially when a whole bunch of documentation formats can be converted to ~orgmode~ with it.

Consider the following beautifully highlighted snippet of an ~rst~ (ReStructured Text) [[https://sublime-and-sphinx-guide.readthedocs.io/en/latest/tables.html][list table]].
#+BEGIN_SRC rest
.. list-table:: Title
   :widths: 25 25 50
   :header-rows: 1

   ,* - Heading row 1, column 1
     - Heading row 1, column 2
     - Heading row 1, column 3
   ,* - Row 1, column 1
     -
     - Row 1, column 3
   ,* - Row 2, column 1
     - Row 2, column 2
   - Row 2, column 3
#+END_SRC

Trying to run this will generate the sort of obvious error:
#+BEGIN_SRC emacs-lisp
org-babel-execute-src-block: No org-babel-execute function for rst!
#+END_SRC
** Writing an Exporter
For this post, I will be focusing on ~rst~, but this can be defined for any of the ~pandoc~ back-ends. The approach was inspired by [[https://github.com/hoelterhof/ob-markdown/blob/master/ob-markdown.el][ob-markdown]].

#+BEGIN_SRC emacs-lisp
(defun org-babel-execute:rst (body params)
  "Execute a block of rst code with org-babel.
This function is called by `org-babel-execute-src-block'."
  (let* ((result-params (split-string (or (cdr (assoc :results params)) "")))
	 (in-file (org-babel-temp-file "rst-"))
	 (cmdline (cdr (assoc :cmdline params)))
	 (to (cdr (assoc :to params)))
	 (template (cdr (assoc :template params)))
	 (cmd (concat "pandoc"
		      " -t  org"
		      " -i " (org-babel-process-file-name in-file)
		      " -f rst "
		      " " cmdline)))
    (with-temp-file in-file (insert body))
    (message cmd)
    (shell-command-to-string cmd))) ;; Send to results

(defun org-babel-prep-session:rst (session params)
  "Return an error because rst does not support sessions."
  (error "rst does not support sessions"))
#+END_SRC

** Trying it out
With that done, it is pretty trivial to re-run the above example.

#+BEGIN_SRC rest :exports both :results raw
.. list-table:: Title
   :widths: 25 25 50
   :header-rows: 1

   ,* - Heading row 1, column 1
     - Heading row 1, column 2
     - Heading row 1, column 3
   ,* - Row 1, column 1
     -
     - Row 1, column 3
   ,* - Row 2, column 1
     - Row 2, column 2
   - Row 2, column 3
#+END_SRC

#+RESULTS:
| Heading row 1, column 1 | Heading row 1, column 2 | Heading row 1, column 3 |
|-------------------------+-------------------------+-------------------------|
| Row 1, column 1         |                         | Row 1, column 3         |
| Row 2, column 1         | Row 2, column 2         |                         |
|                         |                         |                         |
#+CAPTION: Title

Note that we have used ~rst :exports both :results raw~ as the header argument.

** Conclusions
Will probably follow this up with an actual package, which should handle the entire spectrum of ~pandoc~ back-ends.
* DONE Refactoring Dotfiles For Colemak :@programming:workflow:
CLOSED: [2020-05-02 Sat 20:30]
:PROPERTIES:
:EXPORT_FILE_NAME: colemak-dots-refactor
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A more actionable follow up to [[Switching to Colemak][my personal recollections]] relating to my switch to Colemak.
#+END_QUOTE

** Background
I have, in the past written about how I [[Switching to Colemak][made the switch]] to Colemak. However,
until recently, I was still trying to mimic the VIM keybindings from QWERTY.
This is a post where I discuss the changes I made to ensure that I never have to
stretch my fingers in odd ways again. The main idea is expressed well by [[https://github.com/jooize/vim-colemak][vim-colemak]].
#+BEGIN_SRC shell
Colemak layout:                  |                 QWERTY layout:
`12345 67890-=     Move around:  |  (instead of)   `12345 67890-=
 qwfpg jluy;[]\         e        |       k          qwert yuiop[]\
 arstd HNEIo'         h   i      |     h   l        asdfg HJKL;'
 zxcvb km,./            n        |       j          zxcvb nm,./
#+END_SRC
** Sudoers
It is important to note that the ~sudo~ command does not automatically pick up on your keyboard layout. It is best to set this explicitly. Use ~visudo~ and un-comment ~Defaults env_keep += "LANG LANGUAGE LINGUAS LC_* _XKB_CHARSET"~, or:
#+begin_src bash
su
echo 'Defaults env_keep += "LANG LANGUAGE LINGUAS LC_* _XKB_CHARSET"' >> /etc/sudoers
#+end_src
** Emacs
Though I have [[https://github.com/hlissner/doom-emacs/issues/783#issuecomment-535805347][mentioned publicly]], that I was using the regular QWERTY motion
keys, I realized I had actually started to use the mouse more often, simply
because it was a pain to navigate. Thankfully, ~emacs~ has [[https://github.com/wbolster/evil-colemak-basics][evil-colemak-basics]],
which is fabulous. For reference, these make it really easy for QWERTY users to
make the switch if they're previously used to VIM bindings.
#+BEGIN_SRC rst :results raw :exports results
.. list-table::
   :header-rows: 1

   * - Colemak
     - Qwerty
     - Action
     - States
     - At Qwerty position?
     - Remarks

   * - ``h``, ``n``, ``e``, ``i``
     - ``h``, ``j``, ``k``, ``l``
     - navigate
     - ``mnvo``
     - yes
     -

   * - ``k``, ``K``
     - ``n``, ``N``
     - search next/previous
     - ``mnvo``
     - yes
     -

   * - ``u``, ``U``
     - ``i``, ``I``
     - insert
     - ``_nv_``
     - yes
     -

   * - ``l``
     - ``u``
     - undo
     - ``_nv_``
     - yes
     -

   * - ``N``
     - ``J``
     - join lines
     - ``_nv_``
     - yes
     -

   * - ``E``
     - ``K``
     - lookup
     - ``mnv_``
     - yes
     -

   * - ``u``
     - ``i``
     - inner text object keymap
     - ``___o``
     - yes
     -

   * - ``f``, ``F``
     - ``e``, ``E``
     - jump to end of word
     - ``mnvo``
     - yes
     - with ``t-f-j`` rotation

   * - ``t``, ``T``
     - ``f``, ``f``
     - jump to character
     - ``mnvo``
     - yes
     - with ``t-f-j`` rotation

   * - ``j``, ``J``
     - ``t``, ``T``
     - jump until character
     - ``mnvo``
     - no
     - with ``t-f-j`` rotation

   * - ``j``, ``J``
     - ``e``, ``E``
     - jump to end of word
     - ``mnvo``
     - no
     - without ``t-f-j`` rotation
#+END_SRC

#+RESULTS:
| Colemak            | Qwerty             | Action                   | States | At Qwerty position? | Remarks                  |
|--------------------+--------------------+--------------------------+--------+---------------------+--------------------------|
| =h=, =n=, =e=, =i= | =h=, =j=, =k=, =l= | navigate                 | =mnvo= | yes                 |                          |
| =k=, =K=           | =n=, =N=           | search next/previous     | =mnvo= | yes                 |                          |
| =u=, =U=           | =i=, =I=           | insert                   | =_nv_= | yes                 |                          |
| =l=                | =u=                | undo                     | =_nv_= | yes                 |                          |
| =N=                | =J=                | join lines               | =_nv_= | yes                 |                          |
| =E=                | =K=                | lookup                   | =mnv_= | yes                 |                          |
| =u=                | =i=                | inner text object keymap | =___o= | yes                 |                          |
| =f=, =F=           | =e=, =E=           | jump to end of word      | =mnvo= | yes                 | with =t-f-j= rotation    |
| =t=, =T=           | =f=, =f=           | jump to character        | =mnvo= | yes                 | with =t-f-j= rotation    |
| =j=, =J=           | =t=, =T=           | jump until character     | =mnvo= | no                  | with =t-f-j= rotation    |
| =j=, =J=           | =e=, =E=           | jump to end of word      | =mnvo= | no                  | without =t-f-j= rotation |

Where the table above is from the fantastic [[https://github.com/wbolster/evil-colemak-basics][readme]].

I still had some issues, mostly relating to searching in buffers, so I ended
using ~swiper-isearch~ more which is a bonus too.

*** Visual Lines
Since I tend to keep ~visual-line-mode~ all the time, it makes sense to actually swap working with lines and visual lines.
To work this through this needs [[https://github.com/YourFin/evil-better-visual-line/][evil-better-visual-line]].
#+BEGIN_SRC emacs-lisp
(use-package! evil-better-visual-line
  :after evil-colemak-basics
  :config
  (evil-better-visual-line-on)
  (map! :map evil-colemak-basics-keymap
        (:nvm "n" 'evil-better-visual-line-next-line
         :nvm "e" 'evil-better-visual-line-previous-line
         :nvm "g n" 'evil-next-line
         :nvm "g e" 'evil-previous-line))
)
#+END_SRC
*** Pdf-Tools
For my ~doom-emacs~ configuration, I also set the following map:
#+BEGIN_SRC emacs-lisp
(after! pdf-view
  (add-hook! 'pdf-view-mode-hook (evil-colemak-basics-mode -1))
 (map!
   :map pdf-view-mode-map
   :n "g g"          #'pdf-view-first-page
   :n "G"            #'pdf-view-last-page
   :n "N"            #'pdf-view-next-page-command
   :n "E"            #'pdf-view-previous-page-command
   :n "e"            #'evil-collection-pdf-view-previous-line-or-previous-page
   :n "n"            #'evil-collection-pdf-view-next-line-or-next-page
 )
#+END_SRC
Where the most important thing is the hook which removes the
~evil-colemak-basics~ binding. Since it is a single mode and hook, ~after-hook!~
is the same as ~after-hook~[fn:attribute].
*** Window Management
Somehow these are not part of the ~evil-colemak~ defaults.
#+BEGIN_SRC emacs-lisp
(after! evil
  (map! :map evil-window-map
        (:leader
         (:prefix ("w" . "Select Window")
          :n :desc "Left"  "h" 'evil-window-left
          :n :desc "Up"    "e" 'evil-window-up
          :n :desc "Down"  "n" 'evil-window-down
          :n :desc "Right" "i" 'evil-window-right
          ))
        ))
#+END_SRC
*** Search
Harmonizing with Vimium.
#+BEGIN_SRC emacs-lisp
(after! evil (map! :map evil-motion-state-map
                   (:n :desc "Previous match" "K" 'evil-ex-search-previous
                    :n :desc "Next match" "k" 'evil-ex-search-next
                    :n :desc "Forward search" "/" 'evil-search-forward
                    )
                   ))
#+END_SRC
*** Page Movement
Though this is more of a personal preference, I find it more natural to bind N
and E to page-wise movement instead of join lines and lookup, since I almost
never use those commands, and the movement keys echo what I expect elsewhere.
#+BEGIN_SRC emacs-lisp
(after! evil
  (map! :map evil-colemak-basics-keymap
      :nv "N" 'evil-scroll-page-up
      :nv "E" 'evil-scroll-page-down)
  )
#+END_SRC
*** Evil Org
Annoyingly, ~evil-org-mode~ had a map which kept overriding all my other
settings. Thankfully it has a helper variable to set movement. I also do not
need this anyway, at-least not by default.
#+BEGIN_SRC emacs-lisp
(after! org
  (remove-hook 'org-mode-hook 'evil-org-mode)
  (setq evil-org-movement-bindings
        '((up . "e") (down . "n")
          (left . "h") (right . "i"))
        )
)
#+END_SRC
** Vimium
I use the excellent [[https://vimium.github.io/][vimium]] to make Chrome be a little less annoying. Luckily [[https://github.com/philc/vimium/wiki/colemak][the
Wiki]] seems to have a reasonable suggestion for colemak. The basic idea is to
migrate the underlying keys directly to ensure very few manual changes are
required.
#+BEGIN_SRC conf
mapkey n j
mapkey N J
mapkey e k
mapkey E K
mapkey i l
mapkey I L
mapkey k n
mapkey K N
mapkey l i
mapkey L I
mapkey j e
mapkey J E
#+END_SRC
** Vim
For a lot of terminal edits, ~vim~ is still my editor of choice, and [[https://github.com/jooize/vim-colemak][vim-colemak]] works without any trouble [[https://github.com/HaoZeke/Dotfiles/commit/d6df2743822e6fcf4761d569600a8e9a802a7af4][in my configuration]].
** Zsh
To ensure uniform bindings, I used to use ~bindkey -v~ but will need some minor
changes to that set up. I based this part of my configuration off the bindings
of [[https://github.com/bunnyfly/dotfiles/blob/master/zshrc][bunnyfly]].
#+BEGIN_SRC shell
bindkey -v
# Colemak.
  bindkey -M vicmd "h" backward-char
  bindkey -M vicmd "n" down-line-or-history
  bindkey -M vicmd "e" up-line-or-history
  bindkey -M vicmd "i" forward-char
  bindkey -M vicmd "s" vi-insert
  bindkey -M vicmd "S" vi-insert-bol
  bindkey -M vicmd "k" vi-repeat-search
  bindkey -M vicmd "K" vi-rev-repeat-search
  bindkey -M vicmd "l" beginning-of-line
  bindkey -M vicmd "L" end-of-line
  bindkey -M vicmd "j" vi-forward-word-end
  bindkey -M vicmd "J" vi-forward-blank-word-end

# Sane Undo, Redo, Backspace, Delete.
  bindkey -M vicmd "u" undo
  bindkey -M vicmd "U" redo
  bindkey -M vicmd "^?" backward-delete-char
  bindkey -M vicmd "^[[3~" delete-char

# Keep ctrl+r searching
  bindkey -M viins '^R' history-incremental-pattern-search-forward
  bindkey -M viins '^r' history-incremental-pattern-search-backward
#+END_SRC
** Zathura
There is no better ~pdf~ viewer than [[https://pwmt.org/projects/zathura/][zathura]], and it also works for ~djvu~ and
friends. As a plus point, it normally has very reasonable ~vim~ bindings, and an
excellent configuration system, so we will leverage that. The best part is that
we can just add to it using ~include zathuraColemak~ or whatever so as to be
minimally invasive.

#+BEGIN_SRC vim
map h scroll left
map n scroll down
map e scroll up
map i scroll right

map N scroll half-down
map E scroll half-up

map k search forward
map K search backward

# For TOC navigation
map [index] o toggle_index

# hjkl â†’  hnei
map [index] n navigate_index down
map [index] e navigate_index up
map [index] h navigate_index collapse
map [index] i navigate_index expand

map [index] H navigate_index collapse-all
map [index] I navigate_index expand-all
#+END_SRC
Zathura is a complicated beast, however, and my [[https://github.com/HaoZeke/Dotfiles/blob/master/dotfiles/colemak/.zshColemak][full configuration]] contains a
lot more information.
** i3
I have some bindings set up in terms of $left $right $up and $down, so it was
simple to re-bind them.
#+BEGIN_SRC vim
set $left h
set $down n
set $up e
set $right i
#+END_SRC
** Conclusions
That seems to be it for now. If I think of more programs I use regularly which
allow VIM bindings, or keybindings in general, I'll probably just update this
post. My full dotfiles are [[https://github.com/HaoZeke/Dotfiles][present here]], and now include a ~colemak~ target.
[fn:attribute] The hook fix was suggested by the fantastic [[https://github.com/hlissner/][hlissner]] on the super friendly doom Discord server.
* TODO Reclaiming The Web With RSS :@notes:ramblings:workflow:tools:
:PROPERTIES:
:EXPORT_FILE_NAME: reclaim-web-rss
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
** Background
I came across some fascinating sites recently, which didn't bother to have an RSS feed. I've been an RSS fan for as long as I can remember, so, that's really about it.
** Context
*** Do I need an RSS reader?
If you follow a bunch of talented, but sporadic content creators, then *yes*. If you plan on subscribing to news-feeds and consolidated feeds which spam you with a million links per minute, then *no*. To clarify, the feed reader setup is essentially a mechanism by which the audience opts-in to post subscriptions, but without putting the onus on the creator to harvest user-emails or any other data[fn:privacy].
*** RSS? Atom? asrtinuo?
One of the words in the title is not a feed. That said, the concept of a feed, given the rise of Facebook, is probably not hard for anyone to understand anymore. The specifications are a little different, though in practice from an end-user perspective, there are few (if any) differences.
**** Halp?
There are excellent guidelines for every static site generator, and
** Programs
First of all, *no one* really needs to have an RSS subscription. This actually applies to *every* provider, from Feedly, Inoreader, The Old Reader, etc. to even FOSS projects like Miniflux[fn:supportfoss]. Locally, there are a couple of options, but I will only cover things I have used personally.
*** RSSOwl
RSSOwl was fantastic. It looked great, ran smoothly, and had nifty desktop integration. That is, until it stopped working. Though it is a Java package so it is always possible to downgrade your JRE and JDK, sadly this is probably not worth the effort.
*** News Flash
A successor to the well-known Feedbin, this seems to check most of the boxes for me. Though it does not come with a tray icon, KDocker is quick to step into that breach.
** Personal Workflow
While consuming content, there are only a couple of things to do.
** Conclusions
I am not sure there will ever be a resurgence of RSS feeds, given the rise of newsletters. It would be nice to see more people get back to RSS though.
- [x] Don't be the product
- [x] Don't be beholden to search engine rankings
- [x] Support content creators, not SEO specialists

[fn:supportfoss] That said, you should always try to support FOSS products, so if you really feel the need to have a central server, go FOSS
[fn:privacy] That last bit, is probably why companies like [[http://xanadu.ai/][Xanadu]] and others prefer to have a mailing list....

* DONE Compton to Picom and Zoom Glitches :@notes:workflow:tools:
CLOSED: [2020-05-12 Tue 01:32]
:PROPERTIES:
:EXPORT_FILE_NAME: compton-zoom-shadow
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
** Background
I [[https://rgoswami.me/tags/cs106a/][have been leading]] the fantastic section 881 as a virtual section leader for
the [[https://compedu.stanford.edu/codeinplace/v1/#/course][Stanford CS106A: Code in Place]] initiative for the past four weeks. I have
also spent a lot of time on Zoom, sharing my screen. Fun fact. My screen shares
look like this:

#+DOWNLOADED: screenshot @ 2020-05-12 01:05:38
#+caption: Zoom screen share with weird overlay
[[file:images/Background/2020-05-12_01-05-38_screenshot.png]]

This post is about hunting down what caused this amazing zoom
glitch[fn:whatglitch] and how I finally fixed it.
** Tiling Windows and Compositors
For reasons best left to another post, I use the fabulous [[https://i3wm.org/][i3 window manager]],
with colemak keybindings [[i3][described here]]. Recall that, [[https://en.wikipedia.org/wiki/Compositing_window_manager][from Wikipedia]]:
#+BEGIN_QUOTE
A compositing window manager is a window manager that provides applications with an off-screen buffer for each window. The window manager composites the window buffers into an image representing the screen and writes the result into the display memory.
#+END_QUOTE

For reasons I can no longer recall, ~compton~ has been a traditional aspect of my
workflow. As per my last update back in April last year; my configuration [[https://github.com/HaoZeke/Dotfiles/blob/8213473f313aa8fb9eb4c22d6b36a801b1584df6/dotfiles/archLinux/.config/compton.conf][is here]].
** Compton to Picom
Some time ago (actually [[https://github.com/yshui/picom/issues/222][many months ago]]), ~compton~
itself transitioned over to ~picom~, but remained largely compatible with my old
configuration[fn:archwikiplug].
To be clear, the transition was largely painless, with ample warnings in the
terminal showing up; along with very reasonable fallbacks. The key aspect of my ~compton.conf~ which caused the shadowing was:
#+BEGIN_SRC shell
shadow = true;
shadow-radius = 5;
shadow-offset-x = -5;
shadow-offset-y = -5;
shadow-opacity = 0.5;
#+END_SRC
The corrective measure was simply to set ~shadow-opacity~ to nothing; that is:
#+BEGIN_SRC shell
shadow-opacity = 0.0;
#+END_SRC
The rest of the configuration [[https://github.com/HaoZeke/Dotfiles/blob/master/dotfiles/archLinux/.config/picom.conf][is here]]; and contains a lot more, mostly
pertaining to opacity and other pretty effects[fn:plugdots].
** Conclusion
Finally we have achieved the goal of having normal screen sharing capabilities;
as seen below:

#+DOWNLOADED: screenshot @ 2020-05-12 01:13:37
#+caption: Just in time to see an excellent pun
[[file:images/Conclusion/2020-05-12_01-13-37_screenshot.png]]

The struggle was real, though the cause was trivial, and really highlights the
need to always know your system packages. In this case, no doubt my students
would have preferred not having to suffer through the darkness of my
screen[fn:bojackplug]. This has been a rather trivial post, but one to keep in
mind none-the-less.

** Comments
The older commenting system was implemented with [[https://utteranc.es][utteranc.es]] as seen below.
@@html:<script src="https://utteranc.es/client.js" repo="haozeke/haozeke.github.io" issue-term="pathname" theme="photon-dark" label="Utterance ðŸ’¬" crossorigin="anonymous" async></script>@@

[fn:whatglitch] To be clear, none of the windows were the glitch. The issue was the darkened overlay
[fn:bojackplug] Though it might have also served as a metaphor for *darkness*
[fn:plugdots] The rest of [[https://github.com/HaoZeke/Dotfiles][my Dotfiles]], managed by the [[https://github.com/kobus-v-schoor/dotgit][excellent dotgit]] are also worth a look

[fn:archwikiplug] As always, the [[https://wiki.archlinux.org/index.php/Picom][ArchLinux Wiki]] is a great place for more information

* TODO Fancy Unicode XeTeX in Orgmode :@notes:tools:emacs:workflow:orgmode:
:PROPERTIES:
:EXPORT_FILE_NAME: fancy-unicode-orgmode
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:

\begin{align}
\nonumber W_{r\rightarrow\infty}=&-\int_{r}^{\infty}\!F\,\mathrm{d}y=-    \int_r^\infty \!     \dfrac{1}{4\pi \epsilon_0} \dfrac{q^2}{\alpha^2}     \dfrac{\alpha^3}{y^3}\left(1-    \dfrac{\alpha^2}    {y^2}\right)^{-2}\,\mathrm{d}y\\
=&-\dfrac{1}{4\pi \epsilon_0} \dfrac{q^2}{\alpha^2}\alpha^3     \underbrace{\int_r^\infty     \! y^{-3} \left(1-\dfrac{\alpha^2}    {y^2}\right)^{-2} \,\mathrm{d}y}_{I} \label{eq:WcondI}
\end{align}

* DONE LosslessCut, Zoom and an AMA for CS106A :@notes:teaching:cs106a:tools:
CLOSED: [2020-05-20 Wed 20:21]
:PROPERTIES:
:EXPORT_FILE_NAME: losslesscut-zoom-ama
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments false
:END:
** Background
I recently had the opportunity to take part in an AMA (ask me anything) session
for the CS106A students on Machine Learning for the Physical Sciences. This is a
post about the technical issues, and also includes a video if you read through.
** Zoom and LosslessCut
Zoom recordings are one of the nicer ways to deal with switching windows and
screen sharing, especially after fixing the [[Compton to Picom and Zoom Glitches][dark screen glitch]]. However, though
[[https://github.com/mifi/lossless-cut][LosslessCut]] works really well to get cut-points, exporting and merging the file
into one caused a bunch of glitches.
** Enter Handbrake
To not beat around the bush, the solution was to simply encode the Zoom
recording with [[https://handbrake.fr/][Handbrake]] before using LosslessCut[fn:whatsettings]. Since the
conversion takes a while, it is also neat to note that you can directly export
the cut points made with LosslessCut on the original video, then import them
onto the newly encoded file.
** Conclusions
I am not really sure how this will turn out, but it is a useful thing to keep in
mind. The introductory video turned out to be:

{{< youtube aOuqgyHHOK4 >}}

[fn:whatsettings] For me, the Vimeo Youtube HQ 1080p60 preset worked out well
* TODO FOSS Maintenance and Me :@personal:ramblings:explanations:thoughts:
Write about being the [[https://github.com/chriskempson/base16/issues/225#issuecomment-639739616][new maintainer]] for the base16-zathura thing and the AUR packages I maintain.
* DONE Nix with R and devtools :@programming:tools:nix:workflow:R:
CLOSED: [2020-06-06 Sat 05:49]
:PROPERTIES:
:EXPORT_FILE_NAME: nix-r-devtools
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
This post discusses briefly, the ~nix-shell~ environment for reproducible
programming. In particular, there is an emphasis on extensions for installing
and working with packages not in [[https://cran.r-project.org/web/packages/][CRAN]], i.e. packages off Github which are
normally installed with ~devtools~.
#+END_QUOTE

** Background
The entire [[https://nixos.org/][nix ecosystem]] is fantastic, and is the main packaging
system used by [[https://dseams.info][d-SEAMS]] as well.
Recently I began working through the [[https://xcelab.net/rm/statistical-rethinking/][excellent second edition]] of "Statistical
Rethinking" by [[https://twitter.com/rlmcelreath][Richard McElreath]][fn:whatwhy].

Unfortunately, the ~rethinking~ package which is a major component of the book
itself depends on the V8 engine for some reason. The reigning AUR[fn:whutaur]
package ([[https://aur.archlinux.org/packages/v8-r/][V8-r]]) broke with a [[https://aur.archlinux.org/packages/v8-r/#comment-749561][fun error message]] I couldn't be bothered to deal
with. Ominously, the [[https://pastebin.com/CbdCMZ8d][rest of the logs]] prominently featured ~Warning: Running
gclient on Python 3.~. Given that older ~python~ versions have been permanently
retired, this seemed like a bad thing to deal with[fn:sonothing]. In any case,
having weaned off non-nix dependency tools for ~python~ and friends, it seemed
strange to not do the same for R.

The standard installation for the package entails obtaining ~rstan~ (which is trivial with ~nixpkgs~) and then using:
#+BEGIN_SRC R
install.packages(c("coda","mvtnorm","devtools","loo","dagitty"))
library(devtools)
devtools::install_github("rmcelreath/rethinking")
#+END_SRC
We will break this down and work through this installation in Nix space.
** Nix and R
The standard approach to setting up a project ~shell.nix~ is simply by using the
~mkshell~ function. There are some common aspects to this workflow, with more
language specific details documented here. A simple first version might be:

#+BEGIN_SRC nix
let
    pkgs = import <nixpkgs> { };
in pkgs.mkShell {
    buildInputs = with pkgs; [
        zsh
        R
        rPackages.ggplot
        rPackages.data_table
    ];
    shellHook = ''
    echo "hello"
    '';
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

Where we note that we can install CRAN packages as easily as regular packages
(like R), except for the fact that they are kept in a ~pkgs.rPackages~
environment, as opposed to ~pkgs~. This is actually a common convention most
languages with central repos. The most interesting thing to note is that,
similar to the convention for ~nix-python~ setups, packages with a dot in the
name will be converted to having an underscore, i.e. ~data.table~ ->
~data_table~.

However, for the rethinking package, and many others, there is no current CRAN
package, and so the ~rPackages~ approach fails.

The ~LOCALE_ARCHIVE~ needs to be set for Linux machines, and is required for
working with other packages.

** Nix-R and Devtools
To work with non-CRAN packages, we need to modify our package setup a little. We
will also simplify our file to split the ~pkgs~ and the ~r-pkgs~.
*** Naive Approach
The naive approach works by using the ~shellHook~ to set ~R_LIBS_USER~ to save
user packages per-project.
#+BEGIN_SRC nix
{ pkgs ? import <nixpkgs> { } }:
with pkgs;
let
  my-r-pkgs = rWrapper.override {
    packages = with rPackages; [
      ggplot2
      knitr
      rstan
      tidyverse
      V8
      dagitty
      coda
      mvtnorm
      shape
      Rcpp
      tidybayes
    ];
  };
in mkShell {
  buildInputs = = with pkgs;[ git glibcLocales openssl openssh curl wget ];
  inputsFrom = [ my-r-pkgs ];
  shellHook = ''
    mkdir -p "$(pwd)/_libs"
    export R_LIBS_USER="$(pwd)/_libs"
  '';
  GIT_SSL_CAINFO = "${cacert}/etc/ssl/certs/ca-bundle.crt";
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

Note that here we will also need to set the ~GIT_SSL_CAINFO~ to prevent some
errors during the build process[fn:seenelsewhere].
*** Native Approach
The native approach essentially leverages the ~nix~ method for building ~R~
packages. This is the most reproducible of the lot, and also has the useful
property of storing the files in the ~nix-store~ so re-using packages across
different projects will not store, build or download the package again. The
values required can be calculated from ~nix-prefetch-git~ as follows:

#+BEGIN_SRC bash
nix-env -i nix-prefetch-git
nix-prefetch-git https://github.com/rmcelreath/rethinking.git
#+END_SRC

The crux of this approach is the following snippet[fn:defnfrm]:
#+BEGIN_SRC nix
(buildRPackage {
  name = "rethinking";
  src = fetchFromGitHub {
    owner = "rmcelreath";
    repo = "rethinking";
    rev = "d0978c7f8b6329b94efa2014658d750ae12b1fa2";
    sha256 = "1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0";
  };
  propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ];
 })
#+END_SRC

**** Project Shell
This formulation for some strange reason does not work from the shell or environment by default, but does work with ~nix-shell --run bash --pure~.
#+BEGIN_SRC nix
{ pkgs ? import <nixpkgs> { } }:
with pkgs;
let
  my-r-pkgs = rWrapper.override {
    packages = with rPackages; [
      ggplot2
      knitr
      rstan
      tidyverse
      V8
      dagitty
      coda
      mvtnorm
      shape
      Rcpp
      tidybayes
      (buildRPackage {
        name = "rethinking";
        src = fetchFromGitHub {
          owner = "rmcelreath";
          repo = "rethinking";
          rev = "d0978c7f8b6329b94efa2014658d750ae12b1fa2";
          sha256 = "1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0";
        };
        propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ];
      })
    ];
  };
in mkShell {
  buildInputs = with pkgs; [ git glibcLocales openssl which openssh curl wget my-r-pkgs ];
  shellHook = ''
    mkdir -p "$(pwd)/_libs"
    export R_LIBS_USER="$(pwd)/_libs"
    echo ${my-r-pkgs}/bin/R
  '';
  GIT_SSL_CAINFO = "${cacert}/etc/ssl/certs/ca-bundle.crt";
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

The reason behind this is simply that ~rWrapper~ forms an extra package which
has lower precedence than the user profile ~R~, which is documented in more
detail here on the [[https://nixos.wiki/wiki/R][NixOS wiki]].
**** User Profile
This is a more general approach which defines the environment for R with all the
relevant libraries and is described in the [[https://nixos.org/nixpkgs/manual/#r][nixpkgs manual]]. The following code
should be placed in ~$HOME/.config/nixpkgs/config.nix~:

#+BEGIN_SRC nix
{
  packageOverrides = super:
    let self = super.pkgs;
    in {
      rEnv = super.rWrapper.override {
        packages = with self.rPackages; [
          ggplot2
          knitr
          tidyverse
          tidybayes
          (buildRPackage {
            name = "rethinking";
            src = self.fetchFromGitHub {
              owner = "rmcelreath";
              repo = "rethinking";
              rev = "d0978c7f8b6329b94efa2014658d750ae12b1fa2";
              sha256 = "1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0";
            };
            propagatedBuildInputs =
              [ coda MASS mvtnorm loo shape rstan dagitty ];
          })
        ];
      };
    };
}
#+END_SRC

This snippet allows us to use our ~R~ as follows:
#+BEGIN_SRC bash
# Install things
nix-env -f "<nixpkgs>" -iA rEnv
# Fix locale
export LOCALE_ARCHIVE="$(nix-build --no-out-link "<nixpkgs>" -A glibcLocales)/lib/locale/locale-archive"
# Profit
R
#+END_SRC

Note that in this method, on Linux systems, the locale problem has to be fixed
with the explicit export. This means that this should be used mostly with
project level environments, instead of populating the global shell RC files.

*Update:* There is [[Emacs for Nix-R][another post]] with methods to reload this configuration automatically
** Conclusions
Of the methods described, the most useful method for working with packages not
hosted on CRAN is through the user-profile, while the ~shell.nix~ method is
useful in conjunction, for managing various projects. So the ideal approach is
then to use the user profile for installing anything which normally uses
~devtools~ and then use ~shell.nix~ for the rest.

Note that if the [[Project Shell]] is used with a [[User Profile]] as described in the
next section, all packages defined there can be dropped and then the project
shell does not need to execute ~R~ by default. The simplified ~shell.nix~ is
then simply:

#+BEGIN_SRC nix
{ pkgs ? import <nixpkgs> { } }:
with pkgs;
let
  my-r-pkgs = rWrapper.override {
    packages = with rPackages; [
      ggplot2
    ];
  };
in mkShell {
  buildInputs = with pkgs;[ git glibcLocales openssl which openssh curl wget my-r-pkgs ];
  inputsFrom = [ my-r-pkgs ];
  shellHook = ''
    mkdir -p "$(pwd)/_libs"
    export R_LIBS_USER="$(pwd)/_libs"
  '';
  GIT_SSL_CAINFO = "${cacert}/etc/ssl/certs/ca-bundle.crt";
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

The entire workflow for
~rethinking~ is [[Statistical Rethinking and Nix][continued here]].

[fn:whatwhy] As part of [[https://ugla.hi.is/kennsluskra/index.php?sid=&tab=nam&chapter=namskeid&id=71055920203][a summer course]] at the University of Iceland relating to their successful [[http://covid.hi.is/][COVID-19 model]]
[fn:whutaur] The [[https://wiki.archlinux.org/index.php/Arch_User_Repository][Arch User Repository]] is the port of first call for most ArchLinux users
[fn:sonothing] Though, like any good AUR user, I did post a bug report
[fn:seenelsewhere] This approach is also [[https://churchman.nl/tag/r/][discussed here]]
[fn:defnfrm] As discussed on [[https://github.com/NixOS/nixpkgs/issues/44290][this issue]], this [[https://stackoverflow.com/questions/55176609/how-to-install-r-and-packages-through-configuration-nix-and-how-to-add-packages][stackoverflow question]] and also [[https://github.com/rikhuijzer/nix-with-r/blob/master/default.nix][seen here]]

* DONE Statistical Rethinking and Nix :@programming:tools:nix:workflow:R:
CLOSED: [2020-06-07 Sun 04:24]
:PROPERTIES:
:EXPORT_FILE_NAME: rethinking-r-nix
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:

#+BEGIN_QUOTE
This post describes how to set up a transparent automated setup for reproducible
~R~ workflows using ~nixpkgs~, ~niv~, and ~lorri~. The explanatory example used
throughout the post is one of setting up the ~rethinking~ package and running
some examples
from the [[https://xcelab.net/rm/statistical-rethinking/][excellent second edition]] of "Statistical
Rethinking" by [[https://twitter.com/rlmcelreath][Richard McElreath]].
#+END_QUOTE

** Background
As detailed [[Nix with R and devtools][in an earlier post]][fn:wherehook], I had set up Nix to work with
non-CRAN packages. If the rest of this section is unclear, please refer back to [[Nix with R and devtools][the earlier post]].
*** Setup

For the remainder of the post, we will set up a basic project structure:
#+BEGIN_SRC bash
mkdir tryRnix/
#+END_SRC

#+RESULTS:

Now we will create a ~shell.nix~ as[fn:explaaiin]:
#+BEGIN_SRC  nix
# shell.nix
{ pkgs ? import <nixpkgs> { } }:
with pkgs;
let
  my-r-pkgs = rWrapper.override {
    packages = with rPackages; [
      ggplot2
      tidyverse
      tidybayes
      tidybayes.rethinking
      (buildRPackage {
        name = "rethinking";
        src = fetchFromGitHub {
          owner = "rmcelreath";
          repo = "rethinking";
          rev = "d0978c7f8b6329b94efa2014658d750ae12b1fa2";
          sha256 = "1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0";
        };
        propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ];
      })
    ];
  };
in mkShell {
  buildInputs = with pkgs; [ git glibcLocales openssl which openssh curl wget ];
  inputsFrom = [ my-r-pkgs ];
  shellHook = ''
    mkdir -p "$(pwd)/_libs"
    export R_LIBS_USER="$(pwd)/_libs"
  '';
  GIT_SSL_CAINFO = "${cacert}/etc/ssl/certs/ca-bundle.crt";
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

So we have:
#+BEGIN_SRC bash :exports both
tree tryRnix
#+END_SRC

#+RESULTS:
| tryRnix |              |   |      |
| â””â”€â”€     | shell.nix    |   |      |
| 0       | directories, | 1 | file |

*** Introspection
At this point:
- I was able to install packages (system and ~R~) arbitrarily
- I was able to use project specific folders
- Unlike ~npm~, ~pipenv~, ~poetry~, ~conda~ and friends, my system was not bloated by downloading and setting up the same packages every-time I used them in different projects

However, though this is a major step up from being chained to RStudio and my
system package manager, it is still perhaps not immediately obvious how this
workflow is reproducible. Admittedly, I have defined my packages in a nice
functional manner; but someone else might have a different upstream channel they
are tracking, and thus will have different packages. Indeed the only packages
which I could be sure of were the ~R~ packages I built from Github, since those
were tied to a hash. Finally, the setup described for each project is pretty
onerous, and it is not immediately clear how to leverage fantastic tools like
~direnv~ for working through this.
** Towards Reproducible Environments
The astute reader will have noticed that I mentioned that the ~R~ packages were
reproducible since they were tied to a *hash*, and might reasonable argue that
the entire Nix ecosystem is about *hashing* in the first place. Once we realize
that, the rest is relatively simple[fn:callbacktopost].
*** Niv and Pinning
[[https://github.com/nmattia/niv/][Niv]] essentially keeps track of the channel from which all the packages are installed. Setup is pretty minimal.
#+BEGIN_SRC bash
cd tryRnix/
nix-env -i niv
niv init
#+END_SRC

At this point, we have:
#+BEGIN_SRC bash :exports both
tree tryRnix
#+END_SRC

#+RESULTS:
| tryRnix |            |              |       |
| â”œâ”€â”€     | nix        |              |       |
| â”‚Â Â      | â”œâ”€â”€        | sources.json |       |
| â”‚Â Â      | â””â”€â”€        | sources.nix  |       |
| â””â”€â”€     | shell.nix  |              |       |
|         |            |              |       |
| 1       | directory, | 3            | files |

We will have to update our ~shell.nix~ to use the new sources.

#+BEGIN_SRC nix
let
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  stdenv = pkgs.stdenv;
  my-r-pkgs = pkgs.rWrapper.override {
    packages = with pkgs.rPackages; [
      ggplot2
      tidyverse
      tidybayes
    ];
  };
in pkgs.mkShell {
  buildInputs = with pkgs;[ git glibcLocales openssl which openssh curl wget my-r-pkgs ];
  shellHook = ''
    mkdir -p "$(pwd)/_libs"
    export R_LIBS_USER="$(pwd)/_libs"
  '';
  GIT_SSL_CAINFO = "${pkgs.cacert}/etc/ssl/certs/ca-bundle.crt";
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${pkgs.glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

We could inspect and edit these sources by hand, but it is much more convenient
to simply use ~niv~ again when we need to update these.

#+BEGIN_SRC bash
cd tryRnix/
niv update nixpkgs -b nixpkgs-unstable
#+END_SRC

At this stage we have a reproducible set of packages ready to use. However it is
still pretty annoying to have to go through the trouble of writing ~nix-shell~
and also waiting while it rebuilds when we change things.
*** Lorri and Direnv
[[Poetry and Direnv][In the past]], I have made my admiration for ~direnv~ very clear (especially for
~python-poetry~). However, though ~direnv~ does allow us to include arbitrary ~bash~ logic into our projects, it would be nice to have something which has some defaults for nix. Thankfully, the folks at TweagIO developed [[https://github.com/target/lorri][lorri]] to scratch that itch.

The basic setup is simple:

#+BEGIN_SRC bash
nix-env -i lorri
cd tryRnix/
lorri init
#+END_SRC

#+BEGIN_SRC bash :exports both
tree -a tryRnix/
#+END_SRC

#+RESULTS:
| tryRnix/ |            |              |       |
| â”œâ”€â”€      | .envrc     |              |       |
| â”œâ”€â”€      | nix        |              |       |
| â”‚Â Â       | â”œâ”€â”€        | sources.json |       |
| â”‚Â Â       | â””â”€â”€        | sources.nix  |       |
| â””â”€â”€      | shell.nix  |              |       |
|          |            |              |       |
| 1        | directory, | 4            | files |

We can and should inspect the environment ~lorri~ wants us to load with ~direnv~ file:

#+BEGIN_SRC bash :exports both
cat tryRnix/.envrc
#+END_SRC

#+RESULTS:
: $(lorri direnv)

In and of itself that is not too descriptive, so we should run  that on our own first.

#+BEGIN_SRC bash
EVALUATION_ROOT="$HOME/.cache/lorri/gc_roots/407bd4df60fbda6e3a656c39f81c03c2/gc_root/shell_gc_root"

watch_file "/run/user/1000/lorri/daemon.socket"
watch_file "$EVALUATION_ROOT"

#!/usr/bin/env bash
# ^ shebang is unused as this file is sourced, but present for editor
# integration. Note: Direnv guarantees it *will* be parsed using bash.

function punt () {
    :
}

# move "origPreHook" "preHook" "$@";;
move() {
    srcvarname=$1 # example: varname might contain the string "origPATH"
    # drop off the source variable name
    shift

    destvarname=$1 # example: destvarname might contain the string "PATH"
    # drop off the destination variable name
    shift

    # like: export origPATH="...some-value..."
    export "${@?}";

    # set $original to the contents of the variable $srcvarname
    # refers to
    eval "$destvarname=\"${!srcvarname}\""

    # mark the destvarname as exported so direnv picks it up
    # (shellcheck: we do want to export the content of destvarname!)
    # shellcheck disable=SC2163
    export "$destvarname"

    # remove the export from above, ie: export origPATH...
    unset "$srcvarname"
}

function prepend() {
    varname=$1 # example: varname might contain the string "PATH"

    # drop off the varname
    shift

    separator=$1 # example: separator would usually be the string ":"

    # drop off the separator argument, so the remaining arguments
    # are the arguments to export
    shift

    # set $original to the contents of the the variable $varname
    # refers to
    original="${!varname}"

    # effectfully accept the new variable's contents
    export "${@?}";

    # re-set $varname's variable to the contents of varname's
    # reference, plus the current (updated on the export) contents.
    # however, exclude the ${separator} unless ${original} starts
    # with a value
    eval "$varname=${!varname}${original:+${separator}${original}}"
}

function append() {
    varname=$1 # example: varname might contain the string "PATH"

    # drop off the varname
    shift

    separator=$1 # example: separator would usually be the string ":"
    # drop off the separator argument, so the remaining arguments
    # are the arguments to export
    shift


    # set $original to the contents of the the variable $varname
    # refers to
    original="${!varname:-}"

    # effectfully accept the new variable's contents
    export "${@?}";

    # re-set $varname's variable to the contents of varname's
    # reference, plus the current (updated on the export) contents.
    # however, exclude the ${separator} unless ${original} starts
    # with a value
    eval "$varname=${original:+${original}${separator}}${!varname}"
}

varmap() {
    if [ -f "$EVALUATION_ROOT/varmap-v1" ]; then
        # Capture the name of the variable being set
        IFS="=" read -r -a cur_varname <<< "$1"

        # With IFS='' and the `read` delimiter being '', we achieve
        # splitting on \0 bytes while also preserving leading
        # whitespace:
        #
        #    bash-3.2$ printf ' <- leading space\0bar\0baz\0' \
        #                  | (while IFS='' read -d $'\0' -r x; do echo ">$x<"; done)
        #    > <- leading space<
        #    >bar<
        #    >baz<```
        while IFS='' read -r -d '' map_instruction \
           && IFS='' read -r -d '' map_variable \
           && IFS='' read -r -d '' map_separator; do
            unset IFS

            if [ "$map_variable" == "${cur_varname[0]}" ]; then
                if [ "$map_instruction" == "append" ]; then
                    append "$map_variable" "$map_separator" "$@"
                    return
                fi
            fi
        done < "$EVALUATION_ROOT/varmap-v1"
    fi


    export "${@?}"
}

function declare() {
    if [ "$1" == "-x" ]; then shift; fi

    # Some variables require special handling.
    #
    # - punt:    don't set the variable at all
    # - prepend: take the new value, and put it before the current value.
    case "$1" in
        # vars from: https://github.com/NixOS/nix/blob/92d08c02c84be34ec0df56ed718526c382845d1a/src/nix-build/nix-build.cc#L100
        "HOME="*) punt;;
        "USER="*) punt;;
        "LOGNAME="*) punt;;
        "DISPLAY="*) punt;;
        "PATH="*) prepend "PATH" ":" "$@";;
        "TERM="*) punt;;
        "IN_NIX_SHELL="*) punt;;
        "TZ="*) punt;;
        "PAGER="*) punt;;
        "NIX_BUILD_SHELL="*) punt;;
        "SHLVL="*) punt;;

        # vars from: https://github.com/NixOS/nix/blob/92d08c02c84be34ec0df56ed718526c382845d1a/src/nix-build/nix-build.cc#L385
        "TEMPDIR="*) punt;;
        "TMPDIR="*) punt;;
        "TEMP="*) punt;;
        "TMP="*) punt;;

        # vars from: https://github.com/NixOS/nix/blob/92d08c02c84be34ec0df56ed718526c382845d1a/src/nix-build/nix-build.cc#L421
        "NIX_ENFORCE_PURITY="*) punt;;

        # vars from: https://www.gnu.org/software/bash/manual/html_node/Bash-Variables.html (last checked: 2019-09-26)
        # reported in https://github.com/target/lorri/issues/153
        "OLDPWD="*) punt;;
        "PWD="*) punt;;
        "SHELL="*) punt;;

        # https://github.com/target/lorri/issues/97
        "preHook="*) punt;;
        "origPreHook="*) move "origPreHook" "preHook" "$@";;

        *) varmap "$@" ;;
    esac
}

export IN_NIX_SHELL=impure

if [ -f "$EVALUATION_ROOT/bash-export" ]; then
    # shellcheck disable=SC1090
    . "$EVALUATION_ROOT/bash-export"
elif [ -f "$EVALUATION_ROOT" ]; then
    # shellcheck disable=SC1090
    . "$EVALUATION_ROOT"
fi

unset declare

Jun 06 19:02:32.368 INFO lorri has not completed an evaluation for this project yet, expr: $HOME/Git/Github/WebDev/Mine/haozeke.github.io/content-org/tryRnix/shell.nix
Jun 06 19:02:32.368 WARN `lorri direnv` should be executed by direnv from within an `.envrc` file, expr: $HOME/Git/Github/WebDev/Mine/haozeke.github.io/content-org/tryRnix/shell.nix
#+END_SRC

Upon inspection, that seems to check out. So now we can enable this.

#+BEGIN_SRC bash
direnv allow
#+END_SRC

Additionally, we will need to stick to using a pure environment as much as
possible to prevent unexpected situations. So we set:

#+BEGIN_SRC bash
# .envrc
eval "$(lorri direnv)"
nix-shell --run bash --pure
#+END_SRC

There's still a catch though. We need to have ~lorri daemon~ running to make
sure the packages are built automatically without us having to exit the shell
and re-run things. We can [[https://github.com/target/lorri/blob/master/contrib/daemon.md][turn to the documentation]] for this. Essentially, we
need to have a user-level systemd socket file and service for ~lorri~.

#+BEGIN_SRC bash
# ~/.config/systemd/user/lorri.socket
[Unit]
Description=Socket for Lorri Daemon

[Socket]
ListenStream=%t/lorri/daemon.socket
RuntimeDirectory=lorri

[Install]
WantedBy=sockets.target
#+END_SRC

#+BEGIN_SRC bash
# ~/.config/systemd/user/lorri.service
[Unit]
Description=Lorri Daemon
Requires=lorri.socket
After=lorri.socket

[Service]
ExecStart=%h/.nix-profile/bin/lorri daemon
PrivateTmp=true
ProtectSystem=strict
ProtectHome=read-only
Restart=on-failure
#+END_SRC

With that we are finally ready to start working with our auto-managed,
reproducible environments.

#+BEGIN_SRC bash
systemctl --user daemon-reload && \
systemctl --user enable --now lorri.socket
#+END_SRC

** Rethinking
As promised, we will first test the setup to see that everything is working. Now
is also a good time to try the ~tidybayes.rethinking~ package. In order to use
it, we will need to define the ~rethinking~ package in a way so we can pass it
to the ~buildInputs~ for ~tidybayes.rethinking~. We will modify new ~shell.nix~
as follows:

#+BEGIN_SRC nix :tangle tryRnix/shell.nix
# shell.nix
let
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  stdenv = pkgs.stdenv;
  rethinking = with pkgs.rPackages;
    buildRPackage {
      name = "rethinking";
      src = pkgs.fetchFromGitHub {
        owner = "rmcelreath";
        repo = "rethinking";
        rev = "d0978c7f8b6329b94efa2014658d750ae12b1fa2";
        sha256 = "1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0";
      };
      propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ];
    };
  tidybayes_rethinking = with pkgs.rPackages;
    buildRPackage {
      name = "tidybayes.rethinking";
      src = pkgs.fetchFromGitHub {
        owner = "mjskay";
        repo = "tidybayes.rethinking";
        rev = "df903c88f4f4320795a47c616eef24a690b433a4";
        sha256 = "1jl3189zdddmwm07z1mk58hcahirqrwx211ms0i1rzbx5y4zak0c";
      };
      propagatedBuildInputs =
        [ dplyr tibble rlang MASS tidybayes rethinking rstan ];
    };
  rEnv = pkgs.rWrapper.override {
    packages = with pkgs.rPackages; [
      ggplot2
      tidyverse
      tidybayes
      devtools
      modelr
      cowplot
      ggrepel
      RColorBrewer
      purrr
      forcats
      rstan
      rethinking
      tidybayes_rethinking
    ];
  };
in pkgs.mkShell {
  buildInputs = with pkgs; [ git glibcLocales which ];
  inputsFrom = [ rEnv ];
  shellHook = ''
    mkdir -p "$(pwd)/_libs"
    export R_LIBS_USER="$(pwd)/_libs"
  '';
  GIT_SSL_CAINFO = "${pkgs.cacert}/etc/ssl/certs/ca-bundle.crt";
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${pkgs.glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

The main thing to note here is that we need the output of the derivation we
create here, i.e. we need to use ~inputsFrom~ and NOT ~buildInputs~ for ~rEnv~.

Let us try to get a nice graphic for the conclusion.

#+BEGIN_SRC R :tangle tryRnix/tesPlot.R
library(magrittr)
library(dplyr)
library(purrr)
library(forcats)
library(tidyr)
library(modelr)
library(tidybayes)
library(tidybayes.rethinking)
library(ggplot2)
library(cowplot)
library(rstan)
library(rethinking)
library(ggrepel)
library(RColorBrewer)

theme_set(theme_tidybayes())
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())


set.seed(5)
n = 10
n_condition = 5
ABC =
  tibble(
    condition = factor(rep(c("A","B","C","D","E"), n)),
    response = rnorm(n * 5, c(0,1,2,1,-1), 0.5)
  )

mtcars_clean = mtcars %>%
  mutate(cyl = factor(cyl))

m_cyl = ulam(alist(
    cyl ~ dordlogit(phi, cutpoint),
    phi <- b_mpg*mpg,
    b_mpg ~ student_t(3, 0, 10),
    cutpoint ~ student_t(3, 0, 10)
  ),
  data = mtcars_clean,
  chains = 4,
  cores = parallel::detectCores(),
  iter = 2000
)

cutpoints = m_cyl %>%
  recover_types(mtcars_clean) %>%
  spread_draws(cutpoint[cyl])

# define the last cutpoint
last_cutpoint = tibble(
  .draw = 1:max(cutpoints$.draw),
  cyl = "8",
  cutpoint = Inf
)

cutpoints = bind_rows(cutpoints, last_cutpoint) %>%
  # define the previous cutpoint (cutpoint_{j-1})
  group_by(.draw) %>%
  arrange(cyl) %>%
  mutate(prev_cutpoint = lag(cutpoint, default = -Inf))

fitted_cyl_probs = mtcars_clean %>%
  data_grid(mpg = seq_range(mpg, n = 101)) %>%
  add_fitted_draws(m_cyl) %>%
  inner_join(cutpoints, by = ".draw") %>%
  mutate(`P(cyl | mpg)` =
    # this part is logit^-1(cutpoint_j - beta*x) - logit^-1(cutpoint_{j-1} - beta*x)
    plogis(cutpoint - .value) - plogis(prev_cutpoint - .value)
  )


data_plot = mtcars_clean %>%
  ggplot(aes(x = mpg, y = cyl, color = cyl)) +
  geom_point() +
  scale_color_brewer(palette = "Dark2", name = "cyl")

fit_plot = fitted_cyl_probs %>%
  ggplot(aes(x = mpg, y = `P(cyl | mpg)`, color = cyl)) +
  stat_lineribbon(aes(fill = cyl), alpha = 1/5) +
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2")

png(filename="../images/rethinking.png")
plot_grid(ncol = 1, align = "v",
  data_plot,
  fit_plot
)
dev.off
#+END_SRC

Finally we will run this in our environment.

#+BEGIN_SRC bash
Rscript tesPlot.R
#+END_SRC

[[file:images/rethinking.png]]

** Conclusions
This post was really more of an exploratory follow up to the previous post, and
does not really work in isolation. Then again, at this point everything seems to
have worked out well. ~R~ with Nix has finally become a truly viable combination
for any and every analysis under the sun. Some parts of the workflow are still a
bit janky, but will probably resolve themselves over time.

*Update:* There is [[Emacs for Nix-R][a final part]] detailing automated ways of reloading the system configuration

[fn:wherehook] My motivations were laid out in the [[Nix with R and devtools][aforementioned post]], and will not be repeated
[fn:explaaiin] For why these are the way they are see the this is written, see the [[Nix with R and devtools][aforementioned post]]
[fn:callbacktopost] Christine Dodrill [[https://christine.website/blog/how-i-start-nix-2020-03-08][has a great write up]] on using these tools as well
* DONE Emacs for Nix-R :@programming:tools:nix:workflow:R:emacs:
CLOSED: [2020-06-10 Wed 00:12]
:PROPERTIES:
:EXPORT_FILE_NAME: emacs-nix-r
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A short post on my current set-up for ~R~ with ~nixpkgs~ and ~emacs~ to
auto-compile my system configuration.
#+END_QUOTE
** Background
This is my third post on working with ~nixpkgs~ and ~R~.

- [[Nix with R and devtools][Part I]] covered ways of working effectively with ~R~ and ~nixpkgs~
- [[Statistical Rethinking and Nix][Part II]] dealt with composing dependent ~devtools~ packages in a per-package environment, with a focus on ~rethinking~ and ~tidybayes.rethinking~

This final part is about automating the system-wide configuration using ~emacs~.
Specifically ~doom-emacs~. Naturally, this is the most optimal way to work with
~nix~ packages as well.
*** System Configuration
After experimenting with a per-project layout, I decided to use the full system
configuration instead of the per-project layout. So I simply set:
#+BEGIN_SRC nix
# $HOME/.config/nixpkgs/config.nix
{
  packageOverrides = super:
    let
      self = super.pkgs;
      rethinking = with self.rPackages;
        buildRPackage {
          name = "rethinking";
          src = self.fetchFromGitHub {
            owner = "rmcelreath";
            repo = "rethinking";
            rev = "d0978c7f8b6329b94efa2014658d750ae12b1fa2";
            sha256 = "1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0";
          };
          propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ];
        };
      tidybayes_rethinking = with self.rPackages;
        buildRPackage {
          name = "tidybayes.rethinking";
          src = self.fetchFromGitHub {
            owner = "mjskay";
            repo = "tidybayes.rethinking";
            rev = "df903c88f4f4320795a47c616eef24a690b433a4";
            sha256 = "1jl3189zdddmwm07z1mk58hcahirqrwx211ms0i1rzbx5y4zak0c";
          };
          propagatedBuildInputs =
            [ dplyr tibble rlang MASS tidybayes rethinking rstan ];
        };
    in {
      rEnv = super.rWrapper.override {
        packages = with self.rPackages; [
          tidyverse
          devtools
          modelr
          purrr
          forcats
          ####################
          # Machine Learning #
          ####################
          # MLR3
          mlr3
          mlr3viz
          mlr3learners
          mlr3pipelines
          # Plotting tools
          ggplot2
          cowplot
          ggrepel
          RColorBrewer
          # Stan Stuff
          rstan
          tidybayes
          # Text Utilities
          orgutils
          latex2exp
          kableExtra
          knitr
          data_table
          printr
          # Devtools Stuff
          rethinking
          tidybayes_rethinking
        ];
      };
    };
}
#+END_SRC
If any of these look strange, refer to the [[Nix with R and devtools][earlier posts]].
** Automation Pains
~direnv~, ~lorri~ and
~niv~ (the heroes of [[Statistical Rethinking and Nix][Part II]]) are not really useful for working with the system-wide configuration, but
an elegant solution still exists, which leverages ~firestarter~ and
~after-save-hooks~ in ~emacs~.
*** Firestarter
[[https://depp.brause.cc/firestarter/][Firestarter]] is my favorite method of working with shell commands after saving
things. My setup is simply:
#+begin_src emacs-lisp
; packages.el
(package! firestarter)
#+end_src
This is coupled with a simple configuration.
#+begin_src emacs-lisp
; config.el
(use-package! firestarter
  :ensure t
  :init
  (firestarter-mode)
  :config
  (setq firestarter-default-type t)
)
#+end_src
The default type corresponds to demanding the shell outupt for the commands.
*** Nix-R Stuff
To finalize this setup, we will need to modify our system configuration
slightly. For brevity, we simply note the following local variables.

#+BEGIN_SRC nix
# $HOME/.config/nixpkgs/config.nix
# Local Variables:
# firestarter: "nix-env -f '<nixpkgs>' -iA rEnv"
# firestarter-default-type: (quote failure)
# End:
#+END_SRC

The ~firestarter-default-type~ used here is to ensure that errors are displayed
in a buffer.

To check what is being installed (if anything) simply run:
#+BEGIN_SRC bash
nix-env -f "<nixpkgs>" -iA rEnv --dry-run
#+END_SRC
** Conclusion
This is my current setup. It works out better than most of my other attempts and
seems to be an optimal approach. The packages are versioned, everything is
automated, and I can reproduce changes across all my machines. Will stick with
this.

* DONE Temporary LaTeX Documents with Orgmode :@programming:tools:emacs:workflow:orgmode:
CLOSED: [2020-06-19 Fri 05:07]
:PROPERTIES:
:EXPORT_FILE_NAME: org-arb-tex
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A post on working with transient TeX templates in ~orgmode~ without modifying global configurations. This will also serve as a rudimentary introduction to TeX in ~orgmode~.
#+END_QUOTE
** Background
The sad reality of working in a field dominated by institutional actors which do not care for recognizing software development as a skill is that there are often a lot of ugly LaTeX templates[fn:thatsall]. In particular, often Universities have arbitrary LaTeX templates from the the dark days of 2010 something, which include gratuitous usage of say, ~natbib~ instead of ~biblatex~. In other situations, ~.cls~ files define separate document classes which are not covered by the ~orgmode~ defaults and need to be accounted for.
** Standard methods
Essentially for the exporter, the document is broken into[fn:halptex]:
- *document_class* :: This cannot be changed arbitrarily and [[https://orgmode.org/manual/LaTeX-specific-export-settings.html#LaTeX-specific-export-settings][has to be]] a valid element of ~org-latex-classes~
- *preamble* :: This section of the document is essentially everything before ~\begin{document}~ and after ~\documentclass{...}~
- *body* :: The rest of the document

We will briefly cover the standard methods of entering TeX in each of these sections, in reverse order since that is the direction in which the intuitive aspect decreases.
*** In-Body TeX
The method of writing TeX in ~orgmode~ for the document involves simply writing TeX directly, or wrapping the TeX markup in an export TeX block for font locking[fn:fontwhat]. Essentially, for a document snippet:
#+BEGIN_SRC latex :exports code
% #+BEGIN_SRC latex :exports code
\begin{align}
\pi(x) &= \sum_{n=1}^{\infty}\frac{\mu(n)}{n}\Pi(x^{\frac{1}{n}}) \\
       &= \Pi(x) -\frac{1}{2}\Pi(x^{\frac{1}{2}}) - \frac{1}{3}\Pi(x^{\frac{1}{3}}) - \frac{1}{5}\Pi(x^{\frac{1}{5}}) + \frac{1}{6} \Pi(x^{\frac{1}{6}}) -\cdots,
\end{align}
% #+END_SRC
#+END_SRC

Which will actually be rendered in a real document of course[fn:butwhatformula]:
# It is the MÃ¶bius inversion formula

\begin{align}
\pi(x) &= \sum_{n=1}^{\infty}\frac{\mu(n)}{n}\Pi(x^{\frac{1}{n}}) \\
       &= \Pi(x) -\frac{1}{2}\Pi(x^{\frac{1}{2}}) - \frac{1}{3}\Pi(x^{\frac{1}{3}}) - \frac{1}{5}\Pi(x^{\frac{1}{5}}) + \frac{1}{6} \Pi(x^{\frac{1}{6}}) -\cdots,
\end{align}

There is also the inline form of writing LaTeX with ~@@\sin{x}@@~ which is essentially $\sin{x}$.
*** Preamble
The main use of the preamble is to either add classes or modify class options for loaded packages like ~geometry~. Essentially, for ~orgmode~, anything prefixed with ~#+LATEX_HEADER:~ gets inserted in the preamble.
#+BEGIN_SRC org :exports code
#+LATEX_HEADER: \usepackage{amssymb,amsmath,MnSymbol}
#+LATEX_HEADER: \usepackage{unicode-math}
#+LATEX_HEADER: \usepackage{mathtools}
#+END_SRC
For larger documents, this gets quite annoying for loading packages. We will demonstrate a more aesthetically pleasant form later in this post.
*** LaTeX classes
Working with document classes is the least intuitive of all TeX manipulations, because for some reason, ~#+LATEX_CLASS:~ only accepts values defined in ~org-latex-classes~.

The standard approach to extending the ~orgmode~ TeX backend is to add lines like the following in ~init.el~ or, in my case[fn:whatwhy], ~config.org~:
#+BEGIN_SRC emacs-lisp
(add-to-list 'org-latex-classes
             '("koma-article" "\\documentclass{scrartcl}"
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}" . "\\paragraph*{%s}")
               ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
#+END_SRC

This is alright for often used classes like the ~koma-â‹†~ family of LaTeX document-classes, but it is hardly ideal for one-off TeX templates which are meant for say, grant proposals[fn:grantwhat].
** Elisp to the rescue
The core idea is quite simple.
#+BEGIN_QUOTE
Since ~orgmode~ files are literate documents, and ~emacs~ is self-documenting and completely programmable, it should be possible to execute code to deterministically set the state of ~emacs~ before exporting the document to TeX.
#+END_QUOTE
Practically this has a few moving parts. In the following sections, assume that we have a ~.cls~ file which defines a document-class ~foo~ with a bunch of packages which conflict with our global configuration.
*** Adding Document Classes
Instead of adding the code snippet to our global configuration, we will now add it to the document directly with the comments indicating the appropriate environment.
#+BEGIN_SRC emacs-lisp :exports code  :results none :eval never
;; #+BEGIN_SRC emacs-lisp :exports none  :results none :eval always
(add-to-list 'org-latex-classes
             '("foo" "\\documentclass{foo}"
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}" . "\\paragraph*{%s}")
               ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
;; #+END_SRC
#+END_SRC

Where the header arguments simply ensure that the code and result do not show up in the document, and that the chunk is always evaluated by ~org-babel~.
*** Ensuring Purity
Since we want to stick to an external template defined in ~foo~ *and no other packages* we will need to clear the defaults we lovingly set globally for our convenience.
#+BEGIN_SRC emacs-lisp :exports code  :results none :eval never
;; #+BEGIN_SRC emacs-lisp :exports none  :results none :eval always
(setq org-latex-packages-alist 'nil)
;; #+END_SRC
#+END_SRC
*** Pretty Packages
For packages we really would like to add, we can now leverage the ~elisp~ code instead of the ugly ~#+LATEX_HEADER:~ lines.
#+BEGIN_SRC emacs-lisp :exports code  :results none :eval never
;; #+BEGIN_SRC emacs-lisp :exports none  :results none :eval always
(setq org-latex-default-packages-alist
  '(("utf8" "inputenc"  t)
    ("normalem" "ulem"  t)
    (""     "mathtools"   t)
    ))
;; #+END_SRC
#+END_SRC
Note that for setting options, we will still need to use the ~#+LATEX_HEADER:~ syntax.
*** Automating With Hooks
At this stage, we have a chunk of ~elisp~ we can manually evaluate with ~org-babel~ before exporting with ~org-latex-export-to-pdf~ or ~org-latex-export-to-latex~. However, this can get old quickly, so we will instead have a ~before-save-hook~ to do this for us.
#+BEGIN_SRC org :exports code
# Local Variables:
# before-save-hook: org-babel-execute-buffer
# End:
#+END_SRC
**** Bonus Hook
In [[https://dotdoom.rgoswami.me/config.html#org13eed8a][my own configuration]], I have a function defined for an ~after-save-hook~ which generates the TeX file without having me deal with it. For a per-file configuration of this, or globally, the ~elisp~ is:
#+BEGIN_SRC emacs-lisp :exports code
(defun haozeke/org-save-and-export-latex ()
  (if (eq major-mode 'org-mode)
    (org-latex-export-to-latex)))
#+END_SRC
This indirection is required to call the function as a ~hook~. Now this can be used as:
#+BEGIN_SRC org :exports code
# Local Variables:
# after-save-hook: haozeke/org-save-and-export-latex
# End:
#+END_SRC
** Conclusions
The entire file would look something like this (the ~elisp~ can be anywhere in the ~orgmode~ file):
#+BEGIN_SRC emacs-lisp :exports code  :results none :eval never
;; #+BEGIN_SRC emacs-lisp :exports none  :results none :eval always
(add-to-list 'org-latex-classes
             '("foo" "\\documentclass{foo}"
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}" . "\\paragraph*{%s}")
               ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
(setq org-latex-packages-alist 'nil)
(setq org-latex-default-packages-alist
  '(("utf8" "inputenc"  t)
    (""     "minted"   t)
    (""     "rotating"  nil)
    ("normalem" "ulem"  t)
    (""     "mathtools"   t)
    ))
;; #+END_SRC
#+END_SRC
#+BEGIN_SRC org :exports code
,#+TITLE: Something
,#+AUTHOR: Rohit Goswami
,#+OPTIONS: toc:nil \n:nil
,#+STARTUP: fninline
,#+LATEX_COMPILER: xelatex
,#+LATEX_CLASS: foo
,#+LATEX_HEADER: \setlength\parindent{0pt}
,#+LATEX_HEADER: \addbibresource{whatever.bib}

Blah blah document $\sin{x}$ stuff

# Local Variables:
# before-save-hook: org-babel-execute-buffer
# after-save-hook: haozeke/org-save-and-export-latex
# End:
#+END_SRC
This method could be extended to essentially resetting all ~emacs~ variables on a per-file basis (without ~file-local-variables~ and ~dir-local-variables~) or to potentially execute any ~elisp~ to make ~emacs~ do things, though I cannot really think of another realistic use-case. The method presented here is really general enough to work with any arbitrary LaTeX ~.cls~ file or other draconian measures.

[fn:thatsall] Of course there are more issues stemming from this toxic practice, but that's for another rant
[fn:whatwhy] I use [[https://github.com/hlissner/doom-emacs/][doom-emacs]] with my own [[https://dotdoom.rgoswami.me][literate configuration]]
[fn:fontwhat] Or syntax highlighting for most people
[fn:halptex] For more on document structure in TeX [[https://en.wikibooks.org/wiki/LaTeX/Document_Structure][read the wikibook]]
[fn:grantwhat] I haven't seen a grant proposal template I'd like to store for later, *ever*
[fn:butwhatformula] Props to anyone who recognizes that formula
* TODO Computational Science and Math
One of the earliest memories of feedback I recall was about my handwriting. To this day it remains an ugly scrawl, with legibility tending to nil in cursive.

* DONE LineageOS Maintainer Appreciation :@personal:ramblings:thoughts:
CLOSED: [2020-06-28 Sun 18:35]
:PROPERTIES:
:EXPORT_FILE_NAME: linos-maintainer-appre
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:END:
#+BEGIN_QUOTE
A post on a surprisingly heartwarming community appreciation effort.
#+END_QUOTE

** Background
As probably anyone who has asked me about my programming experience has heard, my first real foray into the FOSS community was being a [[https://forum.xda-developers.com/member.php?u=1964056][LineageOS co-maintainer]] (as HaoZeke) for the Xperia Z5 Dual. I haven't thought about the community all that much for a few years, mostly since XDA became pretty toxic, and Android development just got, less exciting.
** The Email
I recieved two of these from different accounts:
#+BEGIN_QUOTE
Thank you so much for your contribution to making my phone the phone it is!

Have a great day!
#+END_QUOTE

[[file:images/lineageOS.jpg]]
** Conclusion
This was completely unexpected, and really made my day. In reterospect this seems like something which should be made more explicit, more often.

* DONE Analytics II: Goat to Clicky :@notes:tools:rationale:workflow:ideas:
CLOSED: [2020-07-06 Mon 23:09]
:PROPERTIES:
:EXPORT_FILE_NAME: goat-clicky
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A follow up to [[Analytics: Google to Goat][my earlier post on analytics]], and on migrating from Goat Counter to Clicky.
#+END_QUOTE

** Background
A few days ago I recieved the following email:
#+BEGIN_QUOTE
Hi there!

I made some changes to the GoatCounter plans/pricing:

1. GoatCounter now has a "Starter" plan, this is â‚¬5/month, limited to 100k pageviews/month, comes with a custom domain, and allow commercial use. This is mostly the same as the "personal plus" plan there was before, except that it allows commercial use. If you had a "personal plus" for a custom domain before then you now have a Starter plan.

2. Starting on August 1st the data retention will be limited for the Free and Starter plans: the Free plan will be limited to 6 months, the Starter plan to 12 months, and the business plans remain unlimited. There is an export feature if you wish to retain your old pageviews.

Some background on this:

There seems to be a gap between "free for personal use" and "â‚¬15/month for commercial use". I've gotten quite a bit of feedback of small (potential) commercial users who just run a small website, where â‚¬15/month really is prohibitively expensive.

The entire idea behind making it free for personal use is that I'd like GoatCounter to be usable by as many people as possible, while also ensuring commercial users pay their fair share. Redistributing software is free, but developing it is not.

The general thinking is that larger businesses with several employees (who can easily afford â‚¬15/month) will have more than 100k pageviews/month, whereas for most startups and the like 100k should be more than enough.

The entire thing is a bit of a balancing act ðŸ˜… I may tweak the pricing further in the future based on additional experience and feedback.

As for the data retention: the biggest issue here is that some not-especially-active sites have had short bursts of millions of pageviews in a short time because they wrote or made something that got widely shared.

The hits/months limit isn't strictly enforced because I don't want to tell people to get a plan just because they wrote a popular article that got to the front page of HN, Reddit, Twitter, etc, and GoatCounter has no problem handling these levels of pageviews, so that's all fine.

But on the other hand, a million pageviews currently takes up about 400M of disk space including backups (although this could probably be reduced a bit with a more clever backup strategy). Disk space is pretty cheap, but it does add up.

It also means more effort on scaling GoatCounter; limiting the data retention is an easy way to reduce the pressure on this. It also gives people a bit more incentive to get a plan ðŸ˜…

As always, self-hosting isn't affected by any of this. This just applies to the goatcounter.com service.

Feel free to let me know if you've got any questions or feedback.

Cheers,
Martin
#+END_QUOTE
** Personal Repercussions
I should point out that I unequivocally support Martin's decision. It is fair and equitable. That said, continuity is super important to me. I've mentioned earlier that for me, the daily limit is not much of an issue but I do like to look back at my collective history.
** Goat Counter
[[https://www.goatcounter.com/][Goat Counter]] is still definitely my go-to option for both self-hosting and shelling out, if the 15 euro fee is acceptable. Honestly, the best option is probably opening a PR or making a tool to aggregate metrics offline, since it is still possible to export the data. The major drawback is the six month window.
** Clicky
[[https://clicky.com/][Clicky]] is pretty great, and they have a good example of [[https://clicky.com/compare/google][what works out in their favor compared to Google Analytics]].
*** Pros
- Free tier has no time limit
- Has a nice dark theme
*** Cons
- Not open source
- Capped at 3K daily views
- Blocked by some VPN providers (Windscribe)
** Conclusions
It is unfortunate to have had to move, since this does imply losing the past eight months of metrics. Eventually I might even go back to steady dependable Google Analytics. Until this, Clicky will do.
* DONE Multiple Monitors with Touchscreens :@notes:tools:workflow:
CLOSED: [2020-07-11 Sat 22:45]
:PROPERTIES:
:EXPORT_FILE_NAME: multi-monitor-touch
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A short tutorial post on multiple screens for laptops with touch-support and ArchLinux. Also evolved into a long rant, with an Easter egg.
#+END_QUOTE
** Background
Of late, I have been attempting to move away from paper, for environmental reasons[fn:environ]. Years of touch typing in Colemak ([[Switching to Colemak][rationale]], [[Refactoring Dotfiles For Colemak][config changes]]) and a very customized [[https://dotdoom.rgoswami.me][Emacs setup]] (including [[Using Mathematica with Orgmode][mathematica]], [[Temporary LaTeX Documents with Orgmode][temporary latex templates]], [[Emacs for Nix-R][Nix]], and [[An Orgmode Note Workflow][org-roam annotations]]) have more or less kept me away from analog devices. One might even argue that my current website is essentially a set of tricks to move my life into ~orgmode~ completely.

However, there are still a few things I cannot do without a pointing device (and some kind of canvas). Scrawl squiggly lines on papers I'm reviewing. That and, scrawl weird symbols which don't actually follow a coherent mathematical notation but might be later written up in latex to prove a point. Also, and I haven't mastered any of the drawing systems (like [[https://en.wikibooks.org/wiki/LaTeX/PGF/TikZ][Tikz]]) yet, so for squiggly charts I rely on [[https://jamboard.google.com/][Jamboard]] (while teaching) and [[https://github.com/xournalpp/xournalpp][Xournal++]] for collaborations.

I also happen to have a [[https://www.notebookcheck.net/Lenovo-ThinkPad-X380-Yoga-i5-8250U-FHD-Convertible-Review.316442.0.html][ThinkPad X380]] (try ~sudo dmidecode -t system | grep Version~) which has an in-built stylus, and since Linux support for touchscreens from 2018 is known to be incredible, I coupled this with the [[https://www.lenovo.com/us/en/thinkvisionM14/][ThinkVision M14]] as a second screen.
** X-Windows and X-ternal Screens

We will define two separate solutions:
- [[https://github.com/Ventto/mons][mons]] :: Using arbitrary external monitors
- [[https://github.com/phillipberndt/autorandr][autorandr]] :: Setting up profiles for specific monitors

Finally we will leverage both to ensure a constant touchscreen area.

*** Autorandr
I use the [[https://github.com/phillipberndt/autorandr][python rewrite]] simply because that's the one which is in the [[https://www.archlinux.org/packages/community/any/autorandr/][ArchLinux community repo]]. To be honest, I came across this before I (re-)discovered ~mons~. The most relevant aspect of ~autorandr~ is using complicated configurations for different monitors, but it also does a mean job of running generic scripts as ~postswitch~ and ~prefix~ scripts.

*** Mons
~xrandr~ is awesome. Unfortunately, more often than not, I forget the commands to interact with it appropriately.
[[https://github.com/Ventto/mons][mons]] does my dirty work for me[fn:monsgood].

#+BEGIN_SRC bash
# -e is extend
mons -e left
# puts screen on the left
#+END_SRC

That and the similar ~right~ option, covers around 99% of all possible dual screen use-cases.

** Constant Touch

The problem is that by default, the entire combined screen area is assumed to be touch-enabled, which essentially means an awkward area of the screen which is dead to all input (since it doesn't exist). The key insight is that I never have more touch-enabled surfaces than my default screen, no matter how many extended screens are present. So the solution is:

#+BEGIN_QUOTE
Make sure the touch area is constant.
#+END_QUOTE

We need to figure out what the touch input devices are:

#+BEGIN_SRC bash :exports both
xinput --list
#+END_SRC

#+RESULTS:
| âŽ¡ Virtual core pointer                             | id=2  | [master pointer  (3)] |
| âŽœ   â†³ Virtual core XTEST pointer                   | id=4  | [slave  pointer  (2)] |
| âŽœ   â†³ Wacom Pen and multitouch sensor Finger touch | id=10 | [slave  pointer  (2)] |
| âŽœ   â†³ Wacom Pen and multitouch sensor Pen stylus   | id=11 | [slave  pointer  (2)] |
| âŽœ   â†³ ETPS/2 Elantech TrackPoint                   | id=14 | [slave  pointer  (2)] |
| âŽœ   â†³ ETPS/2 Elantech Touchpad                     | id=15 | [slave  pointer  (2)] |
| âŽœ   â†³ Wacom Pen and multitouch sensor Pen eraser   | id=17 | [slave  pointer  (2)] |
| âŽ£ Virtual core keyboard                            | id=3  | [master keyboard (2)] |
| â†³ Virtual core XTEST keyboard                      | id=5  | [slave  keyboard (3)] |
| â†³ Power Button                                     | id=6  | [slave  keyboard (3)] |
| â†³ Video Bus                                        | id=7  | [slave  keyboard (3)] |
| â†³ Power Button                                     | id=8  | [slave  keyboard (3)] |
| â†³ Sleep Button                                     | id=9  | [slave  keyboard (3)] |
| â†³ Integrated Camera: Integrated C                  | id=12 | [slave  keyboard (3)] |
| â†³ AT Translated Set 2 keyboard                     | id=13 | [slave  keyboard (3)] |
| â†³ ThinkPad Extra Buttons                           | id=16 | [slave  keyboard (3)] |

At this point we will leverage ~autorandr~ to ensure that these devices are mapped to the primary (touch-enabled) screen with a ~postswitch~ script. This ~postswitch~ script needs to be:

#+BEGIN_SRC bash
#!/bin/sh
# .config/autorandr/postswitch
xinput --map-to-output 'Wacom Pen and multitouch sensor Finger touch' eDP1
xinput --map-to-output 'Wacom Pen and multitouch sensor Pen stylus' eDP1
xinput --map-to-output 'Wacom Pen and multitouch sensor Pen eraser' eDP1
notify-send "Screen configuration changed"
#+END_SRC

The last line of course is really more of an informative boast.

At this stage, we have the ability to set the touchscreens up by informing ~autorandr~ that our configuration has changed, through the command line for example:
#+BEGIN_SRC bash
autorandr --change
#+END_SRC

** Automatic Touch Configuration

Running a command manually on-change is the sort of thing which makes people think Linux is hard or un-intuitive. So we will instead make use of the incredibly powerful ~systemd~ framework for handling events.

Essentially, we combine our existing workflow with ~autorandr-launcher~ [[https://github.com/smac89/autorandr-launcher][from here]], and then set it all up as follows:

#+BEGIN_SRC bash
git clone https://github.com/smac89/autorandr-launcher.git
cd autorandr-launcher
sudo make install
sudo systemctl--user enable autorandr_launcher.service
#+END_SRC

** Conclusion
We now have a setup which ensures that the touch enabled area is constant, without any explicit manual interventions for when devices are added or removed. There isn't much else to say about this workflow here. Additional screens can be [[Old Laptops as Secondary Monitors][configured from older laptops described here]]. Separate posts can deal with how exactly I meld Zotero, ~org-roam~ and Xournal++ to wreak havoc on all kinds of documents. So, in-lieu of a conclusion, behold a recent scribble with this setup:

#+caption: From the planning of [[http://www.wavelf.org/ij6TzydE3kTSF8cwwIUj][this voluntary course]]
file:images/myXournalScrib.png

[fn:environ] Also because paper is difficult to keep track of and is essentially the antithesis of a computer oriented workflow.
[fn:monsgood] I actually planned a whole post called "An Ode to Mons", when I first found out about it.

* DONE Grant Proposals - I :@personal:ramblings:thoughts:academia:
CLOSED: [2020-07-18 Sat 20:31]
:PROPERTIES:
:EXPORT_FILE_NAME: grant-proposals
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:END:
#+BEGIN_QUOTE
Personal recollections of the academic grant writing process.
#+END_QUOTE
** Background
Of the many types of writing one undertakes in a typical academic career, grant writing stands out as a rather large anomaly. For the purposes of this post, we will note that an academic writing taxonomy would consist of roughly:
- Coursework and Assignments :: These are more or less comparative writing exercises, where the only main thing which is enforced is (or should be) plagiarism checks. In terms of locality in history, these are more or less focused on the past, with little to no original content.
- Peer Reviewed Articles :: Broadly, in this category we can lump society journals, some conference articles, and even reviews to some extent. These are hyper-local in time, with enough historical perspective to make the paper worthwhile for the journal/conference, and originality of content is a key highlight.
- Grants :: Grants are unique. They are both short (in terms of a prospectus) and also long, in that there are a huge number of auxiliary files to be added.
** Grants
  At some stage, every researcher who doesn't bow out of academia ends up faced with the prospect of proving to a funding agency that they are capable and well-adjusted enough to get money for an extended amount of time.
*** Lit Surveys
Unlike papers and reviews, though recent papers are important, it is more relevant to actually project where the field will be in upcoming years to ensure the deliverables are not out-dated. Additionally, it is best to link to widely cited literature, to ensure that the reviewers believe in your holistic understanding.
*** Deliverables
This part is fun to write, building towards a goal, is a special kind of write up. It allows one to really flesh out a research plan with realistic goals.
*** Extras
- For most applications, there is a budgetary requirement, mostly with a spreadsheet component.
- A Gantt chart is also often required
  - I started with [[https://teamgantt.com][teamgantt]], which was neat
  - Of course I eventually ended up regressing to ~orgmode~ and TeX using [[https://github.com/swillner/org-gantt/blob/master/org-gantt-manual.org][org-gantt]]
    - This actually would need a whole other post, but it is great
** Conclusions
If this post seemed short, it is probably because even though a lot else went into the proposal, until I hear otherwise next year, it would be presumptous to write more. That said, it was and is an enjoyable exercise.

* DONE A Short Guide to Statistical RethinkingÂ² :@programming:R:SR2:solutions:workflow:
CLOSED: [2020-07-24 Fri 17:35]
:PROPERTIES:
:EXPORT_FILE_NAME: some-sol-sr2
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A meta post introducing my solutions to the fantastic [[https://xcelab.net/rm/statistical-rethinking/][excellent second edition]] of "Statistical
Rethinking" by [[https://twitter.com/rlmcelreath][Richard McElreath]], a.k.a. Statistical RethinkingÂ². Also discusses strategies to keep up with the material, mostly meant for self-study groups.
#+END_QUOTE

** Background
As [[Statistical Rethinking and Nix][detailed previously]], I recently was part of a course centered around Bayesian modeling for the Icelandic COVID-19 pandemic. The Bayesian mindset needs no introduction, and this post is completely inadequete to explain why anyone should be interested (that's what the book is for!). That said, especially for self-paced study groups, it might help to have some structure.

** Solutions
These are meant to be sample solutions, and everyone *should* solve these for themselves. Each solution contains the packages used, as well as a colophon in the later posts to ensure reproduciblity.
Essentially this consists of four posts:
- [[https://rgoswami.me/posts/sr2-ch2-ch3-ch4/][Week I]] :: Covers the first four chapters {1,2,3,4}
- [[https://rgoswami.me/posts/sr2-ch5-ch6-ch7/][Week II]] :: Covers the next three chapters {5,6,7}
- [[https://rgoswami.me/posts/sr2-ch9-ch11-ch12/][Week III]] :: Covers five chapters {9,11,12}
- [[https://rgoswami.me/posts/sr2-ch13-ch14][Week IV]] :: The last five chapters {13,14}

More concisely:

| *Chapter*                                        | *Solutions* |
| 1. The Golem of Prague                           | N/A         |
| 2. Small Worlds and Large Worlds                 | [[https://rgoswami.me/posts/sr2-ch2-ch3-ch4/#chapter-ii-the-golem-of-prague][here]]        |
| 3. Sampling the Imaginary                        | [[https://rgoswami.me/posts/sr2-ch2-ch3-ch4/#chapter-iii-sampling-the-imaginary][here]]        |
| 4. Geocentric Models                             | [[https://rgoswami.me/posts/sr2-ch2-ch3-ch4/#chapter-iv-geocentric-models][here]]        |
| 5. The Many Variables & The Spurious Waffles     | [[https://rgoswami.me/posts/sr2-ch5-ch6-ch7/#chapter-v-the-many-variables-and-the-spurious-waffles][here]]        |
| 6. The Haunted DAG & The Causal Terror           | [[https://rgoswami.me/posts/sr2-ch5-ch6-ch7/#chapter-vi-the-haunted-dag-and-the-causal-terror][here]]        |
| 7. Ulyssesâ€™ Compass                              | [[https://rgoswami.me/posts/sr2-ch5-ch6-ch7/#chapter-vii-ulysses-compass][here]]        |
| 8. Conditional Manatees                          | N/A         |
| 9. Markov Chain Monte Carlo                      | [[https://rgoswami.me/posts/sr2-ch9-ch11-ch12/#chapter-ix-markov-chain-monte-carlo][here]]        |
| 10. Big Entropy and the Generalized Linear Model | N/A         |
| 11. God Spiked the Integers                      | [[https://rgoswami.me/posts/sr2-ch9-ch11-ch12/#chapter-xi-god-spiked-the-integers][here]]        |
| 12. Monsters and Mixtures                        | [[https://rgoswami.me/posts/sr2-ch9-ch11-ch12/#chapter-xii-monsters-and-mixtures][here]]        |
| 13. Models With Memory                           | [[https://rgoswami.me/posts/sr2-ch13-ch14/#chapter-xiii-models-with-memory][here]]        |
| 14. Adventures in Covariance                     | [[https://rgoswami.me/posts/sr2-ch13-ch14/#chapter-xiv-adventures-in-covariance][here]]        |
| 15. Missing Data and Other Opportunities         | TBA         |
| 16. Generalized Linear Madness                   | TBA         |
| 17. Horoscopes                                   | N/A         |

*** Pacing
The solutions compiled here were from an accelerated 4-week course covering the Statistical RethinkingÂ² in four weeks. The book is more traditionally used in a full-semester course, so that should be kept in mind as well.
** Resources
These are highly opinionated and the following list is in no way complete.
*** Canonical Content
- [[https://xcelab.net/rm/statistical-rethinking/][Richard's Website]]
- The [[https://www.youtube.com/playlist?list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI][YouTube 2019 playlist]]
- [[https://twitter.com/rlmcelreath][Richard on Twitter]]
- The [[https://mc-stan.org/][Stan page]]
*** Additional Content
- An [[https://www.youtube.com/watch?v=eDMGDhyDxuY&list=PLFVtMSYAlp5-MsffvdbR-iqfHSaOBgwpn&index=2&t=19s][introduction to Frequentist and Bayesian statistics]] from LLNL by Kristin Lennox
- A [[https://rpubs.com/bgautijonsson/week4_covid][simple COVID-19 model]] for Iceland
- More [[https://covid.hi.is/english/][complete COVID-19 model]] for Iceland
- [[https://gitlab.com/openresearchlabs/probabilistic_data_analysis_2020/-/blob/master/annurev-statistics-031219-041300.pdf][Convergence Diagnostics for MCMC]]
- Betancourt's [[https://arxiv.org/pdf/1701.02434.pdf][Conceptual Introduction to HMC]]
*** Follow-up Courses
- [[https://github.com/avehtari/BDA_course_Aalto][Bayesian Data Analysis]]
** Conclusions
This has been a short meta post which is essentially meant to collect content posted with dates in the past. Though this is not exactly a complete reference for beginners, it might still help people.

* DONE Explorations with Backlight Controllers :@notes:tools:workflow:
CLOSED: [2020-08-01 Sat 20:00]
:PROPERTIES:
:EXPORT_FILE_NAME: expl-backlight-control
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A short post detailing the many trials and tribulations of setting brightness on multiple monitors in tandem.
#+END_QUOTE

** Background
As regular readers might know, I have a multi-screen setup, which accounts for having touch enabled on my primary laptop screen (detailed [[Multiple Monitors with Touchscreens][here]]). A failing of this setup was that I was not able to control the brightness of both monitors at the same time.
** Existing Stack
Since I use ~i3~, my brightness control is simply done with ~bindsym~ lines as follows[fn:wheredot]:
#+BEGIN_SRC conf
bindsym XF86MonBrightnessDown exec light -U 10
bindsym XF86MonBrightnessUp exec light -A 10
#+END_SRC
Note that to get the right ~bindsym~ I use [[https://github.com/wavexx/screenkey][screenkey]] with the ~keysyms~ preference. My software of choice was
Unfortunately, my existing setup (with ~light~, since that is in the Arch ~community~ repo) did not actually allow dimming external screens arbitarily. To be more exact,
#+BEGIN_SRC bash :results raw
light -h
#+END_SRC

#+begin_example
Usage:
  light [OPTIONS] [VALUE]

Commands:
  -H, -h      Show this help and exit
  -V          Show program version and exit
  -L          List available devices
  -A          Increase brightness by value
  -U          Decrease brightness by value
  -T          Multiply brightness by value (can be a non-whole number, ignores raw mode)
  -S          Set brightness to value
  -G          Get brightness
  -N          Set minimum brightness to value
  -P          Get minimum brightness
  -O          Save the current brightness
  -I          Restore the previously saved brightness

Options:
  -r          Interpret input and output values in raw mode (ignored for -T)
  -s          Specify device target path to use, use -L to list available
  -v          Specify the verbosity level (default 0)
                 0: Values only
                 1: Values, Errors.
                 2: Values, Errors, Warnings.
                 3: Values, Errors, Warnings, Notices.

Copyright (C) 2012 - 2018  Fredrik Haikarainen
This is free software, see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE
#+end_example

Clearly it is possible to target specific devices, but for arbitrary additions this is quite tough. Additionally, the project has been more or less been stuck in [[https://github.com/haikarainen/light/issues/37][maintainence mode for a while now]].
** Multi-head Configurations
*** Exposing Brightness
The first hurdle faced in o was actually getting the external monitor to expose the right controls. This is accomplished neatly with ~ddci-driver~ [[https://gitlab.com/ddcci-driver-linux/ddcci-driver-linux][found here]]. As demonstrated (courtesy of [[https://wiki.archlinux.org/index.php/Backlight#Backlight_utilities][the ArchWiki]]):
#+BEGIN_SRC bash
# Load module
sudo modprobe ddcci_backlight
# Check that it worked
sudo ddcutil capabilities | grep "Feature: 10"
sudo ddcutil getvcp 10
# Set brightness
sudo ddcutil setvcp 10 70
#+END_SRC
One of the obvious caveats of this technique is that ~sudo~ access or a dedicated ~polkit~ agent is required. My preferred method of loading this comes from [[https://aur.archlinux.org/packages/ddcci-driver-linux-dkms/][a comment]] on the ~ddcci-driver-linux-dkms~ page of the AUR:
#+BEGIN_SRC bash
# Put in /etc/systemd/system/ddcci-backlight.service:
# https://aur.archlinux.org/packages/ddcci-driver-linux-dkms/
# Placing "ddcci_backlight" into /etc/modules-load.d
# leads to hang on boot with external (HDMI) monitor
# connected to the laptop, so we need to add the module later.

# And ddcci_backlight can't detect monitor during sddm.service startup.

[Unit]
After=multi-user.target
Before=sddm.service

[Service]
Type=oneshot
ExecStart=/usr/bin/modprobe ddcci_backlight
ExecStop=/usr/bin/modprobe --remove ddcci_backlight
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
#+END_SRC
This is then activated with a standard ~systemctl enable ddcci_backlight.service~ command. At this point, the device interface should be exposed to most backlight controllers.
*** Xrandr
This is the most obvious of all methods, and does not even require the ~ddcci-driver~. We will simply tweak the brightness with ~xrandr~.
#+BEGIN_SRC bash
# Get devices
xrandr | grep " connected"
# Tweak
xrandr --output DP1 --brightness 0.2
#+END_SRC

Note that this is an in-exact method, since it actually adjusts the gamma value instead, and it effectively tints your screen rather than modifying the brightness.
*** Clight
[[https://github.com/FedeDP/Clight][clight]] is an excellent, highly performant alternative to redshift, but it tends to force the main screen brightness to zero. Nothing which can't be configured away, but in practice, I work late and tend to turn off the tint anyway. This requires a daemon to be run, as well as needing to be turned on manually for ~i3~. A very elegant additional feature gained by using ~clight~ is that external monitors are turned off automagically when lockscreens are activated.
*** Brillo
[[https://github.com/CameronNemo/brillo][brillo]] is one of the newer controllers, and is pretty actively developed. The interface is almost exactly like ~light~, and unlike ~clight~ there is no need to use a daemon. It meshes perfectly with a key-press based system like ~i3~ and also has controls for keyboard LEDS as well as for smoothed ramping up and down of the brightness. Most importantly, it features an ~-e~ flag which sets the brightness across all output devices. Essentially this means my configuration is simply modified to:
#+BEGIN_SRC conf
bindsym XF86MonBrightnessDown exec brillo -e -U 10
bindsym XF86MonBrightnessUp exec brillo -e -A 10
#+END_SRC
** Conclusions
*tl;dr* moving from ~light~ to ~brillo~ with ~ddcci-driver-linux-dkms~ was a fantastic idea.

[fn:wheredot] My dotfiles [[https://github.com/HaoZeke/Dotfiles][are here]]

* TODO Switching from Zplug to Zinit
* DONE HPC Dotfiles and LMod :@programming:workflow:projects:hpc:
CLOSED: [2020-08-09 Sun 02:29]
:PROPERTIES:
:EXPORT_FILE_NAME: hpc-dots-lmod
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
** Background
My move away from the powerful, but unimaginatively named HPC clusters of IITK [fn:reallythatbad] brought me in close contact with the Lua based [fn:mustisay] [[https://lmod.readthedocs.io/en/latest/index.html][lmod module system]]. Rather than fall into the rabbit hole of [[https://docs.brew.sh/Homebrew-on-Linux][brew]] we will leverage the existing system to add our new libraries. Not finding any good collections of these composable environments, and [[Provisioning Dotfiles on an HPC][having failed once before]] to install Nix as a user without admin access, I decided to [[https://github.com/HaoZeke/hzHPC_lmod][start my own collection of Lmod recipies]]. The rest of this post details the installation proceedure to be carried out in conjunction with the [[https://github.com/HaoZeke/hzHPC_lmod][hzHPC_lmod repo]].
** Setting Up
These are reproduced from the repo for completeness.
#+BEGIN_SRC bash
git clone https://github.com/kobus-v-schoor/dotgit.git
mkdir -p ~/.local/bin
cp -r dotgit/bin/dotgit* ~/.local/bin
cat dotgit/bin/bash_completion >> ~/.bash_completion
rm -rf dotgit
#+END_SRC
I actually strongly suggest using a target from [[https://github.com/HaoZeke/Dotfiles][my Dotfiles]] in conjunction with this, but it isn't really required, so:
#+BEGIN_SRC
~/.local/bin/dotgit restore hzhpc
#+END_SRC
Note that because of the suggested separation, I have not opted to setup a shell or even ensure that there are scripts here to help keep ~module~ in your path. Those are in [[https://github.com/HaoZeke/Dotfiles][my Dotfiles]].
If, you *opt to not use* these ~dotfiles~, then *do not* run the ~ml load~ commands.
** LMod Libraries
#+BEGIN_QUOTE
*Note that*: [[http://ihpc.is/garpur/][garpur]] already has ~lmod~ and a module for GNU ~gcc~ 9.2.0
#+END_QUOTE

The scripts in this post will also be part of the [[https://github.com/HaoZeke/haozeke.github.io/][repo]], but keep in mind that these are not meant to be robust ways to install anything, and every command should be run by hand because things will probably break badly.
*** GMP
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/gcc/gmp/6.2.0
export PATH
wget https://gmplib.org/download/gmp/gmp-6.2.0.tar.xz
tar xfv gmp-6.2.0.tar.xz
cd gmp-6.2.0
./configure --prefix=$myprefix    \
            --enable-cxx     \
            --docdir=$myprefix/doc/gmp-6.1.2
make -j$(nproc)
make install
#+END_SRC
*** MPFR
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/gcc/mpfr/4.1.0
export PATH
wget https://www.mpfr.org/mpfr-current/mpfr-4.1.0.tar.xz
tar xfv mpfr-4.1.0.tar.xz
cd mpfr-4.1.0
./configure --prefix=$myprefix    \
            --enable-thread-safe     \
            --with-gmp=$HOME/.hpc/gcc/gmp/6.2.0 \
            --docdir=$myprefix/doc/mpfr-4.1.0
make -j$(nproc)
make install
#+END_SRC
*** MPC
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/gcc/mpc/1.2.0
export PATH
wget https://ftp.gnu.org/gnu/mpc/mpc-1.2.0.tar.gz
tar xfv mpc-1.2.0.tar.gz
cd mpc-1.2.0
./configure --prefix=$myprefix    \
            --with-gmp=$HOME/.hpc/gcc/gmp/6.2.0 \
            --with-mpfr=$HOME/.hpc/gcc/mpfr/4.1.0 \
            --docdir=$myprefix/doc/mpc-1.2.0
make -j$(nproc)
make install
#+END_SRC
*** GCC 9.2.0
#+BEGIN_SRC bash :noeval :tangle codeSnips/lmod-help/getGCC920.sh :eval never
mkdir -p ~/tmpHPC
cd $HOME/tmpHPC
myprefix=$HOME/.hpc/gcc/9.2.0
export PATH
export LIBRARY_PATH=/usr/lib64/:$LIBRARY_PATH
wget https://ftp.gnu.org/gnu/gcc/gcc-9.2.0/gcc-9.2.0.tar.xz
tar xfv gcc-9.2.0.tar.xz
cd gcc-9.2.0
case $(uname -m) in
  x86_64)
    sed -e '/m64=/s/lib64/lib/' \
        -i.orig gcc/config/i386/t-linux64
  ;;
esac
mkdir -p build                                         &&
cd    build                                            &&

SED=sed                               \
../configure --prefix=$myprefix            \
             --enable-languages=c,c++,fortran \
             --disable-multilib       \
             --with-gmp=$HOME/.hpc/gcc/gmp/6.2.0 \
             --with-mpfr=$HOME/.hpc/gcc/mpfr/4.1.0 \
             --with-mpc=$HOME/.hpc/gcc/mpc/1.2.0 \
             --disable-bootstrap      \
             --with-system-zlib
make -j$(nproc)
ml load gcc/9.2.0
#+END_SRC
*** Autotools
Following the standard approach outlined in the [[https://www.gnu.org/software/automake/faq/autotools-faq.html#How-can-I-install-software-below-my-home-directory_003f][GNU Autotools FAQ]]:
#+BEGIN_SRC bash :noeval :tangle codeSnips/lmod-help/getAutotools.sh :eval never
mkdir -p ~/tmpHPC
cd $HOME/tmpHPC
myprefix=$HOME/.hpc/autotools
export PATH
wget http://ftp.gnu.org/gnu/m4/m4-1.4.18.tar.gz
wget http://ftp.gnu.org/gnu/autoconf/autoconf-2.69.tar.gz
wget http://ftp.gnu.org/gnu/automake/automake-1.16.2.tar.gz
wget http://ftp.gnu.org/gnu/libtool/libtool-2.4.6.tar.gz
gzip -dc m4-1.4.18.tar.gz | tar xvf -
gzip -dc autoconf-2.69.tar.gz | tar xvf -
gzip -dc automake-1.16.2.tar.gz | tar xvf -
gzip -dc libtool-2.4.6.tar.gz | tar xvf -
cd m4-1.4.18
./configure -C --prefix=$myprefix/m4/1.4.18 && make -j$(nproc) && make install
cd ../autoconf-2.69
./configure -C --prefix=$myprefix/autoconf/2.69 && make -j$(nproc) && make install
cd ../automake-1.16.2
./configure -C --prefix=$myprefix/automake/1.16.2 && make -j$(nproc) && make install
cd ../libtool-2.4.6
./configure -C --prefix=$myprefix/libtool/2.4.6 && make -j$(nproc) && make install
ml load autotools/autotools
#+END_SRC
We also need the archive.
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/autotools
git clone git://git.sv.gnu.org/autoconf-archive.git
cd autoconf-archive
#+END_SRC
Combined with the [[https://lmod.readthedocs.io/en/latest/020_advanced.html][lmod manual]] gives rise to the following definiton (roughly the same for each one):
#+BEGIN_SRC bash
local home    = os.getenv("HOME")
local version = myModuleVersion()
local pkgName = myModuleName()
local pkg     = pathJoin(home,".hpc",pkgName,version,"bin")
prepend_path("PATH", pkg)
#+END_SRC
We will no longer bother with the module definitions for the rest of this post, as they are handled and documented in the repo.
*** Perl
This is essentially the setup from the [[https://learn.perl.org/installing/unix_linux.html][main docs]].
#+BEGIN_SRC bash :noeval :tangle codeSnips/lmod-help/getPerl.sh :eval never
# Get Perl
curl -L http://xrl.us/installperlnix | bash
# Use Perl
ml use perl/5.28.0
cpanm ExtUtils::MakeMaker # For git
ml load perl/5.28.0
#+END_SRC
*** Git
This is very similar to the previous approach. However, since by default the system ~perl~ was being picked up, some slight changes have been made.
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/git/2.9.5
PATH=$myprefix/bin:$PATH
export PATH
wget https://mirrors.edge.kernel.org/pub/software/scm/git/git-2.9.5.tar.gz
gzip -dc git-2.9.5.tar.gz | tar xvf -
cd git-2.9.5
./configure --with-perl=$(which perl) -C --prefix=$myprefix
make -j $(nproc)
make install
ml load git/2.9.5
#+END_SRC
**** Caveat
Also, for TRAMP, we would prefer having a more constant path, so we can set up a symlink:
#+BEGIN_SRC bash
mkdir ~/.hpc/bin
ln ~/.hpc/git/2.9.5/bin/git ~/.hpc/bin/git
#+END_SRC
*** Boost
The [[https://www.boost.org/][boost website]] is utterly incomprehensible. As is the documentation. Also, fun fact, the move from ~svn~ makes things worse. Thankfully, a quick dive into the slightly [[https://github.com/boostorg/wiki/wiki/Getting-Started%3A-Overview][better Github wiki]] led to this nugget:

#+BEGIN_SRC bash
git clone --recursive https://github.com/boostorg/boost.git
cd boost
git checkout tags/boost-1.73.0 # or whatever branch you want to use
./bootstrap.sh
./b2 headers
#+END_SRC

This means we're almost done!

#+BEGIN_SRC bash
./b2
./b2 install --prefix=$HOME/.hpc/boost/boost-1.73.0
ml load boost/boost-1.73.0
#+END_SRC
*** Pkg-Config
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/pkg-config/0.29.2
wget https://pkg-config.freedesktop.org/releases/pkg-config-0.29.2.tar.gz
gzip -dc pkg-config-0.29.2.tar.gz | tar xvf -
cd pkg-config-0.29.2
./configure --prefix=$myprefix --with-internal-glib --disable-host-tool --docdir=$myprefix/share/doc/pkg-config-0.29.2
mkdir $myprefix/lib
make -j $(nproc)
make install
ml load pkg-config/0.29.2
#+END_SRC
*** Zlib
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/zlib/1.2.11
wget http://zlib.net/zlib-1.2.11.tar.gz
gzip -dc zlib-1.2.11.tar.gz | tar xvf -
cd zlib-1.2.11
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load zlib/1.2.11
#+END_SRC
*** XZ Utils
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/xz/5.2.5
wget https://tukaani.org/xz/xz-5.2.5.tar.gz
gzip -dc xz-5.2.5.tar.gz | tar xvf -
cd xz-5.2.5
./configure --prefix=$myprefix --enable-threads=yes
make -j $(nproc)
make install
ml load xz/5.2.5
#+END_SRC
*** OpenSSL
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/openssl/1.1.1d
wget https://www.openssl.org/source/openssl-1.1.1d.tar.gz
gzip -dc openssl-1.1.1d.tar.gz | tar xvf -
cd openssl-1.1.1d
./config --prefix=$myprefix --openssldir=$myprefix/etc/ssl shared zlib-dynamic
make -j $(nproc)
make install
ml load openssl/1.1.1d
#+END_SRC
*** Cmake
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/cmake/3.18.1
wget https://github.com/Kitware/CMake/releases/download/v3.18.1/cmake-3.18.1.tar.gz
gzip -dc cmake-3.18.1.tar.gz | tar xvf -
cd cmake-3.18.1
./bootstrap --prefix=$myprefix
make -j $(nproc)
make install
ml load cmake/3.18.1
#+END_SRC
*** GNU-Make
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/make/4.3
wget http://ftp.gnu.org/gnu/make/make-4.3.tar.gz
gzip -dc make-4.3.tar.gz | tar xvf -
cd make-4.3
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load make/4.3
#+END_SRC
*** Brotli
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/brotli/1.0.1
git clone https://github.com/bagder/libbrotli
cd libbrotli
./autogen.sh
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load brotli/1.0.1
#+END_SRC
*** ncurses
We will need to manually ensure the paths for ~pkg-config~ are in a feasible location.
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/ncurses/6.2
wget https://invisible-mirror.net/archives/ncurses/ncurses-6.2.tar.gz
gzip -dc ncurses-6.2.tar.gz | tar xvf -
cd ncurses-6.2
./configure --prefix=$myprefix --enable-widec --enable-pc-files --with-shared
make -j $(nproc)
make install
mkdir pkgconfig
cp misc/formw.pc misc/menuw.pc misc/ncurses++w.pc misc/ncursesw.pc misc/panelw.pc pkgconfig/
mv pkgconfig $myprefix/lib/
ml load ncurses/6.2
#+END_SRC
*** texinfo
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/texinfo/6.7
wget http://ftp.gnu.org/gnu/texinfo/texinfo-6.7.tar.gz
gzip -dc texinfo-6.7.tar.gz | tar xvf -
cd texinfo-6.7
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load texinfo/6.7
#+END_SRC
*** gperf
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/gperf/3.1
wget http://ftp.gnu.org/gnu/gperf/gperf-3.1.tar.gz
gzip -dc gperf-3.1.tar.gz | tar xvf -
cd gperf-3.1
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load gperf/3.1
#+END_SRC
*** libseccomp
There is a bug, which requires modifying ~src/system.c~ to change ~__NR_seccomp~ to ~_nr_seccomp~.
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/libseccomp/2.5.0
git clone https://github.com/seccomp/libseccomp
cd libseccomp
git checkout tags/v2.5.0
./autogen.sh
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load libseccomp/2.5.0
#+END_SRC
Alternatively, it is easier to work with an older version.
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/libseccomp/2.4.4
wget https://github.com/seccomp/libseccomp/releases/download/v2.4.4/libseccomp-2.4.4.tar.gz
tar xfv libseccomp-2.4.4.tar.gz
cd libseccomp-2.4.4
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load libseccomp/2.4.4
#+END_SRC
*** BDWGC
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/bdwgc/8.0.4
wget https://github.com/ivmai/bdwgc/releases/download/v8.0.4/gc-8.0.4.tar.gz
gzip -dc gc-8.0.4.tar.gz | tar xvf -
cd gc-8.0.4
./configure --prefix=$myprefix --enable-cplusplus
make -j $(nproc)
make install
ml load bdwgc/8.0.4
#+END_SRC
*** pcre
We will prep both ~pcre2~ and ~pcre~.
#+BEGIN_QUOTE
myprefix=$HOME/.hpc/pcre2/10.35
wget https://ftp.pcre.org/pub/pcre/pcre2-10.35.tar.gz
gzip -dc pcre2-10.35.tar.gz | tar xvf -
cd pcre2-10.35
./configure --prefix=$myprefix \
            --enable-pcre2-16  \
            --enable-pcre2-32  \
            --enable-pcre2grep-libz
make -j $(nproc)
make install
ml load pcre2/10.35
#+END_QUOTE

#+BEGIN_QUOTE
myprefix=$HOME/.hpc/pcre/8.44
wget https://ftp.pcre.org/pub/pcre/pcre-8.44.tar.gz
gzip -dc pcre-8.44.tar.gz | tar xvf -
cd pcre-8.44
./configure --prefix=$myprefix \
            --enable-pcre-16  \
            --enable-pcre-32  \
            --enable-pcregrep-libz
make -j $(nproc)
make install
ml load pcre/8.44
#+END_QUOTE
*** bison
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/bison/3.7.1
wget http://ftp.gnu.org/gnu/bison/bison-3.7.1.tar.gz
gzip -dc bison-3.7.1.tar.gz | tar xvf -
cd bison-3.7.1
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load bison/3.7.1
#+END_SRC
*** flex
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/flex/2.6.4
wget https://github.com/westes/flex/releases/download/v2.6.4/flex-2.6.4.tar.gz
gzip -dc flex-2.6.4.tar.gz | tar xvf -
cd flex-2.6.4
./configure --prefix=$myprefix 
make -j $(nproc)
make install
ml load flex/2.6.4
#+END_SRC
*** jq
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/jq/1.6
git clone https://github.com/stedolan/jq.git
cd jq
git submodule update --init
git checkout tags/jq-1.6
autoreconf -fi
./configure --prefix=$myprefix --with-oniguruma=builtin
make -j $(nproc)
make install
ml load jq/1.6
#+END_SRC
*** bzip2
Needed to manually configure it as shown [[https://github.com/samtools/htslib/issues/696#issuecomment-387405020][here]]
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/bzip2/1.0.8
wget https://www.sourceware.org/pub/bzip2/bzip2-1.0.8.tar.gz
gzip -dc bzip2-1.0.8.tar.gz | tar xvf -
cd bzip2-1.0.8
make -f Makefile-libbz2_so
ln -sf libbz2.so.1.0 libbz2.so
mkdir -p $myprefix/include
mkdir -p $myprefix/lib
cp -avf bzlib.h $myprefix/include
cp -avf libbz2.so* $myprefix/lib
make install PREFIX=$myprefix
ml load bzip2/1.0.8
#+END_SRC

*** sqlite
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/sqlite/3.32.3
wget https://www.sqlite.org/2020/sqlite-autoconf-3320300.tar.gz
gzip -dc sqlite-autoconf-3320300.tar.gz | tar xvf -
cd sqlite-autoconf-3320300
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load sqlite/3.32.3
#+END_SRC
*** editline
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/editline/1.17.1
wget https://github.com/troglobit/editline/releases/download/1.17.1/editline-1.17.1.tar.gz
gzip -dc editline-1.17.1.tar.gz | tar xvf -
cd editline-1.17.1
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load editline/1.17.1
#+END_SRC
*** Miniconda
We don't need this very much, but it is still useful for some edge cases, mainly revolving around ~jupyter~ infrastructure.
#+BEGIN_SRC bash
cd $HOME
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
chmod +x Miniconda3-latest-Linux-x86_64.sh
./Miniconda3-latest-Linux-x86_64.sh
# Do not allow it to mess up the shell rc files
eval "$($HOME/miniconda3/bin/conda shell.zsh hook)"
#+END_SRC
Note that we will prefer the manual evaluation since it can be handled in the ~lmod~ file.
** Applications
Libraries and ~git~ aside, there are some tools we might want to have.
*** ag
The silver searcher, along with ~rg~ is very useful to have.
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/the_silver_searcher/2.2.0
wget https://geoff.greer.fm/ag/releases/the_silver_searcher-2.2.0.tar.gz
gzip -dc the_silver_searcher-2.2.0.tar.gz | tar xvf -
cd the_silver_searcher-2.2.0
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load the_silver_searcher/2.2.0
#+END_SRC
*** Neovim
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/nvim/0.5.0
wget https://github.com/neovim/neovim/releases/download/nightly/nvim.appimage
chmod +x nvim.appimage
./nvim.appimage --appimage-extract
mkdir -p $myprefix
mv squashfs-root/usr/* $myprefix
ml load nvim/0.5.0
#+END_SRC
*** Tmux
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/tmux/3.1b
wget https://github.com/tmux/tmux/releases/download/3.1b/tmux-3.1b-x86_64.AppImage
chmod +x tmux-3.1b-x86_64.AppImage
rm -rf squashfs-root
./tmux-3.1b-x86_64.AppImage --appimage-extract
mkdir -p $myprefix
mv squashfs-root/usr/bin squashfs-root/usr/lib squashfs-root/usr/share $myprefix
ml load tmux/3.1b
#+END_SRC

*** Zsh
More of an update than a requirement.
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/zsh/5.8
wget https://github.com/zsh-users/zsh/archive/zsh-5.8.tar.gz
gzip -dc zsh-5.8.tar.gz | tar xvf -
cd zsh-zsh-5.8
./configure --prefix=$myprefix
make -j $(nproc)
make install
ml load zsh/5.8
#+END_SRC
** Conclusion
Having composed a bunch of these, I will of course try to somehow get ~nix~ up and running so it can bootstrap itself and allow me to work in peace. I might also eventually create shell scripts to automate updating these, but hopefully I can set up ~nix~ and not re-create package manager logic in ~lua~.

[fn:reallythatbad] They were called ~hpc2013~ and ~hpc2010~ respectively
[fn:mustisay] I really like Lua, enough to embed it in [[https://dseams.info][d-SEAMS]]
* DONE A Tutorial Introduction to Nix :@programming:tools:nix:workflow:python:
CLOSED: [2020-08-18 Tue 16:18]
:PROPERTIES:
:EXPORT_FILE_NAME: ccon-tut-nix
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:EXPORT_BIBLIOGRAPHY: biblio/CarpentryCon2020.bib
:EXPORT_HUGO_PANDOC_CITATIONS: t
:END:
#+BEGIN_QUOTE
Brief introduction to a nix based project workflow.
#+END_QUOTE

** Background
For [[https://2020.carpentrycon.org/][CarpentryCon@Home 2020]], along with [[https://scholar.google.com/citations?user=mviv92EAAAAJ&hl=en][Amrita Goswami]], I am to prepare and deliver a workshop on "[[https://2020.carpentrycon.org/schedule/#session-10][Reproducible Environments with the Nix Packaging System]]". In particular, as a community of practice lesson, the focus is not on packaging (as is typical of most Nix tutorials) nor on the Nix expression language itself, but instead on the use of Nix as a replacement for virtual environments using ~mkShell~.
** Materials
This is a Carpentries style single page lesson on setting up and working with Nix for reproducible environments. It was concieved to be a complimentary resource to the content of [[https://github.com/HaoZeke/CarpentryCon2020_Nix][this repository]], namely:
- [[https://github.com/HaoZeke/CarpentryCon2020_Nix/blob/master/slides/nixPython/nixPython.pdf][Slides on Python packages with Nix]]
- [[https://rgoswami.me/posts/nix-r-devtools/][Nix with R and devtools]]
- [[https://rgoswami.me/posts/rethinking-r-nix/][Statistical Rethinking and Nix]]
- [[https://pad.carpentries.org/cchome-nix-packaging][An Etherpad]]
- [[https://www.youtube.com/watch?v=np6TPzME0aA][Session recording]]
** Ten seconds into Nix
A few words to keep in mind, in no particular order.
- Nix is based of good academic principles by @dolstraNixSafePolicyFree2004 and @dolstraNixOSPurelyFunctional2010
  + It has been used in large scientific projects for reproducibility (e.g. [[https://dseams.info][d-SEAMS]] of @goswamiDSEAMSDeferredStructural2020)
- The Nix expression language is a *domain specific language*
  + Turing completeness is not a goal or a requirement
- Can leverage binary caches
  + Not _always_ true, only when installed in ~/nix~
** Setup
For this particular tutorial, we will assume the standard Nix installation proceedure, that is, one where the installer has root access to create the initial ~/nix~ directory and set up the build users[fn:whynotsingle]. This follows directly from the [[https://nixos.org/nix/manual/#chap-installation][Nix Manual]]:
#+BEGIN_SRC bash
# You need root permissions for this!!!
sh <(curl -L https://nixos.org/nix/install) --daemon
#+END_SRC
At this point we will also install the canonnical first package, the ~hello~ package, which simply outputs a friendly greeting.
#+BEGIN_SRC bash
nix-env -i hello
#+END_SRC
Note that the basic package search operation is ~nix search~ and it gives outputs which look like:

#+DOWNLOADED: screenshot @ 2020-08-18 14:57:27
#+caption: The ~nix search emacs~ output
[[file:images/Setup/nixSearchSetup.png]]

Though this is not bad by any standard, we will try to get a more interactive management tool.
** Basic Helpers
The first few things to obtain are:
- [[https://github.com/madjar/nox][nox]] :: This is a better package management helper
- [[https://github.com/nmattia/niv][niv]] :: For pinning dependencies as discussed later
- [[https://github.com/target/lorri][lorri]] :: For working seamlessly with project environments

*** Exercise 1
#+BEGIN_QUOTE
Try installing these and use ~nox emacs~ to test the output
#+END_QUOTE

#+DOWNLOADED: screenshot @ 2020-08-18 15:06:17
#+caption: The ~nox emacs~ output
[[file:images/Basic_Helpers/basicHelpers.png]]

** More Dependable Dependencies
*** Standard Channels
Nix works by searching a repository (local or online) of package derivations. Indeed, we can pass ~nix-env~ any local fork of the main [[https://github.com/NixOS/nixpkgs][nixpkgs repo]] as well.

#+BEGIN_SRC bash
# don't run this, it is a large repo
git clone https://github.com/NixOS/nixpkgs.git $mynixdir
# make changes..
$EDITOR $nixpkgs/pkgs/applications/editors/emacs/default.nix
nix-env -i emacs -f $nixpkgs
#+END_SRC

- This might serve as a way to mass modify the ~nixpkgs~ in a pinch
  + However, we will almost *never* use this in practice
  + The overlay approach is much better
- It is also useful if we need to build local derivations
*** Pinning Dependencies
Compared to globally tracking the branches of ~nixpkgs~ or even local changes and forks, for project oriented workflows it is better to use ~niv~ which we obtained previously. In a nutshell, ~niv~ will generate a ~json~ file to keep track of dependencies and wraps it in a ~nix~ file we can subsequently import and use.
** Project Setup
We are now in a position to start working with a project oriented workflow.
#+BEGIN_SRC bash :exports both
# Make directories
mkdir myFirstNix
cd myFirstNix
# Setup
niv init
niv update nixpkgs -b nixpkgs-unstable
#+END_SRC
At this stage your project should have the following structure:
#+BEGIN_SRC bash :exports both
tree myFirstNix
#+END_SRC

#+RESULTS:
| myFirstNix |              |   |       |
| â””â”€â”€        | nix          |   |       |
| â”œâ”€â”€        | sources.json |   |       |
| â””â”€â”€        | sources.nix  |   |       |
|            |              |   |       |
| 1          | directory,   | 2 | files |
We can now move on to the heart of this tutorial, the ~nix-shell~. In a nutshell, running ~nix-shell~ when there is a defined ~shell.nix~ will spawn a virtual environment with the nix packages requested.
*** lorri and direnv
Though we haven't as yet generated a ~shell.nix~ we should point out that writing one by hand will mean that we need to rebuild the enviroment when we make changes using ~nix-shell~ every time. A more elegant approach is to offload the rebuilding of the environment to ~lorri~ which also has a neat ~direnv~ integrration. Let's try that out.
#+BEGIN_SRC bash :exports both
cd myFirstNix
lorri init
#+END_SRC

#+RESULTS:
| Aug | 18 | 15:24:06.524 | INFO | wrote | file, | path: | ./shell.nix |
| Aug | 18 | 15:24:06.524 | INFO | wrote | file, | path: | ./.envrc    |
| Aug | 18 | 15:24:06.524 | INFO | done  |       |       |             |
At this point we should now have:
#+BEGIN_SRC bash :exports both
tree -a myFirstNix
#+END_SRC

#+RESULTS:
| myFirstNix |            |              |       |
| â”œâ”€â”€        | .envrc     |              |       |
| â”œâ”€â”€        | nix        |              |       |
| â”‚Â Â         | â”œâ”€â”€        | sources.json |       |
| â”‚Â Â         | â””â”€â”€        | sources.nix  |       |
| â””â”€â”€        | shell.nix  |              |       |
|            |            |              |       |
| 1          | directory, | 4            | files |

We might want to take a quick look at what is being loaded into the environment and the ~shell.nix~ at this point.

#+caption: ~shell.nix~
#+BEGIN_SRC nix
let
  pkgs = import <nixpkgs> {};
in
pkgs.mkShell {
  buildInputs = [
    pkgs.hello
  ];
}
#+END_SRC

#+caption: ~.envrc~
#+BEGIN_SRC bash
eval "$(lorri direnv)"
#+END_SRC
The ~.envrc~ output is not very useful at a glance, however when we cd into the directory it is very verbose and explicit about what is being set up.

#+DOWNLOADED: screenshot @ 2020-08-18 15:31:31
#+caption: Sample output of the evaluation
[[file:images/Project_Setup/prjSetup.png]]

Note that in order to set ~lorri~ up, we will need to set up a daemon.
#+BEGIN_SRC bash
systemctl --user start lorri
direnv allow
#+END_SRC

*** Pinning with niv
Note that inspite of having set up ~niv~, we have not yet used the sources defined therein. We will now fix this, by modifying ~shell.nix~.
#+caption: ~shell.nix~ with ~niv~
#+BEGIN_SRC nix
let
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  inherit (pkgs.lib) optional optionals;
in
pkgs.mkShell {
  buildInputs = [
    pkgs.hello
  ];
}
#+END_SRC
** Purity and Environments
There are a couple of things to note about this setup.
- The default shell is ~bash~
- On occasion, depending on your Dotfiles you might have paths overriden in an annoying way
One workaround is to use nix shell with an argument:
#+BEGIN_SRC bash
nix-shell --run "bash"
#+END_SRC
- We can also pass ~--pure~ to the function, but at the cost of having to define many more dependencies for our shell
** mkShell
The ~mkShell~ function is the focus of our tutorial, and we will mostly work around passing in different environments and hooks. Let us start by defining a hook.
*** Shell Hooks
Often, we will want to set an environment variable in our shell in advance. We should not use ~direnv~ for this, and instead we will focus on the ~shellHook~ option. Syntactically, we note that this is of the form:
#+BEGIN_SRC nix
let
  hook = ''
  export myvar="Test"
  ''
in pkgs.mkShell {
  shellHook = hook;
}
#+END_SRC
Often we will describe variables in the let section in favor of cluttering the actual function call itself.
** Overriding Global Packages
For overriding global packages, it is best to leverage the ~config.nix~ (which is commonly in ~$HOME/.config/nixpkgs/config.nix~) file instead of the current environment, though it could be managed in a per-project setup as well. Consider the case where we need to disable tests for a particular packages, say ~libuv~.
#+caption: A sample ~config.nix~ file
#+BEGIN_SRC nix
{
  packageOverrides = pkgs:
    with pkgs; {
      libuv = libuv.overrideAttrs (oldAttrs: {
        doCheck = false;
        doInstallCheck = false;
      });
    };
}
#+END_SRC
** Python Dependencies
As R dependency management has been covered [[Nix-R and Devtools][in an earlier post]], we will focus on the management of ~python~ environments.
*** Generic Environments
We can define existing packages as follows (and can check for existence with ~nox~) using the ~let..in~ syntax.
#+caption: Shell with basic ~python~ environment
#+BEGIN_SRC nix
let
  # Niv
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  inherit (pkgs.lib) optional optionals;
  # Python
  pythonEnv = pkgs.python38.withPackages (ps: with ps;[
    numpy
    toolz
  ]);
in pkgs.mkShell {
  buildInputs = with pkgs; [
    pythonEnv

    black
    mypy

    libffi
    openssl
  ];
}
#+END_SRC
*** Project Local Pip
We can leverage a trick from [[https://churchman.nl/2019/01/22/using-nix-to-create-python-virtual-environments/][here]] to set a local directory for ~pip~ installations, which boils down to some path hacking.
#+BEGIN_SRC nix
let
  # Niv
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  inherit (pkgs.lib) optional optionals;
  # Python
  pythonEnv = pkgs.python38.withPackages (ps: with ps;[
    numpy
    toolz
  ]);
  hook = ''
     export PIP_PREFIX="$(pwd)/_build/pip_packages"
     export PYTHONPATH="$(pwd)/_build/pip_packages/lib/python3.8/site-packages:$PYTHONPATH"
     export PATH="$PIP_PREFIX/bin:$PATH"
     unset SOURCE_DATE_EPOCH
  '';
in pkgs.mkShell {
  buildInputs = with pkgs; [
    pythonEnv

    black
    mypy

    libffi
    openssl
  ];
  shellHook = hook;
}

#+END_SRC
*Note that* this is discouraged as we will lose the caching capabilities of nix.
** Non-Standard Python
For more control over the environment, we can define it in more detail with some overlays.
#+BEGIN_SRC nix
let
  python = pkgs.python38.override {
    packageOverrides = self: super: {
      pytest = super.pytest.overridePythonAttrs (old: rec {
        doCheck = false;
        doInstallCheck = false;
      });
    };
  };
  myPy = python.withPackages
    (p: with p; [ numpy pip pytest ]);
in pkgs.mkShell {
  buildInputs = with pkgs; [
    myPy
  ];
}
#+END_SRC
We have used both overriden packages and standard packages in the above formulation.
*** Building Packages
For cases where we are certain that no existing package is present (use ~nox~) we can also build them. Take ~f90wrap~ as an example, and we will use the Github version, rather than the PyPi version (the difference is in the source fetch function).
#+BEGIN_SRC nix
f90wrap = self.buildPythonPackage rec {
  pname = "f90wrap";
  version = "0.2.3";
  src = pkgs.fetchFromGitHub {
    owner = "jameskermode";
    repo = "f90wrap";
    rev = "master";
    sha256 = "0d06nal4xzg8vv6sjdbmg2n88a8h8df5ajam72445mhzk08yin23";
  };
  buildInputs = with pkgs; [ gfortran stdenv ];
  propagatedBuildInputs = with self; [
    setuptools
    setuptools-git
    wheel
    numpy
  ];
  preConfigure = ''
    export F90=${pkgs.gfortran}/bin/gfortran
  '';
  doCheck = false;
  doIstallCheck = false;
};
#+END_SRC
This is quite involved, _discuss_.
*** Setting Versions
We can finally generalize our ~shell.nix~ to default to ~python 3.8~ but also take a command through ~--argstr~:
#+BEGIN_SRC bash
nix-shell --argstr pythonVersion 36 --run "bash"
#+END_SRC
Where we need to simply define the option at the top of the file, with a default.
#+caption: Full ~shell.nix~
#+BEGIN_SRC nix
{ pythonVersion ? "38" }:
# Define
let
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  inherit (pkgs.lib) optional optionals;
  hook = ''
    # Python Stuff
     export PIP_PREFIX="$(pwd)/_build/pip_packages"
     export PYTHONPATH="$(pwd)/_build/pip_packages/lib/python3.8/site-packages:$PYTHONPATH"
     export PATH="$PIP_PREFIX/bin:$PATH"
     unset SOURCE_DATE_EPOCH
  '';
  # Apparently pip needs 1980 or above
  # https://github.com/ento/elm-doc/blob/master/shell.nix
  python = pkgs."python${pythonVersion}".override {
    packageOverrides = self: super: {
      pytest = super.pytest.overridePythonAttrs (old: rec {
        doCheck = false;
        doInstallCheck = false;
      });
      ase = super.ase.overridePythonAttrs (old: rec {
        doCheck = false;
        doInstallCheck = false;
      });
      f90wrap = self.buildPythonPackage rec {
        pname = "f90wrap";
        version = "0.2.3";
        src = pkgs.fetchFromGitHub {
          owner = "jameskermode";
          repo = "f90wrap";
          rev = "master";
          sha256 = "0d06nal4xzg8vv6sjdbmg2n88a8h8df5ajam72445mhzk08yin23";
        };
        buildInputs = with pkgs; [ gfortran stdenv ];
        propagatedBuildInputs = with self; [
          setuptools
          setuptools-git
          wheel
          numpy
        ];
        preConfigure = ''
          export F90=${pkgs.gfortran}/bin/gfortran
        '';
        doCheck = false;
        doInstallCheck = false;
      };
    };
  };
  myPy = python.withPackages
    (p: with p; [ ase ipython ipykernel scipy numpy f90wrap pip ]);
in pkgs.mkShell {
  buildInputs = with pkgs; [
    # Required for the shell
    zsh
    perl
    git
    direnv
    fzf
    ag
    fd

    # Building thigns
    gcc9
    gfortran
    openblas

    myPy
    # https://github.com/sveitser/i-am-emotion/blob/294971493a8822940a153ba1bf211bad3ae396e6/gpt2/shell.nix
  ];
  shellHook = hook;
}
#+END_SRC
This is enough to cover almost all use-cases for ~python~ environments.
** Build Helpers
Note that we can speed up some aspects of fetch with the prefetch commands:
#+BEGIN_SRC bash
nix-prefetch-git $giturl
nix-prefetch-url $url
#+END_SRC
In practice, some trial and error is easier.
** Supplementary Reading Material
Though these are in no means exhaustive, they may offer a slightly more advanced or different focus than the material covered here.
*** Core Content
- Manuals
  - [[https://nixos.org/nix/manual/][Nix]] and Nixpkgs
- [[https://nixos.wiki/][Nix Wiki]]
  - [[https://nixos.wiki/index.php?title=Cheatsheet&useskin=vector][Nix Cheatsheet]]
- [[https://github.com/NixOS/nixpkgs/blob/master/doc/languages-frameworks/][Language Sections]]
*** Learning Paths
- [[https://nixos.org/nixos/nix-pills/][Nix pills]]
- [[https://nixos.org/learn.html][Official tutorials]]
- [[https://nix.dev/][Nix dev]] has some nice opinionated tips
*** Personal Correspondence
Tyson Whitehead from Compute Canada was kind enough to bring the folllowing additional training materials:
- A wiki pertaining to usage of Nix on in [[https://git.computecanada.ca/nix/training/-/tree/ccwiki][an HPC setting]]
- SWC style workshop materials from [[https://git.computecanada.ca/nix/training/-/tree/20180618-tecc][TECC 2018]]
- [[https://git.computecanada.ca/nix/training/-/tree/20180115-sharcnet][SHARCNET live presentation materials]] from 2018
** Conclusions
The standard dive into Nix is based on building derivations and playing with language, which is in no means a bad one, just too long for the time allocated. The best way to get into Nix is to start using it for everything.

[fn:whynotsingle] For reasons pertaining to latency and ease-of-use, we will assume the multi-user installation
* DONE Nix Shells for Node Projects :@programming:tools:nix:workflow:node:
CLOSED: [2020-08-23 Sun 10:09]
:PROPERTIES:
:EXPORT_FILE_NAME: nix-shell-node
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:EXPORT_HUGO_PANDOC_CITATIONS: t
:END:
** Background
As a prelude to writing up the details of how this site is generated, I realized I should write up a ~nix~ oriented workflow for ~node~ packages.
** Tooling and Idea
The basic concepts are:
- Use ~npm~ to generate a ~package-lock.json~ file
- Use ~node2nix~ in a shell to generate a set of ~nix~ derivations
- Enter a shell environment with the ~nix~ inputs
- Profit

However, the nuances of this are a bit annoying at first.
** Packaging Requirements
We will use the standard ~npm~ installation method at first, but since we shouldn't keep installing and removing things, so we need a way to modify ~package.json~ without running ~npm~ and will therefore add ~add-dependency~.
#+BEGIN_SRC bash
npm install add-dependency
#+END_SRC
** Setting up Node2Nix
We will first clean the directory of what we do not need.

#+BEGIN_SRC bash
rm -rf default.nix node-env.nix node-packages.nix node_modules
#+END_SRC

Now we can enter a shell with ~node2nix~ and generate files for the node packages.

#+BEGIN_SRC bash
nix-shell -p 'nodePackages.node2nix'
node2nix -l package-lock.json
#+END_SRC

** A Nix Environment

We will use the standard setup described in [[A Tutorial Introduction to Nix][the tutorial post]]:
#+BEGIN_SRC bash
nix-env -i niv lorri
niv init
niv update nixpkgs -b nixpkgs-unstable
#+END_SRC

This is to be in conjunction with the following ~shell.nix~ file [fn:whythis].

#+BEGIN_SRC nix
{ sources ? import ./nix/sources.nix }:
let
  pkgs = import sources.nixpkgs { };
  nodeEnv = pkgs.callPackage ./node-env.nix { };
  nodePackages = pkgs.callPackage ./node-packages.nix {
    globalBuildInputs = with pkgs; [ zsh ];
    inherit nodeEnv;
  };
in nodePackages.shell
#+END_SRC

Note that we have overridden the ~nodePackages~ shell which is defined in the files created by ~node2nix~.

We can now enter the environment and setup ~node_modules~[fn:whatlorri].
#+BEGIN_SRC bash
nix-shell
ln -s $NODE_PATH node_modules
#+END_SRC

** Updates
Unfortunately, this setup is a little fragile to updates. We will need to exit and re-create the setup. Note that we are removing the lock file now as well.

#+BEGIN_SRC bash
# In the nix-shell
add-dependencies babel-loader @babel/core @babel/preset-env core-js @babel/plugin-transform-regenerator
# Do not run in nix-shell
rm -rf default.nix node-env.nix node-packages.nix node_modules package-lock.json
# Update in a line
nix-shell -p 'nodePackages.node2nix' --run 'node2nix package.json'
#+END_SRC

The single line update mechanism can be run in the ~nix-shell~ itself, making things marginally less painful.
** Conclusions
This has been a short introduction to working with the ~nix-shell~ ecosystem. It isn't as fast as working with the normal setup, and it is a pretty annoying workflow. Given that most CI setups have good support for caching ~npm~ dependencies, it doesn't seem worthwhile at the moment.

[fn:whatlorri] We can't use ~lorri~ yet since we need to selectively add and remove the symbolic link to ~node_modules~
[fn:whythis] There might be a better approach defined in [[https://github.com/svanderburg/node2nix/issues/175][this issue]] later

* DONE Niv and Mach-Nix for Nix Python :@programming:tools:nix:workflow:python:
CLOSED: [2020-08-26 Wed 05:42]
:PROPERTIES:
:EXPORT_FILE_NAME: mach-nix-niv-python
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
Short post on using ~mach-nix~ with ~niv~.
#+END_QUOTE
** Background
In previous posts, there was a [[A Tutorial Introduction to Nix][discussion on a ground up approach to adding packages]] which aren't on the core ~nixpkgs~ channels using GitHub or PyPi sources. However, this lacked a way to do so programmatically, and also a way to convert existing python projects.

** Python Dependency Management
This time, instead of the more pedagogical approach of building packages from PyPi or GitHub, we will use overlays and the excellent [[https://github.com/DavHau/mach-nix][mach-nix]]
 to speed up the process. We will continue to use [[https://github.com/nmattia/niv][niv]].
 #+BEGIN_SRC bash
niv init
niv update nixpkgs -b nixpkgs-unstable
 #+END_SRC

To leverage ~mach-nix~ we will simply need the following setup to work with ~niv~.

#+BEGIN_SRC nix
let
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  inherit (pkgs.lib) optional optionals;
  mach-nix = import (builtins.fetchGit {
    url = "https://github.com/DavHau/mach-nix/";
    ref = "refs/tags/3.1.1";
  }) {
    pkgs = pkgs;

    # optionally specify the python version
    # python = "python38";

    # optionally update pypi data revision from https://github.com/DavHau/pypi-deps-db
    # pypiDataRev = "some_revision";
    # pypiDataSha256 = "some_sha256";
  };
  customPython = mach-nix.mkPython {
    requirements = ''
      copier
      pytest
    '';
    providers = {
      _default = "nixpkgs,wheel,sdist";
      pytest = "nixpkgs";
    };
    pkgs = pkgs;
  };
in pkgs.mkShell { buildInputs = with pkgs; [ customPython ]; }
#+END_SRC

Note that we have essentially written out a ~requirements.txt~ and can actually pass a path there instead as well. The key point to make it work with ~niv~ is the ~pkgs~ parameter. To use the older method of overriding parts of the setup, we can use the ~overrides_pre~ hook as shown below:
#+BEGIN_SRC nix
let
  sources = import ./prjSource/nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  inherit (pkgs.lib) optional optionals;
  mach-nix = import (builtins.fetchGit {
    url = "https://github.com/DavHau/mach-nix/";
    ref = "refs/tags/3.1.1";
  }) {
    pkgs = pkgs;

    # optionally specify the python version
    # python = "python38";

    # optionally update pypi data revision from https://github.com/DavHau/pypi-deps-db
    # pypiDataRev = "some_revision";
    # pypiDataSha256 = "some_sha256";
  };
  customPython = mach-nix.mkPython {
    requirements = ''
      copier
      pytest
      f90wrap
    '';
    providers = {
      _default = "nixpkgs,wheel,sdist";
      pytest = "nixpkgs";
    };
    overrides_pre = [
      (pythonSelf: pythonSuper: {
        pytest = pythonSuper.pytest.overrideAttrs (oldAttrs: {
          doCheck = false;
          doInstallCheck = false;
        });
        f90wrap = pythonSelf.buildPythonPackage rec {
          pname = "f90wrap";
          version = "0.2.3";
          src = pkgs.fetchFromGitHub {
            owner = "jameskermode";
            repo = "f90wrap";
            rev = "master";
            sha256 = "0d06nal4xzg8vv6sjdbmg2n88a8h8df5ajam72445mhzk08yin23";
          };
          buildInputs = with pkgs; [ gfortran stdenv ];
          propagatedBuildInputs = with pythonSelf; [
            setuptools
            setuptools-git
            wheel
            numpy
          ];
          preConfigure = ''
            export F90=${pkgs.gfortran}/bin/gfortran
          '';
          doCheck = false;
          doIstallCheck = false;
        };
      })
    ];
    pkgs = pkgs;
  };
in pkgs.mkShell { buildInputs = with pkgs; [ customPython ]; }
#+END_SRC

We can also pull in overrides from ~poetry2nix~ with ~overrides_post~ as [[https://github.com/DavHau/mach-nix/blob/master/examples.md][described here]].
** Conclusion
With the completion of this final remaining hurdle, ~nix~ is now fully realized as a ~python~ management system. At this point the "only" thing remaining is to find an optimal way of leveraging ~nix~ for setting up re-usable data science and scientific computing projects.

* DONE Local Nix without Root :@programming:workflow:projects:hpc:nix:tools:
CLOSED: [2020-09-07 Mon 18:30]
:PROPERTIES:
:EXPORT_FILE_NAME: local-nix-no-root
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
Monkeying around with ~nix~ for HPC systems which have no root access and NFS filesystems.
#+END_QUOTE
** Background
Nix is not well known for being friendly to users without root access. This is typically made worse by the "exotic" filesystem attributes common to HPC networks (this also plagues [[https://github.com/andrewchambers/hermes/issues/75][hermes]]). An [[Provisioning Dotfiles on an HPC][earlier post]] details how and why ~proot~ failed.
The short pitch is simply:

#+DOWNLOADED: screenshot @ 2020-09-07 17:34:25
#+caption: Does your HPC look like this?
[[file:images/Background/2020-09-07_17-34-25_screenshot.png]]

#+DOWNLOADED: screenshot @ 2020-09-07 17:37:28
#+caption: It really is an HPC
[[file:images/Background/2020-09-07_17-37-28_screenshot.png]]


If your HPC doesn't look that swanky and you'd like it to, then read on. Note that there are all the obvious benefits of ~nix~ as well, but this is a more eye-catchy pitch.
** Setup
#+BEGIN_QUOTE
The basic concept is to install ~nix~ from source, with appropriate patches, and then mess around with paths until it is ready and willing to work with stores which are not ~/nix~ [fn:thateasy]
#+END_QUOTE
This concept is strongly influenced by the work [[https://github.com/jefdaj/nix-no-root][described in this repo]]. The premise is similar to my earlier post [[HPC Dotfiles and LMod][on HPC Dotfiles]]. For the purposes of this post, we will assume that all the packages in the [[HPC Dotfiles and LMod][previous post]]
exist. ~lmod~ is not required, feel free to use an alternative path management system, or even just ~$HOME/.local~ but if ~lmod~ is present, it is highly recommended [fn:whatalternatives]. We *will need* the following:

- Pinned set of nixpkgs :: We would like to be able to modify a lot of paths, which is normally a bad practice, but then we don't normally rebuild all packages either. Grab a copy of the ~nixpkgs~ by following the instructions below. Now is also the time to fork the repo if you'd like to keep track of your changes.
#+BEGIN_SRC bash
mkdir -p $HOME/Git/Github
cd $HOME/Git/Github
git clone https://github.com/NixOS/nixpkgs
#+END_SRC
- dotgit :: We use the older, bash version of [[https://github.com/kobus-v-schoor/dotgit/][the excellent dotgit]] since ~python~ is not always present in HPC environments.
#+BEGIN_SRC bash
git clone https://github.com/kobus-v-schoor/dotgit/
mkdir -p $HOME/.local/bin
cp dotgit/old/bin/bash_completion dotgit/old/bin/dotgit dotgit/old/bin/dotgit_headers dotgit/old/bin/fish_completion.fish $HOME/.local/bin/ -r
#+END_SRC
- lmod packages :: If you do not or cannot use modulefiles as [[HPC Dotfiles and LMod][described in the earlier post]], inspect the module-files being loaded and set paths accordingly.
#+BEGIN_SRC bash
cd $HOME/Git/Github
git clone https://github.com/HaoZeke/hzHPC_lmod
cd hzHPC_lmod
$HOME/.local/bin/dotgit restore hzhpc
#+END_SRC

Now we can start by obtaining the ~nix~ sources.
#+BEGIN_SRC bash
myprefix=$HOME/.hpc/nix/nix-boot
nixdir=$HOME/.nix
nix_version=2.3.7
ml load gcc/9.2.0 flex bison
ml load boost
ml load editline
ml load brotli/1.0.1
ml load libseccomp/2.4.4
ml load bdwgc/8.0.4
ml load bzip2/1.0.8
ml load openssl sqlite
wget http://nixos.org/releases/nix/nix-${nix_version}/nix-${nix_version}.tar.bz2
tar xfv nix-2.3.7.tar.bz2
cd nix-2.3.7
#+END_SRC

Before actually configuring and installing from source, we need some patches.
*** Patches
I suggest carefully typing out the patches, though leave a comment if you want a repo with these changes (if you must star something in the meantime, [[https://github.com/d-SEAMS/seams-core][star this]]).
- Start with [[https://github.com/NixOS/nix/pull/1584/files][this patch]]
- Also [[https://nixos.wiki/wiki/Nix_Installation_Guide][this one]]

Remove the following ~ifdef~ stuff from  ~src/libutil/compression.cc~, leaving only the contents of the ~else~ statement.
#+BEGIN_SRC c++
#ifdef HAVE_LZMA_MT
            lzma_mt mt_options = {};
            mt_options.flags = 0;
            mt_options.timeout = 300; // Using the same setting as the xz cmd line
            mt_options.preset = LZMA_PRESET_DEFAULT;
            mt_options.filters = NULL;
            mt_options.check = LZMA_CHECK_CRC64;
            mt_options.threads = lzma_cputhreads();
            mt_options.block_size = 0;
            if (mt_options.threads == 0)
                mt_options.threads = 1;
            // FIXME: maybe use lzma_stream_encoder_mt_memusage() to control the
            // number of threads.
            ret = lzma_stream_encoder_mt(&strm, &mt_options);
            done = true;
#else
            printMsg(lvlError, "warning: parallel XZ compression requested but not supported, falling back to single-threaded compression");
#endif
#+END_SRC

If there is trouble with the bzip2 library, set ~$HOME/.hpc/bzip2/1.0.8/include/bzlib.h~ in ~src/libutil/compression.cc~, but expand ~$HOME~.

Finally, you will need edit ~nixpkgs~.
#+BEGIN_SRC nix
# vim pkgs/os-specific/linux/busybox/default.nix
  debianName = "busybox_1.30.1-6";
  debianTarball = fetchzip {
    url = "http://deb.debian.org/debian/pool/main/b/busybox/${debianName}.debian.tar.xz";
    sha256 = "05n6mxc8n4zsli4dijrr2x5c9ggwi223i5za4n0xwhgd4lkhqymw";
  };
#+END_SRC

*** User Build
We can now complete the build.
#+BEGIN_SRC bash
./configure  --enable-gc --prefix=$myprefix --with-store-dir=$nixdir/store --localstatedir=$nixdir/var --with-boost=$BOOST_ROOT --disable-seccomp-sandboxing --disable-doc-gen CPPFLAGS="-I$HOME/.hpc/bzip2/1.0.8/include" LDFLAGS="-L$HOME/.hpc/bzip2/1.0.8/lib -Wl,-R$HOME/.hpc/bzip2/1.0.8/lib"
make -j $(nproc)
make install
ml load nix/user # Hooray!
#+END_SRC

Now we still need to set a profile. Inspect  ~.hpc/nix/nix-boot/etc/profile.d/nix.sh~ and check the value of ~NIX_PROFILES~

#+BEGIN_SRC bash
chmod +x .hpc/nix/nix-boot/etc/profile.d/nix.sh
./.hpc/nix/nix-boot/etc/profile.d/nix.sh
# OR, and this is better
nix-env --switch-profile .nix/var/nix/profiles/default
mkdir -p  ~/.nix/var/nix/profiles
#+END_SRC
** Rebuilding Natively
The astute reader will have noticed that we glibly monkeyed around with the ~nix~ source in the previous section, but all will be made well since we can rebuild to use ~nix~ with itself. Do *replace the variable with the corresponding path*:
#+BEGIN_SRC nix
storeDir = "$HOME/.nix/store";
stateDir = "$HOME/.nix/var";
confDif = "$HOME/.nix/etc";
#+END_SRC

We can "speed up" our build by disabling all tests. Go to the copy of ~nixpkgs~ and run:

#+BEGIN_SRC bash
find pkgs  -type f -name 'default.nix' | xargs sed -i 's/doCheck = true/doCheck = false/'
#+END_SRC

#+BEGIN_SRC bash
mkdir -p $HOME/.nix/var/nix/profiles/
nix-env -i nix -f $HOME/Git/Github/nixpkgs -j$(nproc) --keep-going --show-trace -v --cores 4 2>&1 | tee nix-no-root.log
ml load nix/bootstrapped
#+END_SRC

This will still take a couple of hours at least. Around 3-4 hours. Try to set this up on a lazy weekend to evade sysadmins.

** Usage
We have finally obtained a bootstrapped ~nix~ which is bound to our set of ~nixpkgs~. To ensure its use:
#+BEGIN_SRC bash
ml use $HOME/Modulefiles
ml purge
ml load nix/bootstrapped
ml save
#+END_SRC
*** Basic Packages
Now we can get some basic stuff too.
#+BEGIN_SRC bash
nix-env -i tmux zsh lsof pv git -f $HOME/Git/Github/nixpkgs -j$(nproc) --keep-going --show-trace --cores 4 2>&1 | tee nix-install-base.log
#+END_SRC
*** Ruby Caveats
While installing packages which depend on ~ruby~, there will be permission errors inside the build folder. These can be "fixed" by setting very permissive controls on the *build-directory* in question. *Do not* set permissions directly on the ~.nix/store/$HASH~ folder, as doing so will make ~nix~ reject the build artifact.
#+BEGIN_SRC bash
# neovim depends on ruby
nix-env -i neovim -v -f $HOME/Git/Github/nixpkgs
#+END_SRC

A more elegant way to fix permissions involves a slightly more convoluted approach. We can note where the build is occurring (e.g. ~/tmp~) and run a ~watch~ command to fix permissions.

#+BEGIN_SRC bash
watch -n1 -x chmod 777 -R /tmp/nix-build-ruby-2.6.6.drv-0/source/lib/
#+END_SRC

Naturally this must be run in a separate window.

*** Dotfiles
Feel free to set up ~dotfiles~ ([[https://github.com/HaoZeke/Dotfiles][mine]], perhaps) to profit even further. We will consider the process of obtaining my set below.
Minimally, we will want to obtain ~tmux~ and ~zsh~.
#+BEGIN_SRC bash
nix-env -i tmux zsh -v -f $HOME/Git/Github/nixpkgs
#+END_SRC
Now we can set the ~dotfiles~ up.
#+BEGIN_SRC bash
git clone https://github.com/HaoZeke/Dotfiles
cd Dotfiles
$HOME/.local/bin/dotgit restore hzhpc
#+END_SRC
The final installation configures ~neovim~ and ~tmux~.
#+BEGIN_SRC bash
zsh
# Should install things with zinit
tmux
# CTRL+b --> SHIFT+I to install
nvim
#+END_SRC
*** Misc NFS
For issues concerning NFS lock files, consider simply moving the problematic file and let things sort themselves out. Consider:
#+BEGIN_SRC bash
nix-build
# something about a .nfs lockfile in some .nix/$HASH-pkg/.nfs0234234
mv .nix/$HASH-pkg/ .diePKGs/
nix-build # profit
#+END_SRC
** Conclusions
Though this is slow and seems like an inefficient use of cluster resources, the benefits of reproducible environments typically outweighs the cost. Also it is much more pleasant to have a proper package manager which can work with Dotfiles.

[fn:thateasy] Note that this will of course entail rebuilding everything from scratch, every time, which means no binary caches. Thus there is no reasonable defence for trying this out without access to a high powered limited access machine
[fn:whatalternatives] The rest of the post assumes we are on the same page and working towards the same end-goal, substitute and remix at will
* TODO Jupyter for HPC Environments :@programming:workflow:projects:hpc:tools:
** Background
Unfortunately, the closely coupled nature of Jupyter instances require both ~nodejs~ builds and ~python~ extensions. Furthermore there are no real tangible gains to running ~jupyterWith~ instances with ~nix~ on a per-project basis.
** Reproducible Environments
We will use ~conda~ along with ~nvm~ to manage the dependencies of our ~jupyter~ instance, and spin it up only when required as facilitated by ~tmux~.
*** Paths
Unfortunately it is impossible to set the file-system paths in a machine agnostic manner, since the configuration file cannot accept environment variables.
#+BEGIN_SRC bash
jupyter lab --generate-config
vim ~/.jupyter/jupyter_notebook_config.py
# Change c.NotebookApp.notebook_dir to a full path
#+END_SRC
** Automation

* TODO Introduce JuPyYod
#+BEGIN_QUOTE
The rationale behind my ~JuPyYod~ project for working with Python and Nix
#+END_QUOTE

** Background
A lot of my projects have a few common requirements, namely:
- A lot of common packages
- A ~python~ based workflow
  + Often this includes a ~jupyter~ component depending on my collaborators
From my own background, I typically *demand* a couple of things:
- Reproducibility
- Tests
- Coverage
- Docs
Typically these are at odds with the ad-hoc nature of the scripts.
** Tooling
For the rest of this post, I will focus on the components which come together in my latest project, ~JuPyYod~.
*** Nix
- Ensures reproducible code
  + Freely converts to ~docker~
- Has great Travis CI support
- Also useful for building at a later stage
*** Project Generators
For project generators, the options were:
- Cookiecutter :: This is [[https://github.com/audreyr/cookiecutter][well defined and super popular]], but having to write ~json~ by hand was really not a very appealing prospect
- *Copier* :: This is a newer template engine, but with an appealing set of features as [[https://copier.readthedocs.io/en/latest/comparisons/][compared here]]
*** Static Types
Most of my problems stem from ~python~ being sloppy to read compared to statically typed languages like Fortran or C++, so ~mypy~ integration goes a long way towards fixing that.
*** Runtime Type Checks
*** Linting and Formatting
We will use ~black~ ([[https://github.com/psf/black][site]]) and ~flake8~ ([[http://flake8.pycqa.org/en/latest/][site]]) for formatting and linting respectively.
** Inspirations
Some of the following configurations influenced this set-up to a large/small extent:
- [[https://sourcery.ai/blog/python-best-practices/][Sourcery's python project ideas]]

** JupyterLab
Unfortunately, notwithstanding efforts from Tweag, involving JupyterWith, the state of dealing with impure paths is still murky. So it makes sense to set up a user-level ~conda~ setup.
#+BEGIN_SRC bash
nix-env -i conda-shell
#+END_SRC

* TODO Rstudio with Nix :@programming:tools:nix:workflow:R:
:PROPERTIES:
:EXPORT_FILE_NAME: nix-rstudio
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
This post covers the setup of Rstudio with custom packages, along with Qt
patches.
#+END_QUOTE

** Background
After the post on using [[Nix with R and devtools][non-CRAN packages with Nix]], I naively assumed I had covered all possible use-cases involving Nix and R. Unfortunately, because GUI programs which aren't browsers or Emacs tend to fly underneath my personal radar, it wasn't until I was up to teach a [[https://nairps.github.io/2020-09-08-ggc-SBDH-online/][Data Carpentries lesson at Georgia Gwinnett College]] that I realized RStudio has additional messy moving parts.

In particular, the error inspiring this post is:
#+BEGIN_SRC bash
qt.glx: qglx_findConfig: Failed to finding matching FBConfig for QSurfaceFormat(version 2.0, options QFlags<QSurfaceFormat::FormatOption>(), depthBufferSize -1, redBufferSize 1, greenBufferSize 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile  QSurfaceFormat::NoProfile)
qt.glx: qglx_findConfig: Failed to finding matching FBConfig for QSurfaceFormat(version 2.0, options QFlags<QSurfaceFormat::FormatOption>(), depthBufferSize -1, redBufferSize 1, greenBufferSize 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile  QSurfaceFormat::NoProfile)
Could not initialize GLX
zsh: abort (core dumped)  rstudio
#+END_SRC

To put that in context, however, before attempting a fix, we will start at the beginning.

** RStudio and Nix
Similar to the environment created with ~rWrapper~ we will set up a global environment in our ~config.nix~.
#+BEGIN_SRC nix
{
  packageOverrides = super:
    let
      self = super.pkgs;
    in {
      rStudioEnv = super.rstudioWrapper.override {
        packages = with self.rPackages; [ tidyverse ];
      };
    };
}
#+END_SRC

The extensions to this are fairly standard, and follow the CRAN guidelines in [[Nix with R and devtools][the previous post]]. Installation works the same way too.
#+BEGIN_SRC bash
nix-env -f "<nixpkgs>" -iA rStudioEnv
#+END_SRC

At this point , when things seem to be rosy, we obtain the error discussed earlier.

#+BEGIN_SRC bash
rstudio
qt.glx: qglx_findConfig: Failed to finding matching FBConfig for QSurfaceFormat(version 2.0, options QFlags<QSurfaceFormat::FormatOption>(), depthBufferSize -1, redBufferSize 1, greenBufferSize 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile  QSurfaceFormat::NoProfile)
qt.glx: qglx_findConfig: Failed to finding matching FBConfig for QSurfaceFormat(version 2.0, options QFlags<QSurfaceFormat::FormatOption>(), depthBufferSize -1, redBufferSize 1, greenBufferSize 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile  QSurfaceFormat::NoProfile)
Could not initialize GLX
zsh: abort (core dumped)  rstudio
#+END_SRC

** Fixes
The crux of the error is that the drivers [[https://github.com/NixOS/nixpkgs/issues/9415#issuecomment-134051412][are always considered to be impure]]. The fix is [[https://github.com/NixOS/nixpkgs/issues/9415#issuecomment-139655485][described here]], and can be interpreted as follows.

- We need to figure out where the library is to begin with.

#+BEGIN_SRC bash
ldconfig -p | grep libGLX
#+END_SRC

** Conclusion
* DONE Documenting C++ with Doxygen and Sphinx - Exhale :@programming:documentation:workflow:cpp:
CLOSED: [2020-09-22 Tue 06:58]
:PROPERTIES:
:EXPORT_FILE_NAME: doc-cpp-dox-sph-exhale
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
This post outlines a basic workflow for C++ projects using Doxygen, Sphinx, and Exhale.
#+END_QUOTE
** Background
My project proposal for documenting [[https://github.com/symengine/symengine][Symengine]] was recently accepted for the Google Summer of Docs initiative. In the past I have been more than happy to document C++ code using *only* [[https://www.doxygen.nl/][Doxygen]] (with [[https://docs.dseams.info][pretty fantastic results]]), while keeping example usage separate ([[https://wiki.dseams.info][d-SEAMS wiki]]). Though this is still a feasible method, a monolithic multi-project setup might benefit from [[https://www.sphinx-doc.org/][Sphinx]], which is what will be covered.
*** Series
This post is the first in a series based on best C++ documentation practices for Sphinx+Doxygen workflows.
1. *Documenting C++ with Doxygen and Sphinx - Exhale* <-- You are here!
2. [[Publishing Doxygen and Sphinx with Nix and Rake][Publishing Doxygen and Sphinx with Nix and Rake]]
3. Documenting C++ with Doxygen and Sphinx - doxyrest (TBD)
4. Adding Tutorials to Sphinx Projects (TBD)
** Goals
A couple of goals informed this approach:
- We expect our documentation to link to the source files
- We expect a lot of Python developers to contribute
  + Hence Sphinx
- We would like to write ~.ipynb~ files into the docs
  + Another reason to use Sphinx (via [[https://myst-nb.readthedocs.io/en/latest/][MyST{NB}]])
** Folder Structure
#+BEGIN_SRC bash :eval never :exports both
tree -d $prj/ -L 2
#+END_SRC

#+RESULTS:
| .   |             |           |
| â”œâ”€â”€ | docs        |           |
| â”‚Â Â  | â”œâ”€â”€         | Doxygen   |
| â”‚Â Â  | â””â”€â”€         | Sphinx    |
| â”œâ”€â”€ | nix         |           |
| â”‚Â Â  | â””â”€â”€         | pkgs      |
| â”œâ”€â”€ | projects    |           |
| â”‚Â Â  | â””â”€â”€         | symengine |
| â””â”€â”€ | scripts     |           |
|     |             |           |
| 8   | directories |           |

Essentially we have a ~scripts~ directory to store basic build scripts, and two kinds of documentation folders.
** Basic Doxygen
The ~doxygen~ setup is beautifully simple:
#+BEGIN_SRC bash
cd docs/Doxygen
doxygen -g
# Easier to edit
mv Doxyfile Doxyfile.cfg
#+END_SRC
Now we can modify some basic settings in a separate file
#+BEGIN_SRC bash
touch Doxyfile-prj.cfg
vim Doxyfile-prj.cfg # or whatever
#+END_SRC
Edit the file to be (minimally):
#+BEGIN_SRC bash
@INCLUDE                = "./Doxyfile.cfg"
GENERATE_HTML           = NO
GENERATE_XML            = YES
XML_PROGRAMLISTING = NO

# Project Stuff
PROJECT_NAME           = "myProject"
PROJECT_BRIEF          = "Dev docs"
OUTPUT_DIRECTORY       = "./gen_docs"

# Inputs
INPUT                  = "./../../projects/symengine/symengine"
RECURSIVE              = NO
#+END_SRC
With this we will now be able to obtain the ~xml~ files for the rest of this setup.
** Exhale
For our first attempt, we will focus on the automation of Sphinx using the [[https://github.com/svenevs/exhale][exhale tool]].
#+BEGIN_SRC bash
# Basic setup
poetry init
poetry add exhale breathe
#+END_SRC
Now we can generate the basic Sphinx structure.
#+BEGIN_SRC bash
# Separate source and build
sphinx-quickstart --sep --makefile docs/Sphinx \
    --project "My Proj" \
    --author "Juurj" \
    --release "latest" \
    --language "en"
#+END_SRC
This allows us to generate the Sphinx documentation we require, with some changes to the ~docs/Sphinx/source/config.py~ file (lifted from the [[https://github.com/svenevs/exhale][exhale documentation]]):
#+BEGIN_SRC python
extensions = [
    'breathe',
    'exhale',
]

# -- Exhale configuration ---------------------------------------------------
# Setup the breathe extension
breathe_projects = {
    "My Proj": "./../../Doxygen/gen_docs/xml"
}
breathe_default_project = "My Proj"

 # Setup the exhale extension
exhale_args = {
    # These arguments are required
    "containmentFolder":     "./api",
    "rootFileName":          "library_root.rst",
    "rootFileTitle":         "Library API",
    "doxygenStripFromPath":  "..",
    # Suggested optional arguments
    "createTreeView":        True,
    # TIP: if using the sphinx-bootstrap-theme, you need
    # "treeViewIsBootstrap": True,
}

# Tell sphinx what the primary language being documented is.
primary_domain = 'cpp'

# Tell sphinx what the pygments highlight language should be.
highlight_language = 'cpp'
#+END_SRC
We also need to add the output to the ~index.rst~ use the following:
#+BEGIN_SRC rst
.. toctree::
   :maxdepth: 2
   :caption: Contents:

   api/library_root
#+END_SRC
At this point we are ready to manually build our documentation.
#+BEGIN_SRC bash
cd docs/Doxygen
doxygen Doxyfile-prj.cfg
cd ../Sphinx
make html
#+END_SRC
This is still pretty cumbersome though. We can view our documentation in a more pleasant manner with ~darkhttpd~.
#+BEGIN_SRC bash
darkhttpd docs/Sphinx/build/html
#+END_SRC
With this we now beautify the documentation (with the [[https://github.com/executablebooks/sphinx-book-theme][sphinx_book_theme]]):
#+BEGIN_SRC bash
poetry add sphinx-book-theme
#+END_SRC
We need to set the theme as well (in the Sphinx ~config.py~ file):
#+BEGIN_SRC python
html_theme = 'sphinx_book_theme'
#+END_SRC
This leads to some pretty documentation.

#+DOWNLOADED: screenshot @ 2020-09-22 06:41:11
#+caption: Generated documentation (Exhale)
[[file:images/Exhale/2020-09-22_06-41-11_screenshot.png]]


#+DOWNLOADED: screenshot @ 2020-09-22 06:44:48
#+caption: Exhale does a great job with file-based hierarchy.
[[file:images/Exhale/2020-09-22_06-44-48_screenshot.png]]

** Conclusions
At this point, we have a basic setup, which we can tweak with a bunch of themes, and/or different parsers, but this is still pretty rough around the edges. However, a caveat of this setup is that the actual contents of the source are not visible in the generated documentation. [[Publishing Doxygen and Sphinx with Nix and Rake][In the next post]], we will look at automating this setup for deploying with Travis.
* DONE Publishing Doxygen and Sphinx with Nix and Rake :@programming:documentation:workflow:nix:cpp:
CLOSED: [2020-09-22 Tue 10:30]
:PROPERTIES:
:EXPORT_FILE_NAME: pub-doc-cpp-dox-sph-nix
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
Automating documenation deployment with Travis, ~rake~ and ~nix~
#+END_QUOTE
** Background
In [[Documenting C++ with Doxygen and Sphinx - Exhale][the previous post]] we generated documentation using Doxygen with Exhale to handle Sphinx. Now we will clean up the earlier workflow with ~rake~ and ensure the environment is reproducible with ~nix~ while deploying to [[https://travis-ci.com/][Travis CI]].
*** Series
1. [[Documenting C++ with Doxygen and Sphinx - Exhale][Documenting C++ with Doxygen and Sphinx - Exhale]]
2. *Publishing Doxygen and Sphinx with Nix and Rake* <-- You are here
3. Documenting C++ with Doxygen and Sphinx - doxyrest
4. Adding Tutorials to Sphinx Projects
** Setup
A quick reminder of the setup we generated in the last post:

#+BEGIN_SRC bash :eval never :exports both
tree -d $prj/ -L 2
#+END_SRC

#+RESULTS:
| .   |             |           |
| â”œâ”€â”€ | docs        |           |
| â”‚Â Â  | â”œâ”€â”€         | Doxygen   |
| â”‚Â Â  | â””â”€â”€         | Sphinx    |
| â”œâ”€â”€ | nix         |           |
| â”‚Â Â  | â””â”€â”€         | pkgs      |
| â”œâ”€â”€ | projects    |           |
| â”‚Â Â  | â””â”€â”€         | symengine |
| â””â”€â”€ | scripts     |           |
|     |             |           |
| 8   | directories |           |
We had further setup files to enable documentation generation with a manual two stage process (handling ~doxygen~ and ~sphinx~ separately).
#+BEGIN_SRC bash
cd docs/Doxygen
doxygen Doxyfile-prj.cfg
cd ../Sphinx
make html
mv build/html ../../public
#+END_SRC
This might be extracted into a simple ~build.sh~ script, and then we might decide to have a ~clean.sh~ script and then we might try to replicate all the functionality of a good build system with scripts.

Thankfully, we will instead start with a ~build~ script defined as above to transition to ~nix~, before using an actual build tool for our dirty work.
** Adding Nix
It wouldn't make sense for me to not stick ~nix~ into this. I recall the dark days of setting up ~Dockerfiles~ to ensure reproducible environments on Travis.

At this point one might assume we will leverage the ~requirements.txt~ based workflow described earlier in [[Niv and Mach-Nix for Nix Python][Niv and Mach-Nix for Nix Python]]. While this would make sense, there are two barriers to its usage:

- It is slower than a ~poetry~ build, as dependency resolution is performed
- It does not play well with existing projects
  + Most ~python~ projects do not rely solely on ~requirements.txt~ [fn:whatthen]

*** Poetry2Nix
Recall that as ~sphinx~ is originally meant for and most often used for Python projects, we will need to consider the possibility (remote though it is) that there might be users who would like to test the documentation without setting up ~nix~.

Thus we will look to the [[https://github.com/nix-community/poetry2nix#mkPoetryEnv][poetry2nix project instead]]. We note the following:
- The ~poetry2nix~ setup is faster (as it consumes a ~lockfile~ instead of solving dependencies from ~requirements.txt~)
  + ~mach-nix~ however, is more flexible and can make use of the ~poetry2nix~ overrides
- In a strange chicken and egg problem, we will have to manually generate the lockfile, thereby creating an impure ~poetry~ project for every update, though the ~nix~ setup will not need it later
  + This is one of the major reasons to prefer ~mach-nix~ for newer projects
*** Shell Environment
We prep our sources in the usual way, by running ~niv init~ in the project root to generate the ~nix/~ folder and the sources therein. With all that in mind, the ~shell.nix~ file at this point is fairly standard, keeping the general ~niv~ setup in mind (described in a [[A Tutorial Introduction to Nix][previous Nix tutorial]]):

#+BEGIN_SRC nix
# -*- mode: nix-mode -*-
let
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  customPython = pkgs.poetry2nix.mkPoetryEnv { projectDir = ./.; };
in pkgs.mkShell {
  buildInputs = with pkgs; [ doxygen customPython rake darkhttpd ];
}
#+END_SRC

Where the most interesting aspect is that the ~projectDir~ is to be the location of the project root, though both ~poetrylock~ and ~pyproject~ variables are supported.
** Refactoring
We consider the problem of refactoring the ~build.sh~ script:
#+BEGIN_SRC bash
#!/usr/bin/env bash
cd docs/Doxygen
doxygen Doxyfile-prj.cfg
cd ../Sphinx
make html
mv build/html ../../public
#+END_SRC
Without resorting to methods such as ~nix-shell --run build.sh --pure~.
*** Nix Bash
Script in hand, we would like to be able to run it directly in the ~nix~ environment. We modify the script as follows:
#+BEGIN_SRC bash
#! /usr/bin/env nix-shell
#! nix-shell deps.nix -i bash

# Build Doxygen
cd docs/Doxygen
doxygen Doxyfile-syme.cfg

# Build Sphinx
cd ../Sphinx
make html
mv build/html ../../public

# Local Variables:
# mode: shell-script
# End:
#+END_SRC

This calls on a ~deps.nix~[fn:whatshebang] which we shall generate in a manner very reminiscent of the ~shell.nix~ [fn:whybother] as follows:

#+BEGIN_SRC nix
let
  sources = import ./../nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  customPython = pkgs.poetry2nix.mkPoetryEnv { projectDir = ./../.; };
in pkgs.runCommand "dummy" {
  buildInputs = with pkgs; [ doxygen customPython ];
} ""
#+END_SRC

Only the paths have changed, and instead of creating and returning a shell environment with ~mkShell~ we instead "run" a derivation instead. At this point we can run this simply as:

#+BEGIN_SRC bash
./scripts/build.sh
#+END_SRC

This is reasonably ready (as a first draft) for being incorporated into a continuous integration workflow.

** Travis CI
Seeing as Travis provides first class ~nix~ support, as well as excellent integration with GitHub, we will prefer it.
*** Settings
A minor but necessary evil is setting up a PAP ([[https://docs.github.com/apps/building-oauth-apps/scopes-for-oauth-apps/][personal access token]]) [[https://github.com/settings/tokens][from here]]. Depending on what repositories are being used, the scope should encompass ~repo~ permissions (minimally ~public_repo~), and ~admin:org~ permissions might be required.

Having obtained the token, we will need to navigate to the Settings section on the Travis web-UI and add the token as an environment variable, we might be partial to a name like ~GH_TOKEN~.

#+DOWNLOADED: screenshot @ 2020-09-22 09:55:21
#+caption: Settings at ~travis-ci.com/host/proj/settings~
[[file:images/Travis_CI/2020-09-22_09-55-21_screenshot.png]]

*** Build Configuration
We will leverage the following configuration:
#+BEGIN_SRC yaml
language: nix

before_install:
  - sudo mkdir -p /etc/nix
  - echo "substituters = https://cache.nixos.org/ file://$HOME/nix.store" | sudo tee -a /etc/nix/nix.conf > /dev/null
  - echo 'require-sigs = false' | sudo tee -a /etc/nix/nix.conf > /dev/null

before_script:
  - sudo mkdir -p /etc/nix && echo 'sandbox = true' | sudo tee /etc/nix/nix.conf

script:
  - scripts/build.sh

before_cache:
  - mkdir -p $HOME/nix.store
  - nix copy --to file://$HOME/nix.store -f shell.nix buildInputs

cache:
  nix: true
  directories:
    - $HOME/nix.store

deploy:
  provider: pages
  local_dir: ./public/
  skip_cleanup: true
  github_token: $GH_TOKEN # Set in the settings page of your repository, as a secure variable
  keep_history: true
  target_branch: master # Required for user pages
  on:
    branch: src
#+END_SRC

Where all the action is essentially in ~script~ and ~deploy~. Note however, that the ~before_cache~ step should change if there is a ~default.nix~ instead. We will in this case, consider the situation of having an organization or user page being the deploy target.

** Rake
Usable though the preceding setting is, it is still rather unwieldy in that:
- there are a bunch of artifacts which need to be cleaned manually
- it is fragile and tied to the folder names
We can fix this with any of the popular build systems, however here we will focus on the excellent ~rake~ [fn:teachmerake]. We shall commit to our course of action by removing ~make~.
#+BEGIN_SRC bash
cd docs/Sphinx
rm Makefile make.bat # other make cruft
#+END_SRC
*** Components
**** Variables
We will begin by requiring ~rake~ and setting basic variables.
#+BEGIN_SRC ruby
require 'rake'

CWD = File.expand_path(__dir__)
DOXYFILE = "Doxyfile-prj.cfg"
OUTDIR = File.join(CWD,"public")
SPHINXDIR = File.join(CWD,"docs/Sphinx")
#+END_SRC
This section should give a fairly clear idea of how the ~Rakefile~ itself is essentially pure ~ruby~ code. We are now beginning to have more holistic control of how our project is structured.
*** Tasks
The general form of a ~task~ is simply:
#+BEGIN_SRC ruby
desc "Blah blah"
task :name do
# Something
end
#+END_SRC

Some variations of this will be considered when appropriate.
**** Clean
A ~clean~ task is a good first task, being as it is almost trivial in all build systems.
#+BEGIN_SRC ruby
desc "Clean the generated content"
task :clean do
  rm_rf "public"
  rm_rf "docs/Doxygen/gen_docs"
  rm_rf "docs/Sphinx/build"
end
#+END_SRC
**** Serve
We will use the [[https://wiki.alpinelinux.org/wiki/Darkhttpd][lightweight darkhttpd server]] for our generated documentation.
#+BEGIN_SRC ruby
desc "Serve site with darkhttpd"
task :darkServe, [:port] do |task, args|
  args.with_defaults(:port => "1337")
  sh "darkhttpd #{OUTDIR} --port #{args.port}"
end
#+END_SRC
Note that we have leveraged the ~args~ system in this case, and also used the top-level ~OUTDIR~ variable.
**** Doxygen
Since the ~doxygen~ output is a pre-requisite, it makes sense to set it up early on.
#+BEGIN_SRC ruby
desc "Build doxygen"
task :mkDoxy do
  Dir.chdir(to = File.join(CWD,"docs/Doxygen"))
  system('doxygen', DOXYFILE)
end
#+END_SRC
**** Sphinx
This task will depend on having the ~doxygen~ output, so we will express this idiomatically by making the ~doxygen~ task run early on.
#+BEGIN_SRC ruby
desc "Build Sphinx"
task :mkSphinx, [:builder] => ["mkDoxy"] do |task, args|
  args.with_defaults(:builder => "html")
  Dir.chdir(to = File.join(CWD,"docs/Sphinx"))
  sh "poetry install"
  sh "poetry run sphinx-build source #{OUTDIR} -b #{args.builder}"
end
#+END_SRC
There are some subtleties here, notably:
- The task is meant to run *without* ~nix~
- We use the ~args~ setup as before
**** No Nix Meta
With this we can now set up a task to build the documentation without having ~nix~.
#+BEGIN_SRC ruby
desc "Build site without Nix"
task :noNixBuild => "mkSphinx" do
  Rake::Task["darkServe"].execute
end
#+END_SRC
The main take-away here is that we finally call the ~Rake~ library itself, but within the task, which means the dependency tree is respected and we get ~doxygen->sphinx->darkhttpd~ as required.
**** Nix Builder
For ~nix~ use we note that we are unable to enter the ~nix~ environment from within the ~Rakefile~ itself. We work around this by being more descriptive.
#+BEGIN_SRC ruby
desc "Build Nix Sphinx, use as nix-shell --run 'rake mkNixDoc' --pure"
task :mkNixDoc, [:builder] => "mkDoxy" do |task, args|
  args.with_defaults(:builder => "html")
  Dir.chdir(to = SPHINXDIR)
  sh "sphinx-build source #{OUTDIR} -b #{args.builder}"
end
#+END_SRC
*** Final Form
The final ~Rakefile~ shall be (with a default task defined):
#+BEGIN_SRC ruby
require 'rake'

# Variables
CWD = File.expand_path(__dir__)
DOXYFILE = "Doxyfile-prj.cfg"
OUTDIR = File.join(CWD,"public")
SPHINXDIR = File.join(CWD,"docs/Sphinx")

# Tasks
task :default => :darkServe

desc "Clean the generated content"
task :clean do
  rm_rf "public"
  rm_rf "docs/Doxygen/gen_docs"
  rm_rf "docs/Sphinx/build"
end

desc "Serve site with darkhttpd"
task :darkServe, [:port] do |task, args|
  args.with_defaults(:port => "1337")
  sh "darkhttpd #{OUTDIR} --port #{args.port}"
end

desc "Build Nix Sphinx, use as nix-shell --run 'rake mkNixDoc' --pure"
task :mkNixDoc, [:builder] => "mkDoxy" do |task, args|
  args.with_defaults(:builder => "html")
  Dir.chdir(to = SPHINXDIR)
  sh "sphinx-build source #{OUTDIR} -b #{args.builder}"
end

desc "Build site without Nix"
task :noNixBuild => "mkSphinx" do
  Rake::Task["darkServe"].execute
end

desc "Build doxygen"
task :mkDoxy do
  Dir.chdir(to = File.join(CWD,"docs/Doxygen"))
  system('doxygen', DOXYFILE)
end

desc "Build Sphinx"
task :mkSphinx, [:builder] => ["mkDoxyRest"] do |task, args|
  args.with_defaults(:builder => "html")
  Dir.chdir(to = File.join(CWD,"docs/Sphinx"))
  sh "poetry install"
  sh "poetry run sphinx-build source #{OUTDIR} -b #{args.builder}"
end
#+END_SRC
*** Travis
We are now in a position to fix our ~travis~ build configuration. Simply replace the old and fragile ~build.sh~ script section with the following:
#+BEGIN_SRC yaml
script:
  - nix-shell --run "rake mkNixDoc" --show-trace --verbose --pure
#+END_SRC
*** Direnv
As a bonus section, consider the addition of the following ~.envrc~ for those who keep multiple ~ruby~ versions:
#+BEGIN_SRC bash
eval "$(rbenv init -)"
rbenv shell 2.6.2
rake -T
#+END_SRC
Activate this with the usual ~direnv allow~. This has the added benefit of listing the defined tasks when ~cd~'ing into the project directory.
** Conclusions
A lot has happened on the tooling end, even though the documentation itself has not been updated further. We have managed to setup a robust environment which is both reproducible and also amenable to users who do not have ~nix~. We have also setup a build system, which can help us in many more ways as well (asset optimization through the ~rails~ pipeline). In the next post, we will return to the documentation itself for further tinkering.
[fn:whatthen] [[https://python-poetry.org/][Poetry]] and [[https://pipenv-fork.readthedocs.io/][Pipenv]] come to mind
[fn:whybother] In this instance, we could have simply called on ~shell.nix~ instead, but it illustrates a more general concept
[fn:teachmerake] [[https://avdi.codes/tag/rake/page/2/][Avdi's blog has a fantastic introduction]] to ~rake~ and ~Rakefiles~
[fn:whatshebang] Chris Warbo has a good [[http://chriswarbo.net/projects/nixos/nix_shell_shebangs.html][introduction to the nix shebang]]
* TODO Documenting C++ with Doxygen and Sphinx - doxyrest :@programming:documentation:workflow:nix:cpp:
#+BEGIN_QUOTE
Advanced documentation customization with doxyrest for C++ projects
#+END_QUOTE
** Background
*** Series
1. [[Documenting C++ with Doxygen and Sphinx - Exhale][Documenting C++ with Doxygen and Sphinx - Exhale]]
2. [[Publishing Doxygen and Sphinx with Nix and Rake][Publishing Doxygen and Sphinx with Nix and Rake]]
3. *Documenting C++ with Doxygen and Sphinx - doxyrest* <-- You are here
4. [[Adding Tutorials to Sphinx Projects][Adding Tutorials to Sphinx Projects]]
* TODO Adding Tutorials to Sphinx Projects :@programming:documentation:workflow:cpp:
#+BEGIN_QUOTE
Multi-source documentation coupled to tutorials, using ~.ipynb~ and other extensions
#+END_QUOTE
** Background
*** Series
1. [[Documenting C++ with Doxygen and Sphinx - Exhale][Documenting C++ with Doxygen and Sphinx - Exhale]]
2. [[Publishing Doxygen and Sphinx with Nix and Rake][Publishing Doxygen and Sphinx with Nix and Rake]]
3. [[Documenting C++ with Doxygen and Sphinx - doxyrest][Documenting C++ with Doxygen and Sphinx - doxyrest]]
4. *Adding Tutorials to Sphinx Projects* <-- You are here

* DONE Talk Supplements for PyCon India 2020 :@conferences:presentations:ramblings:nix:python:
CLOSED: [2020-10-02 Fri 23:56]
:PROPERTIES:
:EXPORT_FILE_NAME: pycon-in-2020-meta
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A meta-post on my talk at PyCon India 2020
#+END_QUOTE
** Background
I [[https://in.pycon.org/2020/][am to present]] at PyCon IN 2020. Some of the motivating reasons for having a post are:
- I would like to preserve questions
- I would like to collect the video, slides and other miscellaneous stuff in one location [fn:officialsite]
- It would be nice to have my own thoughts here afterwards

Details of this happy circumstance are reproduced below from the [[https://in.pycon.org/cfp/2020/proposals/reproducible-scalable-workflows-with-nix-papermill-and-renku~dNkxD/][CFP here]].
*** Details
- Title :: Reproducible Scalable Workflows with Nix, Papermill and Renku
**** Abstract
#+BEGIN_QUOTE
The provenance of Jupyter notebook interfaces can no longer be denied in the data-science and analysis community. In particular, fledgling and "fresh out of school" researchers and practitioners are used to using Jupyter notebooks for their initial analysis. As might be expected, these workflows are difficult to reproduce and also store. Caching efficiency and dependency re-use are almost always sub-optimal with virtual environments, compared to native installations, and the same issues (along with additional security concerns) plague docker setups as well. There are a set of Jupyter tools which have evolved to close this gap, like JupyText. However, the fundamental aspect of reproducing workflows on high performance computing clusters, of being able to compose programmatically, compilation rules which efficiently use underlying hardware with minimal user intervention is still not a solved problem. In this talk, I will discuss packaging Python applications and workflows in an end-to-end composable manner using the Nix ecosystem, which leverages a functional programming paradigm and then show how this allows for both user-friendly low-compute analysis, while being scalable on large clusters. To that end, the tools introduced will be:

The Nix programming language (emphasis on developer environments for python with mkShell)
Jupyter Python kernels (the Xeus kernel for Python debugging) and Jupytext
Papermill for parameterizing notebooks
Renku for tracing provenance
The goal is to have the audience familiarized with the best practices for reproducibility and analysis. The focus will be on scientific HPC applications, though any managed cluster can and will benefit from the practices described.
#+END_QUOTE
**** Other Content
A more in-depth introductory workshop on Nix itself given by me (and Amrita Goswami) at [[https://2020.carpentrycon.org/schedule/#session-10][CarpentryCon2020]] is here:
- [[https://github.com/HaoZeke/CarpentryCon2020_Nix][CarpentryCon2020 Materials]]
- [[https://rgoswami.me/posts/ccon-tut-nix/][A tutorial introduction to Nix and Python]]
** Slides
The slides are embedded below. The ~orgmode~ source [[https://github.com/HaoZeke/haozeke.github.io/blob/src/presentations/PyConIN2020/nixPyconIN.org][is here on the site's GH repo]].
#+BEGIN_EXPORT html
<script async class="speakerdeck-embed" data-id="6db471426fc24d6cbfb433f2464b8146" data-ratio="1.77966101694915" src="//speakerdeck.com/assets/embed.js"></script>
#+END_EXPORT
** Video

{{< youtube 2GX9TK4uNfU >}}

[fn:officialsite] One location I am going to be able to keep track of

* DONE Replacing Zoom with Open Broadcaster Software :@notes:workflow:tools:
CLOSED: [2020-10-03 Sat 17:05]
:PROPERTIES:
:EXPORT_FILE_NAME: rep-zoom-obs
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A post on local recordings
#+END_QUOTE
** Background
Since the advent of the COVID-19 situation, there has been an increase in the demand for recorded materials. Standard approaches involve Zoom, which is not only proprietary, but also quite a bit of a privacy nightmare. The last straw was the random placement of my speaker bauble head.

#+DOWNLOADED: screenshot @ 2020-10-03 16:51:07
#+caption: Zoom webcam placement
[[file:images/Background/2020-10-03_16-51-07_screenshot.png]]

At this point, given that I was to set up a pre-recorded video for [[Talk Supplements for PyCon India 2020][PyCon India 2020]], I decided to look into alternatives.
** Alternatives
The search for alternative screen recording systems isn't really a very new one. For group work (like [[https://wc3m.github.io/][W3cm]] [fn:whatthat]), I tend to prefer Skype, since it handles speaker galleries very well. Unfortunately, Skype has no capacity for recording single person calls, at least as yet. This is not the place for an extended debate on the pros and cons of Skype, or Google Meet (only records corporate accounts), or the rest. Instead, lets sum up all these issue with the simple understanding that, if *one person* wants to record a webcam connected to their local computer, along with the screen, it is insane to imagine that the only way to get this is by:
- Making an account somewhere (Zoom, Meet, Skype, anything)
- Giving a cloud service permission to record our screens
At the same time, a lot of standard tools for screen recording do not play nice with webcam recorders (like [[https://www.maartenbaert.be/simplescreenrecorder/][Simple Screen Recorder]] and [[https://help.gnome.org/users/cheese/stable/][Cheese]]).
** Open Broadcaster Software
The [[https://duckduckgo.com/?q=obs+studio&ia=web][OBS studio project]] is a godsend. It allows for simultaneously managing multiple streams, of both audio and video. Furthermore, since these are implemented as overlays, it is possible to fine-tune the positioning of each of these, which is something Zoom and friends lack.

#+DOWNLOADED: screenshot @ 2020-10-03 16:53:51
#+caption: Into the matrix
[[file:images/Open_Broadcaster_Software/2020-10-03_16-53-51_screenshot.png]]

The ability to resize the webcam is best shown in the figure below.

#+DOWNLOADED: screenshot @ 2020-10-03 16:58:26
#+caption: Almost Zoom, only better
[[file:images/Open_Broadcaster_Software/2020-10-03_16-58-26_screenshot.png]]

OBS also generates beautifully small videos and supports live-streaming.
*** Common Caveats
- The standard setting is set to work with hardware acceleration, which may not be present for many users
  + Use the settings to change this back to the software setting
** Conclusions
I cannot imagine going back to Zoom to record anything local. It is an added bonus that OBS is both cross-platform and FOSS. It is only incredible more people do not use it.

[fn:whatthat] Water, Chemicals and more with Computers for Chemistry, a computational chemistry course aimed at middle school students taught with my sister [[http://scholar.google.com/citations?user=mviv92EAAAAJ&hl=en][Amrita Goswami]]

* DONE Old Laptops as Secondary Monitors
CLOSED: [2020-10-23 Fri 23:22]
:PROPERTIES:
:EXPORT_FILE_NAME: laptop-as-second-screens
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
Dual screen workflows without screens across operating systems
#+end_quote
** Background
My X380 sadly has been having port issues. This meant that my M14 was no longer a viable option for my second screen needs.
** Outline
The general form of the solution works in one of two ways:
- VNC Viewer :: Where the (second-screen) laptop connects to a VNC server on the primary laptop
- Peripheral Shares :: Where the secondary laptop runs a server to enable proxying mouse and keyboard access from the primary laptop
** VNC and Windows
For laptops running Windows, I personally just set up [[https://tightvnc.com/][TightVNC]]. The standard settings work well enough for the peripheral share described below.
*** Comments
This is best used for working with Windows only stuff like Office.
** VNC and Linux
*** Peripheral Share
For the secondary laptop we need to run a server (~tigervnc~) without setting an external screen.
#+begin_src bash
x0vncserver -rfbauth ~/.vnc/passwd
#+end_src
Now on the main laptop, we will simply leverage ~x2vnc~ to extend into the secondary laptop.
#+begin_src bash
x2vnc $ip -west
#+end_src
Where we can get the IP (local) by checking with ~ifconfig~ on the secondary laptop.
**** Meta
This works best when combined with a networked file-system, since then you can interact with files in tandem. Otherwise, there is quite a bit of ~git~ based back and forth.
** VNC and Android
There are two parts to this solution. Note that, as Android devices don't run X11 systems in a meaningful way, the direct access method is through a paid application, a2vnc server lite, which also did not work well in my tests. We will therefore focus on setting a VNC viewer up to connect to the primary laptop.
*** Primary Settings
**** XRandR Setup
For the primary laptop, we will start by obtaining our present screens configuration.
#+BEGIN_SRC bash :exports both :results raw :eval never
xrandr | grep " connected"
#+END_SRC

#+BEGIN_SRC bash
eDP1 connected primary 1920x1080+1920+0 (normal left inverted right x axis y axis) 290mm x 170mm
#+END_SRC

Naturally your output will differ. We also need the resolution of the Android device. In my case, they are the same. At this point we are ready to figure out the mode-line.

#+BEGIN_SRC bash :results raw
gtf 1920 1080 60
#+END_SRC

#+BEGIN_SRC bash
  # 1920x1080 @ 60.00 Hz (GTF) hsync: 67.08 kHz; pclk: 172.80 MHz
  Modeline "1920x1080_60.00"  172.80  1920 2040 2248 2576  1080 1081 1084 1118  -HSync +Vsync
#+END_SRC

Let us now use this information to create a bunch of modelines.

#+BEGIN_SRC bash
xrandr --newmode "1920x1080_60.00"  172.80  1920 2040 2248 2576  1080 1081 1084 1118  -HSync +Vsync
#+END_SRC

Note that we can create more of these in the same manner. We can now move forward with making a virtual screen.

#+BEGIN_SRC bash
xrandr --addmode VIRTUAL1 1920x1080_60.00
#+END_SRC

We can now finally set up the output.

#+BEGIN_SRC bash
xrandr --output VIRTUAL1 --mode 1920x1080_60.00 --left-of eDP1
#+END_SRC

Note that it is better to use ~mons~ to work with our newly created virtual screen.

#+BEGIN_SRC bash
mons -e left
#+END_SRC

This is still a bit ugly, since the process needs to be repeated with each reboot.

**** VNC Setup
Now we need prepare our VNC. ~x11vnc~ is recommended at the moment.
#+BEGIN_SRC bash
x11vnc -vencrypt nodh:only-ssl -ssl SAVE -clip 1920x1080+0+0
#+END_SRC
*** Android Settings
For this section, I personally use [[https://play.google.com/store/apps/details?id=com.iiordanov.bVNC&hl=en&gl=US][bVNC Pro]]. The setup is pretty dead simple. A basic VNC connection is all that is required.
*** Comments
In practice, I use the x86 setup, with the secondary laptop acting as a viewer for a virtual screen, mostly because that way I can tune into multiple Zoom meetings (a bonus).
** Conclusion
The final setup is quite robust to changes. Future posts might go into setting up the kind of local networking tools to help move files, code and more between both machines, to improve on the peripheral share approach. Additionally, there are still some manual steps which can and should be automated. I'm not super pleased with the setup, it takes longer than a wireless screen. This post is complimented by the [[Multiple Monitors with Touchscreens][work and setup with touchscreens here]].

* TODO Haskell with Nix for Hacktoberfest :@programming:tools:nix:workflow:python:
:PROPERTIES:
:EXPORT_FILE_NAME: haskell-with-nix-hacktober
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
Short post on Haskell Nix, and Hacktoberfest
#+END_QUOTE
** Background
Full disclaimer. This post is not like my other ~nix~ posts, which are /ex-cathedra/ expositions. My working knowledge of Haskell, however, is essentially some non-production hacking of [[https://github.com/jaspervdj/hakyll][Hakyll]] and [[https://github.com/jgm/pandoc][Pandoc]]. That said, I thought I'd take advantage of the [[https://github.com/kowainik/learn4haskell#how-to-get-started][learn4haskell initiative]] of [[https://hacktoberfest.digitalocean.com/][Hacktoberfest]]. I truly do believe incentivized FOSS development is terrible [fn:fightme], though incentivized learning is not that bad, so I was pleasantly surprised to see this excellent initiative. Plus, I'd always wanted to get into Haskell. Naturally, working through something without a take is pointless, so I thought I'd work through the exercises with a bit of Nix.
** Setup
There are several methods of working with ~nix~ and Haskell, typically centered around ~cabal2nix~ and ~stack2nix~ (including [[https://docs.haskellstack.org/en/stable/README/][this official approach]]). I will implement and leverage the [[https://input-output-hk.github.io/haskell.nix/][haskell.nix approach]]. Additionally, I will leverage some [[https://www.youtube.com/watch?v=5p2Aq3bRuL0][concepts from Tsoding]] and [[https://github.com/hlissner/doom-emacs][doom-emacs]] as always.

*** Haskell-Nix
This section follows directly from the [[https://input-output-hk.github.io/haskell.nix/tutorials/getting-started/][haskell.nix documentation]], as adapted to the ~learn4haskell~ project.

#+begin_src bash
nix-env -iA cachix -f https://cachix.org/api/v1/install
cachix use iohk
cachix use ghcide-nix
#+end_src

#+RESULTS:
: Configured https://iohk.cachix.org binary cache in /home/haozeke/.config/nix/nix.conf

We will also setup the optional keys in ~nix.conf~. Assuming that someone is "following along" we will configure our *fork* of the ~learn4haskell~ project in the standard manner. In case you haven't done so already, and you have set up the very excellent [[https://github.com/github/hub][hub wrapper for git]].

#+begin_src bash
# wherever you keep git stuff
git clone https://github.com/kowainik/learn4haskell
cd learn4haskell
hub fork
#+end_src

Anyway now we can start by pinning dependencies, with some inspiration from [[https://github.com/Ptival/haskell-nix-template/][this template]].

#+begin_src bash :eval never
nix-env -i niv
niv init
niv add input-output-hk/haskell.nix
niv add cachix/ghcide-nix
#+end_src

With that out of the way, we will start working on a ~default.nix~ [fn:whyverbose].

#+begin_src nix :tangle ~/Git/Github/Haskell/learn4haskell/default.nix :comments link
let
  # Variables
  name = "learn4haskell";
  compiler-nix-name = "ghc884";
  # Niv
  sources = import ./nix/sources.nix;
  # Load haskellNix first
  haskellNix = import (fetchTarball { inherit (sources."haskell.nix") url sha256; }) {
      sourcesOverride = {
        hackageSrc = fetchTarball { inherit (sources."hackage.nix") url sha256; };
      };
  };
  # Override pkgs with the cached versions
  pkgs = import haskellNix.sources.nixpkgs haskellNix.nixpkgsArgs;
  inherit (pkgs.lib) optional optionals;
in pkgs.haskell-nix.project {
  inherit compiler-nix-name;
  src = pkgs.haskell-nix.haskellLib.cleanGit {
    inherit name;
    src = ./.;
  };
}
#+end_src

Now for a nice development environment in a ~shell.nix~ file.

#+begin_src nix :tangle ~/Git/Github/Haskell/learn4haskell/shell.nix :comments link
let
  hsPkgs = import ./default.nix { inherit pkgs; };
in hsPkgs.shellFor {
    # Include only the *local* packages of your project.
    packages = ps: with ps; [
      learn4haskell
    ];
    withHoogle = true;
    tools = { cabal = "3.2.0.0"; hlint = "2.2.11"; };
    buildInputs = with pkgs.haskellPackages;
      [ ghcid ];
    # Prevents cabal from choosing alternate plans, so that
    # *all* dependencies are provided by Nix.
    exactDeps = true;
}
#+end_src


[fn:fightme] [[https://joel.net/how-one-guy-ruined-hacktoberfest2020-drama][This is an extreme case]], but all "part-time" developers end up contributing features which rot over time
[fn:whyverbose] If the post seems overly verbose, it is because these are tangled directly

* DONE Talk Supplements for NixCon 2020 :@conferences:presentations:ramblings:nix:hpc:
CLOSED: [2020-10-17 Sat 10:40]
:PROPERTIES:
:EXPORT_FILE_NAME: nixcon-in-2020-meta
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A meta-post on my lightning talk at NixCon 2020
#+END_QUOTE
** Background
Much the same as the rationale behind my [[Talk Supplements for PyCon India 2020][meta-post on my talk at PyCon India 2020]], that is:
- I would like to preserve questions
- I would like to collect the video, slides and other miscellaneous stuff in one location [fn:officialsite]
- It would be nice to have my own thoughts here afterwards
*** Details
- Title :: Nix from the dark ages (without Root)
- Proposal :: [[https://cfp.nixcon.org/nixcon2020/talk/CUE78W/][See the cfp response here]]
**** Abstract
#+BEGIN_QUOTE
Short comments from the trenches of High Performance Clusters on working with Nix on kernel locked-in systems without proot support.
#+END_QUOTE
**** Linked Posts
- [[Local Nix without Root][Local Nix without Root]] :: Motivation and installation
  + [[Provisioning Dotfiles on an HPC][Provisioning Dotfiles on an HPC]] :: Looks into how standard approaches fail (~proot~)
  + [[HPC Dotfiles and LMod][HPC Dotfiles and LMod]] :: All the ugly manual install steps
**** Other Content
Anything on this site [[https://rgoswami.me/tags/nix/][tagged with Nix]]. Also an introduction to ~nix~ given by me (and Amrita Goswami) at [[https://2020.carpentrycon.org/schedule/#session-10][CarpentryCon2020]] is here:
- [[https://github.com/HaoZeke/CarpentryCon2020_Nix][CarpentryCon2020 Materials]]
- [[https://rgoswami.me/posts/ccon-tut-nix/][A tutorial introduction to Nix and Python]]
** Slides
The slides are embedded below. The ~orgmode~ source [[https://github.com/HaoZeke/haozeke.github.io/blob/src/presentations/NixCon2020/darkNix.org][is here on the site's GH repo]].
#+BEGIN_EXPORT html
<script async class="speakerdeck-embed" data-id="2cc4e5d0cca445be95e1e77827ea782c" data-ratio="1.37081659973226" src="//speakerdeck.com/assets/embed.js"></script>
#+END_EXPORT

[fn:officialsite] One location I am going to be able to keep track of

* TODO Streaming Music with Mopidy :@personal:tools:workflow:
:PROPERTIES:
:EXPORT_FILE_NAME: stream-music-mopidy
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
Short post on multi-device (android devices and laptops) streaming with Mopidy and Icecast.
#+END_QUOTE
** Background
Back home, backed by my ridiculously large local music library, I had an elegant, efficient MPD setup for streaming music throughout the house. Now that I'm constrained to two laptops (one superannuated) and two phones, in a drastically smaller space, I thought I'd do without any streaming setups at all. However, the Spotify desktop application is ridiculous. It is bloated and doesn't minimize well on ~i3~ and is just plain annoying.

** Tools
We will require, in broad strokes:
- A music daemon
  + Should be lightweight (MPD)
  + Will collate a bunch of sources and provide the ignorant end user with a unified interface (Mopidy)
- A streaming server
  + A good streaming server will minimally support multiple listeners (Icecast)
  + An excellent server will allow for synchronous output (Snapcast)
- A client for every OS
  + That's two clients for every OS, and 3 web interfaces [fn:oddname]
  + Cantata, Sonata, M.A.L.P., MPDroid, RompR, Iris (we'll see them show up later)

*** Mopidy
I'm still leery of ~python~ projects, though, given that mopidy supports Spotify (with a premium account), I didn't have much of a choice. The idea is to set up the configuration locally first.
#+begin_src bash
vim ~/.conf/mopidy/mopidy.conf
mopidy # Test things
#+end_src
**** Spotify
This setup essentially entails filling in the ~[spotify]~ block in the configuration file. The only thing to really keep in mind is that the client and secret values [[https://mopidy.com/ext/spotify/][are from here]].
**** Systemd
Once the configuration works as expected, transfer it to the [[https://docs.mopidy.com/en/latest/running/service/?highlight=systemctl#service-management-with-systemd][system configuration location]]. It is best to manually edit this, instead of over-writing this.
**** Comments
At this point, if all that was needed was control over a single PC, then we'd be done. There's no need fo anything more complicated than an MPD client for controlling the running ~mopidy~ server.
*** Icecast

[fn:oddname] An [[https://en.wikipedia.org/wiki/Two_Cars_in_Every_Garage_and_Three_Eyes_on_Every_Fish][old Simpsons pun]]

* DONE Anki Decks with Orgmode :@programming:workflow:projects:tools:emacs:orgmode:
CLOSED: [2020-10-27 Tue 01:05]
:PROPERTIES:
:EXPORT_FILE_NAME: anki-decks-orgmode
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
Setting up unicode math and ~orgmode~ for painless Anki deck building
#+end_quote

** Background
A recent [[https://news.ycombinator.com/item?id=24878171][Hacker News post]] reminded me of [[https://docs.ankiweb.net/#/getting-started][Anki]], and that brought back memories of
my Anki ~orgmode~ setup. I thought I'd re-create and immortalize it.

The standard way of working with Anki, is with a pretty awkward GUI. There are
changes to be made here, which make life a little easier, including the setup of
custom cards, but the inherent concerns of the WYSIWYG editor are basically
insurmountable.

#+DOWNLOADED: screenshot @ 2020-10-27 00:13:31
#+caption: Anki GUI
[[file:images/Background/2020-10-27_00-13-31_screenshot.png]]


The goal is to get this a better workflow than manual editing of Anki decks.
~orgmode~ is perfect for making cards, especially in the larger context of using
it for storing images and rich ~pdfs~.

#+DOWNLOADED: screenshot @ 2020-10-26 23:53:05
#+caption: A pleasant way to make anki decks
[[file:images/Background/2020-10-26_23-53-05_screenshot.png]]

** Methodology
To accomplish this, we basically need to have the following:
- [[https://github.com/louietan/anki-editor][anki-editor]]  :: This ~emacs~ plugin will facilitate the conversion from our ~orgmode~ files to the Anki markup
- [[https://github.com/FooSoft/anki-connect][anki-connect]] :: We need a server of sorts set up to allow us to push pull and get errors from the running Anki server, this is an Anki plugin
- [[https://ankiweb.net/shared/info/937148547][LaTeX process editor]] :: It wouldn't be much better than manually making cards in Anki if we couldn't leverage ~unicode~ characters, so we need to modify the internal Anki build process for TeX
*** Anki Editor
As with all ~emacs~ related setup snippets on this site, these should be modified and adapted as needed, especially for those not using [[https://github.com/hlissner/doom-emacs/][doom-emacs]]. 
#+BEGIN_SRC emacs-lisp :tangle no :eval never
(use-package anki-editor
  :after org-noter
  :config
  ; I like making decks
  (setq anki-editor-create-decks 't))
#+END_SRC
Also, my [[https://dotdoom.rgoswami.me/config.html#text-3][full configuration]] has additional non-essential quality of life keybindings amongst other things.
*** Anki Connect
~CTRL+Shift+A~ will bring up the addon settings, and Anki has to be restarted after installing the addons. [[https://github.com/FooSoft/anki-connect][Anki Connect]] itself does not need any further configuration, though the ~readme~ is very comprehensive.
*** TeX Setup
The [[https://ankiweb.net/shared/info/937148547][LaTeX process editor]] can be set in two stages, wherein we will first ensure that we can use ~xelatex~ and that we can generate an ~svg~.
#+begin_src json
{
    "svgCommands": [
        [
            "xelatex",
            "--no-pdf",
            "-interaction=nonstopmode",
            "tmp.tex"
        ],
        [
            "dvisvgm",
            "--no-fonts",
            "-Z",
            "2",
            "tmp.xdv",
            "-o",
            "tmp.svg"
        ]
    ]
}
#+end_src
The ~png~ settings can be modified in a similar manner if required, but it is better to generate ~svg~ files, which will set up in the cosmetics section. Note that we pass ~--no-pdf~ to get the ~xdv~ file which has replaced ~dvi~ files for ~xelatex~.
*** Cosmetics
The final aspect of this is to be configured with the GUI. The easiest option is to clone the Basic card type and customize that. ~CTRL+Shift+N~ should bring up the card editor. The relevant styles are[fn:wherevert] (from the ~Cards~ option):
#+begin_src css
.card {
 font-family: Literata;
 font-size: 26px;
 text-align: center;
 color: black;
 background-color: white;
}
img {
max-height:1000px;
height: auto;
width: auto;
}
img[src*="latex"] {
  vertical-align: middle;
}
#+end_src

Now we need setup our TeX headers as well, and enable the ~Create scalable
images with dvisvgm~ option. The header needs to have (minimally):

#+begin_src tex
\documentclass[12pt]{article}
\special{papersize=3in,5in}
\usepackage{geometry} 
\usepackage{unicode-math}
\usepackage{mathtools}
\pagestyle{empty}
\setlength{\parindent}{0in}
\begin{document}
#+end_src

While the ~footer~ is simply ~\end{document}~. With this, we have achieved
pretty formatting.

#+DOWNLOADED: screenshot @ 2020-10-26 23:53:25
#+caption: Pretty card formatting
[[file:images/Background/2020-10-26_23-53-25_screenshot.png]]
*** Font Locking
Inspired by [[https://yiufung.net/post/anki-org/][this post]], we will also use [[https://github.com/gongzhitaao/orgcss][orgcss]] to obtain some ~orgmode~ font-locking. We will add the following styles:
#+begin_src css
:not(pre) > code {
  padding: 2px 5px;
  margin: auto 1px;
  border: 1px solid #ddd;
  border-radius: 3px;
  background-clip: padding-box;
  color: #333;
  font-size: $code-size;
}

.org-src-container {
  border: 1px solid #ccc;
  box-shadow: 3px 3px 3px #eee;
  font-family: $monospace;
  font-size: $code-size;
  margin: 1em auto;
  padding: 0.1em 0.5em;
  position: relative;
}

.org-src-container > pre {
  overflow: auto;
}

.org-src-container > pre:before {
  display: block;
  position: absolute;
  background-color: #b3b3b3;
  top: 0;
  right: 0;
  padding: 0 0.5em;
  border-bottom-left-radius: 8px;
  border: 0;
  color: white;
  font-size: $code-size;
}

/* from http://demo.thi.ng/org-spec/ */

.org-src-container > pre.src-sh:before {
  content: "sh";
}
.org-src-container > pre.src-bash:before {
  content: "bash";
}
.org-src-container > pre.src-emacs-lisp:before {
  content: "Emacs Lisp";
}
.org-src-container > pre.src-R:before {
  content: "R";
}
.org-src-container > pre.src-org:before {
  content: "Org";
}
.org-src-container > pre.src-cpp:before {
  content: "C++";
}
.org-src-container > pre.src-c:before {
  content: "C";
}
.org-src-container > pre.src-html:before {
  content: "HTML";
}
.org-src-container > pre.src-js:before {
  content: "Javascript";
}
.org-src-container > pre.src-javascript:before {
  content: "Javascript";
}

// More languages from http://orgmode.org/worg/org-contrib/babel/languages.html

.org-src-container > pre.src-abc:before {
  content: "ABC";
}
.org-src-container > pre.src-asymptote:before {
  content: "Asymptote";
}
.org-src-container > pre.src-awk:before {
  content: "Awk";
}
.org-src-container > pre.src-C:before {
  content: "C";
}
.org-src-container > pre.src-calc:before {
  content: "Calc";
}
.org-src-container > pre.src-clojure:before {
  content: "Clojure";
}
.org-src-container > pre.src-comint:before {
  content: "comint";
}
.org-src-container > pre.src-css:before {
  content: "CSS";
}
.org-src-container > pre.src-D:before {
  content: "D";
}
.org-src-container > pre.src-ditaa:before {
  content: "Ditaa";
}
.org-src-container > pre.src-dot:before {
  content: "Dot";
}
.org-src-container > pre.src-ebnf:before {
  content: "ebnf";
}
.org-src-container > pre.src-forth:before {
  content: "Forth";
}
.org-src-container > pre.src-F90:before {
  content: "Fortran";
}
.org-src-container > pre.src-gnuplot:before {
  content: "Gnuplot";
}
.org-src-container > pre.src-haskell:before {
  content: "Haskell";
}
.org-src-container > pre.src-io:before {
  content: "Io";
}
.org-src-container > pre.src-java:before {
  content: "Java";
}
.org-src-container > pre.src-latex:before {
  content: "LaTeX";
}
.org-src-container > pre.src-ledger:before {
  content: "Ledger";
}
.org-src-container > pre.src-ly:before {
  content: "Lilypond";
}
.org-src-container > pre.src-lisp:before {
  content: "Lisp";
}
.org-src-container > pre.src-makefile:before {
  content: "Make";
}
.org-src-container > pre.src-matlab:before {
  content: "Matlab";
}
.org-src-container > pre.src-max:before {
  content: "Maxima";
}
.org-src-container > pre.src-mscgen:before {
  content: "Mscgen";
}
.org-src-container > pre.src-Caml:before {
  content: "Objective";
}
.org-src-container > pre.src-octave:before {
  content: "Octave";
}
.org-src-container > pre.src-org:before {
  content: "Org";
}
.org-src-container > pre.src-perl:before {
  content: "Perl";
}
.org-src-container > pre.src-picolisp:before {
  content: "Picolisp";
}
.org-src-container > pre.src-plantuml:before {
  content: "PlantUML";
}
.org-src-container > pre.src-python:before {
  content: "Python";
}
.org-src-container > pre.src-ruby:before {
  content: "Ruby";
}
.org-src-container > pre.src-sass:before {
  content: "Sass";
}
.org-src-container > pre.src-scala:before {
  content: "Scala";
}
.org-src-container > pre.src-scheme:before {
  content: "Scheme";
}
.org-src-container > pre.src-screen:before {
  content: "Screen";
}
.org-src-container > pre.src-sed:before {
  content: "Sed";
}
.org-src-container > pre.src-shell:before {
  content: "shell";
}
.org-src-container > pre.src-shen:before {
  content: "Shen";
}
.org-src-container > pre.src-sql:before {
  content: "SQL";
}
.org-src-container > pre.src-sqlite:before {
  content: "SQLite";
}
.org-src-container > pre.src-stan:before {
  content: "Stan";
}
.org-src-container > pre.src-vala:before {
  content: "Vala";
}
.org-src-container > pre.src-axiom:before {
  content: "Axiom";
}
.org-src-container > pre.src-browser:before {
  content: "HTML";
}
.org-src-container > pre.src-cypher:before {
  content: "Neo4j";
}
.org-src-container > pre.src-elixir:before {
  content: "Elixir";
}
.org-src-container > pre.src-request:before {
  content: "http";
}
.org-src-container > pre.src-ipython:before {
  content: "iPython";
}
.org-src-container > pre.src-kotlin:before {
  content: "Kotlin";
}
.org-src-container > pre.src-Flavored Erlang lfe:before {
  content: "Lisp";
}
.org-src-container > pre.src-mongo:before {
  content: "MongoDB";
}
.org-src-container > pre.src-prolog:before {
  content: "Prolog";
}
.org-src-container > pre.src-rec:before {
  content: "rec";
}
.org-src-container > pre.src-ML sml:before {
  content: "Standard";
}
.org-src-container > pre.src-Translate translate:before {
  content: "Google";
}
.org-src-container > pre.src-typescript:before {
  content: "Typescript";
}
.org-src-container > pre.src-rust:before {
  content: "Rust";
}
#+end_src
However, in the interests of sanity, we will leverage the [[https://ankiweb.net/shared/info/1972239816][Syntax Highlighting Anki plugin]] for managing the actual style-sheets instead of manual edits to each card type.
#+caption: A screencast from the plugin readme
https://raw.githubusercontent.com/ijgnd/syntax-highlighting/master/screenshots/demo_config_with_nm_toggle_addon.gif

At this stage, we have a card which can gracefully handle both XeLaTeX and code in an elegant manner. An example is presented in the next section.
** Usage
For the sample card[fn:lolwut] shown, the markup is dead simple.

#+include: "tmpCards/tCardbasic.org" src org

Essentially:
- Enable and load ~anki-editor~
  + Add local variable section to ensure we load ~anki-editor~. This is essentially via ~eval: (anki-editor-mode)~ in the Local variables block
- Fire up Anki
- Export at will, and continue adding more cards or non-card details to the ~orgmode~ file

The [[https://raw.githubusercontent.com/louietan/anki-editor/master/examples.org][Anki editor examples]] file is excellent and the [[https://github.com/louietan/anki-editor/issues/30][issue tracker]] also has a ton
of information.
*** Code

#+include: "tmpCards/testCard.org" src org

#+DOWNLOADED: screenshot @ 2020-10-28 13:31:43
#+caption: Code card with TeX
[[file:images/Methodology/2020-10-28_13-35-16_screenshot.png]]
*** More Content
- [[https://github.com/Anton-Latukha/Fundamental-Haskell][Fundamental Haskell]] :: An excellent example of how a multiple frontend learning repository can be, written with ~org-drill~[fn:whynotdrill]
- [[https://yiufung.net/post/anki-org/][Anki powerups with orgmode]] :: A post brought to my attention after I had published this, an excellent introduction with videos
** Conclusions
Some final comments:
- Screenshots and other images linked are automatically synced
- The TeX is best rendered on the PC first, so run through these at-least once
A missing link in this setup is the ability to use a touch screen and stylus to
write proofs or skip the TeX setup altogether, but that would require another
post altogether. Additionally, all the standard bells and whistles of having an
~orgmode~ document can be applied, including, crucially, the ability to have
long-form notes as well, a coherent approach to this can also be covered later.

[fn:lolwut] It _is_ a *gag* card, no judgement here
[fn:whynotdrill] ~org-drill~ doesn't support any kind of mobile synchronization
[fn:wherevert] The alignment trick is from [[https://clementc.github.io/blog/2018/08/15/anki_setup/][this post]]

* TODO Rediscovering Digital Books :@personal:workflow:tools:
:PROPERTIES:
:EXPORT_FILE_NAME: redis-digi-books
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
A discourse on the greater good embodied by ~k2pdfopt~ and a Kobo Aura HD
#+end_quote

** Background
The sharp eyed reader might notice, that unlike the majority of posts on this site, this one is *not* in the ~programming~ category, though there will be code. Essentially this is a bit of an expository rant on the nature of consumerism and the lost days of durable products.
** Kobo Aura HD
Many many years ago, my parents let me get an e-ink device. At the time, I spent many hours lurking on the fantastic Mobilread forums. Back in those early days.
*** Pros
- KSM menu
  + It is discontinued now, but seeing as I have no desire to update this further, it works great
*** Cons
- The USB connectivity seems to be gone for good
  + Though it charges
** Conclusions
I have no intention of getting a newer e-ink device which does not have an SD-card slot and an active developer community.

* TODO The Decline of Linux Email :@personal:workflow:tools:email:
:PROPERTIES:
:EXPORT_FILE_NAME: declining-linux-email
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
A scathing indictment of Linux Email clients and a methodology to revert to command line oriented workflows.
#+end_quote
** Background
I have loved and used email for over 16 years now. Back in middle school, my dad would travel, and along with my mom and sister, we'd send emails. Of course, over the years, nothing in software has declined so much as email clients.

#+DOWNLOADED: screenshot @ 2020-11-23 08:11:55
#+caption: Ah yes, another casualty of Mozilla's epic life choices
[[file:images/Background/2020-11-23_08-11-55_screenshot.png]]


In brief order:

- [[https://www.thunderbird.net/en-US/][Thunderbird]] :: This was simply impossible to beat, back in the days of XUL. Infact, Mozilla and the entire "Foundation" should have a special circle reserved for them in hell for what they did to both Firefox (making it a chrome clone) and Thunderbird (stripping it of all the loving crafted extensions)[fn:noreally]. Even after the ridiculous new engine, the community bounced back to make a good set of extensions, some of which [[https://addons.thunderbird.net/en-US/thunderbird/collections/rgoswami/favorites/][are enumerated here]]. Most recently, once again version 78 has broken all addons and made my client unusable again. The *last straw*. Also, of late, Thunderbird needs a paid extension to work well with Exchange accounts.
- [[https://getmailspring.com/][Mailspring]] :: Always leery of Electron apps, this is a pathetically heavy, plodding mess. Constantly disconnecting and requiring re-authentication, while also being very coy about losing emails. Their "pro" feature set includes a privacy invasive method of figuring out when people read what you send. This has been championed a lot by the Tech media, but it is simply not a working mail client, and the themes are drab anyway. Not to mention the fact that it is *paid*, and *not FOSS*, and had an ugly history with Nylas mail.
- [[https://www.postbox-inc.com/][Postbox]] :: A fork from Thunderbird, without official Linux support, this [[https://appdb.winehq.org/objectManager.php?sClass=application&iId=9199][runs surprisingly well via Wine]]. That said, it is still annoying to not have a native client, and there are no good extensions.
- [[https://wiki.gnome.org/Apps/Geary][Geary]] :: The shining hope of the GNOME desktop. Good, but not extensible enough yet. Also lacks a lot of user-facing options, due to the ridiculous design paradigm of GNOME, which appears to be "what if we had no options, people like that right, that's why they use Linux!"


Perhaps most importantly, none of these clients are stable. They change at the whims of a shadowy development cabal which has no consideration for users. Many years ago, back when Thunderbird ditched XUL, ~mutt~ presented itself as an alternative. At the time, however, it was
** Conclusions
[fn:noreally] A full description would span several posts, and detracts from the main goal

* DONE Reclaiming Email with Astroid :@personal:rant:tools:email:workflow:
CLOSED: [2020-12-30 Wed 06:37]
:PROPERTIES:
:EXPORT_FILE_NAME: reclaim-email-astroid
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
Migrating Imap, Gmail and Exchange, mail accounts from GUI clients to [[https://astroidmail.github.io/][Astroid]]
#+end_quote
** Background
Initially, I had planned this post to start with a brief history of the decline of email clients for Linux. That quickly got out of hand, and was therefore spun out into a post of its own (TBD). To keep things brief. Thanks to the incredible ineptitude of the Thunderbird steering committee, I ended up requiring a new mail client. Having despaired of the GUI based bloat heavy approaches of most clients, I decided to go the old fashioned route and build one up in a modular manner.
*** Series
This post is part of a series on email.
1. The Decline of Linux Email (TBD)
2. *Reclaiming Email with Astroid* <-- You are here!
3. Emacs and Email (TBD)
** The Accounts
In no order of preference, for a variety of reasons, I have 23 distinct email accounts. However, these are actually broken down into a few basic types and associated mail fetching software.
- Generic IMAP :: Yahoo, Mail.ru and the rest are managed with [[https://isync.sourceforge.io/][isync]]
- Exchange :: A school account of mine uses Office 365, and will be handled with [[http://davmail.sourceforge.net/][davmail]] in conjunction with ~isync~
- Gmail :: There are a few of these, two personal, and two institutional, all of which are handled with [[http://lieer.gaute.vetsj.com/][lieer]]
** InSync
For general IMAP accounts (anything which isn't backed by ~gmail~ or ~outlook~) an ~mbsync~ approach works best. Exhange accounts need ~davmail~ and are described in a separate sub-section. Before we get to the ~poll.sh~ script and ~astroid~, it is a good idea to run each of these as we set them up, especially as the first run can take quite a long time (around an hour for 9205 messages).

#+begin_src bash
mbsync -V blah
#+end_src

*** General IMAP
For a standard IMAP account (anything which isn't Exchange or Gmail) the only thing we need to keep track of is a good way to obfuscate passwords. We will use ~pass~ for this [fn:whatkey].

#+begin_src bash
pass git init $GPG_KEY # Optional, use a secure private git repo
pass init $GPG_KEY
pass add mail/rgoswami.iitk
#+end_src

With this, we can configure a general IMAP account (an IITK webmail in this instance, but it could be anything).

#+begin_src tcl
IMAPAccount rgoswami.iitk
Host qasid.iitk.ac.in
User rgoswami
PassCmd "pass show mail/rgoswami.iitk"
AuthMechs LOGIN
SSLType IMAPS

IMAPStore rgoswami.iitk-remote
Account rgoswami.iitk

MaildirStore rgoswami.iitk-local
SubFolders Verbatim
Path /run/media/Storage/.mail/rgoswami_iitk/
INBOX /run/media/Storage/.mail/rgoswami_iitk/Inbox
Trash trash:///

Channel rgoswami.iitk
Master :rgoswami.iitk-remote:
Slave :rgoswami.iitk-local:
Patterns *
Create Both
SyncState *
#+end_src

*** Exchange Accounts
For Exchange accounts we need to setup ~davmail~. Thankfully the process is actually excessively simple for a single account. For each account, a ~.properties~ file needs to be generated. The password section is kept blank in this case, since we will authenticate with the ~O365Interactive~ protocol.

#+begin_src bash
# The default setup
davmail ~/.davmail.properties
#+end_src

Use the following properties in the ~.davemail.properties~ file to prevent timeout errors:
#+begin_src bash
davmail.folderSizeLimit=50
davmail.clientSoTimeout=0
davmail.enableKeepAlive=true
#+end_src

Further information is available at [[http://davmail.sourceforge.net/faq.html][the official docs]].

#+DOWNLOADED: screenshot @ 2020-12-29 21:54:39
#+caption: ~davmail~ setup
[[file:images/InSync/2020-12-29_21-54-39_screenshot.png]]

#+DOWNLOADED: screenshot @ 2020-12-29 21:54:20
#+caption: Interactive login workflow
[[file:images/InSync/2020-12-29_21-54-20_screenshot.png]]

Now we can configure the ~.mbsyncrc~ in a manner analogous to the standard IMAP setup, but with the port and host where ~davmail~ is running.
#+begin_src tcl
IMAPAccount rog32
Host localhost
User rog32@hi.is
Pass dummy
Port 1143
SSLType None
AuthMechs LOGIN

IMAPStore rog32-remote
Account rog32
MaildirStore rog32-local
SubFolders Verbatim
Path /run/media/Storage/.mail/rog32/
Inbox /run/media/Storage/.mail/rog32/Inbox

Channel rog32
Master :rog32-remote:
Slave :rog32-local:
Patterns *
Create Both
SyncState *
#+end_src

The ~SSLType~ and ~AuthMechs~ are important parameters. Note that the ~Pass~ is truly not important since we an OAuth token is stored in the ~properties~ file.

#+DOWNLOADED: screenshot @ 2020-12-29 23:51:10
#+caption: First run of a large inbox with ~davmail~ and ~isync~
[[file:images/InSync/2020-12-29_23-51-10_screenshot.png]]

** Gmaileer
The general setup ~gmi init blah@gmail.com~ works well for personal accounts. However, there was an additional step for the IEEE account.
*** IEEE
Some issues with tags led to the following changes in the ~.gmaileer.json~ file
#+begin_src js
{"replace_slash_with_dot": false, "account": "rgoswami@ieee.org", "timeout": 600, "drop_non_existing_label": false, "ignore_empty_history": false, "ignore_tags": ["TODO","new"], "ignore_remote_labels": ["CATEGORY_SOCIAL", "CATEGORY_FORUMS", "CATEGORY_PROMOTIONS", "CATEGORY_PERSONAL", "CATEGORY_UPDATES"], "remove_local_messages": true, "file_extension": ""}
#+end_src
Essentially just the addition of two new ~ignore_tags~.
** Notmuch
At this point, we have all mail synced into local directories, but we have no access to view or interact with them. We will start by setting up ~notmuch~ to search and index our mail. This is pretty basic for now.
#+begin_src toml
[database]
path=/run/media/Storage/.mail/
[user]
name=Person
primary_email=primary@domain.com
other_email=one@domain.com;two@domain.com
[new]
tags=new;unread;inbox;TODO;
ignore=/.*[.](json|lock|bak)$/
[search]
exclude_tags=deleted;spam;
[maildir]
synchronize_flags=true
#+end_src

There are a lot more options which may be configured, but this is enough to get started.
** Sending Email
At this stage we need a way to actually send email. A simple configuration can be setup in ~/.config/msmtp/config~ is given below (where we use ~pass~ again):
#+begin_src tcl
defaults
port 587
tls on
auth on
logfile ~/.config/msmtp/.msmtp.log
tls_trust_file /etc/ssl/certs/ca-certificates.crt

account r95g10
host smtp.gmail.com
from r95g10@gmail.com
user r95g10@gmail.com
tls_starttls on
tls on
auth on
port 587
passwordeval pass show mail/r95g10.gmail

account rog32
host localhost
from rog32@hi.is
user rog32@hi.is
tls_starttls off
tls off
auth plain
port 1025
password dummy
#+end_src

Note that the exchange account again uses a dummy password, and the real password will be prompted for on the first run. The permissions of the file above should be ~600~. It is prudent to test at-least the exchange section to login.

#+begin_src bash
echo "Hello world" | msmtp --account=rog32 r95g10@gmail.com
#+end_src
** Astroid
The bulk of this setup is [[https://github.com/astroidmail/astroid/wiki/Astroid-setup][by the numbers]], with the exception of the poll script. However, minimally configure ~astroid~ with the location of our ~notmuch~ configuration.
#+begin_src bash
astroid --new-config
vim ~/.config/astroid/config
#+end_src

The relevant sections are:
#+begin_src json
"notmuch_config": "\/home\/whoever\/.notmuch-config",
#+end_src

The accounts section is fairly self explanatory, but we will need to use the following ~sendmail~ line as well:

#+begin_src json
            "sendmail": "msmtp --read-envelope-from -i -t",
#+end_src


*** Nix Python Poll Script
Natively only bash appears to be supported. However, with ~nix~, we can use a reproducible python script with [[https://amoffat.github.io/sh][the sh library]] to call system functions instead.
#+begin_src python
from pathlib import Path
import sh

# For generic IMAP maildirs

ISYNC_LABELS = ["rgoswami.iitk", "rog32"]

for isync in ISYNC_LABELS:
    sh.mbsync("-V",isync,_bg=True)


# Gmaileer
GMAIL_IDENTIFIERS = ["gmail", "univ", "ieee"]

path = Path(r"/run/media/haozeke/Storage/.mail/")

for dirs in path.iterdir():
    if dirs.is_dir():
        for gmi in GMAIL_IDENTIFIERS:
            if gmi in dirs.name:
                print(f"Syncing {dirs.name}")
                sh.gmi("sync", _cwd=dirs, _fg=True)
#+end_src
This needs to be coupled with the standard ~nix~ shebang in the ~poll.sh~ file:
#+begin_src bash
#!/usr/bin/env nix-shell
#!nix-shell -i python3 -p "python38.withPackages(ps: [ ps.numpy ps.sh ])"
#+end_src
*** Navigation
For working with HTML emails, we need to highlight the notice about potentially sketchy HTML using (defaults) ~j~ or ~k~ and then hit ~enter~ or ~o~.

#+DOWNLOADED: screenshot @ 2020-12-30 01:07:09
#+caption: Unhelpful default
[[file:images/Astroid/2020-12-30_01-07-09_screenshot.png]]


#+DOWNLOADED: screenshot @ 2020-12-30 01:08:45
#+attr_html: :width 600
#+caption: After viewing the sketchy bit
[[file:images/Astroid/2020-12-30_01-08-45_screenshot.png]]

Note that since most email is actually sent with ~text/html~ it might make more sense to configure the ~thread_view~ in the configuration file.

#+begin_src json :hl_lines 3
    "thread_view": {
        "open_html_part_external": "false",
        "preferred_type": "html",
        "preferred_html_only": "false",
        "allow_remote_when_encrypted": "false",
        "open_external_link": "xdg-open",
        "default_save_directory": "~",
        "indent_messages": "false",
        "gravatar": {
            "enable": "true"
        },
        "mark_unread_delay": "0.5",
        "expand_flagged": "true"
    },
#+end_src

**** Deletion
There are again, two different approaches to deletion.

- Gmail :: For ~gmail~ accounts it is simple, just adding a ~trash~ tag will do the trick, so we can select multiple emails with ~t~ and then hit ~+~ to add ~trash~ and everything works out
- Other Accounts :: We can *delete mail* forever (from the server as well), by using a safe-tag and then using ~notmuch~

The generic non-~lieer~ method requires:

#+begin_src bash
notmuch search --output=files --format=text0 tag:killed | xargs -r0 rm
#+end_src

A ~pre-new~ hook will work.
# ** Systemd Integration
# Albert Weichselbraun has [[https://semanticlab.net/desktop/e-mail/linux/sysadmin/Managing-DavMail-with-systemd-and-preventing-service-timeouts-after-network-reconnects/][an excellent setup]] for working with ~systemd~ and ~davmail~ which we will extend to cover the entire workflow using dispatcher scripts.

**** Composition
Thankfully, ~astroid~ supports GPG encryption as well as markdown support. This makes for simple integration with any popular editor.


#+DOWNLOADED: screenshot @ 2020-12-30 06:27:38
#+caption: Composition with ~emacs~ and ~astroid~
[[file:images/Astroid/2020-12-30_06-27-38_screenshot.png]]


** Conclusions
This is enough to get started for a while, but it isn't yet at the stage where I can replace ~thunderbird~ unfortunately. However, there are several pain points to be addressed, which will be covered in a future post. Some of these are essentially related to network fluctuations, and the awkward deletion setup.

*** Update
- ~astroid~ appears to be having a [[https://github.com/astroidmail/astroid/issues/669][bit of a development crisis]]
  + The [[https://github.com/gauteh/ragnarok][sequel (ragnarok)]] unfortunately seems to have gone towards a [[https://github.com/gauteh/ragnarok/tree/master/astroid][browser frontend]], which is quite unacceptable to me
    - This makes it far too similar to (in principle) Mailspring (though with less problematic connectivity issues)
    - This means I won't be moving forward with the next set of posts regarding ~astroid~ and will move towards ~emacs~ again
- [[https://mehl.mx/][Max Mehl]] has a [[https://src.mehl.mx/mxmehl/mail-config/src/branch/master/astroid/scripts][good collection of scripts]] for ~astroid~

[fn:whatkey] The GPG key itself can be stored with ~keybase~
* DONE Remapping Keys with XKB and KLFC :@personal:workflow:tools:
CLOSED: [2020-12-05 Sat 22:05]
:PROPERTIES:
:EXPORT_FILE_NAME: remap-keys-xkb-klfc
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
An introduction to hacking keyboard layouts with X keyboard extension (XKB) and ~klfc~, focused on Colemak and vim bindings
#+end_quote
** Background
Inspite of maximizing ergonomic bindings for most common software (e.g. Vimium, doom-emacs), every operation with the arrow keys still trouble me. Here I will lay out my experiments transitioning to a stable, uniquely defined setup with the X keyboard extension.
*** Series
This post is part of a series on Colemak and keyboard management in general.
1. [[Switching to Colemak][Switching to Colemak]]
2. [[Refactoring Dotfiles For Colemak][Refactoring Dotfiles For Colemak]] 
3. *Remapping Keys Globally and Persistently with XKB* <-- You are here!
** Keyboard Basics
Some terms to keep in mind for this post are[fn:morespecifics]:
- Dead Keys :: These don't actually output anything, but modify the next key pressed. Like applying an umlaut on the subsequent letter.
- Lock Keys :: State modifiers which are toggled, like Caps Lock
- Compose Key :: A key which interprets a series of subsequent key strokes. A dead key on steroids.

Also the different levels ([[https://fsymbols.com/keyboard/linux/choosers/#:~:text=Shift%20is%20called%20the%20second,on%20your%20usual%20keyboard%20layout.][from here]]) are concisely defined in the following table.

#+caption: Levels for a keyboard
| *Level* | *Modifier*   | *Keys*                                          |
|---------+--------------+-------------------------------------------------|
|       1 | None         | Lowercase letters, numbers other symbols        |
|       2 | Shift        | Uppercase letters, symbols placed above numbers |
|       3 | AltGr        | Accented characters, symbols, some dead keys    |
|       4 | Shift+AltGr  | More dead keys and symbols                      |
|       5 | Extend       | User-defined                                    |
|       6 | Shift+Extend | User-defined                                    |
|---------+--------------+-------------------------------------------------|

** Modification Strategies
Common approaches to quick remapping of keys involves ~xmodmap~, which does not persist between reboots. Manually recreating or spinning off of XKB configuration files was also not very appealing.
*** KLFC
A more elegant approach is by using the excellent [[https://github.com/39aldo39/klfc/][klfc Haskell binary]]. To install this from source:
#+begin_src bash :eval never
git clone https://github.com/39aldo39/klfc
cd klfc
# Kanged from the AUR https://aur.archlinux.org/packages/klfc/
cabal v1-sandbox init
cabal v1-update
cabal v1-install --only-dependencies --ghc-options=-dynamic --force-reinstalls
cabal v1-configure --prefix=/usr --ghc-options=-dynamic
cabal v1-build
#+end_src

The output binary is in ~./dist/build/klfc/klfc~.

*Note that* the set of keys mapped by the ~json~ files are relative to the QWERTY layout, that is:

#+DOWNLOADED: screenshot @ 2020-12-01 20:38:58
[[file:images/Modification_Strategies/2020-12-01_20-38-58_screenshot.png]]

This means that we have to ensure that the keys are mapped relative to QWERTY as well, *not* relative to the modified base layout.
** Remapping
Some goals were:
- Programming (particularly in ~python~ and ~lisp~) put a lot of stress on the right hand pinky for Colemak users[fn:whycolemak]
- VIM keys should be global but toggled with a lock
My primary use case is currently my ThinkPad X380, which comes with a basic [[http://www.keyboard-layout-editor.com/#/layouts/ae55dc3c6b6c80904d0901d856247486][QWERTY contracted layout]] as shown in Fig. [[fig:qwerty]].

#+name: fig:qwerty
#+caption: Basic X380 QWERTY
file:images/keyboards/qwerty_x380.png
*** Colemak - Layers 1 and 2
The first mapping is a [[http://www.keyboard-layout-editor.com/#/gists/93c31b6e61608c19c26933b0a212b262][basic Colemak setup]] as shown in Fi. fig:colemak.

#+name: fig:colemak
#+caption: Basic X380 Colemak
[[file:images/keyboards/colemak_x380.jpg]]

It wouldn't make much sense to remap the first two layers. We can use the ~json~ from the examples of the ~klfc~ repository.
#+begin_src json
// Base Colemak layout
// https://colemak.com
{
  "fullName": "Colemak",
  "name": "colemak",
  "localeId": "00000409",
  "copyright": "Public Domain",
  "company": "2006-01-01 Shai Coleman",
  "version": "1.0",
  "shiftlevels": [ "None", "Shift" ],
  "singletonKeys": [
    [ "CapsLock", "Backspace" ]
  ],
  "keys": [
    { "pos": "~", "letters": [ "`", "~" ] },
    { "pos": "1", "letters": [ "1", "!" ] },
    { "pos": "2", "letters": [ "2", "@" ] },
    { "pos": "3", "letters": [ "3", "#" ] },
    { "pos": "4", "letters": [ "4", "$" ] },
    { "pos": "5", "letters": [ "5", "%" ] },
    { "pos": "6", "letters": [ "6", "^" ] },
    { "pos": "7", "letters": [ "7", "&" ] },
    { "pos": "8", "letters": [ "8", "*" ] },
    { "pos": "9", "letters": [ "9", "(" ] },
    { "pos": "0", "letters": [ "0", ")" ] },
    { "pos": "-", "letters": [ "-", "_" ] },
    { "pos": "+", "letters": [ "=", "+" ] },
    { "pos": "Q", "letters": [ "q", "Q" ] },
    { "pos": "W", "letters": [ "w", "W" ] },
    { "pos": "E", "letters": [ "f", "F" ] },
    { "pos": "R", "letters": [ "p", "P" ] },
    { "pos": "T", "letters": [ "g", "G" ] },
    { "pos": "Y", "letters": [ "j", "J" ] },
    { "pos": "U", "letters": [ "l", "L" ] },
    { "pos": "I", "letters": [ "u", "U" ] },
    { "pos": "O", "letters": [ "y", "Y" ] },
    { "pos": "P", "letters": [ ";", ":" ] },
    { "pos": "[", "letters": [ "[", "{" ] },
    { "pos": "]", "letters": [ "]", "}" ] },
    { "pos": "\\", "letters": [ "\\", "|" ] },
    { "pos": "A", "letters": [ "a", "A" ] },
    { "pos": "S", "letters": [ "r", "R" ] },
    { "pos": "D", "letters": [ "s", "S" ] },
    { "pos": "F", "letters": [ "t", "T" ] },
    { "pos": "G", "letters": [ "d", "D" ] },
    { "pos": "H", "letters": [ "h", "H" ] },
    { "pos": "J", "letters": [ "n", "N" ] },
    { "pos": "K", "letters": [ "e", "E" ] },
    { "pos": "L", "letters": [ "i", "I" ] },
    { "pos": ";", "letters": [ "o", "O" ] },
    { "pos": "'", "letters": [ "'", "\"" ] },
    { "pos": "Z", "letters": [ "z", "Z" ] },
    { "pos": "X", "letters": [ "x", "X" ] },
    { "pos": "C", "letters": [ "c", "C" ] },
    { "pos": "V", "letters": [ "v", "V" ] },
    { "pos": "B", "letters": [ "b", "B" ] },
    { "pos": "N", "letters": [ "k", "K" ] },
    { "pos": "M", "letters": [ "m", "M" ] },
    { "pos": ",", "letters": [ ",", "<" ] },
    { "pos": ".", "letters": [ ".", ">" ] },
    { "pos": "/", "letters": [ "/", "?" ] }
  ],
  "variants": [
    {
      "name": "mod-dh",
      "shiftlevels": [ "None", "Shift" ],
      "keys": [
        { "pos": "V", "letters": [ "d", "D" ] },
        { "pos": "B", "letters": [ "v", "V" ] },
        { "pos": "G", "letters": [ "g", "G" ] },
        { "pos": "T", "letters": [ "b", "B" ] },
        { "pos": "H", "letters": [ "k", "K" ] },
        { "pos": "N", "letters": [ "m", "M" ] },
        { "pos": "M", "letters": [ "h", "H" ] }
      ]
    }
  ]
}
#+end_src
*** VIM Extensions
The additions are primarily through the Extend Layer[fn:whyext] (Fig. [[fig:extlayer]]), with a Shift addition (Fig. [[fig:extshift]]) and more keys with AltGr (Fig. [[fig:altgrlayer]]).
As mentioned before, we have to continue mapping relative to QWERTY, so these mappings can be used by QWERTY users as well. We will essentially use the ~ISO_5~ shift key.

#+name: fig:extlayer
#+caption: Extend layer mapping
[[file:images/keyboards/extendLayer_hz.jpg]]

- Maps basic ~vim~ movement

#+name: fig:extshift
#+caption: Extend+Shift layer mapping
[[file:images/keyboards/extShift_hz.jpg]]

- Relatively empty, just has a bracket

#+name: fig:altgrlayer
#+caption: Extend+AltGr layer mapping
[[file:images/keyboards/extAltGr_hz.png]]

- Includes deletions

These are defined in a single ~json~ as shown.
#+begin_src json
{
  "filter": "no klc,keylayout",
  "singletonKeys": [
    [ "CapsLock", "Extend" ],
    [ "Alt_L", "AltGr" ]
  ],
  "shiftlevels": [ "Extend", "Shift+Extend", "AltGr+Extend" ],
  "keys": [
    { "pos": "H", "letters": [ "Left", "", "Backspace" ] },
    { "pos": "J", "letters": [ "Down" ] },
    { "pos": "K", "letters": [ "Up" ] },
    { "pos": "L", "letters": [ "Right", "", "Delete" ] },
    { "pos": ";", "letters": [ "Enter" ] },
    { "pos": "N", "letters": [ "(", "[", "{" ] },
    { "pos": "V", "letters": [ ")", "]" , "}"] }
  ]
}
#+end_src
- The key idea is to have the ~AltGr~ keys placed symmetrically
- One major issue is that ~Backspace~ has gone from a single stroke of ~CapsLock~ to a three-key-combo
  + This is the least intuitive, and might need to be changed
- The third level (~Extend+AltGr~) is more accessible than the second in this layout
*** Usage
To generate the files needed to load the new layout:
#+begin_src bash :eval never
klfc colemak.json extendVIM.json -o coleVIM
cd coleVIM/xkb
./run-session.sh    # to try them out
./install-system.sh && ./scripts/install-xcompose.sh # to install them
#+end_src

** Conclusions
The layout takes a bit of time to get used to, but it is a lot more transparent in the end compared to manually remapping to Colemak's NEIO instead of HJKL for movement. It is both persistent and easily extended, though it is likely that more needs to be done. Perhaps some metrics [fn:whatmetrics] might be collected as well.

[fn:whycolemak] Colemak, unlike Dvorak, prioritises finger rolls over alternating hands
[fn:morespecifics] For more details the [[https://en.wikipedia.org/wiki/Keyboard_layout][Wikipedia article on Keyboard Layouts]] is useful
[fn:whyext] [[https://forum.colemak.com/topic/2014-extend-extra-extreme/][DreymaR's Extend mappings]] might be good for QWERTY people 
[fn:whatmetrics] The metric collection of [[https://github.com/mw8/white_keyboard_layout][Michael White]] or the [[http://mkweb.bcgsc.ca/carpalx/][CARPALX metrics]]

* DONE Private Github Actions without PAT
CLOSED: [2020-12-23 Wed 14:34]
:PROPERTIES:
:EXPORT_FILE_NAME: priv-gh-actions
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
A workflow for managing private submodules in a private repository without personal access tokens for Github actions
#+end_quote
** Background
Ever since Travis CI decided to drink [[https://blog.travis-ci.com/2020-11-02-travis-ci-new-billing][corporate kool-aid]], the search for a new CI has been brought to the fore again. [[https://github.com/features/actions][Github Actions]] seem uniquely suited for private repositories, since most CIs bill for private repos. However, the basic authentication setup for the [[https://github.com/actions/checkout][checkout action]] involves using one SSH key, effectively a personal access token, for both the [[https://github.com/actions/checkout/issues/116#issuecomment-708092052][main project and all submodules]]. This is untenable for anyone working with a team.
** Solution
The fix, as it were, is in two steps. We will first require a deploy key to be set for the private submodule, and then store the private portion in the private repo. We will begin with a concrete setup.
*** Setup
Consider a standard C++ build setup:
#+begin_src yaml
name: Fake secret project

on:
  push:
    branches: [master, development]
  pull_request:
    branches: [master]

jobs:
  build:
    runs-on: ${{ matrix.os }}

    strategy:
      max-parallel: 4
      matrix:
        os: [ubuntu-18.04]
        cpp-version: [g++-7, g++-9, clang++]

    steps:
      - uses: actions/checkout@v2
      - name: build
        env:
          CXX: ${{ matrix.cpp-version }}
        run: |
          mkdir build && cd build
          cmake -DCMAKE_CXX_COMPILER="$CXX" -DCMAKE_CXX_FLAGS="-std=c++11" ../
          make -j$(nproc)
      - name: run
        run: |
          ./super_secret
#+end_src
*** Key Generation
This section is standard. We will generate a deploy key essentially. Note that it isn't necessary to set a password, using one would only minimally improve security and bring in some annoying script modifications.
#+begin_src bash
# Anywhere safe
ssh-keygen -t rsa -b 4096 -C "Fake Deployment Key" -f 'priv_sub_a' -N ''
#+end_src
*** Private Submodule Repo
We will store the *public portion of the key* as a deploy key in the *private submodule repository*.
#+begin_src bash
# Copy contents
cat priv_sub_a.pub | xclip -selection clipboard
#+end_src

Note that, as shown in Fig. [[fig:privsub]] we do not need to give write access to this key.

#+name: fig:privsub
#+caption: Deploy key setup in the private submodule
#+DOWNLOADED: screenshot @ 2020-12-23 14:18:40
[[file:images/Solution/2020-12-23_14-18-40_screenshot.png]]

*** Private Project Repo
Now we will need the *private portion of the key* as a secret in the *private project repository* (see Fig. [[fig:privprj]]).
#+begin_src bash
# Copy contents of private key
cat priv_sub_a | xclip -selection clipboard
#+end_src

#+name: fig:privprj
#+DOWNLOADED: screenshot @ 2020-12-23 14:21:36
#+caption: Secret setup in the private project
[[file:images/Solution/2020-12-23_14-21-36_screenshot.png]]
*** Workflow Modifications
Now we will simply update our workflow [fn:givecredit]. We will simply add the following step:
#+begin_src yaml
      - name: get_subm
        env:
          SSHK: ${{ secrets.SUB_SSHK_A }}
        run: |
          mkdir -p $HOME/.ssh
          echo "$SSHK" > $HOME/.ssh/ssh.key
          chmod 600 $HOME/.ssh/ssh.key
          export GIT_SSH_COMMAND="ssh -i $HOME/.ssh/ssh.key"
          git submodule update --init --recursive
#+end_src
This will work for a single private submodule and multiple public submodules. For multiple private submodules, we would not initialize them recursively, but instead use a separate key for each.
#+begin_src yaml
      - name: get_subm_a
        env:
          SSHK: ${{ secrets.SUB_SSHK_A }}
        run: |
          mkdir -p $HOME/.ssh
          echo "$SSHK" > $HOME/.ssh/ssh.key
          chmod 600 $HOME/.ssh/ssh.key
          export GIT_SSH_COMMAND="ssh -i $HOME/.ssh/ssh.key"
          git submodule update --init -- <specific relative path to submodule A>
      - name: get_subm_b
        env:
          SSHK: ${{ secrets.SUB_SSHK_B }}
        run: |
          mkdir -p $HOME/.ssh
          echo "$SSHK" > $HOME/.ssh/ssh.key
          chmod 600 $HOME/.ssh/ssh.key
          export GIT_SSH_COMMAND="ssh -i $HOME/.ssh/ssh.key"
          git submodule update --init -- <specific relative path to submodule B>
#+end_src
Note that it is not possible to use the same SSH key in multiple submodule repositories, as each deploy key can only be associated with one repository.
*** Complete Workflow
Putting the above steps together, for the case of a single private submodule and multiple public submodules, we have:
#+begin_src yaml
name: Fake secret project

on:
  push:
    branches: [master, development]
  pull_request:
    branches: [master]

jobs:
  build:
    runs-on: ${{ matrix.os }}

    strategy:
      max-parallel: 4
      matrix:
        os: [ubuntu-18.04]
        cpp-version: [g++-7, g++-9, clang++]

    steps:
      - uses: actions/checkout@v2
      - name: get_subm
        env:
          SSHK: ${{ secrets.SUB_SSHK_A }}
        run: |
          mkdir -p $HOME/.ssh
          echo "$SSHK" > $HOME/.ssh/ssh.key
          chmod 600 $HOME/.ssh/ssh.key
          export GIT_SSH_COMMAND="ssh -i $HOME/.ssh/ssh.key"
          git submodule update --init --recursive
      - name: build
        env:
          CXX: ${{ matrix.cpp-version }}
        run: |
          mkdir build && cd build
          cmake -DCMAKE_CXX_COMPILER="$CXX" -DCMAKE_CXX_FLAGS="-std=c++11" ../
          make -j$(nproc)
      - name: run
        run: |
          ./super_secret
#+end_src
** Conclusions
We have demonstrated a minimally invasive setup for working with a private submodule, which is trivially extensible to multiple such submodules. With this, it appears that GH actions might a viable option (as opposed to say, [[https://app.wercker.com/][Wercker]]), at least for private teams. A more full comparative post might be warranted at a later date.

 [fn:givecredit] [[https://github.com/jwsi/submodule-checkout][submodule-checkout]] uses a similar concept but unfortunately does not extend to multiple submodules and the submodules are checked out as ~root~

* TODO My Life in E-ink :@personal:workflow:tools:
:PROPERTIES:
:EXPORT_FILE_NAME: my-life-in-eink
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
Collection of odds and ends relating to e-readers
#+end_quote
** Background
I have been reading on my Kobo Aura HD for almost a decade now, ever since its release. A lot of these can be adapted widely to other e-ink readers, and are written as such.

** Zotero Sync
Calibre provides a handy ZMI plugin which allows for exported papers to be imported into calibre and from then into the e-reader as expected.
- Best to have a separate library for these

* TODO Nix For Static Sites
* TODO Improving Astroid :@personal:rant:tools:email:workflow:
:PROPERTIES:
:EXPORT_FILE_NAME: improving-astroid
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+begin_quote
Adding encryption and scripts for better mail management
#+end_quote
** Background
** Conclusions
