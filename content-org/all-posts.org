#+author: Rohit Goswami

#+hugo_base_dir: ../
#+hugo_front_matter_format: yaml
#+bibliography: biblio/refs.bib

#+seq_todo: TODO DRAFT DONE
#+seq_todo: TEST__TODO | TEST__DONE

#+property: header-args :eval never-export

#+startup: indent

* About
:PROPERTIES:
:EXPORT_HUGO_SECTION: /
:EXPORT_FILE_NAME: about
:EXPORT_DATE: 1995-08-10
:END:

*Hi.*

I'm [[https://orcid.org/0000-0002-2393-8056][Rohit Goswami]], better known across the web as ~HaoZeke~. I'm not the
first of my name, which is why instead of ~rgoswami~, I occasionally use ~rg0swami~ when I need to be
identified by something closer to my name.

The actual username is a throwback to back when people *liked* being anonymous (and with multiple personalities)
online, so that ought to give an idea of how old I am. A curriculum vitae is
[[https://github.com/HaoZeke/CV/blob/master/RG_Latest-cv.pdf][available here]].

It is difficult to keep this section short and not let it spill
into an unstructured memoir. For a while I considered trying to consolidate my
online presences but that turned out to be completely impossible without a
series of posts and avatars[fn:notrefs].
** Intangible Positions
This is a set of things which are primarily online and/or voluntary in a
non-academic sense.
- I administer and design a bunch of websites, mostly verified [[https://keybase.io/HaoZeke][on Keybase]]
- I am a certified Software Carpentries [[https://static.carpentries.org/instructors#HaoZeke][instructor]]
- I [[https://static.carpentries.org/maintainers/#HaoZeke][officially maintain]], for the Software Carpentries, the lesson on R (r-novice-inflammation)
- I [[https://aur.archlinux.org/packages/?SeB=m&K=HaoZeke][also maintain]] some packages on the AUR (ArchLinux User Repository)
- I hone coursework development and teaching [[https://www.univ.ai/teams/rohit-goswami][with univ.ai]]
- I [[https://forum.xda-developers.com/xperia-z5/orig-development/cm-14-1-lineageos-t3536846][maintain(ed)]] the official LineageOS image for the Xperia Z5 Dual
** Historical Places
What follows is a more informal set of places I am or have been associated with or are of significance to
me[fn:growingUp].
*** Reykjavík
- I [[https://english.hi.is/staff/rog32][am associated]] with the [[https://notendur.hi.is/hj/researchgroup.html][reputed Jonsson group]] of the [[http://raunvisindastofnun.hi.is/the_science_institute][Science Institute]] at the
  [[https://english.hi.is/school_of_engineering_and_natural_sciences][University of Iceland]], where I benefit from the
  guidance of the erudite and inspiring [[https://notendur.hi.is/hj/indexE.html][Prof. Hannes Jonsson]]
- My doctoral committee is here, which includes the very excellent inputs of
  [[https://english.hi.is/staff/elvarorn][Dr. Elvar Jonsson]]
- I have also benefited from sitting in on some formal coursework here, which
  has been a fascinatingly useful experience
*** Kanpur
- I [[https://femtolab.science/people/rohit][retain a close association]] with the fantastic [[https://femtolab.science/][Femtolab]] at [[http://home.iitk.ac.in/~dgoswami/][IIT Kanpur]] under
  [[https://femtolab.science/people/dgoswami][Prof. Debabrata Goswami]], who has provided constant guidance throughout my career
- I am the co-lead developer of the FOSS scientific [[https://dseams.info][d-SEAMS software suite]] for
  [[https://wiki.dseams.info/#citation][graph theoretic approaches to structure determination]] of molecular dynamics
  simulations, along with my exceptional co-lead [[https://www.researchgate.net/profile/Amrita_Goswami2][Amrita Goswami]] of the CNS Lab
  under Prof. Jayant K. Singh at IITK
- I worked with the Nair group as part of the [[http://surge.iitk.ac.in/AnnualReport/report2017.pdf][Summer Undergraduate in Research Excellence]] (SURGE) program, also at IITK
- Harcourt Butler Technical Institute (HBTI) Kanpur, or the [[http://hbtu.ac.in/][Harcourt Butler Technological University]], as it is now called, was where I trained to be a chemical engineer
*** Bombay
- I [[https://rajarshichakrabarti.wixsite.com/rajarshichakrabarti/team][spent a formative summer]] under [[https://rajarshichakrabarti.wixsite.com/rajarshichakrabarti][Prof. Rajarshi Chakrabarti]] of the IIT Bombay
  Chemistry department, who has been instrumental in developing my interests
- I also spent some time discussing experiments with [[https://www.che.iitb.ac.in/online/faculty/rajdip-bandyopadhyaya][Prof. Rajdip Bandyopadhyaya]]
  of the IIT Bombay Chemical Engineering department during an industrial
  internship in fragnance compounding at the R&D department of KEVA Ltd. under
  [[https://in.linkedin.com/in/debojit-chakrabarty-b9a2262][Dr. Debojit Chakrabarty]]
*** Bangalore
- At IISc, I had the good fortune to meet Prof. Hannes Jonsson at a summer
  workshop [[https://chemeng.iisc.ac.in/rare-events/index.html][on Rare events]]
- At the [[http://bangaloreinternationalcentre.org/][BIC]], I undertook formal machine learning and artificial intelligence
  training under Harvard's [[https://www.extension.harvard.edu/faculty-directory/rahul-dave][Dr. Rahul Dave]] and [[https://iacs.seas.harvard.edu/people/pavlos-protopapas][Dr. Pavlos Protopapas]] as part of the [[https://univ.ai][univ.ai]]
  summer course
*** Chennai
- I spent a very fruitful summer on quantum tomography under [[https://www.imsc.res.in/~sibasish/qis.html][Prof. Sibashish Ghosh]] at the [[https://www.imsc.res.in/][Institute for Mathematical Sciences]] (IMSc Chennai)
** Avatars
I thought it might be of use to list a few of my more official visages. This is
mostly to ensure people do not confuse me with a Sasquatch[fn:notpersonal].
These mugshots are exactly that, mugshots for profile icons[fn:mountaintapir].

#+caption: A collage of mugshots, shuffled and not ordered by date to confuse people trying to kill me
[[file:images/avatarCollage.jpg]]

[fn:growingUp] I grew up on the verdant and beautiful [[https://www.tifr.res.in/][TIFR Mumbai]] campus, and
completed high school and undergraduate stuff while playing with peacocks and things on
the [[https://www.iitk.ac.in][IIT Kanpur]] campus
[fn:notrefs] I didn't think it would be necessary, but just in case it isn't
clear, people listed here are not necessarily all references or anything, this is
a personal list of people associated with each city, not a cover letter
[fn:notpersonal] This is not a replacement for [[https://www.instagram.com/rg0swami/][an Instagram feed]] or a [[https://www.facebook.com/rg0swami][Facebook
wall]], or even a [[https://www.researchgate.net/profile/Rohit_Goswami2][ResearchGate]] or [[https://publons.com/researcher/2911170/rohit-goswami/][Publons]] or [[https://orcid.org/0000-0002-2393-8056][ORCID]] page; all of which I do sporadically remember I have
[fn:mountaintapir] Made with the [[https://github.com/tttppp/mountain_tapir][Mountain Tapir Collage Maker]]
* Site Rationale
:PROPERTIES:
:EXPORT_FILE_NAME: rationale
:EXPORT_HUGO_CATEGORIES: projects
:EXPORT_HUGO_TAGS: ramblings explanations
:EXPORT_DATE: 2020-02-11 23:28
:END:
** Why this site exists
I have a lot of online presences. I have been around (or at-least, lurking) for
over ten years. Almost as long as I have been programming. Anyway, I have a
penchant lately for using ~emacs~ and honestly there isn't very good support for
~org-mode~ files. There are options recently with ~gatsby~ as well, but this
seemed kinda neat.
** What 'this' is
- This site is [[http://gohugo.io/][built by Hugo]]
- The posts are [[https://ox-hugo.scripter.co/][generated with ox-hugo]]
- The theme is based of this [[https://github.com/rhazdon/hugo-theme-hello-friend-ng][excellent one]] and my modifications [[https://github.com/HaoZeke/hugo-theme-hello-friend-ng-hz][are here]]
** What is here
- Mostly random thoughts I don't mind people knowing
- Some tech stuff which isn't coherent enough to be put in any form with
  references
- Emacs specific workflows which I might want to write about more than [[https://dotdoom.grimoire.science/][short
  notes on the config]]
** What isn't here
- More coherent thoughts will *not* be here, that should and will go to my [[https://grimoire.science][grimoire]]
- My [[https://dotdoom.grimoire.science/][doom-emacs configuration]]
- Academic stuff is better tracked on [[https://publons.com/researcher/2911170/rohit-goswami/][Publons]] or [[https://scholar.google.co.in/citations?user=36gIdJMAAAAJ&hl=en][Google Scholar]] or my pages
  hosted by my favorite [[https://femtolab.science/people/rohit][IITK group]] or [[https://www.hi.is/starfsfolk/rog32][UI group]]
* Taming Github Notifications
:PROPERTIES:
:EXPORT_FILE_NAME: ghNotif
:EXPORT_HUGO_CATEGORIES: notes
:EXPORT_HUGO_TAGS: tools github workflow
:EXPORT_DATE: 2020-02-12 11:36
:END:
** Background
As a member of several large organizations, I get a lot of github notifications.
Not all of these are of relevance to me. This is especially true of
~psuedo-monorepo~ style repositories like the [[https://github.com/openjournals/joss-reviews][JOSS review system]] and
*especially* the [[https://github.com/exercism/v3/][exercism community]].

- I recently (re-)joined the [[https://exercism.io/][exercism community]] as a maintainer for the C++
  lessons after having been a (sporadic) teacher
- This was largely in response to a community call to action as the group needed
  new blood to usher in *v3* of the exercism project

Anyway, I have since found that at the small cost of possibly much of my public
repo data, I can manage my notifications better with [[https://octobox.io/][Octobox]]

** Octobox
- It appears to be free for now
- It syncs on demand (useful)
- I can search things quite easily
- They have a neat logo
- There appear to be many features I probably won't use

It looks like this:

#+caption: Octobox Stock Photo
[[file:images/octoboxSample.png]]
* Poetry and Direnv
:PROPERTIES:
:EXPORT_FILE_NAME: poetry-direnv
:EXPORT_HUGO_CATEGORIES: programming
:EXPORT_HUGO_TAGS: tools direnv workflow python
:EXPORT_DATE: 2020-02-13 21:36
:END:
** Background
- I end up writing about using [[https://python-poetry.org/][poetry]] a lot
- I almost always [[https://direnv.net/][use direnv]] in real life too
- I don't keep writing mini scripts in my ~.envrc~

Honestly there's nothing here anyone using the [[https://github.com/direnv/direnv/wiki/Python][direnv wiki]] will find surprising,
but then it is still neat to link back to.

** Setting Up Poetry
This essentially works by simply modifying the global ~.direnvrc~ which
essentially gets sourced by every local ~.envrc~ anyway.
#+BEGIN_SRC sh
vim $HOME/.direnvrc
#+END_SRC
So what we put in there is the following snippet derived from other snippets [[https://github.com/direnv/direnv/wiki/Python][on
the wiki]], and is actually now there too.

#+BEGIN_SRC bash
# PUT this here
layout_poetry() {
  if [[ ! -f pyproject.toml ]]; then
    log_error 'No pyproject.toml found.  Use `poetry new` or `poetry init` to create one first.'
    exit 2
  fi

  local VENV=$(dirname $(poetry run which python))
  export VIRTUAL_ENV=$(echo "$VENV" | rev | cut -d'/' -f2- | rev)
  export POETRY_ACTIVE=1
  PATH_add "$VENV"
}
#+END_SRC

Now we can just make ~.envrc~ files with ~layout_poetry~ and everything will
/just work™/.

* Replacing Jupyter with Orgmode
:PROPERTIES:
:EXPORT_FILE_NAME: jupyter-orgmode
:EXPORT_HUGO_CATEGORIES: programming
:EXPORT_HUGO_TAGS: tools emacs workflow python
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:EXPORT_DATE: 2020-02-13 22:36
:END:
** Background
- I dislike Jupyter notebooks (and [[https://jupyter.org/][JupyterHub]]) a lot
- [[https://tkf.github.io/emacs-ipython-notebook/][EIN]] is really not much of a solution either

In the past I have written some posts on [[https://grimoire.science/latex-and-jupyterhub/][TeX with JupyterHub]] and discussed ways
to use virtual [[https://grimoire.science/python-and-jupyterhub/][Python with JupyterHub]] in a more reasonable manner.

However, I personally found that EIN was a huge pain to work with, and I mostly
ended up working with the web-interface anyway.

It is a bit redundant to do so, given that at-least for my purposes, the end
result was a LaTeX document. Breaking down the rest of my requirements went a
bit like this:

- What exports well to TeX? :: *Org*, Markdown, anything which goes into pandoc
- What displays code really well? :: LaTeX, Markdown, *Org*
- What allows easy visualization of code snippets? :: Rmarkdown, RStudio,
  JupyterHub, *Org* with babel

Clearly, [[https://orgmode.org/manual/][orgmode]] is the common denominator, and ergo, a perfect JupyterHub alternative.
** Setup
Throughout this post I will assume the following structure:
#+BEGIN_SRC bash :exports both
tree tmp
mkdir -p tmp/images
touch tmp/myFakeJupyter.org
#+END_SRC

#+RESULTS:
| tmp |                   |   |      |
| ├── | images            |   |      |
| └── | myFakeJupyter.org |   |      |
| 1   | directory,        | 1 | file |

As is evident, we have a folder ~tmp~ which will have all the things we need for
dealing with our setup.

*** Virtual Python
Without waxing too eloquent on the whole reason behind doing this, since I will
rant about virtual python management systems elsewhere, here I will simply
describe my preferred method, which is [[https://python-poetry.org/][using poetry]].

#+BEGIN_SRC bash
# In a folder above tmp
poetry init
poetry add numpy matplotlib scipy pandas
#+END_SRC

The next part is optional, but a good idea if you figure out [[https://direnv.net/][using direnv]] and
have configured ~layout_poetry~ as [[https://rgoswami.me/posts/poetry-direnv][described here]]:
#+BEGIN_SRC bash
# Same place as the poetry files
echo "layout_poetry()" >> .envrc
#+END_SRC

*Note:*
- We can nest an arbitrary number of the ~tmp~ structures under a single place
  we define the poetry setup
- I prefer using ~direnv~ to ensure that I never forget to hook into the right environment
** Orgmode
This is not an introduction to org, however in particular, there are some basic
settings to keep in mind to make sure the set-up works as expected.

*** Indentation
Python is notoriously weird about whitespace, so we will ensure that our export
process does not mangle whitespace and offend the python interpreter. We will
have the following line at the top of our ~orgmode~ file:

#+BEGIN_SRC orgmode :tangle tmp/myFakeJupyter.org :exports code
# -*- org-src-preserve-indentation: t; org-edit-src-content: 0; -*-
#+END_SRC

*Note:*
- this post is actually generating the file being discussed here by
[[https://orgmode.org/manual/Extracting-Source-Code.html][tangling the file]]
- You can get the [[https://github.com/HaoZeke/haozeke.github.io/blob/src/content-org/tmp/myFakeJupyter.org][whole file here]]
*** TeX Settings
These are also basically optional, but at the very least you will need the
following:

#+BEGIN_SRC orgmode :tangle tmp/myFakeJupyter.org
#+author: Rohit Goswami
#+title: Whatever
#+subtitle: Wittier line about whatever
#+date: \today
#+OPTIONS: toc:nil
#+END_SRC

I actually use a lot of math using the ~TeX~ input mode in Emacs, so I like the
following settings for math:

#+BEGIN_SRC orgmode :tangle tmp/myFakeJupyter.org
# For math display
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{unicode-math}
#+END_SRC

There are a bunch of other settings which may be used, but these are the bare
minimum, more on that would be in a snippet anyway.

*Note:*
- rendering math in the ~orgmode~ file in this manner requires that we
 use ~XeTeX~ to compile the final file
*** Org-Python
We essentially need to ensure that:
- Babel uses our virtual python
- The same session is used for each block

We will get our poetry python pretty easily:
#+BEGIN_SRC bash
which python
#+END_SRC

#+RESULTS:
: /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python

Now we will use this as a common ~header-arg~ passed into the property drawer to
make sure we don't need to set them in every code block.

We can use the following structure in our file:

#+BEGIN_SRC orgmode :tangle tmp/myFakeJupyter.org :exports code
\* Python Stuff
  :PROPERTIES:
  :header-args:    :python /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python :session One :results output :exports both
  :END:
Now we can simply work with code as we normally would
\#+BEGIN_SRC python
print("Hello World")
\#+END_SRC
#+END_SRC

*Note:*
- For some reason, this property needs to be set on *every* heading (as of Feb 13 2020)
- In the actual file you will want to remove extraneous  \ symbols:
  - \* → *
  - \#+BEGIN_SRC → #+BEGIN_SRC
  - \#+END_SRC → #+END_SRC
*** Python Images and Orgmode
To view images in ~orgmode~ as we would in a JupyterLab notebook, we will use a
slight trick.
- We will ensure that the code block returns a file object with the arguments
- The code block should end with a print statement to actually generate the file
  name

 So we want a code block like this:

#+begin_example
#+BEGIN_SRC python :results output file :exports both
import matplotlib.pyplot as plt
from sklearn.datasets.samples_generator import make_circles
X, y = make_circles(100, factor=.1, noise=.1)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plt.xlabel('x1')
plt.ylabel('x2')
plt.savefig('images/plotCircles.png', dpi = 300)
print('images/plotCircles.png') # return filename to org-mode
#+end_src
#+end_example

Which would give the following when executed:

#+begin_example
#+RESULTS:
[[file:images/plotCircles.png]]
#+end_example

Since that looks pretty ugly, this will actually look like this:

#+BEGIN_SRC python :results output file :exports both
import matplotlib.pyplot as plt
from sklearn.datasets.samples_generator import make_circles
X, y = make_circles(100, factor=.1, noise=.1)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plt.xlabel('x1')
plt.ylabel('x2')
plt.savefig('images/plotCircles.png', dpi = 300)
print('images/plotCircles.png') # return filename to org-mode
#+end_src

[[file:tmp/images/plotCircles.png]]

*** Bonus
A better way to simulate standard ~jupyter~ workflows is to just specify the
properties once at the beginning.

#+BEGIN_SRC orgmode
#+PROPERTY: header-args:python :python /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python :session One :results output :exports both
#+END_SRC

This setup circumvents having to set the properties per sub-tree, though for
very large projects, it is useful to use different processes.
** Conclusions
- The last step is of course to export the file as to a ~TeX~ file and then
  compile that with something like ~latexmk -pdfxe -shell-escape file.tex~

There are a million and one variations of this of course, but this is enough to
get started.

The whole file is also [[https://github.com/HaoZeke/haozeke.github.io/blob/src/content-org/tmp/myFakeJupyter.org][reproduced here]].
* TODO Orgmode and Hugo
:PROPERTIES:
:EXPORT_FILE_NAME: hugo-orgmode
:EXPORT_HUGO_CATEGORIES: projects
:EXPORT_HUGO_TAGS: tools emacs webdev hugo
# :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:EXPORT_DATE: 2020-02-14 05:57
:END:
** Background
- This is about the site you are reading
- It is also a partial rant
- It has a lot to do with web development in general
* DONE Switching to Colemak
:PROPERTIES:
:EXPORT_FILE_NAME: colemak-switch
:EXPORT_HUGO_CATEGORIES: notes
:EXPORT_HUGO_TAGS: workflow programming personal
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:EXPORT_DATE: 2020-02-29 14:06
:END:
** Background
I just realized that it has been over two years since I switched from QWERTY to
COLEMAK but somehow never managed to write about it. It was a major change in my
life, and it took forever to get acclimatized to. I do not think I'll ever again be
in a position to make such a change in my life again, but it was definitely
worth it.
** Touch Typing
My interest in touch typing in I decided to digitize my notes for posterity, during the
last two years of my undergraduate studies back in Harcourt Butler Technical
Institute (HBTI) Kanpur, India. in one of my many instances of yak shaving, I
realized I could probably consume and annotate a lot more content by typing
faster. Given that at that stage I was already a fast talker, it seemed like a
natural extension. There was probably an element of nostalgia involved as well.
That and the end of a bachelors involves the thesis, which generally involves a
lot of typing.

There were (and are) some fantastic resources for learning to touch type
nowadays, I personally used:
- [[https://www.typing.com/][Typing.com]] :: This is short, but a pretty good basic setup. The numbering and
  special characters are a bit much to take in at the level of practice you get
  by completing all the previous exercises, but eventually they make for a good workout.
- [[https://www.typingclub.com/en-gb/login/][TypingClub]] :: This is what I ended up working my way through. It is
  comprehensive, beautiful, and fun.

Also, later, I ended up using [[https://www.keybr.com/][keybr]] a lot, simply because typing gibberish is a
good way of practicing, and it is independent of the keyboard layout.

Just to foreshadow things, the enemy facing me at this point was the layout
itself[fn:img] .

https://www.keyboard-design.com/kb-images/qwerty-kla.jpg

** Alternate layouts
Having finally broken into the giddy regimes of 150+ wpm, I was ecstatic, and
decided to start working my way through some longer reports. However, I quickly
realized I was unable to type for more than a couple of minutes without getting
terribly cramped. Once it got to the point of having to visit a physiotherapist,
I had to call it quits. At that stage, relearning the entire touch typing
corpus, given that I already was used to QWERTY, seemed pretty bleak.

It took forever, and I ended up applying my choices to my phone keyboard as
well, which presumably helped me in terms of increasing familiarity, had the
unintended effect of making me seem distant to people I was close to, since my
verbose texts suddenly devolved to painful one-liners.

The alternative layouts I tried were:

- [[https://www.dvorak-keyboard.com/][DVORAK]] :: At the time, TypingClub only supported QWERTY and DVORAK, so it was
  pretty natural for me to try it out. There are also some [[https://www.dvzine.org/][very nice comics
  about it]]. I remember that it was pretty neat, with
  a good even distribution, until I tried coding. The placement of the
  semicolons make it impossible to use while programming. I would still say it
  makes for a comfortable layout, as long as special characters are not required.

https://www.keyboard-design.com/kb-images/dvorak-kla.jpg

- [[http://mkweb.bcgsc.ca/carpalx][CarpalX]] :: I experimented with the entire carpalx family, but I was unable to get
  used to it. I liked QFMLWY best. I do recommend reading the training methodology, especially if
  anyone is interested in numerical optimization in general. More importantly,
  though it was relatively easy to set up on my devices and operating systems,
  the fact that it wasn't natively supported meant a lot of grief whenever I
  inevitably had to use a public computer.

https://www.keyboard-design.com/kb-images/qgmlwy-kla.jpg

- Colemak :: Eventually I decided to go with [[https://colemak.com/][Colemak]], especially since it is
  widely available. Nothing is easier than ~setxkbmap us -variant colemak -option grp:alt_shift_toggle~ on public machines and it's easy on Windows as
  well. Colemak seems like a good compromise. I personally have not been able to
  reach the same speeds I managed with QWERTY, even after a year, but then
  again, I can be a lot more consistent, and it hurts less. Nowadays, Colemak
  has made its way onto most typing sites as well, including TypingClub

https://www.keyboard-design.com/kb-images/colemak-kla.jpg

*** What about VIM?
- DVORAK makes it impossible, so do most other layouts, but there are some
  tutorials purporting to help use vim movement with DVORAK
- Colemak isn't any better, but the fact of the matter is that once you know VIM
  on QWERTY, and have separately internalized colemak or something else, hitting
  keys is just hitting keys

All that said, I still occasionally simply remap HJKL (QWERTY movement) to HNEI
(Colemak analog) when it is feasible.
** Conclusion
Changing layouts was a real struggle. Watching my WPM drop back to lower than
hunt and peck styles was pretty humiliating, especially since the reports kept
coming in, and more than once I switched to QWERTY. However, since then, I have
managed to stay on course. I guess if I think about it, it boils down to a few
scattered thoughts:
- Typing is kinda like running a marathon, knowing how it is done and doing it
  are two different things
- Tell *everyone*, so people can listen to you lament your reduced speed and not
  hate you for replying slowly
- Practice everyday, because, well, it works out in the long run, even when you
  plateau
- Alternate shifts! That's really something which should show up more in
  tutorials, especially for listicles, not changing the shifts will really hurt
- Try and get a mechanical keyboard (like the [[https://www.annepro.net/][Anne Pro 2]] or the [[https://www.coolermaster.com/catalog/peripheral/keyboards/masterkeys-pro-l-white/][Coolermaster Masterkeys]]), they're fun and easy to change layouts on

[fn:img] The images are [[https://www.keyboard-design.com/best-keyboard-layouts.html][from here]], where there's also an effort based metric
used to score keyboard layouts.
* TODO Replacing Rstudio with Emacs
:PROPERTIES:
:EXPORT_FILE_NAME: rstudio-emacs
:EXPORT_HUGO_CATEGORIES: programming
:EXPORT_HUGO_TAGS: tools emacs workflow R
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:EXPORT_DATE: 2020-02-15 04:38
:END:
** Background
RStudio is one of the best IDEs around, in that it is essentially a text editor
and terminal with some pretty printing and object viewing functionality. It is
really great, but it is also relatively resource intensive. It turns out that
thanks to Emacs ESS, it is possible to circumvent Rstudio completely in favor of
an Emacs-native workflow.
* TODO Role models and colleges
* TODO My current courses
* TODO Rude college admissions
* Pandora and Proxychains
:PROPERTIES:
:EXPORT_FILE_NAME: pandora-proxychains
:EXPORT_HUGO_CATEGORIES: personal
:EXPORT_HUGO_TAGS: tools workflow
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :comments true
:EXPORT_DATE: 2020-02-15 05:28
:END:
** Background
- Pandora doesn't work outside the states
- I keep forgetting how to set-up ~proxychains~
** Proxychains
Technically this article [[https://github.com/rofl0r/proxychains-ng][expects proxychains-ng]], which seems to be the more
up-to-date fork of the original ~proxychains~.

1. Install ~proxychains-ng~
   #+BEGIN_SRC bash
# I am on archlinux..
sudo pacman -S proxychains-ng
   #+END_SRC
2. Copy the configuration to the ~$HOME~ directory
   #+BEGIN_SRC bash
cp /etc/proxychains.conf .
   #+END_SRC
3. Edit said configuration to add some US-based proxy

In my particular case, I don't keep the tor section enabled.
#+BEGIN_SRC bash :exports both :results raw
tail $HOME/proxychains.conf
#+END_SRC

#+RESULTS:
#+begin_example
#
#       proxy types: http, socks4, socks5
#        ( auth types supported: "basic"-http  "user/pass"-socks )
#
[ProxyList]
# add proxy here ...
# meanwile
# defaults set to "tor"
# socks4 	127.0.0.1 9050
#+end_example

I actually use [[https://windscribe.com][Windscribe]] for my VPN needs, and they have a neat [[https://windscribe.com/getconfig/socks][SOCKS5 proxy
setup]]. This works out to a line like ~socks5 $IP $PORT $USERNAME $PASS~ being
added. The default generator gives you a pretty server name, but to get the IP
I use ~ping $SERVER~ and put that in the ~conf~ file.
** Pandora
I use the excellent ~pianobar~ frontend.
1. Get [[https://github.com/PromyLOPh/pianobar][pianobar]]
   #+BEGIN_SRC bash
sudo pacman -S pianobar
   #+END_SRC
2. Use it with ~proxychains~
   #+BEGIN_SRC bash
proxychains pianobar
   #+END_SRC
3. Profit

I also like setting up some defaults to make life easier:
#+BEGIN_SRC bash
mkdir -p ~/.config/pianobar
vim ~/.config/pianobar/config
#+END_SRC
I normally set the following (inspired by the [[https://wiki.archlinux.org/index.php/Pianobar][ArchWiki]]):
#+BEGIN_SRC conf
audio_quality = {high, medium, low}
autostart_station = $ID
password = "$PASS"
user = "$emailID"
#+END_SRC

The ~autostart_station ID~ can be obtained by inspecting the terminal output
during an initial run. I usually set it to the QuickMix station.
* DONE Bojack Horseman
:PROPERTIES:
:EXPORT_FILE_NAME: bojack-horseman
:EXPORT_HUGO_CATEGORIES: personal
:EXPORT_HUGO_TAGS: thoughts random review TV
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :comments false
:EXPORT_DATE: 2020-02-27 22:28
:END:
** Background
For a while I was worried about writing about a TV show here. I thought it might
be frivolous, or worse, might outweigh the other kinds of articles I would like
to write. However, like most things, that which is ignored just grows, so it is
easier to just write and forget about it.
** The Show
Much has been said about how Bojack Horseman is one of the best shows ever, and
they're all correct. For that matter I won't be going into the details of how
every episode ties together a tapestry of lives in a meaningful way, or any of
that. The show was amazingly poignant. The characters felt real. Which actually
leads me to the real issue.
** The End
The end of Bojack was *good*. It was the way it was meant to be. For a
slice-of-life show, it is a natural conclusion. It isn't necessary that any
catharsis occurs or that the characters change or become better or all that
jazz. It isn't about giving the viewers closure. It is simply about a window
onto the lives of (fictional) characters being shut. To that end, I disliked
attempts to bring closure in the show itself.

One of the main reasons why I felt strongly enough to write this, is simply
because when I looked around, the prevailing opinion was that the main character
should have been killed off, _for his sins_. This strikes me as a very flippant
attitude to take. It reeks of people trying to make the show a cautionary tale,
which is frankly speaking a weird approach to take towards any fictional story.
The idea that the character should be redeemed also seemed equally weak, for
much the same reasons.

The fact that the characters are hypocrites, and that none of them are as good
or bad as they make themselves out to be is one of the best parts of the show.

** Conclusion
That's actually all I have to say about this. I thought of adding relevant memes
or listing episodes or name dropping sites, but this isn't buzzfeed. The show is
incredible, and there are far better ways of proving that. Bust out your
favorite search engine + streaming content provider / digital piracy eye-patch
and give it a whirl. The only thing I'd suggest is watching everything in order,
it's just that kind of show.

* TODO The Morpho Language
:PROPERTIES:
:EXPORT_FILE_NAME: morpho-lang
:EXPORT_HUGO_CATEGORIES: programming
:EXPORT_HUGO_TAGS: programming review
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:EXPORT_DATE: 2020-02-29 17:06
:END:
* TODO Towards DOOM-Emacs
:PROPERTIES:
:EXPORT_FILE_NAME: towards-doom-emacs
:EXPORT_HUGO_CATEGORIES: personal
:EXPORT_HUGO_TAGS: programming workflow review
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments false
:EXPORT_DATE: 2020-02-29 17:06
:END:
** Background
[[https://dotdoom.grimoire.science/][My doom-emacs configuration]] gets a rather insane number of views every month.
Statistically, it accounts for 90% of the traffic to [[https://grimoire.science][my other site]], and that is
essentially around three times time traffic on the rest of my presences,
combined. I followed a pretty standard path to finally reach doom-emacs.
However, before delving into it, I thought I'd discuss the chronological aspects
of my road to doom. In a nutshell it was just:

Word → Notepad++ → Sublime Text 3 → VIM → Emacs (Spacemacs) → Emacs (doom-emacs)
* DONE Provisioning Dotfiles on an HPC
:PROPERTIES:
:EXPORT_FILE_NAME: prov-dots
:EXPORT_HUGO_CATEGORIES: programming
:EXPORT_HUGO_TAGS: programming workflow projects hpc
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:EXPORT_DATE: 2020-03-16 00:06
:END:
** Background
[[https://github.com/HaoZeke/Dotfiles][My dotfiles]] turned 4 years old a few months ago (since 9th Jan 2017) and remains one of my most
frequently updated projects for obvious reasons. Going through the changes
reminds me of a whole of posts I never got around to writing.

Anyway, recently I gained access to another HPC cluster, with a standard configuration
(bash, old CentOS) and decided to track my provisioning steps. This is really a
very streamlined experience by now, since I've used the same setup across scores
of machines. This is actually also a generic intro to configuring user setups on
HPC (high performance cluster) machines, if one is inclined to read it in that
manner. To that end, sections of this post involve restrictions relating to user
privileges which aren't normally part of most Dotfile setups.
*** Aside
- Dotfiles define most people who maintain them
- No two sets are ever exactly alike
- They fall somewhere between winging it for each machine and using something
  like [[https://www.habitat.sh/learn/][Chef]] or [[https://www.ansible.com/][Ansible]]
- Tracking dotfiles is really close to having a sort of out-of-context journal

Before I settled on using [[https://github.com/kobus-v-schoor/dotgit][the fabulous dotgit]], I considered several
alternatives, most notably [[https://www.gnu.org/software/stow/][GNU stow]].
** Preliminaries
It is important to note the environment into which I had to get my
setup.
*** SSH Setup
- The very first thing to do is to use a new ~ssh-key~
#+BEGIN_SRC bash
export myKey="someName"
ssh-keygen -f $HOME/.ssh/$myKey
# I normally don't set a password
ssh-add $HOME/.ssh/$myKey
ssh-copy-id $myHPC
# myHPC being an IP address
#+END_SRC
I more often than not tend to back this up with a cutesy alias, also because I
do not always get my username of choice on these machines. So in
~$HOME/.ssh/config~ I use:
#+BEGIN_SRC conf
Host myHPC
 Hostname 127.0.0.1
 User somethingIgot
 IdentityFile ~/.ssh/myKey
#+END_SRC
*** Harvesting Information
- I normally use [[https://github.com/dylanaraps/neofetch][neofetch]] on new machines
#+BEGIN_SRC bash
mkdir -p $HOME/Git/Github
cd $HOME/Git/Github
git clone https://github.com/dylanaraps/neofetch.git
cd neofetch
./neofetch
#+END_SRC

#+caption: Neofetch Output
[[file:images/sampleHPC.png]]

Where the top has been tastefully truncated. Just for context, the latest ~bash~
as of this writing is ~v5.0.16~ so, that's not too bad, given that ~neofetch~
works for ~bash~ ≥ 3.2

** TODO Circumventing User Restrictions with Nix
- A post in and of itself would be required to explain why and how users are
  normally restricted from activities in cluster nodes
- Here, we leverage the [[https://nixos.org/nix/manual/#chap-installation][nix-package management system]] to circumvent these
- User installation of ~nix~ is sadly non-trivial, so this might be of some use [fn:nixUsr]
*** Testing nix-user-chroot
1. We will first check namespace support
#+BEGIN_SRC bash
# Errored out
unshare --user --pid echo YES
# Worked!
zgrep CONFIG_USER_NS /boot/config-$(uname -r)
# CONFIG_USER_NS=y
#+END_SRC

Thankfully we have support for namespaces, so we can continue with ~nix-user-chroot~.

2. Since we definitely do not have ~rustup~ or ~rustc~ on the HPC, we will use [[https://github.com/nix-community/nix-user-chroot/releases][a
   prebuilt binary]] of ~nix-user-chroot~

#+BEGIN_SRC bash
cd $HOME && wget -O nix-user-chroot  https://github.com/nix-community/nix-user-chroot/releases/download/1.0.2/nix-user-chroot-bin-1.0.2-x86_64-unknown-linux-musl
#+END_SRC

3. Similar to [[https://nixos.wiki/wiki/Nix_Installation_Guide#Installing_without_root_permissions][the wiki example]], we will use ~$HOME/.nix~

#+BEGIN_SRC bash
cd ~/
chmod +x nix-user-chroot
mkdir -m 0755 ~/.nix
./nix-user-chroot ~/.nix bash -c 'curl https://nixos.org/nix/install | sh'
#+END_SRC

- Only, this *doesn't work*

Turns out that since ~unshare~ is too old, ~nix-user-chroot~ won't work either.

*** Using PRoot
PRoot is pretty neat in general, they even have a [[https://proot-me.github.io/][nice website describing it]].
0. Set a folder up for local installations (this is normally done by my
   Dotfiles, but we might as well have one here too)
#+BEGIN_SRC bash
mkdir -p $HOME/.local/bin
export PATH=$PATH:$HOME/.local/bin
#+END_SRC
1. Get a binary from the [[https://gitlab.com/proot/proot/-/jobs][GitLab artifacts]]
#+BEGIN_SRC bash
cd $HOME
mkdir tmp
cd tmp
wget -O artifacts.zip https://gitlab.com/proot/proot/-/jobs/452350181/artifacts/download
unzip artifacts.zip
mv dist/proot $HOME/.local/bin
#+END_SRC
2. Bind and install ~nix~
#+BEGIN_SRC bash
mkdir ~/.nix
export PROOT_NO_SECCOMP=1
proot -b ~/.nix:/nix
export PROOT_NO_SECCOMP=1
curl https://nixos.org/nix/install | sh
#+END_SRC

If you're very unlucky, like I was, you may be greeted by a lovely little error
message along the lines of:

#+begin_example
/nix/store/ddmmzn4ggz1f66lwxjy64n89864yj9w9-nix-2.3.3/bin/nix-store: /opt/ohpc/pub/compiler/gcc/5.4.0/lib64/libstdc++.so.6: version `GLIBCXX_3.4.22' not found (required by /nix/store/c0b76xh2za9r9r4b0g3iv4x2lkw1zzcn-aws-sdk-cpp-1.7.90/lib/libaws-cpp-sdk-core.so)
#+end_example

Which basically is as bad as it sounds. At this stage, we need a newer compiler
to even get ~nix~ up and running, but can't without getting an OS update. This
chicken and egg situation calls for the drastic measure of leveraging ~brew~
first[fn:brewStuff].

#+BEGIN_SRC bash
sh -c "$(curl -fsSL https://raw.githubusercontent.com/Linuxbrew/install/master/install.sh)"
#+END_SRC

Note that nothing in this section suggests the best way is not to lobby your
sys-admin to install ~nix~ system-wide in multi-user mode.
** Giving Up with Linuxbrew
- Somewhere around this point, [[https://docs.brew.sh/Homebrew-on-Linux][linuxbrew]] is a good idea
- More on this later
** Shell Stuff
~zsh~ is my shell of choice, and is what my ~Dotfiles~ expect and work best with.
- I did end up making a quick change to update the ~dotfiles~ with a target
  which includes a snippet to transition to ~zsh~ from the default ~bash~ shell
** Dotfiles
The actual installation steps basically tracks [[https://github.com/HaoZeke/Dotfiles][the readme instructions]].

#+BEGIN_SRC bash
git clone https://github.com/kobus-v-schoor/dotgit.git
mkdir -p ~/.bin
cp -r dotgit/bin/dotgit* ~/.bin
cat dotgit/bin/bash_completion >> ~/.bash_completion
rm -rf dotgit
# echo 'export PATH="$PATH:$HOME/.bin"' >> ~/.bashrc
echo 'export PATH="$PATH:$HOME/.bin"' >> ~/.zshrc
#+END_SRC

[fn:nixUsr] Much of this section is directly adapted from [[https://nixos.wiki/wiki/Nix_Installation_Guide#Installing_without_root_permissions][the NixOS wiki]]
[fn:brewStuff] This used to be called linuxbrew, but the [[https://docs.brew.sh/Homebrew-on-Linux][new site]] makes it clear
that it's all one ~brew~ now.
* Shorter Posts
:PROPERTIES:
:EXPORT_FILE_NAME: shortpost
:EXPORT_HUGO_CATEGORIES: notes
:EXPORT_HUGO_TAGS: tools rationale workflow ideas
:EXPORT_DATE: 2020-03-16 00:16
:END:
** Background
Sometime this year, I realized that I no longer have access to a lot of my older
communication. This included, a lot of resources I enjoyed and shared with the
people who were around me at that point in time. To counter this, I have decided
to opt for shorter posts, even if they don't always include the same level of
detail I would prefer to provide.

*** Alternatives
- I have an automated system based around IFTTT combined with Twitter, Diigo,
  and even Pocket
- However, that doesn't really tell me much, and trawling through a massive glut
  of data is often pointless as well
- There's always Twitter, but I don't really care to hear the views of others
  when I want to revisit my own ideas
** Conclusions
- I will be making shorter posts here, like the random one on [[https://rgoswami.me/posts/ghnotif/][octobox]]
* D3 for Git
:PROPERTIES:
:EXPORT_FILE_NAME: d3git
:EXPORT_HUGO_CATEGORIES: notes
:EXPORT_HUGO_TAGS: tools rationale workflow ideas
:EXPORT_DATE: 2020-03-16 00:17
:END:
** Background
- I have had a lot of discussions regarding the teaching of ~git~
- This is mostly as a part of [[https://static.carpentries.org/maintainers/#HaoZeke][the SoftwareCarpentries]], or in view of my
  [[https://www.univ.ai/teams/rohit-goswami][involvement with univ.ai]], or simply in every public space I am associated with
- Without getting into my views, I just wanted to keep this resource in mind
** The site
- Learning ~git~ is a highly contentious thing
- People seem to be fond of GUI tools, especially since on non *nix systems, it
  seems that there is a lot of debate surrounding obtaining the ~git~ utility in
  the first place

One of the best ways of understanding (without installing stuff) the mental
models required for working with ~git~ is [[https://onlywei.github.io/explain-git-with-d3/#checkout][this site]]

#+caption: A screenshot of the site
[[file:images/d3git.png]]

- However, as is clear, this is not exactly a replacement for a good old command-line.

- It does make for a good resource for teaching with slides, or for generating
  other static visualizations, where live coding is not an option
* DONE Trees and Bags
:PROPERTIES:
:EXPORT_FILE_NAME: trees-and-bags
:EXPORT_HUGO_CATEGORIES: notes
:EXPORT_HUGO_TAGS: theory statistics
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:EXPORT_HUGO_CUSTOM_FRONT_MATTER+: :link-citations true
:EXPORT_HUGO_PANDOC_CITATIONS: t
:EXPORT_DATE: 2020-03-26 00:28
:END:
# :EXPORT_HUGO_CUSTOM_FRONT_MATTER+: :nocite '(@hastieElementsStatisticalLearning2009)

#+BEGIN_QUOTE
  Explain why using bagging for prediction trees generally improves
  predictions over regular prediction trees.
#+END_QUOTE


** Introduction

Bagging (or Bootstrap Aggregation) is one of the most commonly used
ensemble method for improving the prediction of trees. We will broadly
follow a historical development trend to understand the process. That
is, we will begin by considering the Bootstrap method. This in turn
requires knowledge of the Jacknife method, which is understandable from
a simple bias variance perspective. Finally we will close out the
discussion by considering the utility and trade-offs of the Bagging
technique, and will draw attention to the fact that the Bagging method
was contrasted to another popular ensemble method, namely the Random
Forest method, in the previous section.

Before delving into the mathematics, recall that the approach taken by
bagging is essentially (@cichoszDataMiningAlgorithms2015):

- create base models with *bootstrap* samples of the training set
- combine models by unweighted voting (for classification) or by
  averaging (for regression)

The reason for covering the Jacknife method is to develop an intuition
relating to the sampling of data described in the following table:

| Data-set   Size  per   sample | Estimator         |
| Reduces                       | Jacknife          |
| Remains    the   same         | Bootstrap         |
| Increases                     | data-augmentation |

** Bias Variance Trade-offs

We will recall, for this discussion, the bias variance trade off which
is the basis of our model accuracy estimates (for regression) as per the
formulation of @jamesIntroductionStatisticalLearning2013.

\begin{equation}
E(y₀-\hat{f}(x₀))²=\mathrm{Var}(\hat{f}(x₀))+[\mathrm{Bias(\hat{f(x₀)})}]²+\mathrm{Var}(ε)
\end{equation}

Where:

- $E(y_{0}-\hat{f}(x_{0}))²$ is the expected test MSE, or the average
  test MSE if $f$ is estimated with a large number of training sets and
  tested at each $x₀$
- The variance is the amount by which our approximation $\hat{f}$ will
  change if estimated by a different training set, or the *flexibility*
  error
- The bias is the (reducible) *approximation* error, caused by not
  fitting to the training set exactly
- $\mathrm{Var}(ε)$ is the *irreducible* error

We will also keep in mind, going forward the following requirements of a
good estimator:

- Low variance AND low bias
- Typically, the variance increases while the bias decreases as we use
  more flexible methods (i.e. methods which fit the training set
  better[fn:smooth])

Also for the rest of this section, we will need to recall from
@hastieElementsStatisticalLearning2009, that the bias is given by:

\begin{equation}
[E(\hat{f_{k}}(x₀)-f(x₀)]²
\end{equation}

Where the expectation averages over the randomness in the training data.

To keep things in perspective, recall from
@hastieElementsStatisticalLearning2009:

#+CAPTION: Test and training error as a function of model complexity
[[file:images/biasVar.png]]

** Jacknife Estimates
    :PROPERTIES:
    :CUSTOM_ID: jacknife-estimates
    :END:

We will model our discussion on the work of
@efronJackknifeBootstrapOther1982. Note that:

- The $\hat{θ}$ symbol is an estimate of the true quantity $θ$
- This is defined by the estimate being $\hat{θ}=θ(\hat{F})$
- $\hat{F}$ is the empirical probability distribution, defined by mass
  $1/n$ at $xᵢ ∀ i∈I$, i is from 1 to n

The points above establishes our bias to be given by
$E_Fθ(\hat{F})-θ(F)$ such that $E_F$ is the expectation under x₁⋯xₙ~F.

To derive the Jacknife estimate $(\tilde{θ})$ we will simply
sequentially delete points xᵢ (changing $\hat{F}$), and recompute our
estimate $\hat{θ}$, which then simplifies to:

\begin{equation}
\tilde{θ}\equiv n\hat{θ}-(\frac{n-1}{n})∑_{i=1}ⁿ\hat{θ}
\end{equation}

In essence, the Jacknife estimate is obtained by making repeated
estimates on increasingly smaller data-sets. This intuition lets us
imagine a method which actually makes estimates on larger data-sets
(which is the motivation for data augmentation) or, perhaps not so
intuitively, on estimates on data-sets of the same size.

** Bootstrap Estimates

Continuing with the same notation, we will note that the bootstrap is
obtained by draw random data-sets with replacement from the training
data, where each sample is the same size as the original
(@hastieElementsStatisticalLearning2009).

We will consider the bootstrap estimate for the standard deviation of
the $\hat{θ}$ operator, which is denoted by $σ(F,n,\hat{\theta})=σ(F)$

The bootstrap is simple the standard deviation at the approximate F,
i.e., at $F=\hat{F}$:

\begin{equation}
\hat{\mathrm{SD}}=\sigma(\hat{F})
\end{equation}

Since we generally have no closed form analytical form for $σ(F)$ we
must use a Monte Carlo algorithm:

1. Fit a non parametric maximum likelihood estimate (MLE) of F,
   i.e. $\hat{F}$
2. Draw a sample from $\hat{F}$ and calculate the estimate of $\hat{θ}$
   on that sample, say, $\hat{θ}^*$
3. Repeat 2 to get multiple (say B) replications of $\hat{θ}^*$

Now we know that as $B→∞$ then our estimate would match $σ(\hat{F})$
perfectly, however, since that itself is an estimate of the value we are
actually interested in, in practice there is no real point using a very
high B value.

Note that in actual practice we simply use the given training data with
repetition and do not actually use an MLE of the approximate true
distribution to generate samples. This causes the bootstrap estimate to
be unreasonably good, since there is always significant overlap between
the training and test samples during the model fit. This is why cross
validation demands non-overlapping data partitions.

*** Connecting Estimates

The somewhat surprising result can be proved when $\hat{θ}=θ(\hat{F}$ is
a quadratic functional, namely:

\begin{equation}\hat{\mathrm{Bias}}_{boot}=\frac{n-1}{n} \hat{\mathrm{Bias}}_{jack}\end{equation}

In practice however, we will simply recall that the Jacknife tends to
overestimate, and the Bootstrap tends to underestimation.

** Bagging

Bagging, is motivated by using the bootstrap methodology to improve the
estimate or prediction directly, instead of using it as a method to
asses the accuracy of an estimate. It is a representative of the
so-called parallel ensemble methods where the base learners are
generated in parallel. As such, the motivation is to reduce the error by
exploiting the independence of base learners (true for mathematically
exact bootstrap samples, but not really true in practice).

Mathematically the formulation of @hastieElementsStatisticalLearning2009
establishes a connection between the Bayesian understanding of the
bootstrap mean as a posterior average, however, here we will use a more
heuristic approach.

We have noted above that the bagging process simply involves looking at
different samples in differing orders. This has some stark repercussions
for tree-based methods, since the trees are grown with a /greedy/
approach.

- Bootstrap samples may cause different trees to be produced
- This causes a reduction in the *variance*, especially when not too
  many samples are considered
- Averaging, reduces variance while leaving bias unchanged

Practically, these separate trees being averaged allows for varying
importance values of the variables to be calculated.

In particular, following @hastieElementsStatisticalLearning2009, it is
possible to see that the MSE tends to decrease by bagging.

\begin{align}
 E_P[Y-\hat{f}^*(x)]² & = & E_P[Y-f*{ag}(x)+f^*_{ag}(x)-\hat{f}^*(x)]² \\
& = & E_P[Y-f^*_{ag}(x)]²+E_P[\hat{f}^*(x)-f^*_{ag}(x)]² ≥ E_P[Y-f^*_{ag}(x)]²
\end{align}

Where:

- The training observations are independently drawn from a distribution
  $P$
- $f_{ag}(x)=E_P\hat{f}^*(x)$ is the ideal aggregate estimator

For the formulation above, we assume that $f_{ag}$ is a true bagging
estimate, which draws samples from the actual population. The upper
bound is obtained from the variance of the $\hat{f}^*(x)$ around the
mean, $f_{ag}$

Practically, we should note the following:

- The regression trees are deep
- The greedy algorithm growing the trees cause them to be unstable
  (sensitive to changes in input data)
- Each tree has a high variance, and low bias
- Averaging these trees reduces the variance

Missing from the discussion above is how exactly the training and test
sets are used in a bagging algorithm, as well as an estimate for the
error for each base learner. This has been reported in the code above as
the OOB error, or out of bag error. We have, as noted by
@zhouEnsembleMethodsFoundations2012 and @breimanBaggingPredictors1996
the following considerations.

- Given $m$ training samples, the probability that the iᵗʰ sample is
  selected 0,1,2... times is approximately Poisson distributed with
  $λ=1$
- The probability of the iᵗʰ example will occur at least once is then
  $1-(1/e)≈0.632$
- This means for each base learner, there are around $36.8$ % original
  training samples which have not been used in its training process

The goodness can thus be estimated using these OOB error, which is
simply an estimate of the error of the base tree on the OOB samples.

As a final note, random forests are conceptually easily understood by combining
bagging with subspace sampling, which is why in most cases and packages, we used
bagging as a special case of random forests, i.e. when no subspace sampling is
performed, random forests algorithms perform bagging.


[fn:smooth] This is mostly true for reasonably smooth true functions
* TODO d-SEAMS got published
