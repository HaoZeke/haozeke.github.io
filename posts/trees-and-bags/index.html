<!doctype html><html lang=en itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content="[Rohit Goswami]"><meta name=description content="Explain why using bagging for prediction trees generally improves predictions over regular prediction trees.
Introduction Bagging (or Bootstrap Aggregation) is one of the most commonly used ensemble method for improving the prediction of trees. We will broadly follow a historical development trend to understand the process. That is, we will begin by considering the Bootstrap method. This in turn requires knowledge of the Jacknife method, which is understandable from a simple bias variance perspective."><meta name=keywords content="personal,theory,statistics,math"><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://rgoswami.me/posts/trees-and-bags/><meta name=generator content="Wooframework"><title>Trees and Bags :: Rohit Goswami ‚Äî Reflections</title><link href=https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css rel=stylesheet type=text/css><link rel=stylesheet href=/main.min.6388ac937eea2d898585c5534e2073aa9c7bc7cb0b7dc90ffc488291d123cd42.css><meta itemprop=name content="Trees and Bags"><meta itemprop=description content="Explain why using bagging for prediction trees generally improves predictions over regular prediction trees.
Introduction Bagging (or Bootstrap Aggregation) is one of the most commonly used ensemble method for improving the prediction of trees. We will broadly follow a historical development trend to understand the process. That is, we will begin by considering the Bootstrap method. This in turn requires knowledge of the Jacknife method, which is understandable from a simple bias variance perspective."><meta itemprop=datePublished content="2020-03-26T00:28:00+00:00"><meta itemprop=dateModified content="2020-03-26T00:28:00+00:00"><meta itemprop=wordCount content="1485"><meta itemprop=image content="https://rgoswami.me/images/RG.png"><meta itemprop=keywords content="theory,statistics,math,"><meta property="og:title" content="Trees and Bags"><meta property="og:description" content="Explain why using bagging for prediction trees generally improves predictions over regular prediction trees.
Introduction Bagging (or Bootstrap Aggregation) is one of the most commonly used ensemble method for improving the prediction of trees. We will broadly follow a historical development trend to understand the process. That is, we will begin by considering the Bootstrap method. This in turn requires knowledge of the Jacknife method, which is understandable from a simple bias variance perspective."><meta property="og:type" content="article"><meta property="og:url" content="https://rgoswami.me/posts/trees-and-bags/"><meta property="og:image" content="https://rgoswami.me/images/RG.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-03-26T00:28:00+00:00"><meta property="article:modified_time" content="2020-03-26T00:28:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rgoswami.me/images/RG.png"><meta name=twitter:title content="Trees and Bags"><meta name=twitter:description content="Explain why using bagging for prediction trees generally improves predictions over regular prediction trees.
Introduction Bagging (or Bootstrap Aggregation) is one of the most commonly used ensemble method for improving the prediction of trees. We will broadly follow a historical development trend to understand the process. That is, we will begin by considering the Bootstrap method. This in turn requires knowledge of the Jacknife method, which is understandable from a simple bias variance perspective."><meta property="article:section" content="notes"><meta property="article:published_time" content="2020-03-26 00:28:00 +0000 UTC"></head><body class=dark-theme><div class=container><header class=header><div class=header__inner><div class=header__left><div class=logo><a href=/ style=text-decoration:none><img src=/images/home.png alt></a></div></div><div class=header__mid><button class=button><div class=button__text><a href=https://rgoswami.me//search>Search</a></div></button></div><div class=header__right><nav class=menu><ul class=menu__inner><li><a href=/about>About</a></li><li><a href=/posts>Blog</a></li></ul><ul class=menu__inner><li><a href=/categories>Categories</a></li><li><a href=/tags>Tags</a></li></ul></nav><button id=toggleMenu><div class=menu-trigger><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></div></button>
<button class=theme-toggle id=toggleTheme><svg class="theme-toggler" width="32" height="32" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></button></div></div></header><script src=/js/mathjax-config.js></script>
<script id=MathJax-script async src=https://unpkg.com/mathjax@3/es5/tex-chtml.js></script><aside class=sidebar><div class=hideTOC><button id=tocTog>TOC</button><div class="sideTOC m-fadeOut"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#bias-variance-trade-offs>Bias Variance Trade-offs</a></li><li><a href=#jacknife-estimates>Jacknife Estimates</a></li><li><a href=#bootstrap-estimates>Bootstrap Estimates</a><ul><li><a href=#connecting-estimates>Connecting Estimates</a></li></ul></li><li><a href=#bagging>Bagging</a></li><li><a href=#references>References</a></li></ul></nav></div></div></aside><main class=post><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>7 minutes<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>Written: 2020-03-26 00:28 +0000</p></p></div><article class=h-entry><h1 class="post-title p-name"><a class=u-url href=https://rgoswami.me/posts/trees-and-bags/>Trees and Bags</a></h1><div class=post-info><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-pen-tool"><path d="M12 19l7-7 3 3-7 7-3-3z"/><path d="M18 13l-1.5-7.5L2 2l3.5 14.5L13 18l5-5z"/><path d="M2 2l7.586 7.586"/><circle cx="11" cy="11" r="2"/></svg><span class="author u-author p-author"><a href=/author/rohit-goswami>Rohit Goswami</a></span></div><hr><aside id=toc class=sidebar><div class=toc-title>Table of Contents</div><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#bias-variance-trade-offs>Bias Variance Trade-offs</a></li><li><a href=#jacknife-estimates>Jacknife Estimates</a></li><li><a href=#bootstrap-estimates>Bootstrap Estimates</a><ul><li><a href=#connecting-estimates>Connecting Estimates</a></li></ul></li><li><a href=#bagging>Bagging</a></li><li><a href=#references>References</a></li></ul></nav></aside><hr><div class="post-content e-content"><blockquote><p>Explain why using bagging for prediction trees generally improves
predictions over regular prediction trees.</p></blockquote><h2 id=introduction>Introduction</h2><p>Bagging (or Bootstrap Aggregation) is one of the most commonly used
ensemble method for improving the prediction of trees. We will broadly
follow a historical development trend to understand the process. That
is, we will begin by considering the Bootstrap method. This in turn
requires knowledge of the Jacknife method, which is understandable from
a simple bias variance perspective. Finally we will close out the
discussion by considering the utility and trade-offs of the Bagging
technique, and will draw attention to the fact that the Bagging method
was contrasted to another popular ensemble method, namely the Random
Forest method, in the previous section.</p><p>Before delving into the mathematics, recall that the approach taken by
bagging is given as per Cichosz
(<a href=#ref-cichoszDataMiningAlgorithms2015>2015</a>) to be:</p><ul><li>create base models with <strong>bootstrap</strong> samples of the training set</li><li>combine models by unweighted voting (for classification) or by
averaging (for regression)</li></ul><p>The reason for covering the Jacknife method is to develop an intuition
relating to the sampling of data described in the following table:</p><table><thead><tr><th>Data-set Size per sample</th><th>Estimator</th></tr></thead><tbody><tr><td>Reduces</td><td>Jacknife</td></tr><tr><td>Remains the same</td><td>Bootstrap</td></tr><tr><td>Increases</td><td>data-augmentation</td></tr></tbody></table><h2 id=bias-variance-trade-offs>Bias Variance Trade-offs</h2><p>We will recall, for this discussion, the bias variance trade off which
is the basis of our model accuracy estimates (for regression) as per the
formulation of James et al.
(<a href=#ref-jamesIntroductionStatisticalLearning2013>2013</a>).</p><p>\begin{equation}
E(y‚ÇÄ-\hat{f}(x‚ÇÄ))¬≤=\mathrm{Var}(\hat{f}(x‚ÇÄ))+[\mathrm{Bias(\hat{f(x‚ÇÄ)})}]¬≤+\mathrm{Var}(Œµ)
\end{equation}</p><p>Where:</p><ul><li>\(E(y_{0}-\hat{f}(x_{0}))¬≤\) is the expected test MSE, or the
average test MSE if \(f\) is estimated with a large number of
training sets and tested at each \(x‚ÇÄ\)</li><li>The variance is the amount by which our approximation \(\hat{f}\)
will change if estimated by a different training set, or the
<strong>flexibility</strong> error</li><li>The bias is the (reducible) <strong>approximation</strong> error, caused by not
fitting to the training set exactly</li><li>\(\mathrm{Var}(Œµ)\) is the <strong>irreducible</strong> error</li></ul><p>We will also keep in mind, going forward the following requirements of a
good estimator:</p><ul><li>Low variance AND low bias</li><li>Typically, the variance increases while the bias decreases as we use
more flexible methods (i.e.¬†methods which fit the training set
better<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>)</li></ul><p>Also for the rest of this section, we will need to recall from Hastie,
Tibshirani, and Friedman
(<a href=#ref-hastieElementsStatisticalLearning2009>2009</a>), that the bias is
given by:</p><p>\begin{equation}
[E(\hat{f_{k}}(x‚ÇÄ)-f(x‚ÇÄ)]¬≤
\end{equation}</p><p>Where the expectation averages over the randomness in the training data.</p><p>To keep things in perspective, recall from Hastie, Tibshirani, and
Friedman (<a href=#ref-hastieElementsStatisticalLearning2009>2009</a>):</p><figure><img src=/ox-hugo/biasVar.png alt="Figure 1: Test and training error as a function of model complexity"><figcaption><p><span class=figure-number>Figure 1: </span>Test and training error as a function of model complexity</p></figcaption></figure><h2 id=jacknife-estimates>Jacknife Estimates</h2><p>We will model our discussion on the work of Efron
(<a href=#ref-efronJackknifeBootstrapOther1982>1982</a>). Note that:</p><ul><li>The \(\hat{Œ∏}\) symbol is an estimate of the true quantity \(Œ∏\)</li><li>This is defined by the estimate being \(\hat{Œ∏}=Œ∏(\hat{F})\)</li><li>\(\hat{F}\) is the empirical probability distribution, defined by
mass \(1/n\) at \(x·µ¢ ‚àÄ i‚ààI\), i is from 1 to n</li></ul><p>The points above establishes our bias to be given by
\(E_FŒ∏(\hat{F})-Œ∏(F)\) such that \(E_F\) is the expectation under
x‚ÇÅ‚ãØx‚Çô~F.</p><p>To derive the Jacknife estimate \((\tilde{Œ∏})\) we will simply
sequentially delete points x·µ¢ (changing \(\hat{F}\)), and recompute
our estimate \(\hat{Œ∏}\), which then simplifies to:</p><p>\begin{equation}
\tilde{Œ∏}\equiv n\hat{Œ∏}-(\frac{n-1}{n})‚àë_{i=1}‚Åø\hat{Œ∏}
\end{equation}</p><p>In essence, the Jacknife estimate is obtained by making repeated
estimates on increasingly smaller data-sets. This intuition lets us
imagine a method which actually makes estimates on larger data-sets
(which is the motivation for data augmentation) or, perhaps not so
intuitively, on estimates on data-sets of the same size.</p><h2 id=bootstrap-estimates>Bootstrap Estimates</h2><p>Continuing with the same notation, we will note that the bootstrap is
obtained by draw random data-sets with replacement from the training
data, where each sample is the same size as the original; as noted by
Hastie, Tibshirani, and Friedman
(<a href=#ref-hastieElementsStatisticalLearning2009>2009</a>).</p><p>We will consider the bootstrap estimate for the standard deviation of
the \(\hat{Œ∏}\) operator, which is denoted by
\(œÉ(F,n,\hat{\theta})=œÉ(F)\)</p><p>The bootstrap is simple the standard deviation at the approximate F,
i.e., at \(F=\hat{F}\):</p><p>\begin{equation}
\hat{\mathrm{SD}}=\sigma(\hat{F})
\end{equation}</p><p>Since we generally have no closed form analytical form for \(œÉ(F)\) we
must use a Monte Carlo algorithm:</p><ol><li>Fit a non parametric maximum likelihood estimate (MLE) of F,
i.e.¬†\(\hat{F}\)</li><li>Draw a sample from \(\hat{F}\) and calculate the estimate of
\(\hat{Œ∏}\) on that sample, say, \(\hat{Œ∏}^*\)</li><li>Repeat 2 to get multiple (say B) replications of \(\hat{Œ∏}^*\)</li></ol><p>Now we know that as \(B‚Üí‚àû\) then our estimate would match
\(œÉ(\hat{F})\) perfectly, however, since that itself is an estimate of
the value we are actually interested in, in practice there is no real
point using a very high B value.</p><p>Note that in actual practice we simply use the given training data with
repetition and do not actually use an MLE of the approximate true
distribution to generate samples. This causes the bootstrap estimate to
be unreasonably good, since there is always significant overlap between
the training and test samples during the model fit. This is why cross
validation demands non-overlapping data partitions.</p><h3 id=connecting-estimates>Connecting Estimates</h3><p>The somewhat surprising result can be proved when
\(\hat{Œ∏}=Œ∏(\hat{F}\) is a quadratic functional, namely:</p><p>\begin{equation}\hat{\mathrm{Bias}}_{boot}=\frac{n-1}{n} \hat{\mathrm{Bias}}_{jack}\end{equation}</p><p>In practice however, we will simply recall that the Jacknife tends to
overestimate, and the Bootstrap tends to underestimation.</p><h2 id=bagging>Bagging</h2><p>Bagging, is motivated by using the bootstrap methodology to improve the
estimate or prediction directly, instead of using it as a method to
asses the accuracy of an estimate. It is a representative of the
so-called parallel ensemble methods where the base learners are
generated in parallel. As such, the motivation is to reduce the error by
exploiting the independence of base learners (true for mathematically
exact bootstrap samples, but not really true in practice).</p><p>Mathematically the formulation of Hastie, Tibshirani, and Friedman
(<a href=#ref-hastieElementsStatisticalLearning2009>2009</a>) establishes a
connection between the Bayesian understanding of the bootstrap mean as a
posterior average, however, here we will use a more heuristic approach.</p><p>We have noted above that the bagging process simply involves looking at
different samples in differing orders. This has some stark repercussions
for tree-based methods, since the trees are grown with a <em>greedy</em>
approach.</p><ul><li>Bootstrap samples may cause different trees to be produced</li><li>This causes a reduction in the <strong>variance</strong>, especially when not too
many samples are considered</li><li>Averaging, reduces variance while leaving bias unchanged</li></ul><p>Practically, these separate trees being averaged allows for varying
importance values of the variables to be calculated.</p><p>In particular, following Hastie, Tibshirani, and Friedman
(<a href=#ref-hastieElementsStatisticalLearning2009>2009</a>), it is possible to
see that the MSE tends to decrease by bagging.</p><p>\begin{align}
E_P[Y-\hat{f}^*(x)]¬≤ & = & E_P[Y-f*{ag}(x)+f^*_{ag}(x)-\hat{f}^*(x)]¬≤ \\
& = & E_P[Y-f^*_{ag}(x)]¬≤+E_P[\hat{f}^*(x)-f^*_{ag}(x)]¬≤ ‚â• E_P[Y-f^*_{ag}(x)]¬≤
\end{align}</p><p>Where:</p><ul><li>The training observations are independently drawn from a
distribution \(P\)</li><li>\(f_{ag}(x)=E_P\hat{f}^*(x)\) is the ideal aggregate estimator</li></ul><p>For the formulation above, we assume that \(f_{ag}\) is a true
bagging estimate, which draws samples from the actual population. The
upper bound is obtained from the variance of the \(\hat{f}^*(x)\)
around the mean, \(f_{ag}\)</p><p>Practically, we should note the following:</p><ul><li>The regression trees are deep</li><li>The greedy algorithm growing the trees cause them to be unstable
(sensitive to changes in input data)</li><li>Each tree has a high variance, and low bias</li><li>Averaging these trees reduces the variance</li></ul><p>Missing from the discussion above is how exactly the training and test
sets are used in a bagging algorithm, as well as an estimate for the
error for each base learner. This has been reported in the code above as
the OOB error, or out of bag error. We have, as noted by Zhou
(<a href=#ref-zhouEnsembleMethodsFoundations2012>2012</a>) and Breiman
(<a href=#ref-breimanBaggingPredictors1996>1996</a>) the following
considerations.</p><ul><li>Given \(m\) training samples, the probability that the i·µó ∞ sample
is selected 0,1,2&mldr; times is approximately Poisson distributed with
\(Œª=1\)</li><li>The probability of the i·µó ∞ example will occur at least once is then
\(1-(1/e)‚âà0.632\)</li><li>This means for each base learner, there are around \(36.8\) %
original training samples which have not been used in its training
process</li></ul><p>The goodness can thus be estimated using these OOB error, which is
simply an estimate of the error of the base tree on the OOB samples.</p><p>As a final note, random forests are conceptually easily understood by
combining bagging with subspace sampling, which is why in most cases and
packages, we used bagging as a special case of random forests, i.e.¬†when
no subspace sampling is performed, random forests algorithms perform
bagging.</p><h2 id=references>References</h2><div id=refs class="references csl-bib-body hanging-indent"><div id=ref-breimanBaggingPredictors1996 class=csl-entry><p>Breiman, Leo. 1996. &ldquo;Bagging Predictors.&rdquo; <em>Machine Learning</em> 24 (2):
123&ndash;40. <a href=https://doi.org/10.1023/A:1018054314350>https://doi.org/10.1023/A:1018054314350</a>.</p></div><div id=ref-cichoszDataMiningAlgorithms2015 class=csl-entry><p>Cichosz, Pawel. 2015. <em>Data Mining Algorithms: Explained Using R</em>.
Chichester, West Sussex ; Malden, MA: John Wiley & Sons Inc.</p></div><div id=ref-efronJackknifeBootstrapOther1982 class=csl-entry><p>Efron, Bradley. 1982. <em>The Jackknife, the Bootstrap, and Other
Resampling Plans</em>. CBMS-NSF Regional Conference Series in Applied
Mathematics 38. Philadelphia, Pa: Society for Industrial and Applied
Mathematics.</p></div><div id=ref-hastieElementsStatisticalLearning2009 class=csl-entry><p>Hastie, Trevor, Robert Tibshirani, and J. H. Friedman. 2009. <em>The
Elements of Statistical Learning: Data Mining, Inference, and
Prediction</em>. 2nd ed. Springer Series in Statistics. New York, NY:
Springer.</p></div><div id=ref-jamesIntroductionStatisticalLearning2013 class=csl-entry><p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.
2013. <em>An Introduction to Statistical Learning</em>. Vol. 103. Springer
Texts in Statistics. New York, NY: Springer New York.
<a href=https://doi.org/10.1007/978-1-4614-7138-7>https://doi.org/10.1007/978-1-4614-7138-7</a>.</p></div><div id=ref-zhouEnsembleMethodsFoundations2012 class=csl-entry><p>Zhou, Zhi-Hua. 2012. <em>Ensemble Methods: Foundations and Algorithms</em>. 0th
ed. Chapman and Hall/CRC. <a href=https://doi.org/10.1201/b12207>https://doi.org/10.1201/b12207</a>.</p></div></div><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>This is mostly true for reasonably smooth true functions&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></article><hr><div class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg><span class=tag><a href=/tags/theory>theory</a></span><span class=tag><a href=/tags/statistics>statistics</a></span><span class=tag><a href=/tags/math>math</a></span></p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/><polyline points="10 9 9 9 8 9"/></svg>1485 Words</p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>Posted: <time class=dt-published>2020-03-26 00:28 +0000</time></p><a class=resp-sharing-button__link href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frgoswami.me%2fposts%2ftrees-and-bags%2f" target=_blank rel=noopener aria-label=Facebook><div class="resp-sharing-button resp-sharing-button--facebook resp-sharing-button--medium"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--circle"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><circle cx="12" cy="12" r="11.5"/><path d="M15.84 9.5H13.5V8.48c0-.53.35-.65.6-.65h1.4v-2.3h-2.35c-2.3.0-2.65 1.7-2.65 2.8V9.5h-2v2h2v7h3v-7h2.1l.24-2z"/></svg></div>Facebook</div></a><a class=resp-sharing-button__link href="https://twitter.com/intent/tweet/?text=%22Trees%20and%20Bags%22%20seems%20like%20an%20interesting%20read%20from%20%40rg0swami&amp;url=https%3a%2f%2frgoswami.me%2fposts%2ftrees-and-bags%2f" target=_blank rel=noopener aria-label=Twitter><div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--medium"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--circle"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18.5 7.4l-2 .2c-.4-.5-1-.8-2-.8C13.3 6.8 12 8 12 9.4v.6c-2 0-4-1-5.4-2.7-.2.4-.3.8-.3 1.3.0 1 .4 1.7 1.2 2.2-.5.0-1 0-1.2-.3.0 1.3 1 2.3 2 2.6-.3.4-.7.4-1 0 .2 1.4 1.2 2 2.3 2-1 1-2.5 1.4-4 1 1.3 1 2.7 1.4 4.2 1.4 4.8.0 7.5-4 7.5-7.5v-.4c.5-.4.8-1.5 1.2-2z"/><circle cx="12" cy="12" r="11.5"/></svg></div>Twitter</div></a><a class=resp-sharing-button__link href="mailto:?subject=%22Trees%20and%20Bags%22%20seems%20interesting...&amp;body=https%3a%2f%2frgoswami.me%2fposts%2ftrees-and-bags%2f" target=_self rel=noopener aria-label=E-Mail><div class="resp-sharing-button resp-sharing-button--email resp-sharing-button--medium"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--circle"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19.5 16c0 .8-.7 1.5-1.5 1.5H6c-.8.0-1.5-.7-1.5-1.5V8c0-.8.7-1.5 1.5-1.5h12c.8.0 1.5.7 1.5 1.5v8zm-2-7.5L12 13 6.5 8.5m11 6-4-2.5m-7 2.5 4-2.5"/><circle cx="12" cy="12" r="11.5"/></svg></div>E-Mail</div></a><a class=resp-sharing-button__link href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frgoswami.me%2fposts%2ftrees-and-bags%2f&amp;title=%22Trees%20and%20Bags%22%20seems%20interesting...&amp;summary=%22Trees%20and%20Bags%22%20seems%20interesting...&amp;source=https%3a%2f%2frgoswami.me%2fposts%2ftrees-and-bags%2f" target=_blank rel=noopener aria-label=LinkedIn><div class="resp-sharing-button resp-sharing-button--linkedin resp-sharing-button--medium"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--circle"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><circle cx="12" cy="12" r="11.5"/><path d="M15 12.5c-.28.0-.5.22-.5.5v3.5h-3s.03-6.48.0-7h3v.83s.46-.75 1.7-.75c1.56.0 2.3 1.12 2.3 3.3v3.62h-3V13c0-.28-.23-.5-.5-.5zm-7.5-3h2v7h-2z"/><circle cx="8.5" cy="6.5" r="1"/></svg></div>LinkedIn</div></a><a class=resp-sharing-button__link href="https://reddit.com/submit/?url=https%3a%2f%2frgoswami.me%2fposts%2ftrees-and-bags%2f&amp;resubmit=true&amp;title=%22Trees%20and%20Bags%22%20seems%20interesting..." target=_blank rel=noopener aria-label=Reddit><div class="resp-sharing-button resp-sharing-button--reddit resp-sharing-button--medium"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--circle"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><circle cx="12" cy="12" r="11.5"/><ellipse cx="12" cy="14.37" rx="6.2" ry="4.24"/><path d="M14.3 16.25c-.62.36-1.42.57-2.3.57s-1.7-.2-2.32-.58"/><circle cx="14.61" cy="13.39" r=".98"/><circle cx="9.39" cy="13.39" r=".98"/><path d="M16.4 11.38c.26-.55.82-.92 1.47-.92.9.0 1.63.73 1.63 1.63.0.8-.6 1.47-1.38 1.6"/><circle cx="17.22" cy="7.52" r="1.63"/><path d="M7.6 11.38c-.26-.54-.82-.92-1.47-.92-.9.0-1.63.73-1.63 1.63.0.8.6 1.47 1.38 1.6M12 10.12s-.08-4.82 3.6-2.6"/></svg></div>Reddit</div></a><a class=resp-sharing-button__link href="whatsapp://send?text=%22Trees%20and%20Bags%22%20seems%20interesting...%20https%3a%2f%2frgoswami.me%2fposts%2ftrees-and-bags%2f" target=_blank rel=noopener aria-label=WhatsApp><div class="resp-sharing-button resp-sharing-button--whatsapp resp-sharing-button--medium"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--circle"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><circle xmlns="http://www.w3.org/2000/svg" cx="12" cy="12" r="11.5"/><path stroke-width="1" d="M17.6 6.2c-1.5-1.5-3.4-2.3-5.5-2.3-4.3.0-7.8 3.5-7.8 7.8.0 1.4.4 2.7 1 3.9l-1.1 4 4.1-1.1c1.1.6 2.4.9 3.7.9 4.3.0 7.8-3.5 7.8-7.8.1-2-.7-3.9-2.2-5.4zm-5.5 11.9c-1.2.0-2.3-.3-3.3-.9l-.2-.1-2.4.6.7-2.4-.2-.2c-.6-1-1-2.2-1-3.4.0-3.6 2.9-6.5 6.5-6.5 1.7.0 3.3.7 4.6 1.9 1.2 1.2 1.9 2.8 1.9 4.6-.1 3.5-3 6.4-6.6 6.4zm3.5-4.8c-.2-.1-1.1-.6-1.3-.6-.2-.1-.3-.1-.4.1s-.5.6-.6.8c-.1.1-.2.1-.4.0s-.8-.3-1.6-1c-.6-.5-1-1.2-1.1-1.3-.1-.2.0-.3.1-.4l.3-.3s.1-.2.2-.3c.1-.1.0-.2.0-.3s-.4-1.1-.6-1.4c-.2-.4-.3-.3-.4-.3h-.4s-.3.0-.5.2-.7.7-.7 1.6c0 1 .7 1.9.8 2s1.4 2.1 3.3 2.9c.5.2.8.3 1.1.4.5.1.9.1 1.2.1.4-.1 1.1-.5 1.3-.9.2-.5.2-.8.1-.9.0-.2-.2-.3-.4-.4z"/></svg></div>WhatsApp</div></a></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button previous"><a href=https://rgoswami.me/posts/goat-google/><span class=button__icon>‚Üê</span>
<span class=button__text>Analytics: Google to Goat</span></a></span>
<span class="button next"><a href=https://rgoswami.me/posts/mackay-all-a/><span class=button__text>"Everyone Should Get an A - David MacKay"</span>
<span class=button__icon>‚Üí</span></a></span></div></div><div id=comments class=thin><div class=pagination__title><span class=pagination__title-h>Comments</span><hr></div><script src=https://utteranc.es/client.js repo=haozeke/haozeke.github.io issue-term=pathname theme=photon-dark label="Utterance üí¨" crossorigin=anonymous async></script></div></main><footer class=footer><p>&copy; 2023
<span><a href=https://rgoswami.me/>Rohit Goswami (HaoZeke)</a></span>
<a href=/posts/index.xml target=_blank title=rss><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a><br><a rel=license href=http://creativecommons.org/licenses/by-nc-sa/4.0/>CC BY-NC-SA 4.0</a>.
Powered by <a href=http://gohugo.io><u>Hugo</u></a> and <a href=https://github.com/HaoZeke/hugo-theme-hello-friend-ng-hz><u>this theme</u></a>.</br>Hosted with <u><a href=https://www.netlify.com/>Netlify</a></u>.</br><script src=https://liberapay.com/rohit/widgets/button.js></script><noscript><a href=https://liberapay.com/rohit/donate><img alt="Donate using Liberapay" src=https://liberapay.com/assets/widgets/donate.svg></a></noscript></p></footer></div><script src=/bundle.min.5fae9992ea406007408dc33b0b3a8376d61e2f1d6ac50ba1dad7580ac50afb5199bcef3bbc3677e77553a9980bf882cf71edc2f755687dd7725f12ece4304ac2.js></script>
<script>(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","//www.google-analytics.com/analytics.js","ga"),ga("create","UA-109503488-16","auto"),ga("send","pageview")</script><script type=text/javascript>(function(e,t,n,s,o,i,a){e[n]=e[n]||function(){(e[n].q=e[n].q||[]).push(arguments)},i=t.createElement(s),i.async=1,i.src="https://www.clarity.ms/tag/"+o,a=t.getElementsByTagName(s)[0],a.parentNode.insertBefore(i,a)})(window,document,"clarity","script","3z2xvwqu4u")</script><script data-goatcounter=https://rgoswami.goatcounter.com/count async src=//gc.zgo.at/count.js></script>
<script>var clicky_site_ids=clicky_site_ids||[];clicky_site_ids.push("101265002")</script><script async src=https://static.getclicky.com/js></script>
<script src=https://cdn.jsdelivr.net/npm/readmore-js@3.0.0-beta-1/dist/readmore.min.js></script>
<script src=/js/codefold.min.f0790269b87613b3bcff64022840fe1991eee3ea384e874c78a24022c9936bd888a3c14a05a10955881eebb1558e4fcf37576dac120cd0bf4ebaf7aca27b35b1.js></script></body></html>