
@article{prernaStudyIceNucleation2019,
  langid = {english},
  title = {Study of Ice Nucleation on Silver Iodide Surface with Defects},
  issn = {0026-8976, 1362-3028},
  url = {https://www.tandfonline.com/doi/full/10.1080/00268976.2019.1657599},
  doi = {10/gf62dn},
  abstract = {In this work, we have considered the crystallisation behaviour of supercooled water in the presence of surface defects of varying size (surface fraction, α from 1 to 0.5). Ice nucleation on Ag exposed βAgI (0001 plane) surface is investigated by molecular dynamics simulation at a temperature of 240 K. For systems with α {$>$} 0.67, the surface layers crystallise within 150 ns. In the system with defects, we observe two distinct stacking patterns in the layers near the surface and find that systems with AA stacking cause a monotonic decrease in the early nucleation dynamics with an increase in defect size. Where AB stacking (α = 0.833) is observed, the effect of the defect is diminished and the dynamics are similar to the plain AgI surface. This is supported by the variation in the orientational dynamics, hydrogen bond network stability, and tetrahedrality with respect to the defects. We quantify results in terms of the network topology using double-diamond cages (DDCs) and hexagonal cages (HCs). The configurations of the initially formed layers of ice strongly affect the subsequent growth even at long timescales. We assert that the retarded ice growth due to defects can be explained by the relative increase in DDCs with respect to HCs.},
  journaltitle = {Molecular Physics},
  shortjournal = {Molecular Physics},
  urldate = {2019-08-26},
  date = {2019-08-25},
  pages = {1-13},
  author = {{Prerna} and Goswami, Rohit and Metya, Atanu K. and Shevkunov, S. V. and Singh, Jayant K.},
  file = {/home/haozeke/Zotero/storage/7YXK8YLM/Prerna et al. - 2019 - Study of ice nucleation on silver iodide surface w.pdf},
  note = {00000}
}

@inproceedings{goswamiSpaceFillingCurves2019,
  langid = {english},
  location = {{New Delhi, India}},
  title = {Space {{Filling Curves}}: {{Heuristics For Semi Classical Lasing Computations}}},
  isbn = {978-90-825987-5-9},
  url = {https://my.pcloud.com/publink/show?code=XZ8vxr7ZHFxXSXuD2J4ntMcQoyPGXFodmegy},
  doi = {10/gf5mqk},
  shorttitle = {Space {{Filling Curves}}},
  abstract = {For semi classical lasing, the FDTD (finite difference time domain) formulation including nonlinearities is often used. We determine the computational efficiency of such schemes quantitatively and present a hueristic based on space filling curves to minimize complexity. The sparse matrix kernel is shown to be optimized by the utilization of Bi-directional Incremental Compressed Row Storage (BICRS). Extensions to high performance clusters and parallelization are also derived.},
  eventtitle = {2019 {{URSI Asia}}-{{Pacific Radio Science Conference}} ({{AP}}-{{RASC}})},
  booktitle = {2019 {{URSI Asia}}-{{Pacific Radio Science Conference}} ({{AP}}-{{RASC}})},
  publisher = {{IEEE}},
  urldate = {2019-08-01},
  date = {2019-03},
  pages = {1-4},
  keywords = {_tablet},
  author = {Goswami, Rohit and Goswami, Amrita and Goswami, Debabrata},
  file = {/home/haozeke/Zotero/storage/CSI4JTYW/dgoswamiFemtoLab-Goswami_et_al_2019_Space_Filling_Curves.pdf},
  note = {00000}
}

@inproceedings{medelModellingPerformanceResource2016,
  title = {Modelling {{Performance Resource Management}} in {{Kubernetes}}},
  abstract = {Containers are rapidly replacing Virtual Machines (VMs) as the compute instance of choice in cloud-based deployments. The significantly lower overhead of deploying containers (compared to VMs) has often been cited as one reason for this. We analyse performance of the Kubernetes system and develop a Reference net-based model of resource management within this system. Our model is characterised using real data from a Kubernetes deployment, and can be used as a basis to design scalable applications that make use of Kubernetes.},
  eventtitle = {2016 {{IEEE}}/{{ACM}} 9th {{International Conference}} on {{Utility}} and {{Cloud Computing}} ({{UCC}})},
  booktitle = {2016 {{IEEE}}/{{ACM}} 9th {{International Conference}} on {{Utility}} and {{Cloud Computing}} ({{UCC}})},
  date = {2016-12},
  pages = {257-262},
  keywords = {Analytical models,cloud computing,Cloud computing,Computational modeling,containers,Containers,Kubernetes deployment,Kubernetes system,Monitoring,PaaS clouds,performance model,Petri nets,platform-as-a-service clouds,reference net-based model,resource allocation,resource management,virtual machines,Virtual machining,VM},
  author = {Medel, V. and Rana, O. and Bañares, J. Á and Arronategui, U.},
  file = {/home/haozeke/Zotero/storage/P7BVM37F/dgoswamiFemtoLab-Medel_et_al_2016_Modelling_Performance_Resource_Management_in_Kubernetes.pdf;/home/haozeke/Zotero/storage/97TAU38U/7881642.html},
  note = {00016}
}

@inproceedings{joyPerformanceComparisonLinux2015,
  title = {Performance Comparison between {{Linux}} Containers and Virtual Machines},
  doi = {10/gf7tnp},
  abstract = {With the advent of cloud computing and virtualization, modern distributed applications run on virtualized environments for hardware resource utilization and flexibility of operations in an infrastructure. However, when it comes to virtualization, resource overhead is involved. Linux containers can be an alternative to traditional virtualization technologies because of its high resource utilization and less overhead. This paper provides a comparison between Linux containers and virtual machines in terms of performance and scalability.},
  eventtitle = {2015 {{International Conference}} on {{Advances}} in {{Computer Engineering}} and {{Applications}}},
  booktitle = {2015 {{International Conference}} on {{Advances}} in {{Computer Engineering}} and {{Applications}}},
  date = {2015-03},
  pages = {342-346},
  keywords = {cloud computing,containers,Containers,virtual machines,Virtual machining,docker,hardware resource utilization,hypervisor,kubernetes,Linux,Linux containers,resource overhead,Scalability,Servers,Virtual machine monitors,virtualisation,virtualization,Virtualization,virtualization technologies,virtualized environments},
  author = {Joy, A. M.},
  file = {/home/haozeke/Zotero/storage/4YU2K28F/dgoswamiFemtoLab-Joy_2015_Performance_comparison_between_Linux_containers_and_virtual_machines.pdf;/home/haozeke/Zotero/storage/3IQYDT93/7164727.html},
  note = {00126}
}

@article{rodehBTRFSLinuxBTree2013,
  title = {{{BTRFS}}: {{The Linux B}}-{{Tree Filesystem}}},
  volume = {9},
  issn = {1553-3077},
  url = {http://doi.acm.org/10.1145/2501620.2501623},
  doi = {10/gf7tqx},
  shorttitle = {{{BTRFS}}},
  abstract = {BTRFS is a Linux filesystem that has been adopted as the default filesystem in some popular versions of Linux. It is based on copy-on-write, allowing for efficient snapshots and clones. It uses B-trees as its main on-disk data structure. The design goal is to work well for many use cases and workloads. To this end, much effort has been directed to maintaining even performance as the filesystem ages, rather than trying to support a particular narrow benchmark use-case. Linux filesystems are installed on smartphones as well as enterprise servers. This entails challenges on many different fronts. ---Scalability. The filesystem must scale in many dimensions: disk space, memory, and CPUs. ---Data integrity. Losing data is not an option, and much effort is expended to safeguard the content. This includes checksums, metadata duplication, and RAID support built into the filesystem. ---Disk diversity. The system should work well with SSDs and hard disks. It is also expected to be able to use an array of different sized disks, which poses challenges to the RAID and striping mechanisms. This article describes the core ideas, data structures, and algorithms of this filesystem. It sheds light on the challenges posed by defragmentation in the presence of snapshots, and the tradeoffs required to maintain even performance in the face of a wide spectrum of workloads.},
  number = {3},
  journaltitle = {Trans. Storage},
  urldate = {2019-09-10},
  date = {2013-08},
  pages = {9:1--9:32},
  keywords = {B-trees,concurrency,copy-on-write,filesystem,RAID,shadowing,snapshots},
  author = {Rodeh, Ohad and Bacik, Josef and Mason, Chris},
  note = {00290}
}

@article{dolstraNixOSPurelyFunctional2010,
  langid = {english},
  title = {{{NixOS}}: {{A}} Purely Functional {{Linux}} Distribution},
  volume = {20},
  issn = {1469-7653, 0956-7968},
  doi = {10/dfrgtj},
  shorttitle = {{{NixOS}}},
  abstract = {Existing package and system configuration management tools suffer from an imperative model, where system administration actions such as package upgrades or changes to system configuration files are stateful: they destructively update the state of the system. This leads to many problems, such as the inability to roll back changes easily, to deploy multiple versions of a package side-by-side, to reproduce a configuration deterministically on another machine, or to reliably upgrade a system. In this paper we show that we can overcome these problems by moving to a purely functional system configuration model. This means that all static parts of a system (such as software packages, configuration files and system startup scripts) are built by pure functions and are immutable, stored in a way analogous to a heap in a purely functional language. We have implemented this model in NixOS, a non-trivial Linux distribution that uses the Nix package manager to build the entire system configuration from a modular, purely functional specification.},
  number = {5-6},
  journaltitle = {Journal of Functional Programming},
  date = {2010-11},
  pages = {577-615},
  author = {Dolstra, Eelco and Löh, Andres and Pierron, Nicolas},
  file = {/home/haozeke/Zotero/storage/FHNM3UYX/dgoswamiFemtoLab-Dolstra_et_al_2010_NixOS.pdf;/home/haozeke/Zotero/storage/VDABXBEC/C1ACBA2A51D2E5466820F5B5086EA2CE.html},
  note = {00046}
}

@article{chlumskyExtensionTORQUEScheduler2012,
  langid = {english},
  title = {The Extension of {{TORQUE}} Scheduler Allowing the Use of Planning and Optimization in Grids},
  issn = {1508-2806},
  url = {https://www.infona.pl//resource/bwmeta1.element.baztech-article-AGH1-0028-0199},
  doi = {10/gf7tp7},
  abstract = {In this work we present a major extension of the open source TORQUE Resource Manager system. We have replaced a naive scheduler provided in the TORQUE distribution with complex scheduling system that allows to plan job execution ahead and predict the behavior of the system. It is based on the application of job schedule, which represents the jobs’ execution plan. Such a functionality is very useful as the plan can be used by the users to see when and where their jobs will be executed. Moreover, created plans can be easily evaluated in order to identify possible inefficiencies. Then, repair actions can be taken immediately and the inefficiencies can be fixed, producing better schedules with respect to considered criteria.},
  issue = {Vol. 13 (2)},
  journaltitle = {Computer Science},
  urldate = {2019-09-10},
  date = {2012},
  pages = {5-19},
  author = {Chlumsky, V. and Klusacek, D. and Ruda, M.},
  note = {00012}
}

@inproceedings{yooSLURMSimpleLinux2003,
  langid = {english},
  title = {{{SLURM}}: {{Simple Linux Utility}} for {{Resource Management}}},
  isbn = {978-3-540-39727-4},
  shorttitle = {{{SLURM}}},
  abstract = {A new cluster resource management system called Simple Linux Utility Resource Management (SLURM) is described in this paper. SLURM, initially developed for large Linux clusters at the Lawrence Livermore National Laboratory (LLNL), is a simple cluster manager that can scale to thousands of processors. SLURM is designed to be flexible and fault-tolerant and can be ported to other clusters of different size and architecture with minimal effort. We are certain that SLURM will benefit both users and system architects by providing them with a simple, robust, and highly scalable parallel job execution environment for their cluster system.},
  booktitle = {Job {{Scheduling Strategies}} for {{Parallel Processing}}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer Berlin Heidelberg}},
  date = {2003},
  pages = {44-60},
  keywords = {Exit Status,Lawrence Livermore National Laboratory,Message Authentication Code,Remote Execution,Resource Management System},
  author = {Yoo, Andy B. and Jette, Morris A. and Grondona, Mark},
  editor = {Feitelson, Dror and Rudolph, Larry and Schwiegelshohn, Uwe},
  note = {00745}
}

@article{sochatScientificFilesystem2018,
  langid = {english},
  title = {The {{Scientific Filesystem}}},
  volume = {7},
  url = {https://academic.oup.com/gigascience/article/7/5/giy023/4931737},
  doi = {10/gdwq7f},
  abstract = {AbstractBackground.  Here, we present the Scientific Filesystem (SCIF), an organizational format that supports exposure of executables and metadata for discover},
  number = {5},
  journaltitle = {GigaScience},
  shortjournal = {Gigascience},
  urldate = {2019-09-10},
  date = {2018-05-01},
  author = {Sochat, Vanessa},
  note = {00002}
}

@book{nguyenLinuxFilesystemHierarchy,
  langid = {english},
  title = {Linux {{Filesystem Hierarchy}}},
  abstract = {This document outlines the set of requirements and guidelines for file and directory placement under the Linux operating system according to those of the FSSTND v2.3 final (January 29, 2004) and also its actual implementation on an arbitrary system. It is meant to be accessible to all members of the Linux community, be distribution independent and is intended discuss the impact of the FSSTND and how it has managed to increase the efficiency of support interoperability of applications, system administration tools, development tools, and scripts as well as greater uniformity of documentation for these systems.},
  pagetotal = {113},
  publisher = {{Binh Nguyen}},
  keywords = {Computers / General},
  author = {Nguyen, Binh},
  eprinttype = {googlebooks},
  eprint = {wLJWBQAAQBAJ}
}

@online{NixManual,
  title = {Nix Manual},
  url = {https://nixos.org/nix/manual/},
  urldate = {2019-09-10},
  file = {/home/haozeke/Zotero/storage/F7KM58TL/manual.html},
  note = {00000}
}

@article{jackmanLinuxbrewHomebrewCrossPlatform,
  langid = {english},
  title = {Linuxbrew and {{Homebrew}} for C{{Cross}}P{{Platform Package Management}}},
  pages = {1},
  keywords = {⛔ No DOI found},
  author = {Jackman, Shaun D},
  file = {/home/haozeke/Zotero/storage/C77VMCL9/storage\:Jackman - Linuxbrew and Homebrew  for Cross-Platform Package.pdf},
  note = {00007}
}

@article{poolDistccFastFree,
  langid = {english},
  title = {Distcc, a Fast Free Distributed Compiler},
  pages = {11},
  keywords = {⛔ No DOI found},
  author = {Pool, Martin},
  file = {/home/haozeke/Zotero/storage/LVLRFM2C/Pool - distcc, a fast free distributed compiler.pdf}
}

@online{TweagIntroducingLorri,
  title = {Tweag {{I}}/{{O}} - {{Introducing}} Lorri, Your Project's Nix-Env},
  url = {https://www.tweag.io/posts/2019-03-28-introducing-lorri.html},
  urldate = {2019-09-10},
  file = {/home/haozeke/Zotero/storage/RUBQAUR7/2019-03-28-introducing-lorri.html},
  note = {00000}
}

@article{dolstraHydraDeclarativeApproach,
  langid = {english},
  title = {Hydra: {{A Declarative Approach}} to {{Continuous Integration}}},
  abstract = {There are many tools to support continuous integration: the process of automatically and continuously building a project from a version management repository. However, they do not have good support for variability in the build environment: dependencies such as compilers, libraries or testing tools must typically be installed manually on all machines on which automated builds are performed. In this paper we present Hydra, a continuous build tool based on Nix, a package manager that has a purely functional language for describing package build actions and their dependencies. This allows the build environment for projects to be produced automatically and deterministically, and so significantly reduces the effort to maintain a continuous integration environment.},
  pages = {18},
  keywords = {⛔ No DOI found},
  author = {Dolstra, Eelco and Visser, Eelco},
  file = {/home/haozeke/Zotero/storage/GIGXFVJP/Dolstra and Visser - Hydra A Declarative Approach to Continuous Integr.pdf}
}

@article{medelCharacterisingResourceManagement2018,
  title = {Characterising Resource Management Performance in {{Kubernetes}}},
  volume = {68},
  issn = {0045-7906},
  url = {http://www.sciencedirect.com/science/article/pii/S0045790617315240},
  doi = {10/gf7tnw},
  abstract = {A key challenge for supporting elastic behaviour in cloud systems is to achieve a good performance in automated (de-)provisioning and scheduling of computing resources. One of the key aspects that can be significant is the overheads associated with deploying, terminating and maintaining resources. Therefore, due to their lower start up and termination overhead, containers are rapidly replacing Virtual Machines (VMs) in many cloud deployments, as the computation instance of choice. In this paper, we analyse the performance of Kubernetes achieved through a Petri net-based performance model. Kubernetes is a container management system for a distributed cluster environment. Our model can be characterised using data from a Kubernetes deployment, and can be exploited for supporting capacity planning and designing Kubernetes-based elastic applications.},
  journaltitle = {Computers \& Electrical Engineering},
  shortjournal = {Computers \& Electrical Engineering},
  urldate = {2019-09-10},
  date = {2018-05-01},
  pages = {286-297},
  keywords = {Petri nets,Cloud resource management,Container lifecycle,Kubernetes,Performance models},
  author = {Medel, Víctor and Tolosana-Calasanz, Rafael and Bañares, José Ángel and Arronategui, Unai and Rana, Omer F.},
  file = {/home/haozeke/Zotero/storage/EKG9MNVP/dgoswamiFemtoLab-Medel_et_al_2018_Characterising_resource_management_performance_in_Kubernetes.pdf},
  note = {00006}
}

@inproceedings{joyPerformanceComparisonLinux2015a,
  title = {Performance Comparison between {{Linux}} Containers and Virtual Machines},
  doi = {10/gf7tnp},
  abstract = {With the advent of cloud computing and virtualization, modern distributed applications run on virtualized environments for hardware resource utilization and flexibility of operations in an infrastructure. However, when it comes to virtualization, resource overhead is involved. Linux containers can be an alternative to traditional virtualization technologies because of its high resource utilization and less overhead. This paper provides a comparison between Linux containers and virtual machines in terms of performance and scalability.},
  eventtitle = {2015 {{International Conference}} on {{Advances}} in {{Computer Engineering}} and {{Applications}}},
  booktitle = {2015 {{International Conference}} on {{Advances}} in {{Computer Engineering}} and {{Applications}}},
  date = {2015-03},
  pages = {342-346},
  keywords = {cloud computing,containers,Containers,virtual machines,Virtual machining,docker,hardware resource utilization,hypervisor,kubernetes,Linux,Linux containers,resource overhead,Scalability,Servers,Virtual machine monitors,virtualisation,virtualization,Virtualization,virtualization technologies,virtualized environments},
  author = {Joy, A. M.},
  file = {/home/haozeke/Zotero/storage/48L9ZITX/dgoswamiFemtoLab-Joy_2015_Performance_comparison_between_Linux_containers_and_virtual_machines.pdf;/home/haozeke/Zotero/storage/8I6688IW/7164727.html},
  note = {00126}
}

@inproceedings{devresseNixBasedFully2015,
  location = {{New York, NY, USA}},
  title = {Nix {{Based Fully Automated Workflows}} and {{Ecosystem}} to {{Guarantee Scientific Result Reproducibility Across Software Environments}} and {{Systems}}},
  isbn = {978-1-4503-4012-0},
  url = {http://doi.acm.org/10.1145/2830168.2830172},
  doi = {10/gf7tnn},
  abstract = {Reproducibility is a key requirement to scientific development. Any scientific process, including software simulations, must be able to be replicated in order to prove the robustness of its process and the validity of its results. If an approach based on the extensive documentation of the process itself maybe considered as sufficient to guarantee reproducibility of results in domains like Physics or Biology, such a requirement proves to be incomplete for software being executed on high performance computing platforms. The specifics of the customized and exotic HPC architectures, the fast evolution of the software development environment as well as the various variables that can pollute the software development and building process are just few of the many possible sources of scientific result corruption. We describe in this paper how the developers of the Blue Brain Project built a software development ecosystem based on the Nix packaging and build system in order to guarantee the full portability, traceability and reproducibility of scientific results.},
  booktitle = {Proceedings of the 3rd {{International Workshop}} on {{Software Engineering}} for {{High Performance Computing}} in {{Computational Science}} and {{Engineering}}},
  series = {{{SE}}-{{HPCCSE}} '15},
  publisher = {{ACM}},
  urldate = {2019-09-10},
  date = {2015},
  pages = {25--31},
  keywords = {software engineering,high performance computing,scientific reproducibility,software deployment,traceability},
  author = {Devresse, Adrien and Delalondre, Fabien and Schürmann, Felix},
  venue = {Austin, Texas},
  note = {00006}
}

@inproceedings{bzeznikNixHPCPackage2017,
  langid = {english},
  location = {{Denver, CO, USA}},
  title = {Nix as {{HPC}} Package Management System},
  isbn = {978-1-4503-5130-0},
  url = {http://dl.acm.org/citation.cfm?doid=3152493.3152556},
  doi = {10/gf7tnc},
  abstract = {Modern High Performance Computing systems are becoming larger and more heterogeneous. The proper management of software for the users of such systems poses a significant challenge. These users run very diverse applications that may be compiled with proprietary tools for specialized hardware. Moreover, the application life-cycle of these software may exceed the lifetime of the HPC systems themselves. These difficulties motivate the use of specialized package management systems. In this paper, we outline an approach to HPC package development, deployment, management, sharing, and reuse based on the Nix functional package manager. We report our experience with this approach inside the GRICAD HPC center[GRICAD 2017a] in Grenoble over a 12 month period and compare it to other existing approaches.},
  eventtitle = {The {{Fourth International Workshop}}},
  booktitle = {Proceedings of the {{Fourth International Workshop}} on {{HPC User Support Tools}}  - {{HUST}}'17},
  publisher = {{ACM Press}},
  urldate = {2019-09-10},
  date = {2017},
  pages = {1-6},
  author = {Bzeznik, Bruno and Henriot, Oliver and Reis, Valentin and Richard, Olivier and Tavard, Laure},
  file = {/home/haozeke/Zotero/storage/WTC5M8M7/Bzeznik et al. - 2017 - Nix as HPC package management system.pdf},
  note = {00001}
}

@article{buiAnalysisDockerSecurity2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1501.02967},
  primaryClass = {cs},
  title = {Analysis of {{Docker Security}}},
  url = {http://arxiv.org/abs/1501.02967},
  abstract = {Over the last few years, the use of virtualization technologies has increased dramatically. This makes the demand for efficient and secure virtualization solutions become more obvious. Container-based virtualization and hypervisor-based virtualization are two main types of virtualization technologies that have emerged to the market. Of these two classes, container-based virtualization is able to provide a more lightweight and efficient virtual environment, but not without security concerns. In this paper, we analyze the security level of Docker, a well-known representative of container-based approaches. The analysis considers two areas: (1) the internal security of Docker, and (2) how Docker interacts with the security features of the Linux kernel, such as SELinux and AppArmor, in order to harden the host system. Furthermore, the paper also discusses and identifies what could be done when using Docker to increase its level of security.},
  urldate = {2019-09-10},
  date = {2015-01-13},
  keywords = {Computer Science - Cryptography and Security},
  author = {Bui, Thanh},
  file = {/home/haozeke/Zotero/storage/F5RXDU2E/dgoswamiFemtoLab-Bui_2015_Analysis_of_Docker_Security.pdf;/home/haozeke/Zotero/storage/Z2BZ33T7/1501.html},
  note = {00118}
}

@inproceedings{azabEnablingDockerContainers2017,
  title = {Enabling {{Docker Containers}} for {{High}}-{{Performance}} and {{Many}}-{{Task Computing}}},
  doi = {10/gf7tmq},
  abstract = {Docker is the most popular and user friendly platform for running and managing Linux containers. This is proven by the fact that vast majority of containerized tools are packaged as Docker images. A demanding functionality is to enable running Docker containers inside HPC job scripts for researchers to make use of the flexibility offered by containers in their real-life computational and data intensive jobs. The main two questions before implementing such functionality are: how to securely run Docker containers within cluster jobs? and how to limit the resource usage of a Docker job to the borders defined by the HPC queuing system? This paper presents Socker, a secure wrapper for running Docker containers on Slurm and similar queuing systems. Socker enforces the execution of containers within Slurm jobs as the submitting user instead of root, as well as enforcing the inclusion of containers in the cgroups assigned by the queuing system to the parent jobs. Different from other Docker supported containers-for-hpc platform, socker uses the underlaying Docker engine instead of replacing it. To eveluate socker, it has been tested for running MPI Docker jobs on Slurm. It has been also tested for Many-task computing (MTC) on interconnected clusters. Socker has proven to be secure, as well as introducing no additional overhead to the one introduced already by the Docker engine.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Cloud Engineering}} ({{IC2E}})},
  booktitle = {2017 {{IEEE International Conference}} on {{Cloud Engineering}} ({{IC2E}})},
  date = {2017-04},
  pages = {279-285},
  keywords = {high-performance computing,Tools,containers,Containers,Linux,Linux containers,Docker,Docker containers,Docker images,Docker job resource usage,Engines,HPC,HPC job scripts,HPC queuing system,interconnected clusters,Kernel,many-task computing,message passing,MPI Docker jobs,parallel processing,Production,secure wrapper,security of data,Slurm,Socker},
  author = {Azab, A.},
  file = {/home/haozeke/Zotero/storage/B275JRIG/dgoswamiFemtoLab-Azab_2017_Enabling_Docker_Containers_for_High-Performance_and_Many-Task_Computing.pdf;/home/haozeke/Zotero/storage/A9NLWZ2G/7923813.html},
  note = {00013}
}

@inproceedings{higginsOrchestratingDockerContainers2015,
  langid = {english},
  title = {Orchestrating {{Docker Containers}} in the {{HPC Environment}}},
  isbn = {978-3-319-20119-1},
  abstract = {Linux container technology has more than proved itself useful in cloud computing as a lightweight alternative to virtualisation, whilst still offering good enough resource isolation. Docker is emerging as a popular runtime for managing Linux containers, providing both management tools and a simple file format. Research into the performance of containers compared to traditional Virtual Machines and bare metal shows that containers can achieve near native speeds in processing, memory and network throughput. A technology born in the cloud, it is making inroads into scientific computing both as a format for sharing experimental applications and as a paradigm for cloud based execution. However, it has unexplored uses in traditional cluster and grid computing. It provides a run time environment in which there is an opportunity for typical cluster and parallel applications to execute at native speeds, whilst being bundled with their own specific (or legacy) library versions and support software. This offers a solution to the Achilles heel of cluster and grid computing that requires the user to hold intimate knowledge of the local software infrastructure. Using Docker brings us a step closer to more effective job and resource management within the cluster by providing both a common definition format and a repeatable execution environment. In this paper we present the results of our work in deploying Docker containers in the cluster environment and an evaluation of its suitability as a runtime for high performance parallel execution. Our findings suggest that containers can be used to tailor the run time environment for an MPI application without compromising performance, and would provide better Quality of Service for users of scientific computing.},
  booktitle = {High {{Performance Computing}}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  date = {2015},
  pages = {506-513},
  keywords = {Linux containers,Docker,Cluster,Grids,Run time environment},
  author = {Higgins, Joshua and Holmes, Violeta and Venters, Colin},
  editor = {Kunkel, Julian M. and Ludwig, Thomas},
  note = {00030}
}

@inproceedings{rahoKVMXenDocker2015,
  title = {{{KVM}}, {{Xen}} and {{Docker}}: {{A}} Performance Analysis for {{ARM}} Based {{NFV}} and Cloud Computing},
  doi = {10/gf7tmp},
  shorttitle = {{{KVM}}, {{Xen}} and {{Docker}}},
  abstract = {Virtualization is a mature technology which has shown to provide computing resource and cost optimization while enabling consolidation, isolation and hardware abstraction through the concept of virtual machine. Recently, by sharing the operating system resources and simplifying the deployment of applications, containers are getting a more and more popular alternative to virtualization for specific use cases. As a result, today these two technologies are competing to provide virtual instances for cloud computing, Network Functions Virtualization (NFV), High Performance Computing (HPC), avionic and automotive platforms. In this paper, the performance of the most important open source hypervisor (KVM and Xen) and container (Docker) solutions are compared on the ARM architecture, which is rapidly emerging in the server world. The extensive system and Input/Output (I/O) performance measurements included in this paper show a slightly better performance for containers in CPU bound workloads and request/response networking; conversely, thanks to their caching mechanisms, hypervisors perform better in most disk I/O operations and TCP streaming benchmark.},
  eventtitle = {2015 {{IEEE}} 3rd {{Workshop}} on {{Advances}} in {{Information}}, {{Electronic}} and {{Electrical Engineering}} ({{AIEEE}})},
  booktitle = {2015 {{IEEE}} 3rd {{Workshop}} on {{Advances}} in {{Information}}, {{Electronic}} and {{Electrical Engineering}} ({{AIEEE}})},
  date = {2015-11},
  pages = {1-8},
  keywords = {cloud computing,Containers,virtual machines,Linux,Virtual machine monitors,virtualisation,virtualization,Virtualization,Docker,Kernel,application deployment,ARM based NFV,caching mechanisms,Computer architecture,computer network performance evaluation,computing resource,consolidation,cost optimization,CPU bound workloads,disk I/O operations,Hardware,hardware abstraction,I/O performance measurements,input/output performance measurements,isolation,KVM,network function virtualization,open source hypervisor,operating system resource sharing,performance analysis,public domain software,request/response networking,TCP streaming benchmark,transport protocols,virtual instances,virtual machine,Xen},
  author = {Raho, M. and Spyridakis, A. and Paolino, M. and Raho, D.},
  file = {/home/haozeke/Zotero/storage/6AACTPTJ/dgoswamiFemtoLab-Raho_et_al_2015_KVM,_Xen_and_Docker.pdf;/home/haozeke/Zotero/storage/JWG7LVLH/7367280.html},
  note = {00046}
}

@inproceedings{ismailEvaluationDockerEdge2015,
  title = {Evaluation of {{Docker}} as {{Edge}} Computing Platform},
  doi = {10/gf7tmm},
  abstract = {High latency, network congestion and network bottleneck are some of problems in cloud computing. Moving from centralized to decentralized paradigm, Edge computing could offload the processing to the edge which indirectly reduces application response time and improves overall user experience. This paper evaluate Docker, a container based technology as a platform for Edge Computing. 4 fundamental criteria were evaluated 1) deployment and termination, 2) resource \& service management, 3) fault tolerance and 4) caching. Based on our evaluation and experiment Docker provides fast deployment, small footprint and good performance which make it potentially a viable Edge Computing platform.},
  eventtitle = {2015 {{IEEE Conference}} on {{Open Systems}} ({{ICOS}})},
  booktitle = {2015 {{IEEE Conference}} on {{Open Systems}} ({{ICOS}})},
  date = {2015-08},
  pages = {130-135},
  keywords = {simulation,cloud computing,Cloud computing,Containers,Linux,Servers,Docker,application response time,caching criteria,centralized paradigm,Cloudlet,Container,decentralized paradigm,deployment and termination criteria,Docker container based technology,Docker evaluation,Edge Computing,edge computing platform,fault tolerance criteria,Fog Computing,Image edge detection,Mobile Cloud,Mobile communication,Mobile Edge Computing,resource and service management criteria,Time factors,user experience},
  author = {Ismail, B. I. and Goortani, E. Mostajeran and Karim, M. B. Ab and Tat, W. Ming and Setapa, S. and Luke, J. Y. and Hoe, O. Hong},
  file = {/home/haozeke/Zotero/storage/MD839W9K/dgoswamiFemtoLab-Ismail_et_al_2015_Evaluation_of_Docker_as_Edge_computing_platform.pdf;/home/haozeke/Zotero/storage/GFT68UUH/7377291.html},
  note = {00114}
}

@inproceedings{chungUsingDockerHigh2016,
  title = {Using {{Docker}} in High Performance Computing Applications},
  doi = {10/gf7tmf},
  abstract = {Virtualization technology plays a vital role in cloud computing. In particular, benefits of virtualization are widely employed in high performance computing (HPC) applications. Recently, virtual machines (VMs) and Docker containers known as two virtualization platforms need to be explored for developing applications efficiently. We target a model for deploying distributed applications on Docker containers, among using well-known benchmarks to evaluate performance between VMs and containers. Based on their architecture, we propose benchmark scenarios to analyze the computing performance and the ability of data access on HPC system. Remarkably, Docker container has more advantages than virtual machine in terms of data intensive application and computing ability, especially the overhead of Docker is trivial. However, Docker architecture has some drawbacks in resource management. Our experiment and evaluation show how to deploy efficiently high performance computing applications on Docker containers and VMs.},
  eventtitle = {2016 {{IEEE Sixth International Conference}} on {{Communications}} and {{Electronics}} ({{ICCE}})},
  booktitle = {2016 {{IEEE Sixth International Conference}} on {{Communications}} and {{Electronics}} ({{ICCE}})},
  date = {2016-07},
  pages = {52-57},
  keywords = {Libraries,cloud computing,Cloud computing,Containers,resource management,virtual machines,Virtual machining,VM,Virtual machine monitors,virtualisation,Virtualization,high performance computing,Docker,HPC,parallel processing,Computer architecture,data handling,data intensive application,Graph500,HPL,performance evaluation,virtualization technology},
  author = {Chung, M. T. and Quang-Hung, N. and Nguyen, M. and Thoai, N.},
  file = {/home/haozeke/Zotero/storage/9QZUAELJ/dgoswamiFemtoLab-Chung_et_al_2016_Using_Docker_in_high_performance_computing_applications.pdf;/home/haozeke/Zotero/storage/AUG4RTSK/7562612.html},
  note = {00043}
}

@inproceedings{adufuContainerbasedTechnologyWinner2015,
  title = {Is Container-Based Technology a Winner for High Performance Scientific Applications?},
  doi = {10/gf7tmh},
  abstract = {High Performance Computing (HPC) applications require systems with environments for maximum use of limited resources to facilitate efficient computations. However, these systems are faced with a large trade-off between efficient resource allocation and minimum execution times for the applications executed on them. Also, deploying applications in newer environments is exacting. To alleviate this challenge, container-based systems are recently being deployed to reduce the trade-off. In this paper, we investigate container-based technology as an efficient virtualization technology for running high performance scientific applications. We select Docker as the container-based technology for our test bed. We execute autodock3, a molecular modeling simulation software mostly used for Protein-ligand docking, in Docker containers and VMs created using OpenStack. We compare the execution times of the docking process in both Docker containers and in VMs.},
  eventtitle = {2015 17th {{Asia}}-{{Pacific Network Operations}} and {{Management Symposium}} ({{APNOMS}})},
  booktitle = {2015 17th {{Asia}}-{{Pacific Network Operations}} and {{Management Symposium}} ({{APNOMS}})},
  date = {2015-08},
  pages = {507-510},
  keywords = {Containers,resource allocation,virtual machines,Servers,virtualisation,Virtualization,Docker,parallel processing,virtualization technology,application program interfaces,autodock3,Cloud Computing,Container-Based Technology,Container-based virtualization,High Performance Computing (HPC),high performance computing application,high performance scientific application,HPC application,Hypervisor-based virtualization (HPV),Memory management,molecular modeling simulation software,OpenStack,Operating systems,Protein-ligand docking,Random access memory,Resource management},
  author = {Adufu, T. and Choi, J. and Kim, Y.},
  file = {/home/haozeke/Zotero/storage/KYCI6VZ6/dgoswamiFemtoLab-Adufu_et_al_2015_Is_container-based_technology_a_winner_for_high_performance_scientific.pdf;/home/haozeke/Zotero/storage/HXFJYIGD/7275379.html},
  note = {00045}
}

@article{larusSingularitySystem2010,
  langid = {english},
  title = {The {{Singularity}} System},
  volume = {53},
  issn = {00010782},
  url = {http://portal.acm.org/citation.cfm?doid=1787234.1787253},
  doi = {10/b9sp93},
  number = {8},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  urldate = {2019-09-10},
  date = {2010-08-01},
  pages = {72},
  author = {Larus, James and Hunt, Galen},
  file = {/home/haozeke/Zotero/storage/RLPF629N/Larus and Hunt - 2010 - The Singularity system.pdf}
}

@article{kurtzerSingularityScientificContainers2017,
  langid = {english},
  title = {Singularity: {{Scientific}} Containers for Mobility of Compute},
  volume = {12},
  issn = {1932-6203},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177459},
  doi = {10/f969fz},
  shorttitle = {Singularity},
  abstract = {Here we present Singularity, software developed to bring containers and reproducibility to scientific computing. Using Singularity containers, developers can work in reproducible environments of their choosing and design, and these complete environments can easily be copied and executed on other platforms. Singularity is an open source initiative that harnesses the expertise of system and software engineers and researchers alike, and integrates seamlessly into common workflows for both of these groups. As its primary use case, Singularity brings mobility of computing to both users and HPC centers, providing a secure means to capture and distribute software and compute environments. This ability to create and deploy reproducible environments across these centers, a previously unmet need, makes Singularity a game changing development for computational science.},
  number = {5},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2019-09-10},
  date = {2017-05-11},
  pages = {e0177459},
  keywords = {Computer software,Software design,Software development,Software tools,Open source software,Operating systems,Research validity,Tar},
  author = {Kurtzer, Gregory M. and Sochat, Vanessa and Bauer, Michael W.},
  file = {/home/haozeke/Zotero/storage/HXRH5H7L/dgoswamiFemtoLab-Kurtzer_et_al_2017_Singularity.pdf;/home/haozeke/Zotero/storage/8UUMFVVZ/article.html}
}

@inproceedings{gamblinSpackPackageManager2015,
  location = {{New York, NY, USA}},
  title = {The {{Spack Package Manager}}: {{Bringing Order}} to {{HPC Software Chaos}}},
  isbn = {978-1-4503-3723-6},
  url = {http://doi.acm.org/10.1145/2807591.2807623},
  doi = {10/gf7tmd},
  shorttitle = {The {{Spack Package Manager}}},
  abstract = {Large HPC centers spend considerable time supporting software for thousands of users, but the complexity of HPC software is quickly outpacing the capabilities of existing software management tools. Scientific applications require specific versions of compilers, MPI, and other dependency libraries, so using a single, standard software stack is infeasible. However, managing many configurations is difficult because the configuration space is combinatorial in size. We introduce Spack, a tool used at Lawrence Livermore National Laboratory to manage this complexity. Spack provides a novel, recursive specification syntax to invoke parametric builds of packages and dependencies. It allows any number of builds to coexist on the same system, and it ensures that installed packages can find their dependencies, regardless of the environment. We show through real-world use cases that Spack supports diverse and demanding applications, bringing order to HPC software chaos.},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  series = {{{SC}} '15},
  publisher = {{ACM}},
  urldate = {2019-09-10},
  date = {2015},
  pages = {40:1--40:12},
  author = {Gamblin, Todd and LeGendre, Matthew and Collette, Michael R. and Lee, Gregory L. and Moody, Adam and de Supinski, Bronis R. and Futral, Scott},
  options = {useprefix=true},
  venue = {Austin, Texas}
}

@article{dolstraNixSafePolicyFree2004,
  langid = {english},
  title = {Nix: {{A Safe}} and {{Policy}}-{{Free System}} for {{Software Deployment}}},
  abstract = {Existing systems for software deployment are neither safe nor sufficiently flexible. Primary safety issues are the inability to enforce reliable specification of component dependencies, and the lack of support for multiple versions or variants of a component. This renders deployment operations such as upgrading or deleting components dangerous and unpredictable. A deployment system must also be flexible (i.e., policy-free) enough to support both centralised and local package management, and to allow a variety of mechanisms for transferring components. In this paper we present Nix, a deployment system that addresses these issues through a simple technique of using cryptographic hashes to compute unique paths for component instances.},
  date = {2004},
  pages = {15},
  author = {Dolstra, Eelco and de Jonge, Merijn and Visser, Eelco},
  options = {useprefix=true},
  file = {/home/haozeke/Zotero/storage/JIHFRGAE/Dolstra et al. - 2004 - Nix A Safe and Policy-Free System for Software De.pdf}
}

@thesis{dolstraPurelyFunctionalSoftware2006,
  langid = {english},
  location = {{S.l.}},
  title = {The Purely Functional Software Deployment Model},
  institution = {{[s.n.]}},
  date = {2006},
  author = {Dolstra, Eelco},
  file = {/home/haozeke/Zotero/storage/7IBL44V4/Dolstra - 2006 - The purely functional software deployment model.pdf},
  note = {00039 
OCLC: 71702886}
}

@inproceedings{goswamiQuantumDistributedComputing2016,
  langid = {english},
  location = {{Kanpur}},
  title = {Quantum {{Distributed Computing}} with {{Shaped Laser Pulses}}},
  isbn = {978-1-943580-22-4},
  doi = {10/gf5mrr},
  abstract = {Shaped laser pulses can control decoherence under quantum adiabatic method of logic operations to result in a possible scalable quantum computer by distributing the computing load on a set of optically adiabatic quantum computing nodes.},
  eventtitle = {International {{Conference}} on {{Fibre Optics}} and {{Photonics}}},
  booktitle = {13th {{International Conference}} on {{Fiber Optics}} and {{Photonics}}},
  publisher = {{OSA}},
  date = {2016},
  pages = {W4C.3},
  keywords = {_tablet},
  author = {Goswami, Rohit and Goswami, Debabrata},
  file = {/home/haozeke/Zotero/storage/BDII67V4/dgoswamiFemtoLab-Goswami_Goswami_2016_Quantum_Distributed_Computing_with_Shaped_Laser_Pulses.pdf},
  note = {00000}
}

@article{goswamiDSEAMSDeferredStructural2020a,
  title = {D-{{SEAMS}}: {{Deferred Structural Elucidation Analysis}} for {{Molecular Simulations}}},
  shorttitle = {D-{{SEAMS}}},
  author = {Goswami, Rohit and Goswami, Amrita and Singh, Jayant K.},
  date = {2020-04-27},
  journaltitle = {Journal of Chemical Information and Modeling},
  shortjournal = {J. Chem. Inf. Model.},
  volume = {60},
  number = {4},
  pages = {2169--2177},
  issn = {1549-9596, 1549-960X},
  doi = {10.1021/acs.jcim.0c00031},
  url = {https://pubs.acs.org/doi/10.1021/acs.jcim.0c00031},
  urldate = {2022-03-17},
  abstract = {Structural analyses are an integral part of computational research on nucleation and supercooled water, whose accuracy and efficiency can impact the validity and feasibility of such studies. The underlying molecular mechanisms of these often elusive and computationally expensive processes can be inferred from the evolution of ice-like structures, determined using appropriate structural analysis techniques. We present d-SEAMS, a free and open-source postprocessing engine for the analysis of molecular dynamics trajectories, which is specifically able to qualitatively classify ice structures in both strong-confinement and bulk systems. For the first time, recent algorithms for confined ice structure determination have been implemented, along with topological network criteria for bulk ice structure determination. We also propose and validate a new order parameter for identifying the building blocks of quasi-one-dimensional ice. Recognizing the need for customization in structural analysis, d-SEAMS has a unique code architecture built with nix and employing a YAML-Lua scripting pipeline. The software has been designed to be userfriendly and extensible. The engine outputs are compatible with popular graphics software suites, allowing for immediate visual insights into the systems studied. We demonstrate the features of d-SEAMS by using it to analyze nucleation in the bulk regime and for quasi-one- and quasi-two-dimensional systems. Structural time evolution and quantitative metrics are determined for heterogeneous ice nucleation on a silver-exposed β-AgI surface, homogeneous ice nucleation, flat monolayer square ice formation, and freezing of an ice nanotube.},
  langid = {english},
}

@article{courtesFunctionalPackageManagement2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1305.4584},
  primaryClass = {cs},
  title = {Functional {{Package Management}} with {{Guix}}},
  url = {http://arxiv.org/abs/1305.4584},
  abstract = {We describe the design and implementation of GNU Guix, a purely functional package manager designed to support a complete GNU/Linux distribution. Guix supports transactional upgrades and roll-backs, unprivileged package management, per-user profiles, and garbage collection. It builds upon the low-level build and deployment layer of the Nix package manager. Guix uses Scheme as its programming interface. In particular, we devise an embedded domain-specific language (EDSL) to describe and compose packages. We demonstrate how it allows us to benefit from the host general-purpose programming language while not compromising on expressiveness. Second, we show the use of Scheme to write build programs, leading to "two-tier" programming system.},
  urldate = {2019-11-13},
  date = {2013-05-20},
  keywords = {Computer Science - Programming Languages},
  author = {Courtès, Ludovic},
  file = {/home/haozeke/Zotero/storage/HV9VL42L/Courtès - 2013 - Functional Package Management with Guix.pdf;/home/haozeke/Zotero/storage/9Q76QICB/1305.html}
}

@incollection{dieguezcastroNixOS2016,
  langid = {english},
  location = {{Berkeley, CA}},
  title = {{{NixOS}}},
  isbn = {978-1-4842-1392-6},
  url = {https://doi.org/10.1007/978-1-4842-1392-6_14},
  abstract = {Thus Nix was created as part of academic research and NixOS was created as a proof of concept that Nix could be used to manage a whole Linux distro. Dolstra also would develop other tools like Hydra, which is a Nix-based continuous integration tool, and NixOps, which is a tool for provisioning and deploying NixOS machines.},
  booktitle = {Introducing {{Linux Distros}}},
  publisher = {{Apress}},
  urldate = {2019-11-13},
  date = {2016},
  pages = {301-327},
  keywords = {Configuration File,Desktop Environment,Official Channel,Package Management,Professional Certification},
  author = {Dieguez Castro, Jose},
  editor = {Dieguez Castro, Jose},
  doi = {10.1007/978-1-4842-1392-6_14}
}

@inproceedings{courtesReproducibleUserControlledSoftware2015,
  langid = {english},
  location = {{Cham}},
  title = {Reproducible and {{User}}-{{Controlled Software Environments}} in {{HPC}} with {{Guix}}},
  isbn = {978-3-319-27308-2},
  doi = {10.1007/978-3-319-27308-2_47},
  abstract = {Support teams of high-performance computing (HPC) systems often find themselves between a rock and a hard place: on one hand, they understandably administrate these large systems in a conservative way, but on the other hand, they try to satisfy their users by deploying up-to-date tool chains as well as libraries and scientific software. HPC system users often have no guarantee that they will be able to reproduce results at a later point in time, even on the same system—software may have been upgraded, removed, or recompiled under their feet, and they have little hope of being able to reproduce the same software environment elsewhere. We present GNU Guix and the functional package management paradigm and show how it can improve reproducibility and sharing among researchers with representative use cases.},
  booktitle = {Euro-{{Par}} 2015: {{Parallel Processing Workshops}}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  date = {2015},
  pages = {579-591},
  keywords = {Directed Acyclic Graph,Environment Module,Software Environment,Support Team,Tool Chain},
  author = {Courtès, Ludovic and Wurmus, Ricardo},
  editor = {Hunold, Sascha and Costan, Alexandru and Giménez, Domingo and Iosup, Alexandru and Ricci, Laura and Gómez Requena, María Engracia and Scarano, Vittorio and Varbanescu, Ana Lucia and Scott, Stephen L. and Lankes, Stefan and Weidendorfer, Josef and Alexander, Michael},
  file = {/home/haozeke/Zotero/storage/KE6S2PDY/Courtès and Wurmus - 2015 - Reproducible and User-Controlled Software Environm.pdf}
}

@inproceedings{tovarAutomaticDependencyManagement2018,
  title = {Automatic {{Dependency Management}} for {{Scientific Applications}} on {{Clusters}}},
  doi = {10.1109/IC2E.2018.00026},
  abstract = {Software installation remains a challenge in scientific computing. End users require custom software stacks that are not provided through commodity channels. The resulting effort needed to install software delays research in the first place, and creates friction for moving applications to new resources and new users. Ideally, end-users should be able to manage their own software stacks without requiring administrator privileges. To that end, we describe vc3-builder, a tool for deploying software environments automatically on clusters. Its primary application comes in cloud and opportunistic computing, where deployment must be performed in batch as a side effect of job execution. vc3-builder uses workflow technologies as a means of exploiting cluster resources for building in a portable way. We demonstrate the use of vc3-builder on three applications with complex dependencies: MAKER, Octave, and CVMFS, building and running on three different cluster facilities in sequential, parallel, and distributed modes.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Cloud Engineering}} ({{IC2E}})},
  booktitle = {2018 {{IEEE International Conference}} on {{Cloud Engineering}} ({{IC2E}})},
  date = {2018-04},
  pages = {41-49},
  keywords = {Python,scientific computing,software engineering,Tools,automatic dependency management,Buildings,cluster resources,commodity channels,custom software stacks,CVMFS application,installation of scientific sofware,MAKER application,Middleware,Octave application,opportunistic computing,software delays research,software dependency management,software environments,software installation,Software packages,Task analysis,vc3-builder},
  author = {Tovar, Benjamin and Hazekamp, Nicholas and Kremer-Herman, Nathaniel and Thain, Douglas},
  file = {/home/haozeke/Zotero/storage/36TYCNHH/Tovar et al. - 2018 - Automatic Dependency Management for Scientific App.pdf;/home/haozeke/Zotero/storage/RZLXEIR2/8360311.html}
}

@article{merkelDockerLightweightLinux2014,
  title = {Docker: {{Lightweight Linux Containers}} for {{Consistent Development}} and {{Deployment}}},
  volume = {2014},
  issn = {1075-3583},
  url = {http://dl.acm.org/citation.cfm?id=2600239.2600241},
  shorttitle = {Docker},
  abstract = {Docker promises the ability to package applications and their dependencies into lightweight containers that move easily between different distros, start up quickly and are isolated from each other.},
  number = {239},
  journaltitle = {Linux J.},
  urldate = {2019-11-14},
  date = {2014-03},
  author = {Merkel, Dirk}
}

@inproceedings{morabitoHypervisorsVsLightweight2015,
  title = {Hypervisors vs. {{Lightweight Virtualization}}: {{A Performance Comparison}}},
  doi = {10.1109/IC2E.2015.74},
  shorttitle = {Hypervisors vs. {{Lightweight Virtualization}}},
  abstract = {Virtualization of operating systems provides a common way to run different services in the cloud. Recently, the lightweight virtualization technologies claim to offer superior performance. In this paper, we present a detailed performance comparison of traditional hypervisor based virtualization and new lightweight solutions. In our measurements, we use several benchmarks tools in order to understand the strengths, weaknesses, and anomalies introduced by these different platforms in terms of processing, storage, memory and network. Our results show that containers achieve generally better performance when compared with traditional virtual machines and other recent solutions. Albeit containers offer clearly more dense deployment of virtual machines, the performance difference with other technologies is in many cases relatively small.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Cloud Engineering}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Cloud Engineering}}},
  date = {2015-03},
  pages = {386-393},
  keywords = {Albeit containers,anomalies,Benchmark testing,Benchmarking,benchmarks tools,Container,Containers,Hypervisor,hypervisors,lightweight virtualization technologies,Linux,memory,network,operating systems,Operating systems,operating systems (computers),Performance,processing,storage,strengths,Virtual machine monitors,virtual machines,virtualisation,Virtualization,weaknesses},
  author = {Morabito, Roberto and Kjällman, Jimmy and Komu, Miika},
  file = {/home/haozeke/Zotero/storage/HC8I3RBS/morabito2015.pdf;/home/haozeke/Zotero/storage/KKSITRN2/7092949.html}
}

@article{ermakovTestingDockerPerformance2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.05592},
  primaryClass = {cs},
  title = {Testing {{Docker Performance}} for {{HPC Applications}}},
  url = {http://arxiv.org/abs/1704.05592},
  abstract = {The main goal for this article is to compare performance penalties when using KVM virtualization and Docker containers for creating isolated environments for HPC applications. The article provides both data obtained using commonly accepted synthetic tests (High Performance Linpack) and real life applications (OpenFOAM). The article highlights the influence on resulting application performance of major infrastructure configuration options: CPU type presented to VM, networking connection type used.},
  urldate = {2019-11-14},
  date = {2017-04-18},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Performance},
  author = {Ermakov, Alexey and Vasyukov, Alexey},
  file = {/home/haozeke/Zotero/storage/S9T3PW6D/Ermakov and Vasyukov - 2017 - Testing Docker Performance for HPC Applications.pdf;/home/haozeke/Zotero/storage/ASBVKERT/1704.html}
}

@inproceedings{duarteEmpiricalStudyDocker2018,
  title = {An {{Empirical Study}} of {{Docker Vulnerabilities}} and of {{Static Code Analysis Applicability}}},
  doi = {10.1109/LADC.2018.00013},
  abstract = {Containers are a lighter solution to traditional virtualization, avoiding the overhead of starting and configuring the virtual machines. Docker is very popular due to its portability, ease of deployment and configuration. However, the security problems that it may have are still not completely understood. This paper aims at understanding Docker security vulnerabilities and what could have been done to avoid them. For this, we performed a detailed analysis of the security reports and respective vulnerabilities, systematizing them according to causes, effects, and consequences. Then, we analyzed the applicability of static code analyzers in Docker codebase, trying to understand, in hindsight, the usefulness of tools reports. For a deeper understanding, we analyzed concrete exploits for some vulnerabilities. The results show a prevalence of bypass and gain privileges, and that the used tools are rather ineffective, not helping to identify the analyzed vulnerabilities. We also observed that some vulnerabilities would be easy to find using robustness or penetration testing, while others would be really challenging.},
  eventtitle = {2018 {{Eighth Latin}}-{{American Symposium}} on {{Dependable Computing}} ({{LADC}})},
  booktitle = {2018 {{Eighth Latin}}-{{American Symposium}} on {{Dependable Computing}} ({{LADC}})},
  date = {2018-10},
  pages = {27-36},
  keywords = {cloud computing,Cloud computing,cloud technologies,containers,Containers,Docker codebase,Docker security vulnerabilities,Kernel,program diagnostics,security,Security,security of data,software vulnerabilities,source code (software),static code analysis,static code analyzers,Tools,virtual machines,Virtualization},
  author = {Duarte, Ana and Antunes, Nuno},
  file = {/home/haozeke/Zotero/storage/8IHWHQAY/8671641.html}
}

@article{chaePerformanceComparisonLinux2019,
  langid = {english},
  title = {A Performance Comparison of Linux Containers and Virtual Machines Using {{Docker}} and {{KVM}}},
  volume = {22},
  issn = {1573-7543},
  url = {https://doi.org/10.1007/s10586-017-1511-2},
  doi = {10.1007/s10586-017-1511-2},
  abstract = {Virtualization is a foundational element of cloud computing. Since cloud computing is slower than a native system, this study analyzes ways to improve performance. We compared the performance of Docker and Kernel-based virtual machine (KVM). KVM uses full virtualization, including [Math Processing Error]×\textbackslash{}times 86 hardware virtualization extensions. Docker is a solution provided by isolation in userspace instead of creating a virtual machine. The performance of KVM and Docker was compared in three ways. These comparisons show that Docker is faster than KVM.},
  number = {1},
  journaltitle = {Cluster Computing},
  shortjournal = {Cluster Comput},
  urldate = {2019-11-14},
  date = {2019-01-01},
  pages = {1765-1775},
  keywords = {Container,Docker,KVM,Performance comparison,Virtual machine},
  author = {Chae, MinSu and Lee, HwaMin and Lee, Kiyeol},
  file = {/home/haozeke/Zotero/storage/GLULXZNS/Chae et al. - 2019 - A performance comparison of linux containers and v.pdf}
}

@inproceedings{yadavDockerContainersVirtual2019,
  langid = {english},
  location = {{Singapore}},
  title = {Docker {{Containers Versus Virtual Machine}}-{{Based Virtualization}}},
  isbn = {9789811315015},
  doi = {10.1007/978-981-13-1501-5_12},
  abstract = {Cloud computing is a paradigm based on IT that enables ubiquitous access to large pools of configurable resources which can be shared (such as computer networks, servers, storage, applications, and services) and rapidly provisioned with least effort. Cloud computing implementation in traditional ways is done using virtual machines, but nowadays a new concept of Docker containers is also gaining popularity due to its features. Containerization in some cases is treated as lightweight virtualization technique. Virtualization is used by cloud computing environments and data centres to disassociate the tools and applications from the underlying hardware. To validate this, hardware virtualization and OS-level virtualization is used. Apart from virtualization, a new technique is, containers, gaining popularity and many cloud-based deployments are using this technique. In this paper, both of these technologies are compared such that end-user can use these according to the requirement to get benefitted.},
  booktitle = {Emerging {{Technologies}} in {{Data Mining}} and {{Information Security}}},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  publisher = {{Springer}},
  date = {2019},
  pages = {141-150},
  keywords = {Container,Docker engine,Hypervisor,VMs},
  author = {Yadav, Anuj Kumar and Garg, M. L. and {Ritika}},
  editor = {Abraham, Ajith and Dutta, Paramartha and Mandal, Jyotsna Kumar and Bhattacharya, Abhishek and Dutta, Soumi}
}

@inproceedings{plauthPerformanceSurveyLightweight2017,
  langid = {english},
  location = {{Cham}},
  title = {A {{Performance Survey}} of {{Lightweight Virtualization Techniques}}},
  isbn = {978-3-319-67262-5},
  doi = {10.1007/978-3-319-67262-5_3},
  abstract = {The increasing prevalence of the microservice paradigm creates a new demand for low-overhead virtualization techniques. Complementing containerization, unikernels are emerging as alternative approaches. With both techniques undergoing rapid improvements, the current landscape of lightweight virtualization approaches presents a confusing scenery, complicating the task of choosing a suited technology for an intended purpose. This work provides a comprehensive performance comparison covering containers, unikernels, whole-system virtualization, native hardware, and combinations thereof. Representing common workloads in microservice-based applications, we assess application performance using HTTP servers and a key-value store. With the microservice deployment paradigm in mind, we evaluate further characteristics such as startup time, image size, network latency, and memory footprint.},
  booktitle = {Service-{{Oriented}} and {{Cloud Computing}}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  date = {2017},
  pages = {34-48},
  author = {Plauth, Max and Feinbube, Lena and Polze, Andreas},
  editor = {De Paoli, Flavio and Schulte, Stefan and Broch Johnsen, Einar}
}

@inproceedings{haydelEnhancingUsabilityUtilization2015,
  location = {{Piscataway, NJ, USA}},
  title = {Enhancing the {{Usability}} and {{Utilization}} of {{Accelerated Architectures}} via {{Docker}}},
  isbn = {978-0-7695-5697-0},
  url = {http://dl.acm.org/citation.cfm?id=3233397.3233456},
  abstract = {Accelerated architectures such as GPUs (Graphics Processing Units) and MICs (Many Integrated Cores) have been proven to increase the performance of many algorithms compared to their CPU counterparts and are widely available in local, campus-wide and national infrastructures, however, their utilization is not following the same pace as their deployment. Reasons for the underutilization lay partly on the software side with proprietary and complex interfaces for development and usage. A common API providing an extra layer to abstract the differences and specific characteristics of those architectures would deliver a far more portable interface for application developers. This cloud challenge proposal presents such an API that addresses these issues using a container-based approach. The resulting environment provides Docker-based containers for deploying accelerator libraries, such as CUDA Toolkit, OpenCL and OpenACC, onto a wide variety of different platforms and operating systems. By leveraging the container approach, we can overlay accelerator libraries onto the host without needing to be concerned about the intricacies of underlying operating system of the host. Docker therefore provides the advantage of being easily applicable on diverse architectures, virtualizing the necessary environment and including libraries as well as applications in a standardized way. The novelty of our approach is the extra layer for utilization and device discovery in this layer improving the usability and uniform development of accelerated methods with direct access to resources.},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Utility}} and {{Cloud Computing}}},
  series = {{{UCC}} '15},
  publisher = {{IEEE Press}},
  urldate = {2019-11-14},
  date = {2015},
  pages = {361--367},
  keywords = {accelerated architectures,cloud computing,docker,virtual machine,virtualization},
  author = {Haydel, Nicholas and Madey, Gregory and Gesing, Sandra and Dakkak, Abdul and de Gonzalo, Simon Garcia and Taylor, Ian and Hwu, Wen-mei W.},
  options = {useprefix=true},
  venue = {Limassol, Cyprus}
}

@article{goswamiGeneralTopologicalNetwork2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1909.09827},
  primaryClass = {cond-mat, physics:physics},
  title = {A {{General Topological Network Criterion}} for {{Exploring}} the {{Structure}} of {{Icy Nanoribbons}} and {{Monolayers}}},
  url = {http://arxiv.org/abs/1909.09827},
  abstract = {We develop intuitive metrics for quantifying complex nucleating systems under confinement. These are shown to arise naturally from the analysis of the topological ring network, and are amenable for use as order parameters for such systems. Drawing inspiration from qualitative visual inspection, we introduce a general topological criterion for elucidating the ordered structures of confined water, using a graph theoretic approach. Our criterion is based on primitive rings, and reinterprets the hydrogen-bond-network in terms of these primitives. This approach has no a priori assumptions, except the hydrogen bond definition, and may be used as an exploratory tool for the automated discovery of new ordered phases. We demonstrate the versatility of our criterion by applying it to analyse well-known monolayer ices. Our methodology is then extended to identify the building blocks of one-dimensional \$n\$-sided prismatic nanoribbon ices.},
  urldate = {2019-11-14},
  date = {2019-09-21},
  keywords = {Condensed Matter - Materials Science,Condensed Matter - Other Condensed Matter,Physics - Atomic and Molecular Clusters,Physics - Chemical Physics,Physics - Computational Physics},
  author = {Goswami, Amrita and Singh, Jayant K.}
}



@inproceedings{goswamiWailordParsersReproducibility2022,
  title = {Wailord: {{Parsers}} and {{Reproducibility}} for {{Quantum Chemistry}}},
  shorttitle = {Wailord},
  author = {Goswami, Rohit},
  date = {2022},
  journaltitle = {Proceedings of the 21st Python in Science Conference},
  pages = {193--197},
  doi = {10.25080/majora-212e5952-021},
  url = {https://conference.scipy.org/proceedings/scipy2022/rohit_goswami_wailord.html},
  urldate = {2022-07-14},
  eventtitle = {Proceedings of the 21st {{Python}} in {{Science Conference}}}
}

@article{kedwardStateFortran2022a,
  title = {The {{State}} of {{Fortran}}},
  author = {Kedward, Laurence J. and Aradi, Bálint and Čertík, Ondřej and Curcic, Milan and Ehlert, Sebastian and Engel, Philipp and Goswami, Rohit and Hirsch, Michael and Lozada-Blanco, Asdrubal and Magnin, Vincent and Markus, Arjen and Pagone, Emanuele and Pribec, Ivan and Richardson, Brad and Snyder, Harris and Urban, John and Vandenplas, Jérémie},
  date = {2022-03},
  journaltitle = {Computing in Science \& Engineering},
  volume = {24},
  number = {2},
  pages = {63--72},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2022.3159862},
  abstract = {A community of developers has formed to modernize the Fortran ecosystem. In this article, we describe the high-level features of Fortran that continue to make it a good choice for scientists and engineers in the 21st century. Ongoing efforts include the development of a Fortran standard library and package manager, the fostering of a friendly and welcoming online community, improved compiler support, and language feature development. The lessons learned are common across contemporary programming languages and help reduce the learning curve and increase adoption of Fortran.},
  eventtitle = {Computing in {{Science}} \& {{Engineering}}},
  keywords = {Arrays,Codes,Libraries,Parallel processing,Python,Standards,Syntactics}
}

@article{huberAutomatedReproducibleWorkflows2022,
  title = {Automated Reproducible Workflows and Data Provenance with {{AiiDA}}},
  author = {Huber, Sebastiaan P.},
  date = {2022-07},
  journaltitle = {Nature Reviews Physics},
  shortjournal = {Nat Rev Phys},
  volume = {4},
  number = {7},
  pages = {431--431},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5820},
  doi = {10.1038/s42254-022-00463-1},
  url = {https://www.nature.com/articles/s42254-022-00463-1},
  urldate = {2022-10-11},
  abstract = {Sebastiaan Huber describes AiiDA, an open source system to manage complex workflows and heterogeneous data.},
  issue = {7},
  langid = {english},
  keywords = {Condensed-matter physics}
}

@article{pengReproducibleResearchComputational2011,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  author = {Peng, Roger D.},
  date = {2011-12-02},
  journaltitle = {Science},
  volume = {334},
  number = {6060},
  eprint = {22144613},
  eprinttype = {pmid},
  pages = {1226--1227},
  issn = {0036-8075, 1095-9203},
  doi = {10/fdv356},
  url = {https://science.sciencemag.org/content/334/6060/1226},
  urldate = {2019-09-04},
  abstract = {{$<$}p{$>$}Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.{$<$}/p{$>$}},
  langid = {english},
  annotation = {00860}
}
