[{"categories":["code"],"contents":"I often need to set up quick virtual environments. Unfortunately, the standard approach to work with this in nix deals with the local nixpkgs mechanism for python dependencies:\n1nix-shell -p \u0026#34;python38.withPackages(ps: [ ps.numpy ps.sh ])\u0026#34; However there is a catch for packages which are not officially present upstream.\n1# Fails! 2nix-shell -p \u0026#34;python38.withPackages(ps: [ ps.numpy ps.sh ps.lieer ])\u0026#34; However, the mach-nix project can indeed be used to work around this, at the cost of a somewhat longer command.\n1# Works 2nix-shell -p \u0026#39;(callPackage (fetchTarball https://github.com/DavHau/mach-nix/tarball/3.0.2) {python=\u0026#34;python38\u0026#34;;}).mkPython{requirements=\u0026#34;numpy\\n lieer\\n ipython\u0026#34;;}\u0026#39; --command ipython mkPythonShell does not generate an environment which can be used here, since that is not a derivation which can be built. This is most useful in the context of systems which use asdf or other PATH shim approaches.\n","permalink":"https://rgoswami.me/snippets/mach-nix-shell-env/","tags":["nix","python","cmdline"],"title":"Mach-Nix and Shell Environments"},{"categories":["code"],"contents":"Very quick set of ugly commands to grab build environments. A much better approach is to make a custom Dockerfile or even better, use nix.\nHowever it does work in a pinch.\n1docker pull IMG:TAG 2sudo docker run -v LOCALDIR:DIRINDOCKER -it debian:experimental-20211115 bash 3# Don\u0026#39;t be root for long 4apt update 5apt install sudo vim zsh 6# Use the same username --\u0026gt; easier to manage permissions 7useradd -m -s /bin/zsh $USER -G sudo 8passwd $USER # Some crap 9# Or just add to the sudoers file 10echo \u0026#34;$USERALL=(ALL:ALL) ALL\u0026#34; \u0026gt;\u0026gt; /etc/sudoers 11su $USER 12# numpy stuff 13sudo apt install gcc gfortran libopenblas-dev python3.10-dev python3.10-dbg git python3-distutils python3-setuptools libx11-dev build-essential pkg-config python3-pip python3-pytest python3-hypothesis 14python3.10-dbg runtests.py -s f2py -vvv ","permalink":"https://rgoswami.me/snippets/docker-dev-envs/","tags":["devenv","cmdline"],"title":"Docker Development Environments"},{"categories":["code"],"contents":"Whenever I need to access a server running on an HPC which does not support ngrok or localtunnel or even gsocket; the fallback approach is always to rely on SSH port forwarding.\nThe sample problem here is running an HTTP server for viewing graphics in R via httpgd.\n1# Local 2export PORT=9899 \u0026amp;\u0026amp; ssh -L \u0026#34;${PORT}:localhost:${PORT}\u0026#34; \u0026#34;rog32@krafla.rhi.hi.is\u0026#34; -N -L \u0026#34;${PORT}:localhost:${PORT}\u0026#34; elja 3# New tab 4ssh krafla 5ssh elja 6radian # or R Now in R.\n1library(\u0026#34;httpgd\u0026#34;) 2# else install.packages(c(\u0026#34;httpgd\u0026#34;), repos=\u0026#39;https://cran.hafro.is/\u0026#39;) 3hgd(port=9899) Conda and R Annoyingly if you are depending on conda or micromamba or some variant thereof on the remote sever then install.packages will fail and instead the environment needs to have the corresponding R packages installed after searching anaconda.org.\n1# minimal 2micromamba create -p $(pwd)/.tmp -c conda-forge r-tidyverse git R 3micromamba activate $(pwd)/.tmp 4micromamba install -c nclibz r-httpgd These issues are not present when using a nix-shell.\n","permalink":"https://rgoswami.me/snippets/ssh-port-forwarding/","tags":["ssh"],"title":"SSH Port Forwarding"},{"categories":["code"],"contents":"nix aside1, I recently shifted to using asdf to manage different language versions.\n1git clone https://github.com/asdf-vm/asdf.git ~/.asdf --branch v0.8.1 The main reason to prefer asdf over competing language specific options like rvm or pyenv or nvm and company is simply uniformity of the interface. This can be coupled with zinit snippet OMZ::plugins/asdf for loading the completions. Note that the installation steps can take a while especially if openssl is being installed.\nConfiguration Actually one weird aspect of asdf is that it pollutes $HOME by default instead of respectfully storing its configuration in $HOME/.config/asdf like every other program. To change this; exporting ASDF_CONFIG_FILE works. Essentially a zsh configuration with zinit would look like this2:\n1if [[ ! -d ~/.asdf ]]; then 2mkdir ~/.asdf 3git clone https://github.com/asdf-vm/asdf.git ~/.asdf 4fi 56export ASDF_CONFIG_FILE=\u0026#34;~/.config/asdf/asdfrc\u0026#34; 7zinit snippet OMZ::plugins/asdf asdf has an option; legacy_version_file = yes which is meant to ensure that existing solutions (e.g. .nvmrc or .node-version) are compatible. There is also $HOME/.tool-versions for trickle-down language versions; that is language-version pairs defined in such files are activated for the directory and lower sub-directories. The rest of a reasonable configuration (described here) might look like:\n1legacy_version_file = yes 2always_keep_download = no Ruby This plugin leverages ruby-build and so can take a pick up an extended set of environment variables; it supports .ruby-version files as well.\n1asdf plugin add ruby 2asdf list all ruby 3asdf install ruby 3.0.2 4asdf global ruby 3.0.2 # Convenience NodeJS This plugin respects .nvmrc and .node-version files.\n1asdf plugin add nodejs 2asdf global nodejs latest 3asdf install nodejs latest Python Builds with python-build.\n1asdf plugin add python 2asdf install python 3.9.7 3asdf global python 3.9.7 Direnv I\u0026rsquo;m a huge fan of direnv; and a fantastic little plugin (with neat hyperfine benchmarks) for asdf removes a layer of shim indirection while playing nicely with direnv.\n1asdf plugin-add direnv 2asdf install direnv latest 3asdf global direnv latest This comes with an associated set of shell profile instructions:\n1# File: ~/.zshrc 2# Hook direnv into your shell. 3eval \u0026#34;$(asdf exec direnv hook zsh)\u0026#34; 4# A shortcut for asdf managed direnv. 5direnv() { asdf exec direnv \u0026#34;$@\u0026#34;; } Along with a commiserate $HOME/.config/direnv/direnvrc requirement:\n1source \u0026#34;$(asdf direnv hook asdf)\u0026#34; Now every new .envrc can start with use asdf which will now speed up evaluations.\nConclusions This method appears to be more robust than remembering the various idiosyncrasies and logic of a host of other tools.\n  This was during the build plan dynamism RFCs which themselves were symptomatic of the {yarn,composer,node,*}2nix issues\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This structure can be seen in my own Dotfiles as well\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/snippets/prog-lang-man/","tags":["shell","lang"],"title":"Programming Language Management"},{"categories":["code"],"contents":"My personal favorite for watching files and running context sensitive commands is to use the lovely filewatcher CLI utility written in Ruby.\n1gem install filewatcher 2gem install filewatcher-cli This can then be used with:\n1filewatcher \u0026#39;**/*.js\u0026#39; \u0026#39;node $FILENAME\u0026#39; However this hasn\u0026rsquo;t been updated in a while now and fails on newer versions of Ruby. So now I use watchexec.\n1cargo install watchexec-cli 2watchexec -w source/f2py \u0026#39;make html\u0026#39; ","permalink":"https://rgoswami.me/snippets/watch-files/","tags":["productivity","cmdline","ruby","rust"],"title":"Watching Files"},{"categories":["code"],"contents":"Most often it makes more sense to map the same ports on every intermediate machine.\n1Host super 2Hostname super.machine.location.is 3IdentityFile ~/.ssh/mykey 4User myuser 5LocalForward 8001 localhost:8001 6LocalForward 8002 localhost:8002 7LocalForward 8003 localhost:8003 8LocalForward 8004 localhost:8004 This is good for interactive sessions with multiple servers. For single servers, reverse proxy tunnels are more efficient.\n","permalink":"https://rgoswami.me/snippets/forward-multiport/","tags":["ssh"],"title":"Forwarding Multiple Local Ports"},{"categories":["code"],"contents":"1doxygen191 = pkgs.doxygen.overrideAttrs (_: rec { 2name = \u0026#34;doxygen-1.9.1\u0026#34;; 3src = pkgs.fetchurl { 4urls = [ 5\u0026#34;mirror://sourceforge/doxygen/${name}.src.tar.gz\u0026#34; # faster, with https, etc. 6\u0026#34;http://doxygen.nl/files/${name}.src.tar.gz\u0026#34; 7]; 8sha256 = \u0026#34;1lcif1qi20gf04qyjrx7x367669g17vz2ilgi4cmamp1whdsxbk7\u0026#34;; 9}; 10}); ","permalink":"https://rgoswami.me/snippets/nix-collection-overwrite-attrs/","tags":["nix"],"title":"Overwriting Attributes"},{"categories":["programming"],"contents":" Setup details are described here, and the meta-post about these solutions is here.\n Materials The summmer course1 is based off of the second edition of Statistical Rethinking by Richard McElreath.\n Chapter 13  E{1,2,3,4,5}   Chapter 14  E{1,2,3}    Packages A colophon with details is provided at the end, but the following packages and theme parameters are used throughout.\n1libsUsed\u0026lt;-c(\u0026#34;tidyverse\u0026#34;,\u0026#34;tidybayes\u0026#34;,\u0026#34;orgutils\u0026#34;,\u0026#34;dagitty\u0026#34;, 2\u0026#34;rethinking\u0026#34;,\u0026#34;tidybayes.rethinking\u0026#34;, 3\u0026#34;ggplot2\u0026#34;,\u0026#34;kableExtra\u0026#34;,\u0026#34;dplyr\u0026#34;,\u0026#34;glue\u0026#34;, 4\u0026#34;latex2exp\u0026#34;,\u0026#34;data.table\u0026#34;,\u0026#34;printr\u0026#34;,\u0026#34;devtools\u0026#34;) 5invisible(lapply(libsUsed, library, character.only = TRUE)); 6theme_set(theme_grey(base_size=24)) 7set.seed(1995) Chapter XIII: Models With Memory Easy Questions (Ch13) HOLD 13E1 Which of the following priors will produce more shrinkage in the estimates? (a) \\(α_{\\mathrm{TANK}}∼\\mathrm{Normal}(0,1)\\); (b) \\(α_{\\mathrm{TANK}}∼\\mathrm{Normal}(0,2)\\).\nSolution The normal distribution fits a probability distribution centered around the mean and the spread is given by the standard deviation. Thus the first option, (a) will produce more shrinkage in the estimates, as the prior will be more concentrated.\n1curve(dnorm(x,0,1),from=-10,to=10,col=\u0026#34;red\u0026#34;,ylab=\u0026#34;density\u0026#34;) 2curve(dnorm(x,0,2),add=TRUE) 3legend(\u0026#34;topright\u0026#34;, 4col = c(\u0026#34;red\u0026#34;,\u0026#34;black\u0026#34;), 5pch = 19, 6legend = c(\u0026#34;Normal(0,1)\u0026#34;,\u0026#34;Normal(0,2)\u0026#34;))  13E2 Rewrite the following model as a multilevel model.\nSolution The model can be expressed as:\nThe priors have been chosen to be essentially uninformative, as is appropriate for a situation where no further insight is present for the hyperparameters.\n13E3 Rewrite the following model as a multilevel model.\nSolution The model can be defined as:\nHOLD 13E4 Write a mathematical model formula for a Poisson regression with varying intercepts.\nSolution 13E5 Write a mathematical model formula for a Poisson regression with two different kinds of varying intercepts, a cross-classified model.\nSolution We will use the non-centered form for the cross-classified model.\nChapter XIV: Adventures in Covariance Easy Questions (Ch14) HOLD 14E1 Add to the following model varying slopes on the predictor \\(x\\).\nSolution Following the convention in the physical sciences, I will use square brackets for matrices and parenthesis for vectors.\nWhere we do not have any information so have used a standard weakly informative LKJcorr prior for correlation matrices which is flat for all valid correlation matrices. We also use weakly uninformative priors for the standard deviations among slopes and intercepts.\nHOLD 14E2 Think up a context in which varying intercepts will be positively correlated with varying slopes. Provide a mechanistic explanation for the correlation.\nSolution We note at the onset that the concept of varying intercepts is to account for blocks or sub-groups in our problem. This means that the clusters in our data which have higher average values will show a stronger positive association with predictor variables. To augment the example of the tadpoles in the book, if the data is arranged as:\n Tadpoles in tanks Some tanks have larger tadpoles (different species) which grow faster  For a repeated measurement in an interval of time, there will be a positive correlation between the initial height and the slope.\nHOLD 14E3 When is it possible for a varying slopes model to have fewer effective parameters (as estimated by WAIC or PSIS) than the corresponding model with fixed (unpooled) slopes? Explain.\nSolution The varying effects essentially causes regularization or shrinkage towards the global mean to prevent overfitting to the individual data-points. Consider the example from the text, for the chimpanzee experiment.\n1data(chimpanzees) 2d \u0026lt;- chimpanzees 3d$block_id \u0026lt;- d$block 4d$treatment \u0026lt;- 1L + d$prosoc_left + 2L*d$condition 5dat \u0026lt;- list( 6L = d$pulled_left, 7tid = as.integer(d$treatment), 8actor = d$actor ) We will set up a simple fixed effects model.\n1m14fix \u0026lt;- ulam( 2alist( 3L ~ dbinom( 1 , p ) , 4logit(p) \u0026lt;- alpha[actor] + beta[tid] , 5alpha[actor] ~ dnorm( 0 , 5 ), 6beta[tid] ~ dnorm( 0 , 0.5 ) 7) , data=dat , chains=4 , log_lik=TRUE ) 1SAMPLING FOR MODEL \u0026#39;90fe1cae14bc2bf32f08b4d71c2d1f0d\u0026#39; NOW (CHAIN 1). 2Chain 1: 3Chain 1: Gradient evaluation took 9.2e-05 seconds 4Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.92 seconds. 5Chain 1: Adjust your expectations accordingly! 6Chain 1: 7Chain 1: 8Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) 9Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) 10Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) 11Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) 12Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) 13Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) 14Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) 15Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) 16Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) 17Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) 18Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) 19Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) 20Chain 1: 21Chain 1: Elapsed Time: 0.335488 seconds (Warm-up) 22Chain 1: 0.228533 seconds (Sampling) 23Chain 1: 0.564021 seconds (Total) 24Chain 1: 2526SAMPLING FOR MODEL \u0026#39;90fe1cae14bc2bf32f08b4d71c2d1f0d\u0026#39; NOW (CHAIN 2). 27Chain 2: 28Chain 2: Gradient evaluation took 3.4e-05 seconds 29Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.34 seconds. 30Chain 2: Adjust your expectations accordingly! 31Chain 2: 32Chain 2: 33Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup) 34Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) 35Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) 36Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) 37Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) 38Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) 39Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) 40Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) 41Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) 42Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) 43Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) 44Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) 45Chain 2: 46Chain 2: Elapsed Time: 0.365676 seconds (Warm-up) 47Chain 2: 0.330942 seconds (Sampling) 48Chain 2: 0.696618 seconds (Total) 49Chain 2: 5051SAMPLING FOR MODEL \u0026#39;90fe1cae14bc2bf32f08b4d71c2d1f0d\u0026#39; NOW (CHAIN 3). 52Chain 3: 53Chain 3: Gradient evaluation took 4.3e-05 seconds 54Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.43 seconds. 55Chain 3: Adjust your expectations accordingly! 56Chain 3: 57Chain 3: 58Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) 59Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) 60Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) 61Chain 3: Iteration: 300 / 1000 [ 30%] (Warmup) 62Chain 3: Iteration: 400 / 1000 [ 40%] (Warmup) 63Chain 3: Iteration: 500 / 1000 [ 50%] (Warmup) 64Chain 3: Iteration: 501 / 1000 [ 50%] (Sampling) 65Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) 66Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) 67Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) 68Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) 69Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) 70Chain 3: 71Chain 3: Elapsed Time: 0.356879 seconds (Warm-up) 72Chain 3: 0.352045 seconds (Sampling) 73Chain 3: 0.708924 seconds (Total) 74Chain 3: 7576SAMPLING FOR MODEL \u0026#39;90fe1cae14bc2bf32f08b4d71c2d1f0d\u0026#39; NOW (CHAIN 4). 77Chain 4: 78Chain 4: Gradient evaluation took 4.5e-05 seconds 79Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.45 seconds. 80Chain 4: Adjust your expectations accordingly! 81Chain 4: 82Chain 4: 83Chain 4: Iteration: 1 / 1000 [ 0%] (Warmup) 84Chain 4: Iteration: 100 / 1000 [ 10%] (Warmup) 85Chain 4: Iteration: 200 / 1000 [ 20%] (Warmup) 86Chain 4: Iteration: 300 / 1000 [ 30%] (Warmup) 87Chain 4: Iteration: 400 / 1000 [ 40%] (Warmup) 88Chain 4: Iteration: 500 / 1000 [ 50%] (Warmup) 89Chain 4: Iteration: 501 / 1000 [ 50%] (Sampling) 90Chain 4: Iteration: 600 / 1000 [ 60%] (Sampling) 91Chain 4: Iteration: 700 / 1000 [ 70%] (Sampling) 92Chain 4: Iteration: 800 / 1000 [ 80%] (Sampling) 93Chain 4: Iteration: 900 / 1000 [ 90%] (Sampling) 94Chain 4: Iteration: 1000 / 1000 [100%] (Sampling) 95Chain 4: 96Chain 4: Elapsed Time: 0.296276 seconds (Warm-up) 97Chain 4: 0.261692 seconds (Sampling) 98Chain 4: 0.557968 seconds (Total) 99Chain 4: Now we can extend this to a varying slopes model, where we will consider varying slopes for actors.\n1m14ppool \u0026lt;- ulam( 2alist( 3L ~ dbinom( 1 , p ) , 4logit(p) \u0026lt;- alpha + a[actor]*vary_id + beta[tid], 5alpha ~ dnorm( 0 , 5 ), 6a[actor] ~ dnorm( 0 , 1 ), 7beta[tid] ~ dnorm( 0 , 0.5 ), 8vary_id ~ dexp( 1 ) 9) , data=dat , chains=4 , log_lik=TRUE ) 1SAMPLING FOR MODEL \u0026#39;d14d0bbe9399ac4de917b8e279a6e9e5\u0026#39; NOW (CHAIN 1). 2Chain 1: 3Chain 1: Gradient evaluation took 0.000184 seconds 4Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.84 seconds. 5Chain 1: Adjust your expectations accordingly! 6Chain 1: 7Chain 1: 8Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) 9Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) 10Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) 11Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) 12Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) 13Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) 14Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) 15Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) 16Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) 17Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) 18Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) 19Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) 20Chain 1: 21Chain 1: Elapsed Time: 1.63295 seconds (Warm-up) 22Chain 1: 1.18708 seconds (Sampling) 23Chain 1: 2.82003 seconds (Total) 24Chain 1: 2526SAMPLING FOR MODEL \u0026#39;d14d0bbe9399ac4de917b8e279a6e9e5\u0026#39; NOW (CHAIN 2). 27Chain 2: 28Chain 2: Gradient evaluation took 6.3e-05 seconds 29Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.63 seconds. 30Chain 2: Adjust your expectations accordingly! 31Chain 2: 32Chain 2: 33Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup) 34Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) 35Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) 36Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) 37Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) 38Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) 39Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) 40Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) 41Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) 42Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) 43Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) 44Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) 45Chain 2: 46Chain 2: Elapsed Time: 1.58303 seconds (Warm-up) 47Chain 2: 1.30156 seconds (Sampling) 48Chain 2: 2.88459 seconds (Total) 49Chain 2: 5051SAMPLING FOR MODEL \u0026#39;d14d0bbe9399ac4de917b8e279a6e9e5\u0026#39; NOW (CHAIN 3). 52Chain 3: 53Chain 3: Gradient evaluation took 6.4e-05 seconds 54Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.64 seconds. 55Chain 3: Adjust your expectations accordingly! 56Chain 3: 57Chain 3: 58Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) 59Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) 60Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) 61Chain 3: Iteration: 300 / 1000 [ 30%] (Warmup) 62Chain 3: Iteration: 400 / 1000 [ 40%] (Warmup) 63Chain 3: Iteration: 500 / 1000 [ 50%] (Warmup) 64Chain 3: Iteration: 501 / 1000 [ 50%] (Sampling) 65Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) 66Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) 67Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) 68Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) 69Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) 70Chain 3: 71Chain 3: Elapsed Time: 1.37077 seconds (Warm-up) 72Chain 3: 1.12109 seconds (Sampling) 73Chain 3: 2.49186 seconds (Total) 74Chain 3: 7576SAMPLING FOR MODEL \u0026#39;d14d0bbe9399ac4de917b8e279a6e9e5\u0026#39; NOW (CHAIN 4). 77Chain 4: 78Chain 4: Gradient evaluation took 5.9e-05 seconds 79Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.59 seconds. 80Chain 4: Adjust your expectations accordingly! 81Chain 4: 82Chain 4: 83Chain 4: Iteration: 1 / 1000 [ 0%] (Warmup) 84Chain 4: Iteration: 100 / 1000 [ 10%] (Warmup) 85Chain 4: Iteration: 200 / 1000 [ 20%] (Warmup) 86Chain 4: Iteration: 300 / 1000 [ 30%] (Warmup) 87Chain 4: Iteration: 400 / 1000 [ 40%] (Warmup) 88Chain 4: Iteration: 500 / 1000 [ 50%] (Warmup) 89Chain 4: Iteration: 501 / 1000 [ 50%] (Sampling) 90Chain 4: Iteration: 600 / 1000 [ 60%] (Sampling) 91Chain 4: Iteration: 700 / 1000 [ 70%] (Sampling) 92Chain 4: Iteration: 800 / 1000 [ 80%] (Sampling) 93Chain 4: Iteration: 900 / 1000 [ 90%] (Sampling) 94Chain 4: Iteration: 1000 / 1000 [100%] (Sampling) 95Chain 4: 96Chain 4: Elapsed Time: 1.48561 seconds (Warm-up) 97Chain 4: 1.38809 seconds (Sampling) 98Chain 4: 2.8737 seconds (Total) 99Chain 4: Now we can test the number of parameters.\n1compare(m14ppool,m14fix) %\u0026gt;% toOrg 1| row.names | WAIC | SE | dWAIC | dSE | pWAIC | weight | 2|-----------+------------------+------------------+------------------+------------------+------------------+-------------------| 3| m14ppool | 532.211705503729 | 19.5177343184252 | 0 | NA | 9.12911563615787 | 0.796616007296431 | 4| m14fix | 534.942259470042 | 18.0912938913487 | 2.73055396631241 | 1.66292448384092 | 8.10370201332515 | 0.203383992703568 | As we can see, the model with partial pooling has only one effective additional parameter, even though the model without pooling has \\(n(\\mathrm{actor})\\) intercepts (one per actor) with a standard deviation, while the partial pooling parameter has an additional average intercept and a standard deviation parameter.\nBoth the models have around the same number of effective parameters, which mean that the additional parameters do not actually cause additional overfitting. This simply implies that for Bayesian models, the raw number of model parameters does not correspond necessarily to a model with more overfitting.\nIn general, we should keep in mind that the effective number of parameters, when the variation among clusters is high, is probably going to be lower than the total number of parameters, due to adaptive regularization.\nA: Colophon To ensure that this document is fully reproducible at a later date, we will record the session info.\n1devtools::session_info() 1─ Session info ─────────────────────────────────────────────────────────────── 2setting value 3version R version 4.0.0 (2020-04-24) 4os Arch Linux 5system x86_64, linux-gnu 6ui X11 7language (EN) 8collate en_US.UTF-8 9ctype en_US.UTF-8 10tz Iceland 11date 2020-06-27 1213─ Packages ─────────────────────────────────────────────────────────────────── 14package * version date lib source 15arrayhelpers 1.1-0 2020-02-04 [167] CRAN (R 4.0.0) 16assertthat 0.2.1 2019-03-21 [34] CRAN (R 4.0.0) 17backports 1.1.6 2020-04-05 [68] CRAN (R 4.0.0) 18boot 1.3-24 2019-12-20 [5] CRAN (R 4.0.0) 19broom 0.5.6 2020-04-20 [67] CRAN (R 4.0.0) 20callr 3.4.3 2020-03-28 [87] CRAN (R 4.0.0) 21cellranger 1.1.0 2016-07-27 [55] CRAN (R 4.0.0) 22cli 2.0.2 2020-02-28 [33] CRAN (R 4.0.0) 23coda 0.19-3 2019-07-05 [169] CRAN (R 4.0.0) 24colorspace 1.4-1 2019-03-18 [97] CRAN (R 4.0.0) 25crayon 1.3.4 2017-09-16 [35] CRAN (R 4.0.0) 26curl 4.3 2019-12-02 [26] CRAN (R 4.0.0) 27dagitty * 0.2-2 2016-08-26 [244] CRAN (R 4.0.0) 28data.table * 1.12.8 2019-12-09 [27] CRAN (R 4.0.0) 29DBI 1.1.0 2019-12-15 [77] CRAN (R 4.0.0) 30dbplyr 1.4.3 2020-04-19 [76] CRAN (R 4.0.0) 31desc 1.2.0 2018-05-01 [84] CRAN (R 4.0.0) 32devtools * 2.3.0 2020-04-10 [219] CRAN (R 4.0.0) 33digest 0.6.25 2020-02-23 [42] CRAN (R 4.0.0) 34dplyr * 0.8.5 2020-03-07 [69] CRAN (R 4.0.0) 35ellipsis 0.3.0 2019-09-20 [30] CRAN (R 4.0.0) 36evaluate 0.14 2019-05-28 [82] CRAN (R 4.0.0) 37fansi 0.4.1 2020-01-08 [36] CRAN (R 4.0.0) 38forcats * 0.5.0 2020-03-01 [29] CRAN (R 4.0.0) 39fs 1.4.1 2020-04-04 [109] CRAN (R 4.0.0) 40generics 0.0.2 2018-11-29 [71] CRAN (R 4.0.0) 41ggplot2 * 3.3.0 2020-03-05 [78] CRAN (R 4.0.0) 42glue * 1.4.0 2020-04-03 [37] CRAN (R 4.0.0) 43gridExtra 2.3 2017-09-09 [123] CRAN (R 4.0.0) 44gtable 0.3.0 2019-03-25 [79] CRAN (R 4.0.0) 45haven 2.2.0 2019-11-08 [28] CRAN (R 4.0.0) 46hms 0.5.3 2020-01-08 [44] CRAN (R 4.0.0) 47htmltools 0.4.0 2019-10-04 [112] CRAN (R 4.0.0) 48httr 1.4.1 2019-08-05 [100] CRAN (R 4.0.0) 49inline 0.3.15 2018-05-18 [162] CRAN (R 4.0.0) 50jsonlite 1.6.1 2020-02-02 [101] CRAN (R 4.0.0) 51kableExtra * 1.1.0 2019-03-16 [212] CRAN (R 4.0.0) 52knitr 1.28 2020-02-06 [113] CRAN (R 4.0.0) 53latex2exp * 0.4.0 2015-11-30 [211] CRAN (R 4.0.0) 54lattice 0.20-41 2020-04-02 [6] CRAN (R 4.0.0) 55lifecycle 0.2.0 2020-03-06 [38] CRAN (R 4.0.0) 56loo 2.2.0 2019-12-19 [163] CRAN (R 4.0.0) 57lubridate 1.7.8 2020-04-06 [106] CRAN (R 4.0.0) 58magrittr 1.5 2014-11-22 [21] CRAN (R 4.0.0) 59MASS 7.3-51.5 2019-12-20 [7] CRAN (R 4.0.0) 60matrixStats 0.56.0 2020-03-13 [164] CRAN (R 4.0.0) 61memoise 1.1.0 2017-04-21 [229] CRAN (R 4.0.0) 62modelr 0.1.6 2020-02-22 [107] CRAN (R 4.0.0) 63munsell 0.5.0 2018-06-12 [96] CRAN (R 4.0.0) 64mvtnorm 1.1-0 2020-02-24 [243] CRAN (R 4.0.0) 65nlme 3.1-147 2020-04-13 [11] CRAN (R 4.0.0) 66orgutils * 0.4-1 2017-03-21 [209] CRAN (R 4.0.0) 67pillar 1.4.3 2019-12-20 [39] CRAN (R 4.0.0) 68pkgbuild 1.0.6 2019-10-09 [86] CRAN (R 4.0.0) 69pkgconfig 2.0.3 2019-09-22 [43] CRAN (R 4.0.0) 70pkgload 1.0.2 2018-10-29 [83] CRAN (R 4.0.0) 71plyr 1.8.6 2020-03-03 [73] CRAN (R 4.0.0) 72prettyunits 1.1.1 2020-01-24 [58] CRAN (R 4.0.0) 73printr * 0.1 2017-05-19 [214] CRAN (R 4.0.0) 74processx 3.4.2 2020-02-09 [88] CRAN (R 4.0.0) 75ps 1.3.2 2020-02-13 [89] CRAN (R 4.0.0) 76purrr * 0.3.4 2020-04-17 [50] CRAN (R 4.0.0) 77R6 2.4.1 2019-11-12 [48] CRAN (R 4.0.0) 78Rcpp 1.0.4.6 2020-04-09 [10] CRAN (R 4.0.0) 79readr * 1.3.1 2018-12-21 [45] CRAN (R 4.0.0) 80readxl 1.3.1 2019-03-13 [54] CRAN (R 4.0.0) 81remotes 2.1.1 2020-02-15 [233] CRAN (R 4.0.0) 82reprex 0.3.0 2019-05-16 [108] CRAN (R 4.0.0) 83rethinking * 2.01 2020-06-06 [242] local 84rlang 0.4.5 2020-03-01 [31] CRAN (R 4.0.0) 85rmarkdown 2.1 2020-01-20 [110] CRAN (R 4.0.0) 86rprojroot 1.3-2 2018-01-03 [85] CRAN (R 4.0.0) 87rstan * 2.19.3 2020-02-11 [161] CRAN (R 4.0.0) 88rstudioapi 0.11 2020-02-07 [91] CRAN (R 4.0.0) 89rvest 0.3.5 2019-11-08 [120] CRAN (R 4.0.0) 90scales 1.1.0 2019-11-18 [93] CRAN (R 4.0.0) 91sessioninfo 1.1.1 2018-11-05 [231] CRAN (R 4.0.0) 92shape 1.4.4 2018-02-07 [193] CRAN (R 4.0.0) 93StanHeaders * 2.19.2 2020-02-11 [165] CRAN (R 4.0.0) 94stringi 1.4.6 2020-02-17 [52] CRAN (R 4.0.0) 95stringr * 1.4.0 2019-02-10 [74] CRAN (R 4.0.0) 96svUnit 1.0.3 2020-04-20 [168] CRAN (R 4.0.0) 97testthat 2.3.2 2020-03-02 [81] CRAN (R 4.0.0) 98textutils 0.2-0 2020-01-07 [210] CRAN (R 4.0.0) 99tibble * 3.0.1 2020-04-20 [32] CRAN (R 4.0.0) 100tidybayes * 2.0.3 2020-04-04 [166] CRAN (R 4.0.0) 101tidybayes.rethinking * 2.0.3.9000 2020-06-07 [246] local 102tidyr * 1.0.2 2020-01-24 [75] CRAN (R 4.0.0) 103tidyselect 1.0.0 2020-01-27 [49] CRAN (R 4.0.0) 104tidyverse * 1.3.0 2019-11-21 [66] CRAN (R 4.0.0) 105usethis * 1.6.0 2020-04-09 [238] CRAN (R 4.0.0) 106V8 3.0.2 2020-03-14 [245] CRAN (R 4.0.0) 107vctrs 0.2.4 2020-03-10 [41] CRAN (R 4.0.0) 108viridisLite 0.3.0 2018-02-01 [99] CRAN (R 4.0.0) 109webshot 0.5.2 2019-11-22 [213] CRAN (R 4.0.0) 110withr 2.2.0 2020-04-20 [90] CRAN (R 4.0.0) 111xfun 0.13 2020-04-13 [116] CRAN (R 4.0.0) 112xml2 1.3.2 2020-04-23 [122] CRAN (R 4.0.0) 113114[1] /nix/store/xzd8h53xkyvfm3kvj5ab6znp685wi04w-r-car-3.0-7/library 115[2] /nix/store/mhr8zw9bmxarc3n821b83i0gz2j9zlrq-r-abind-1.4-5/library 116[3] /nix/store/hp86nhr0787vib3l8mkw0gf9nxwb45im-r-carData-3.0-3/library 117[4] /nix/store/vhw7s2h5ds6sp110z2yvilchv8j9jch5-r-lme4-1.1-23/library 118[5] /nix/store/987n8g0zy9sjvfvnsck1bkkcknw05yvb-r-boot-1.3-24/library 119[6] /nix/store/jxxxxyz4c1k5g3drd35gsrbjdg028d11-r-lattice-0.20-41/library 120[7] /nix/store/q9zfm5h53m8rd08xcsdcwaag31k4z1pf-r-MASS-7.3-51.5/library 121[8] /nix/store/kjkm50sr144yvrhl5axfgykbiy13pbmg-r-Matrix-1.2-18/library 122[9] /nix/store/8786z5lgy8h3akfjgj3yq5yq4s17rhjy-r-minqa-1.2.4/library 123[10] /nix/store/93wv3j0z1nzqp6fjsm9v7v8bf8d1xkm2-r-Rcpp-1.0.4.6/library 124[11] /nix/store/akfw6zsmawmz8lmjkww0rnqrazm4mqp0-r-nlme-3.1-147/library 125[12] /nix/store/rxs0d9bbn8qhw7wmkfb21yk5abp6lpq1-r-nloptr-1.2.2.1/library 126[13] /nix/store/8n0jfiqn4275i58qgld0dv8zdaihdzrk-r-RcppEigen-0.3.3.7.0/library 127[14] /nix/store/8vxrma33rhc96260zsi1jiw7dy3v2mm4-r-statmod-1.4.34/library 128[15] /nix/store/2y46pb5x9lh8m0hdmzajnx7sc1bk9ihl-r-maptools-0.9-9/library 129[16] /nix/store/iwf9nxx1v883wlv0p88q947hpz5lhfh7-r-foreign-0.8-78/library 130[17] /nix/store/rl9sjqply6rjbnz5k792ghm62ybv76px-r-sp-1.4-1/library 131[18] /nix/store/ws4bkzyv2vj5pyn1hgwyy6nlp48arz0n-r-mgcv-1.8-31/library 132[19] /nix/store/307dzxrmnqk4p86560a02r64x1fhhmxb-r-nnet-7.3-13/library 133[20] /nix/store/g2zpzkdb9hzkza1wpcbrk58119v1wyaf-r-pbkrtest-0.4-8.6/library 134[21] /nix/store/p0l503fr8960vld70w6ilmknxs5qwq77-r-magrittr-1.5/library 135[22] /nix/store/rmjpcaw3i446kwnjgcxcaid0yac36cj2-r-quantreg-5.55/library 136[23] /nix/store/10mzmnvc5jjgk2xzasia522pk60a30qz-r-MatrixModels-0.4-1/library 137[24] /nix/store/6qwdzvmnnmhjwdnvg2zmvv6wafd1vf91-r-SparseM-1.78/library 138[25] /nix/store/aa9c39a3yiqkh1h7pbngjlbr7czvc7yi-r-rio-0.5.16/library 139[26] /nix/store/2fx4vqlybgwp5rhhy6pssqx7h1a927fn-r-curl-4.3/library 140[27] /nix/store/k4m3fn1kqvvvn8y33kd57gq49hr3ar8y-r-data.table-1.12.8/library 141[28] /nix/store/651hfjylqzmsf565wyx474vyjny771gy-r-haven-2.2.0/library 142[29] /nix/store/a3rnz28irmqvmj8axj5x5j1am2c3gzs4-r-forcats-0.5.0/library 143[30] /nix/store/j8v4gzib137q2cml31hvvfkrc0f60pp5-r-ellipsis-0.3.0/library 144[31] /nix/store/xaswqlnamf4k8vwx0x3wav3l0x60sag0-r-rlang-0.4.5/library 145[32] /nix/store/dqm3xpix2jwhhhr67s6fgrwbw7hizap7-r-tibble-3.0.1/library 146[33] /nix/store/v7xfsq6d97wpn6m0hjrac78w5xawbr8a-r-cli-2.0.2/library 147[34] /nix/store/fikjasr98klhk9cf44x4lhi57vh3pmkg-r-assertthat-0.2.1/library 148[35] /nix/store/3fya6cd38vsqdj0gjb7bcsy00sirlyw1-r-crayon-1.3.4/library 149[36] /nix/store/payqi9bwh216rwhaq07jgc26l4fv1zsb-r-fansi-0.4.1/library 150[37] /nix/store/h6a61ghws7yrdxlg412xl1im37z5r28i-r-glue-1.4.0/library 151[38] /nix/store/y8mjbia1wbnq26dkigr0p3xxwrbzsc2r-r-lifecycle-0.2.0/library 152[39] /nix/store/kwaghh12cnifgvcbvlv2anx0hd5f4ild-r-pillar-1.4.3/library 153[40] /nix/store/k1phn8j10nni7gzvcgp0vc25dby6bb77-r-utf8-1.1.4/library 154[41] /nix/store/k3b77y8v7zsshpp1ccs8jwk2i2g4rm9a-r-vctrs-0.2.4/library 155[42] /nix/store/iibjmbh7vj0d0bfafz98yn29ymg43gkw-r-digest-0.6.25/library 156[43] /nix/store/aqsj4k3pgm80qk4jjg7sh3ac28n6alv0-r-pkgconfig-2.0.3/library 157[44] /nix/store/i7c5v8s4hd9rlqah3bbvy06yywjqwdgk-r-hms-0.5.3/library 158[45] /nix/store/2fyrk58cmcbrxid66rbwjli7y114lvrm-r-readr-1.3.1/library 159[46] /nix/store/163xq2g5nblqgh7qhvzb6mvgg6qdrirj-r-BH-1.72.0-3/library 160[47] /nix/store/dr27b6k49prwgrjs0v30b6mf5lxa36pk-r-clipr-0.7.0/library 161[48] /nix/store/bghvqg9mcaj2jkbwpy0di6c563v24acz-r-R6-2.4.1/library 162[49] /nix/store/nq8jdq7nlg9xns4xpgyj6sqv8p4ny1wz-r-tidyselect-1.0.0/library 163[50] /nix/store/zlwhf75qld7vmwx3d4bdws057ld4mqbp-r-purrr-0.3.4/library 164[51] /nix/store/0gbmmnbpqlr69l573ymkcx8154fvlaca-r-openxlsx-4.1.4/library 165[52] /nix/store/1m1q4rmwx56dvx9rdzfsfq0jpw3hw0yx-r-stringi-1.4.6/library 166[53] /nix/store/mhy5vnvbsl4q7dcinwx3vqlyywxphbfd-r-zip-2.0.4/library 167[54] /nix/store/88sp7f7q577i6l5jjanqiv5ak6nv5357-r-readxl-1.3.1/library 168[55] /nix/store/6q9zwivzalhmzdracc8ma932wirq8rl5-r-cellranger-1.1.0/library 169[56] /nix/store/jh2n6k2ancdzqych5ix8n4rq9w514qq9-r-rematch-1.0.1/library 170[57] /nix/store/22xjqikqd6q556absb5224sbx6q0kp0c-r-progress-1.2.2/library 171[58] /nix/store/9vp32wa1qvv6lkq6p70qlli5whrxzfbi-r-prettyunits-1.1.1/library 172[59] /nix/store/r9rhqb6fsk75shihmb7nagqb51pqwp0y-r-class-7.3-16/library 173[60] /nix/store/z1kad071y43wij1ml9lpghh7jbimmcli-r-cluster-2.1.0/library 174[61] /nix/store/i8wr965caf6j1rxs2dsvpzhlh4hyyb4y-r-codetools-0.2-16/library 175[62] /nix/store/8iglq3zr68a39hzswvzxqi2ffhpw9p51-r-KernSmooth-2.23-16/library 176[63] /nix/store/n3k50zv40i40drpdf8npbmy2y08gkr6w-r-rpart-4.1-15/library 177[64] /nix/store/b4r6adzcvpm8ivflsmis7ja7q4r5hkjy-r-spatial-7.3-11/library 178[65] /nix/store/zqg6hmrncl8ax3vn7z5drf4csddwnhcx-r-survival-3.1-12/library 179[66] /nix/store/4anrihkx11h8mzb269xdyi84yp5v7grl-r-tidyverse-1.3.0/library 180[67] /nix/store/945haq0w8nfm9ib7r0nfngn5lk2i15ix-r-broom-0.5.6/library 181[68] /nix/store/52viqxzrmxl7dk0zji293g5b0b9grwh8-r-backports-1.1.6/library 182[69] /nix/store/zp1k42sw2glqy51w4hnzsjs8rgi8xzx2-r-dplyr-0.8.5/library 183[70] /nix/store/mkjd98mnshch2pwnj6h31czclqdaph3f-r-plogr-0.2.0/library 184[71] /nix/store/kflrzax6y5pwfqwzgfvqz433a3q3hnhn-r-generics-0.0.2/library 185[72] /nix/store/xi1n5h5w17c33y6ax3dfhg2hgzjl9bxz-r-reshape2-1.4.4/library 186[73] /nix/store/vn63z92zkpbaxmmhzpb6mq2fvg0xa26h-r-plyr-1.8.6/library 187[74] /nix/store/wmpyxss67bj44rin7hlnr9qabx66p5hj-r-stringr-1.4.0/library 188[75] /nix/store/330qbgbvllwz3h0i2qidrlk50y0mbgph-r-tidyr-1.0.2/library 189[76] /nix/store/cx3x4pqb65l1mhss65780hbzv9jdrzl6-r-dbplyr-1.4.3/library 190[77] /nix/store/gsj49bp3hpw9jlli3894c49amddryqsq-r-DBI-1.1.0/library 191[78] /nix/store/kvymhwp4gac0343c2yi1qvdpavx4gdn2-r-ggplot2-3.3.0/library 192[79] /nix/store/knv51jvpairvibrkkq48b6f1l2pa1cv8-r-gtable-0.3.0/library 193[80] /nix/store/158dx0ddv20ikwag2860nlg9p3hbh1zc-r-isoband-0.2.1/library 194[81] /nix/store/fprs9rp1jlhxzj7fp6l79akyf8k3p7zd-r-testthat-2.3.2/library 195[82] /nix/store/0pmlnkyn0ir3k9bvxihi1r06jyl64w3i-r-evaluate-0.14/library 196[83] /nix/store/7210bjjqn5cjndxn5isnd4vip00xhkhy-r-pkgload-1.0.2/library 197[84] /nix/store/9a12ybd74b7dns40gcfs061wv7913qjy-r-desc-1.2.0/library 198[85] /nix/store/na9pb1apa787zp7vvyz1kzym0ywjwbj0-r-rprojroot-1.3-2/library 199[86] /nix/store/pa2n7bh61qxyarn5i2ynd62k6knb1np1-r-pkgbuild-1.0.6/library 200[87] /nix/store/1hxm1m7h4272zxk9bpsaq46mvnl0dbss-r-callr-3.4.3/library 201[88] /nix/store/bigvyk6ipglbiil93zkf442nv4y3xa1x-r-processx-3.4.2/library 202[89] /nix/store/370lr0wf7qlq0m72xnmasg2iahkp2n52-r-ps-1.3.2/library 203[90] /nix/store/rr72q61d8mkd42zc5fhcd2rqjghvc141-r-withr-2.2.0/library 204[91] /nix/store/9gw77p7fmz89fa8wi1d9rvril6hd4sxy-r-rstudioapi-0.11/library 205[92] /nix/store/9x4v4pbrgmykbz2801h77yz2l0nmm5nb-r-praise-1.0.0/library 206[93] /nix/store/pf8ssb0dliw5bzsncl227agc8przb7ic-r-scales-1.1.0/library 207[94] /nix/store/095z4wgjrxn63ixvyzrj1fm1rdv6ci95-r-farver-2.0.3/library 208[95] /nix/store/5aczj4s7i9prf5i32ik5ac5baqvjwdb1-r-labeling-0.3/library 209[96] /nix/store/wch26phipzz9gxd4vbr4fynh7v28349j-r-munsell-0.5.0/library 210[97] /nix/store/3w8fh756mszhsjx5fwgwydcpn8vkwady-r-colorspace-1.4-1/library 211[98] /nix/store/8cmaj81v2vm4f8p59ylbnsby8adkbmhd-r-RColorBrewer-1.1-2/library 212[99] /nix/store/h4x4ygax7gpz6f0c2v0xacr62080qwb8-r-viridisLite-0.3.0/library 213[100] /nix/store/qhx0i2nn5syb6vygdn8fdxgl7k56yj81-r-httr-1.4.1/library 214[101] /nix/store/lxnb4aniv02i4jhdvz02aaql1kznbpxb-r-jsonlite-1.6.1/library 215[102] /nix/store/13dcry4gad3vfwqzqb0ii4n06ybrxybr-r-mime-0.9/library 216[103] /nix/store/2can5l8gscc92a3bqlak8hfcg96v5hvf-r-openssl-1.4.1/library 217[104] /nix/store/piwsgxdz5w2ak8c6fcq0lc978qbxwdp1-r-askpass-1.1/library 218[105] /nix/store/3sj5h6dwa1l27d2hvdchclygk0pgffsr-r-sys-3.3/library 219[106] /nix/store/2z0p88g0c03gigl2ip60dlsfkdv1k30h-r-lubridate-1.7.8/library 220[107] /nix/store/1pkmj8nqjg2iinrkg2w0zkwq0ldc01za-r-modelr-0.1.6/library 221[108] /nix/store/bswkzvn8lczwbyw3y7n0p0qp2q472s0g-r-reprex-0.3.0/library 222[109] /nix/store/yid22gad8z49q52d225vfba2m4cgj2lx-r-fs-1.4.1/library 223[110] /nix/store/d185qiqaplm5br9fk1pf29y0srlabw83-r-rmarkdown-2.1/library 224[111] /nix/store/iszqviydsdj31c3ww095ndqy1ld3cibs-r-base64enc-0.1-3/library 225[112] /nix/store/i89wfw4cr0fz3wbd7cg44fk4dwz8b6h1-r-htmltools-0.4.0/library 226[113] /nix/store/qrl28laqwmhpwg3dpcf4nca8alv0px0g-r-knitr-1.28/library 227[114] /nix/store/jffaxc4a3bbf2g6ip0gdcya73dmg53mb-r-highr-0.8/library 228[115] /nix/store/717srph13qpnbzmgsvhx25q8pl51ivpj-r-markdown-1.1/library 229[116] /nix/store/mxqmyq3ybdfyc6p0anhfy2kfw0iz5k4n-r-xfun-0.13/library 230[117] /nix/store/b8g6hadva0359l6j1aq4dbvxlqf1acxc-r-yaml-2.2.1/library 231[118] /nix/store/rrl05vpv7cw58zi0k9ykm7m4rjb9gjv3-r-tinytex-0.22/library 232[119] /nix/store/2ziq8nzah6xy3dgmxgim9h2wszz1f89f-r-whisker-0.4/library 233[120] /nix/store/540wbw4p1g2qmnmbfk0rhvwvfnf657sj-r-rvest-0.3.5/library 234[121] /nix/store/n3prn77gd9sf3z4whqp86kghr55bf5w8-r-selectr-0.4-2/library 235[122] /nix/store/gv28yjk5isnglq087y7767xw64qa40cw-r-xml2-1.3.2/library 236[123] /nix/store/693czdcvkp6glyir0mi8cqvdc643whvc-r-gridExtra-2.3/library 237[124] /nix/store/3sykinp7lyy70dgzr0fxjb195nw864dv-r-future-1.17.0/library 238[125] /nix/store/bqi2l53jfxncks6diy0hr34bw8f86rvk-r-globals-0.12.5/library 239[126] /nix/store/dydyl209klklzh69w9q89f2dym9xycnp-r-listenv-0.8.0/library 240[127] /nix/store/lni0bi36r4swldkx7g4hql7gfz9b121b-r-gganimate-1.0.5/library 241[128] /nix/store/hh92jxs79kx7vxrxr6j6vin1icscl4k7-r-tweenr-1.0.1/library 242[129] /nix/store/0npx3srjnqgh7bib80xscjqvfyzjvimq-r-GGally-1.5.0/library 243[130] /nix/store/x5nzxklmacj6l162g7kg6ln9p25r3f17-r-reshape-0.8.8/library 244[131] /nix/store/q29z7ckdyhfmg1zlzrrg1nrm36ax756j-r-ggfortify-0.4.9/library 245[132] /nix/store/1rvm1w9iv2c5n22p4drbjq8lr9wa2q2r-r-cowplot-1.0.0/library 246[133] /nix/store/rp8jhnasaw1vbv5ny5zx0mw30zgcp796-r-ggrepel-0.8.2/library 247[134] /nix/store/wb7y931mm8nsj7w9xin83bvbaq8wvi4d-r-corrplot-0.84/library 248[135] /nix/store/gdzcqivfvgdrsz247v5kmnnw1v6p9c1p-r-rpart.plot-3.0.8/library 249[136] /nix/store/6yqg37108r0v22476cm2kv0536wyilki-r-caret-6.0-86/library 250[137] /nix/store/6fjdgcwgisiqz451sg5fszxnn9z8vxg6-r-foreach-1.5.0/library 251[138] /nix/store/c3ph5i341gk7jdinrkkqf6y631xli424-r-iterators-1.0.12/library 252[139] /nix/store/sjm1rxshlpakpxbrynfhsjnnp1sjvc3r-r-ModelMetrics-1.2.2.2/library 253[140] /nix/store/vgk4m131d057xglmrrb9rijhzdr2qhhp-r-pROC-1.16.2/library 254[141] /nix/store/bv1kvy1wc2jx3v55rzn3cg2qjbv7r8zp-r-recipes-0.1.10/library 255[142] /nix/store/001h42q4za01gli7avjxhq7shpv73n9k-r-gower-0.2.1/library 256[143] /nix/store/ssffpl6ydffqyn9phscnccxnj71chnzg-r-ipred-0.9-9/library 257[144] /nix/store/baliqip8m6p0ylqhqcgqak29d8ghral1-r-prodlim-2019.11.13/library 258[145] /nix/store/j4n2wsv98asw83qiffg6a74dymk8r2hl-r-lava-1.6.7/library 259[146] /nix/store/hf5wq5kpsf6p9slglq5iav09s4by0y5i-r-numDeriv-2016.8-1.1/library 260[147] /nix/store/s58hm38078mx4gyqffvv09zn575xn648-r-SQUAREM-2020.2/library 261[148] /nix/store/g63ydzd53586pvr9kdgk8kf5szq5f2bc-r-timeDate-3043.102/library 262[149] /nix/store/0jkarmlf1kjv4g8a3svkc7jfarpp77ny-r-mlr3-0.2.0/library 263[150] /nix/store/g1m0n1w7by213v773iyn7vnxr25pkf56-r-checkmate-2.0.0/library 264[151] /nix/store/fc2ah8cz2sj6j2jk7zldvjmsjn1yakpn-r-lgr-0.3.4/library 265[152] /nix/store/0i2hs088j1s0a6i61124my6vnzq8l27m-r-mlbench-2.1-1/library 266[153] /nix/store/vzcs6k21pqrli3ispqnvj5qwkv14srf5-r-mlr3measures-0.1.3/library 267[154] /nix/store/h2yqqaia46bk3b1d1a7bq35zf09p1b1a-r-mlr3misc-0.2.0/library 268[155] /nix/store/c9mrkc928cmsvvnib50l0jb8lsz59nyk-r-paradox-0.2.0/library 269[156] /nix/store/vqpbdipi4p4advl2vxrn765mmgcrabvk-r-uuid-0.1-4/library 270[157] /nix/store/xpclynxnfq4h9218gk4y62nmgyyga6zl-r-mlr3viz-0.1.1/library 271[158] /nix/store/7w6pld5vir3p9bybay67kq0qwl0gnx17-r-mlr3learners-0.2.0/library 272[159] /nix/store/ca50rp6ha5s51qmhb1gjlj62r19xfzxs-r-mlr3pipelines-0.1.3/library 273[160] /nix/store/9hg0xap4pir64mhbgq8r8cgrfjn8aiz5-r-mlr3filters-0.2.0/library 274[161] /nix/store/jgqcmfix0xxm3y90m8wy3xkgmqf2b996-r-rstan-2.19.3/library 275[162] /nix/store/mvv1gjyrrpvf47fn7a8x722wdwrf5azk-r-inline-0.3.15/library 276[163] /nix/store/zmkw51x4w4d1v1awcws0xihj4hnxfr09-r-loo-2.2.0/library 277[164] /nix/store/30xxalfwzxl05bbfvj5sy8k3ysys6z5y-r-matrixStats-0.56.0/library 278[165] /nix/store/fhkww2l0izx87bjnf0pl9ydl1wprp0xv-r-StanHeaders-2.19.2/library 279[166] /nix/store/aflck5pzxa8ym5q1dxchx5hisfmfghkr-r-tidybayes-2.0.3/library 280[167] /nix/store/jhlbhiv4fg0wsbxwjz8igc4hcg79vw94-r-arrayhelpers-1.1-0/library 281[168] /nix/store/fv089zrnvicnavbi08hnzqpi9g1z4inj-r-svUnit-1.0.3/library 282[169] /nix/store/xci2rgjizx1fyb33818jx5s1bgn8v8k6-r-coda-0.19-3/library 283[170] /nix/store/dch9asd38yldz0sdn8nsgk9ivjrkbhva-r-HDInterval-0.2.0/library 284[171] /nix/store/rs8dri2m5cqdmpiw187rvl4yhjn0jg2v-r-e1071-1.7-3/library 285[172] /nix/store/qs1zyh3sbvccgnqjzas3br6pak399zgc-r-pvclust-2.2-0/library 286[173] /nix/store/sh3zxvdazp7rkjn1iczrag1h2358ifm1-r-forecast-8.12/library 287[174] /nix/store/h67kaxqr2ppdpyj77wg5hm684jypznji-r-fracdiff-1.5-1/library 288[175] /nix/store/fh0z465ligbpqyam5l1fwiijc7334kbk-r-lmtest-0.9-37/library 289[176] /nix/store/0lnsbwfg0axr80h137q52pa50cllbjpf-r-zoo-1.8-7/library 290[177] /nix/store/p7k4s3ivf83dp2kcxr1cr0wlc1rfk6jx-r-RcppArmadillo-0.9.860.2.0/library 291[178] /nix/store/ssnxv5x6zid2w11v8k5yvnyxis6n1qfk-r-tseries-0.10-47/library 292[179] /nix/store/zrbskjwaz0bzz4v76j044d771m24g6h8-r-quadprog-1.5-8/library 293[180] /nix/store/2x3w5sjalrfm6hf1dxd951j8y94nh765-r-quantmod-0.4.17/library 294[181] /nix/store/7g55xshf49s9379ijm1zi1qnh1vbsifq-r-TTR-0.23-6/library 295[182] /nix/store/6ilyzph46q6ijyanq4p7f0ccyni0d7j0-r-xts-0.12-0/library 296[183] /nix/store/17xhqghcnqha7pwbf98dxsq1729slqd5-r-urca-1.3-0/library 297[184] /nix/store/722lyn0k8y27pj1alik56r4vpjnncd9z-r-swdft-1.0.0/library 298[185] /nix/store/36n0zgy10fsqcq76n0qmdwjxrwh7pn9n-r-xgboost-1.0.0.2/library 299[186] /nix/store/ac0ar7lf75qx84xsdjv6j02rkdgnhybz-r-ranger-0.12.1/library 300[187] /nix/store/i1ighkq42x10dirqmzgbx2mhbnz1ynkb-r-DALEX-1.2.0/library 301[188] /nix/store/28fqnhsfng1bkphl0wvr7lg5y3p6va46-r-iBreakDown-1.2.0/library 302[189] /nix/store/dpym77x9qc2ksr4mwjm3pb9ar1kvwhdl-r-ingredients-1.2.0/library 303[190] /nix/store/sp4d281w6dpr31as0xdjqizdx8hhb01q-r-DALEXtra-0.2.1/library 304[191] /nix/store/ckhp9kpmjcs0wxb113pxn25c2wip2d0n-r-ggdendro-0.1-20/library 305[192] /nix/store/f3k7dxj1dsmqri2gn0svq4c9fvvl9g7q-r-glmnet-3.0-2/library 306[193] /nix/store/l6ccj6mwkqybjvh6dr8qzalygp0i7jyb-r-shape-1.4.4/library 307[194] /nix/store/418mqfwlafh6984xld8lzhl7rv29qw68-r-reticulate-1.15/library 308[195] /nix/store/qwh982mgxd2mzrgbjk14irqbasywa1jk-r-rappdirs-0.3.1/library 309[196] /nix/store/6sxs76abll23c6372h6nf101wi8fcr4c-r-FactoMineR-2.3/library 310[197] /nix/store/39d2va10ydgyzddwr07xwdx11fwk191i-r-ellipse-0.4.1/library 311[198] /nix/store/4lxym5nxdn8hb7l8a566n5vg9paqcfi2-r-flashClust-1.01-2/library 312[199] /nix/store/wp161zbjjs41fq4kn4k3m244c7b8l2l2-r-leaps-3.1/library 313[200] /nix/store/irghsaplrpb3hg3y7j831bbklf2cqs6d-r-scatterplot3d-0.3-41/library 314[201] /nix/store/09ahkf50g1q9isxanbdykqgcdrp8mxl1-r-factoextra-1.0.7/library 315[202] /nix/store/zi9bq7amsgc6w2x7fvd62g9qxz69vjfm-r-dendextend-1.13.4/library 316[203] /nix/store/wcywb7ydglzlxg57jf354x31nmy63923-r-viridis-0.5.1/library 317[204] /nix/store/pvnpg4vdvv93pmwrlgmy51ihrb68j55f-r-ggpubr-0.2.5/library 318[205] /nix/store/qpapsc4l9pylzfhc72ha9d82hcbac41z-r-ggsci-2.9/library 319[206] /nix/store/h0zg4x3bmkc82ggx8h4q595ffckcqgx5-r-ggsignif-0.6.0/library 320[207] /nix/store/vn5svgbf8vsgv8iy8fdzlj0izp279q15-r-polynom-1.4-0/library 321[208] /nix/store/mc1mlsjx5h3gc8nkl7jlpd4vg145nk1z-r-lindia-0.9/library 322[209] /nix/store/z1k4c8lhabp9niwfg1xylg58pf99ld9r-r-orgutils-0.4-1/library 323[210] /nix/store/ybj4538v74wx4f1l064m0qn589vyjmzg-r-textutils-0.2-0/library 324[211] /nix/store/hhm5j0wvzjc0bfd53170bw8w7mij2wnh-r-latex2exp-0.4.0/library 325[212] /nix/store/njlv5mkxgjyx3x8p984nr84dwa2v1iqp-r-kableExtra-1.1.0/library 326[213] /nix/store/lf2sb84ylh259m421ljbj731a4prjhsl-r-webshot-0.5.2/library 327[214] /nix/store/n6b8ap54b78h8l70kyx9nvayp44rnfzf-r-printr-0.1/library 328[215] /nix/store/02g1v6d3ly8zylpckigwk6w3l1mx2i9d-r-microbenchmark-1.4-7/library 329[216] /nix/store/ri6qm0fp8cyx2qnysxjv2wsk0nndl1x9-r-webchem-0.5.0/library 330[217] /nix/store/cg95rqc1gmaqxf5kxja3cz8m5w4vl76l-r-RCurl-1.98-1.2/library 331[218] /nix/store/qbpinv148778fzdz8372x8gp34hspvy1-r-bitops-1.0-6/library 332[219] /nix/store/1g0lbrx6si76k282sxr9cj0mgknrw0lx-r-devtools-2.3.0/library 333[220] /nix/store/hnvww0128czlx6w8aipjn0zs7nvmvak9-r-covr-3.5.0/library 334[221] /nix/store/p4nv59przmb14sxi49jwqarkv0l40jsp-r-rex-1.2.0/library 335[222] /nix/store/vnysmc3vkgkligwah1zh9l4sahr533a8-r-lazyeval-0.2.2/library 336[223] /nix/store/d638w33ahybsa3sqr52fafvxs2b7w9x3-r-DT-0.13/library 337[224] /nix/store/35nqc34wy2nhd9bl7lv6wriw0l3cghsw-r-crosstalk-1.1.0.1/library 338[225] /nix/store/03838i63x5irvgmpgwj67ah0wi56k9d7-r-htmlwidgets-1.5.1/library 339[226] /nix/store/l4640jxlsjzqhw63c18fziar5vc0xyhk-r-promises-1.1.0/library 340[227] /nix/store/rxrb8p3dxzsg10v7yqaq5pi3y3gk6nqh-r-later-1.0.0/library 341[228] /nix/store/giprr32bl6k18b9n4qjckpf102flarly-r-git2r-0.26.1/library 342[229] /nix/store/bbkpkf44b13ig1pkz7af32kw5dzp12vb-r-memoise-1.1.0/library 343[230] /nix/store/m31vzssnfzapsapl7f8v4m15003lcc8r-r-rcmdcheck-1.3.3/library 344[231] /nix/store/hbiylknhxsin9hp9zaa6dwc2c9ai1mqx-r-sessioninfo-1.1.1/library 345[232] /nix/store/8vwlbx3s345gjccrkiqa6h1bm9wq4s9q-r-xopen-1.0.0/library 346[233] /nix/store/mjnwnlv60cn56ap0rrzvrkqlh5qisszx-r-remotes-2.1.1/library 347[234] /nix/store/1rq4zyzqymml7cc11q89rl5g514ml9na-r-roxygen2-7.1.0/library 348[235] /nix/store/2658mrn1hpkq0fv629rvags91qg65pbn-r-brew-1.0-6/library 349[236] /nix/store/nvjalws9lzva4pd4nz1z2131xsb9b5p6-r-commonmark-1.7/library 350[237] /nix/store/qx900vivd9s2zjrxc6868s92ljfwj5dv-r-rversions-2.0.1/library 351[238] /nix/store/1drg446wilq5fjnxkglxnnv8pbp1hllg-r-usethis-1.6.0/library 352[239] /nix/store/p3f3wa41d304zbs5cwvw7vy4j17zd6nq-r-gh-1.1.0/library 353[240] /nix/store/769g7jh93da8w15ad0wsbn2aqziwwx56-r-ini-0.3.1/library 354[241] /nix/store/p7kifw1l6z2zg68a71s4sdbfj8gdmnv5-r-rematch2-2.1.1/library 355[242] /nix/store/6zhdqip9ld9vl6pvifqcf4gsqy2f5wix-r-rethinking/library 356[243] /nix/store/496p28klmflihdkc83c8p1cywg85mgk4-r-mvtnorm-1.1-0/library 357[244] /nix/store/xb1zn7ab4nka7h1vm678ginzfwg4w9wf-r-dagitty-0.2-2/library 358[245] /nix/store/3zj4dkjbdwgf3mdsl9nf9jkicpz1nwgc-r-V8-3.0.2/library 359[246] /nix/store/qiqsh62w69b5xgj2i4wjamibzxxji0mf-r-tidybayes.rethinking/library 360[247] /nix/store/4j6byy1klyk4hm2k6g3657682cf3wxcj-R-4.0.0/lib/R/library   Summer of 2020\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/sr2-ch13-ch14/","tags":["solutions","R","SR2"],"title":"  \"SR2 :: Solutions for Chapters {13,14}\"\n  "},{"categories":["programming"],"contents":" Setup details are described here, and the meta-post about these solutions is here.\n Materials The summmer course1 is based off of the second edition of Statistical Rethinking by Richard McElreath. This submission covers the following exercise questions:\n Chapter 9  E{3,4,5,6} M{1,2,3}   Chapter 11  E{1,2,3,4} M{2,3,4,5,6,8}   Chapter 12  E{4} H{1,2}    Packages A colophon with details is provided at the end, but the following packages and theme parameters are used throughout.\n1libsUsed\u0026lt;-c(\u0026#34;tidyverse\u0026#34;,\u0026#34;tidybayes\u0026#34;,\u0026#34;orgutils\u0026#34;,\u0026#34;dagitty\u0026#34;, 2\u0026#34;rethinking\u0026#34;,\u0026#34;tidybayes.rethinking\u0026#34;, 3\u0026#34;ggplot2\u0026#34;,\u0026#34;kableExtra\u0026#34;,\u0026#34;dplyr\u0026#34;,\u0026#34;glue\u0026#34;, 4\u0026#34;latex2exp\u0026#34;,\u0026#34;data.table\u0026#34;,\u0026#34;printr\u0026#34;,\u0026#34;devtools\u0026#34;) 5invisible(lapply(libsUsed, library, character.only = TRUE)); 6theme_set(theme_grey(base_size=24)) 7set.seed(1995) Chapter IX: Markov Chain Monte Carlo Easy Questions (Ch9) 9E3 Which sort of parameters can Hamiltonian Monte Carlo not handle? Can you explain why?\nSolution Hamiltonian Monte Carlo is derived by adding the concept of momentum which requires that the Hessian is non-negative, which in term requires a continuous smooth function. Thus HMC cannot handle discrete parameters by construction. More formally, the HMC requires a transform from the D-dimensional parameter space to a 2D-dimensional phase space cite:betancourtConceptualIntroductionHamiltonian2018.\n9E4 Explain the difference between the effective number of samples, n_eff as calculated by Stan, and the actual number of samples.\nSolution We will invoke the precise definition of the effective sample size cite:betancourtConceptualIntroductionHamiltonian2018\n\\[ ESS = \\frac{N}{1+2\\sum_{l=1}^{\\infty}\\rho_{l}} \\]\nWhere we note that \\(\\rho_{l}\\) is the lag-l autocorrelation of \\(f\\) over the Markov chain (in time). In essence, this is the number of independent samples which have equivalent information of the posterior. This is relevant, because the samples from a Marko chain are sequentially correlated (autocorrelated).\n9E5 Which value should Rhat approach, when a chain is sampling the posterior distribution correctly?\nSolution The literature cite:gelmanBayesianDataAnalysis2014 often cites a value of \\(1.01\\) for convergence. However, newer versions of Stan tend are documented to suggest \\(1.05\\) since they use newer formulations of the Rhat value cite:vehtariRanknormalizationFoldingLocalization2020. It should also be noted that cite:royConvergenceDiagnosticsMarkov2020 the Rhat value does not necessarily indicate convergence, it is not a necessary and sufficient condition, but a heuristic, and should be understood as such.\nHOLD 9E6 Sketch a good trace plot for a Markov chain, one that is effectively sampling from the posterior distribution. What is good about its shape? Then sketch a trace plot for a malfunctioning Markov chain. What about its shape indicates malfunction?\nSolution Recall that the \u0026ldquo;health\u0026rdquo; of a chain can be determined by the following qualities in the trace plot.\n Stationarity This ensures that the chain is sampling the high probability portion of the posterior distribution Mixing This ensures that the chain explores the full region Convergence Convergence implies that independent chains agree on the same region of high probability  We will require a sample model to plot.\n1data(rugged) 2rugDat\u0026lt;-rugged 3rugDat\u0026lt;-rugDat %\u0026gt;% dplyr::mutate(logGDP=log(rgdppc_2000)) %\u0026gt;% tidyr::drop_na() %\u0026gt;% dplyr::mutate(logGDP_std=logGDP/mean(logGDP), 4rugged_std=rugged/max(rugged), 5cid=ifelse(cont_africa==1,1,2)) 6datList\u0026lt;-list( 7logGDP_std=rugDat$logGDP_std, 8rugged_std=rugDat$rugged_std, 9cid=as.integer(rugDat$cid) 10) 1m91unif\u0026lt;-ulam( 2alist( 3logGDP_std ~ dnorm(mu,sigma), 4mu\u0026lt;-a[cid] + b[cid]*(rugged_std-0.215), 5a[cid]~dnorm(1,0.1), 6b[cid]~dnorm(0,0.3), 7sigma~dunif(0,1) 8), data=datList, chains=4, cores=4 9) We would like to check the trace and trace rank plots.\n1m91unif %\u0026gt;% traceplot  1m91unif %\u0026gt;% trankplot  Clearly this is a good model, with well mixed chains, as can be seen in the trank and trace plots.\nWe will now check the plots for the unhealthy chain described in the chapter.\n1m9e4un\u0026lt;-ulam( 2alist( 3y ~ dnorm(mu,sigma), 4mu\u0026lt;-alpha, 5alpha ~ dnorm(0,1000), 6sigma~dexp(0.0001) 7),data=list(y=c(-1,1)),chains=4,cores=4 8) 1SAMPLING FOR MODEL \u0026#39;726d002e27cec1633082261fcfedb813\u0026#39; NOW (CHAIN 1). 2Chain 1: 3Chain 1: Gradient evaluation took 8.1e-05 seconds 4Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.81 seconds. 5Chain 1: Adjust your expectations accordingly! 6Chain 1: 7Chain 1: 8Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) 9Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) 1011SAMPLING FOR MODEL \u0026#39;726d002e27cec1633082261fcfedb813\u0026#39; NOW (CHAIN 2). 12Chain 2: 13Chain 2: Gradient evaluation took 3.6e-05 seconds 14Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.36 seconds. 15Chain 2: Adjust your expectations accordingly! 16Chain 2: 17Chain 2: 18Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup) 19Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) 20Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) 21Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) 22Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) 23Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) 24Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) 2526SAMPLING FOR MODEL \u0026#39;726d002e27cec1633082261fcfedb813\u0026#39; NOW (CHAIN 3). 27Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) 28Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) 29Chain 3: 30Chain 3: Gradient evaluation took 2.5e-05 seconds 31Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds. 32Chain 3: Adjust your expectations accordingly! 33Chain 3: 34Chain 3: 35Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) 36Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) 37Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) 38Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) 39Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) 40Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) 41Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) 42Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) 43Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) 44Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) 45Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) 46Chain 2: 47Chain 2: Elapsed Time: 0.069084 seconds (Warm-up) 48Chain 2: 0.023083 seconds (Sampling) 49Chain 2: 0.092167 seconds (Total) 50Chain 2: 51Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) 5253SAMPLING FOR MODEL \u0026#39;726d002e27cec1633082261fcfedb813\u0026#39; NOW (CHAIN 4). 54Chain 4: 55Chain 4: Gradient evaluation took 3.1e-05 seconds 56Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. 57Chain 4: Adjust your expectations accordingly! 58Chain 4: 59Chain 4: 60Chain 4: Iteration: 1 / 1000 [ 0%] (Warmup) 61Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) 62Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) 63Chain 3: Iteration: 300 / 1000 [ 30%] (Warmup) 64Chain 3: Iteration: 400 / 1000 [ 40%] (Warmup) 65Chain 4: Iteration: 100 / 1000 [ 10%] (Warmup) 66Chain 3: Iteration: 500 / 1000 [ 50%] (Warmup) 67Chain 3: Iteration: 501 / 1000 [ 50%] (Sampling) 68Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) 69Chain 4: Iteration: 200 / 1000 [ 20%] (Warmup) 70Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) 71Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) 72Chain 1: 73Chain 1: Elapsed Time: 0.082116 seconds (Warm-up) 74Chain 1: 0.095511 seconds (Sampling) 75Chain 1: 0.177627 seconds (Total) 76Chain 1: 77Chain 4: Iteration: 300 / 1000 [ 30%] (Warmup) 78Chain 4: Iteration: 400 / 1000 [ 40%] (Warmup) 79Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) 80Chain 4: Iteration: 500 / 1000 [ 50%] (Warmup) 81Chain 4: Iteration: 501 / 1000 [ 50%] (Sampling) 82Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) 83Chain 4: Iteration: 600 / 1000 [ 60%] (Sampling) 84Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) 85Chain 4: Iteration: 700 / 1000 [ 70%] (Sampling) 86Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) 87Chain 3: 88Chain 3: Elapsed Time: 0.073606 seconds (Warm-up) 89Chain 3: 0.076388 seconds (Sampling) 90Chain 3: 0.149994 seconds (Total) 91Chain 3: 92Chain 4: Iteration: 800 / 1000 [ 80%] (Sampling) 93Chain 4: Iteration: 900 / 1000 [ 90%] (Sampling) 94Chain 4: Iteration: 1000 / 1000 [100%] (Sampling) 95Chain 4: 96Chain 4: Elapsed Time: 0.064213 seconds (Warm-up) 97Chain 4: 0.091667 seconds (Sampling) 98Chain 4: 0.15588 seconds (Total) 99Chain 4: 100Warning messages: 1011: There were 55 divergent transitions after warmup. Increasing adapt_delta above 0.95 may help. See 102http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 1032: Examine the pairs() plot to diagnose sampling problems 1041053: The largest R-hat is 1.08, indicating chains have not mixed. 106Running the chains for more iterations may help. See 107http://mc-stan.org/misc/warnings.html#r-hat 1084: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. 109Running the chains for more iterations may help. See 110http://mc-stan.org/misc/warnings.html#bulk-ess 1115: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. 112Running the chains for more iterations may help. See 113http://mc-stan.org/misc/warnings.html#tail-ess 1m9e4un %\u0026gt;% traceplot  1m9e4un %\u0026gt;% trankplot  Clearly these plots show a model which is unable to converge.\n1m9e4un %\u0026gt;% precis 1mean sd 5.5% 94.5% n_eff Rhat4 2alpha -22.01 267.51 -414.95 316.61 154 1.02 3sigma 420.97 983.52 5.39 1894.87 151 1.04 This has clear repercussions on the actual predictions as well.\nQuestions of Medium Complexity (Ch9) HOLD 9M1 Re-estimate the terrain ruggedness model from the chapter, but now using a uniform prior for the standard deviation, sigma. The uniform prior should be dunif(0,1). Use ulam to estimate the posterior. Does the different prior have any detectable influence on the posterior distribution of sigma? What or why not?\nSolution Instead of using the complete.cases formulation in the book, we will instead use a more tidyverse friendly approach.\n1data(rugged) 2rugDat\u0026lt;-rugged 3rugDat\u0026lt;-rugDat %\u0026gt;% dplyr::mutate(logGDP=log(rgdppc_2000)) %\u0026gt;% tidyr::drop_na() %\u0026gt;% dplyr::mutate(logGDP_std=logGDP/mean(logGDP), 4rugged_std=rugged/max(rugged), 5cid=ifelse(cont_africa==1,1,2)) 6datList\u0026lt;-list( 7logGDP_std=rugDat$logGDP_std, 8rugged_std=rugDat$rugged_std, 9cid=as.integer(rugDat$cid) 10) We can now formulate a model with a uniform prior on sigma.\n1m91unif\u0026lt;-ulam( 2alist( 3logGDP_std ~ dnorm(mu,sigma), 4mu\u0026lt;-a[cid] + b[cid]*(rugged_std-0.215), 5a[cid]~dnorm(1,0.1), 6b[cid]~dnorm(0,0.3), 7sigma~dunif(0,1) 8), data=datList, chains=4, cores=4 9) 1m91exp\u0026lt;-ulam( 2alist( 3logGDP_std ~ dnorm(mu,sigma), 4mu\u0026lt;-a[cid] + b[cid]*(rugged_std-0.215), 5a[cid]~dnorm(1,0.1), 6b[cid]~dnorm(0,0.3), 7sigma~dexp(1) 8), data=datList, chains=4, cores=4 9) 1SAMPLING FOR MODEL \u0026#39;9b462775c5cc2badb2b667c53f2020c8\u0026#39; NOW (CHAIN 1). 2Chain 1: 3Chain 1: Gradient evaluation took 2.8e-05 seconds 4Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. 5Chain 1: Adjust your expectations accordingly! 6Chain 1: 7Chain 1: 8Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) 910SAMPLING FOR MODEL \u0026#39;9b462775c5cc2badb2b667c53f2020c8\u0026#39; NOW (CHAIN 2). 11Chain 2: 12Chain 2: Gradient evaluation took 3.3e-05 seconds 13Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.33 seconds. 14Chain 2: Adjust your expectations accordingly! 15Chain 2: 16Chain 2: 17Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup) 1819SAMPLING FOR MODEL \u0026#39;9b462775c5cc2badb2b667c53f2020c8\u0026#39; NOW (CHAIN 3). 20Chain 3: 21Chain 3: Gradient evaluation took 2.6e-05 seconds 22Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds. 23Chain 3: Adjust your expectations accordingly! 24Chain 3: 25Chain 3: 26Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) 27Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) 28Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) 29Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) 30Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) 3132SAMPLING FOR MODEL \u0026#39;9b462775c5cc2badb2b667c53f2020c8\u0026#39; NOW (CHAIN 4). 33Chain 4: 34Chain 4: Gradient evaluation took 2.5e-05 seconds 35Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds. 36Chain 4: Adjust your expectations accordingly! 37Chain 4: 38Chain 4: 39Chain 3: Iteration: 300 / 1000 [ 30%] (Warmup) 40Chain 4: Iteration: 1 / 1000 [ 0%] (Warmup) 41Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) 42Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) 43Chain 3: Iteration: 400 / 1000 [ 40%] (Warmup) 44Chain 3: Iteration: 500 / 1000 [ 50%] (Warmup) 45Chain 3: Iteration: 501 / 1000 [ 50%] (Sampling) 46Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) 47Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) 48Chain 4: Iteration: 100 / 1000 [ 10%] (Warmup) 49Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) 50Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) 51Chain 4: Iteration: 200 / 1000 [ 20%] (Warmup) 52Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) 53Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) 54Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) 55Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) 56Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) 57Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) 58Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) 59Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) 60Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) 61Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) 62Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) 63Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) 64Chain 4: Iteration: 300 / 1000 [ 30%] (Warmup) 65Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) 66Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) 67Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) 68Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) 69Chain 4: Iteration: 400 / 1000 [ 40%] (Warmup) 70Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) 71Chain 1: 72Chain 1: Elapsed Time: 0.048459 seconds (Warm-up) 73Chain 1: 0.017728 seconds (Sampling) 74Chain 1: 0.066187 seconds (Total) 75Chain 1: 76Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) 77Chain 2: 78Chain 2: Elapsed Time: 0.043574 seconds (Warm-up) 79Chain 2: 0.018569 seconds (Sampling) 80Chain 2: 0.062143 seconds (Total) 81Chain 2: 82Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) 83Chain 3: 84Chain 3: Elapsed Time: 0.03201 seconds (Warm-up) 85Chain 3: 0.024666 seconds (Sampling) 86Chain 3: 0.056676 seconds (Total) 87Chain 3: 88Chain 4: Iteration: 500 / 1000 [ 50%] (Warmup) 89Chain 4: Iteration: 501 / 1000 [ 50%] (Sampling) 90Chain 4: Iteration: 600 / 1000 [ 60%] (Sampling) 91Chain 4: Iteration: 700 / 1000 [ 70%] (Sampling) 92Chain 4: Iteration: 800 / 1000 [ 80%] (Sampling) 93Chain 4: Iteration: 900 / 1000 [ 90%] (Sampling) 94Chain 4: Iteration: 1000 / 1000 [100%] (Sampling) 95Chain 4: 96Chain 4: Elapsed Time: 0.040804 seconds (Warm-up) 97Chain 4: 0.018479 seconds (Sampling) 98Chain 4: 0.059283 seconds (Total) 99Chain 4: The posterior distributions are simply:\n1m91exp %\u0026gt;% extract.samples %\u0026gt;% .$sigma %\u0026gt;% dens(.,xlab=\u0026#34;sigma\u0026#34;) 2m91unif %\u0026gt;% extract.samples %\u0026gt;% .$sigma %\u0026gt;% dens(.,add=TRUE,col=\u0026#34;blue\u0026#34;) 3mtext(\u0026#34;posterior\u0026#34;)  With the priors being:\n1m91exp %\u0026gt;% extract.prior %\u0026gt;% .$sigma %\u0026gt;% dens(.,xlab=\u0026#34;sigma\u0026#34;) 2m91unif %\u0026gt;% extract.prior %\u0026gt;% .$sigma %\u0026gt;% dens(.,add=TRUE,col=\u0026#34;blue\u0026#34;) 3mtext(\u0026#34;prior\u0026#34;)  This makes sense, since we know that uniform prior is essentially a step function between 0 and 1 with a value of 1, while the exponential function decays normally, but should actually be spiked upwards to 1 as well.\nHOLD 9M2 Modify the terrain ruggedness model again. This times, change the prior for b[cid] to dexp(0.3). What does this do to the posterior distribution? Can you explain it?\nSolution 1m92exp\u0026lt;-ulam( 2alist( 3logGDP_std ~ dnorm(mu,sigma), 4mu\u0026lt;-a[cid] + b[cid]*(rugged_std-0.215), 5a[cid]~dnorm(1,0.1), 6b[cid]~dexp(0.3), 7sigma~dexp(1) 8),data=datList, chains=4, cores=4 9) Priors:\n1m92exp %\u0026gt;% extract.prior %\u0026gt;% .$sigma %\u0026gt;% dens(.,xlab=\u0026#34;sigma\u0026#34;,col=\u0026#34;blue\u0026#34;) 2mtext(\u0026#34;prior\u0026#34;)  Posterior:\n1m92exp %\u0026gt;% extract.samples %\u0026gt;% .$sigma %\u0026gt;% dens(.,xlab=\u0026#34;sigma\u0026#34;,col=\u0026#34;blue\u0026#34;) 2mtext(\u0026#34;posterior\u0026#34;)  1m92exp %\u0026gt;% precis(.,depth = 2) 2m91exp %\u0026gt;% precis(.,depth = 2) 3m91unif %\u0026gt;% precis(.,depth = 2) 1mean sd 5.5% 94.5% n_eff Rhat4 2a[1] 1.01 0.02 0.98 1.03 1161 1 3b[1] 0.13 0.08 0.02 0.27 821 1 4sigma 0.12 0.01 0.10 0.15 1097 1 56mean sd 5.5% 94.5% n_eff Rhat4 7a[1] 1.01 0.02 0.98 1.04 1609 1 8b[1] 0.09 0.10 -0.06 0.24 1518 1 9sigma 0.12 0.01 0.10 0.15 1493 1 1011mean sd 5.5% 94.5% n_eff Rhat4 12a[1] 1.01 0.02 0.97 1.04 1744 1 13b[1] 0.10 0.09 -0.05 0.25 1874 1 14sigma 0.12 0.01 0.10 0.15 1824 1 We can see that there isn\u0026rsquo;t much difference, however, the main difference is in the b parameter, which seems to have fewer samples, and is also no longer takes any negative values.\nHOLD 9M3 Re-estimate one of the Stan models from the chapter, but at different numbers of warmup iterations. Be sure to use the same number of sampling iterations in each case. Compare the n_eff values. How much warmup is enough?\nSolution For brevity, we will re-use the same data and model as used in the previous questions.\n1warmTrial\u0026lt;-seq.int(10,10000,length.out = 10) 2nSampleEff\u0026lt;-matrix(NA,nrow=length(warmTrial),ncol=3) 3nSampleEffExp\u0026lt;-matrix(NA,nrow=length(warmTrial),ncol=3)    Uniform Model\n1for(i in 1:length(warmTrial)){ 2tmp\u0026lt;-ulam(m91unif,chains=4,cores=4,refresh=-1,warmup=warmTrial[i],iter=1000+warmTrial[i]) 3nSampleEff[i,]\u0026lt;-precis(tmp,2)$n_eff 4} 12Chain 1: 3Chain 1: Gradient evaluation took 9.7e-05 seconds 4Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.97 seconds. 5Chain 1: Adjust your expectations accordingly! 6Chain 1: 7Chain 1: 8Chain 1: WARNING: No variance estimation is 9Chain 1: performed for num_warmup \u0026lt; 20 10Chain 1: 11Chain 2: 12Chain 2: Gradient evaluation took 9.6e-05 seconds 13Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.96 seconds. 14Chain 2: Adjust your expectations accordingly! 15Chain 2: 16Chain 2: 17Chain 2: WARNING: No variance estimation is 18Chain 2: performed for num_warmup \u0026lt; 20 19Chain 2: 20Chain 3: 21Chain 3: Gradient evaluation took 0.000107 seconds 22Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.07 seconds. 23Chain 3: Adjust your expectations accordingly! 24Chain 3: 25Chain 3: 26Chain 3: WARNING: No variance estimation is 27Chain 3: performed for num_warmup \u0026lt; 20 28Chain 3: 29Chain 1: 30Chain 1: Elapsed Time: 0.00162 seconds (Warm-up) 31Chain 1: 0.1738 seconds (Sampling) 32Chain 1: 0.17542 seconds (Total) 33Chain 1: 34Chain 3: 35Chain 3: Elapsed Time: 0.003755 seconds (Warm-up) 36Chain 3: 0.059869 seconds (Sampling) 37Chain 3: 0.063624 seconds (Total) 38Chain 3: 39Chain 4: 40Chain 4: Gradient evaluation took 9.2e-05 seconds 41Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.92 seconds. 42Chain 4: Adjust your expectations accordingly! 43Chain 4: 44Chain 4: 45Chain 4: WARNING: No variance estimation is 46Chain 4: performed for num_warmup \u0026lt; 20 47Chain 4: 48Chain 2: 49Chain 2: Elapsed Time: 0.003924 seconds (Warm-up) 50Chain 2: 0.257815 seconds (Sampling) 51Chain 2: 0.261739 seconds (Total) 52Chain 2: 53Chain 4: 54Chain 4: Elapsed Time: 0.003735 seconds (Warm-up) 55Chain 4: 0.127719 seconds (Sampling) 56Chain 4: 0.131454 seconds (Total) 57Chain 4: 58Chain 1: 59Chain 1: Gradient evaluation took 0.000209 seconds 60Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 2.09 seconds. 61Chain 1: Adjust your expectations accordingly! 62Chain 1: 63Chain 1: 64Chain 2: 65Chain 2: Gradient evaluation took 7.8e-05 seconds 66Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.78 seconds. 67Chain 2: Adjust your expectations accordingly! 68Chain 2: 69Chain 2: 70Chain 3: 71Chain 3: Gradient evaluation took 6.2e-05 seconds 72Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.62 seconds. 73Chain 3: Adjust your expectations accordingly! 74Chain 3: 75Chain 3: 76Chain 4: 77Chain 4: Gradient evaluation took 8.2e-05 seconds 78Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.82 seconds. 79Chain 4: Adjust your expectations accordingly! 80Chain 4: 81Chain 4: 82Chain 1: 83Chain 1: Elapsed Time: 0.118609 seconds (Warm-up) 84Chain 1: 0.075057 seconds (Sampling) 85Chain 1: 0.193666 seconds (Total) 86Chain 1: 87Chain 2: 88Chain 2: Elapsed Time: 0.122167 seconds (Warm-up) 89Chain 2: 0.05402 seconds (Sampling) 90Chain 2: 0.176187 seconds (Total) 91Chain 2: 92Chain 3: 93Chain 3: Elapsed Time: 0.118156 seconds (Warm-up) 94Chain 3: 0.035528 seconds (Sampling) 95Chain 3: 0.153684 seconds (Total) 96Chain 3: 97Chain 4: 98Chain 4: Elapsed Time: 0.073505 seconds (Warm-up) 99Chain 4: 0.040445 seconds (Sampling) 100Chain 4: 0.11395 seconds (Total) 101Chain 4: 102Chain 1: 103Chain 1: Gradient evaluation took 9.8e-05 seconds 104Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.98 seconds. 105Chain 1: Adjust your expectations accordingly! 106Chain 1: 107Chain 1: 108Chain 2: 109Chain 2: Gradient evaluation took 7.3e-05 seconds 110Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.73 seconds. 111Chain 2: Adjust your expectations accordingly! 112Chain 2: 113Chain 2: 114Chain 3: 115Chain 3: Gradient evaluation took 0.000107 seconds 116Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.07 seconds. 117Chain 3: Adjust your expectations accordingly! 118Chain 3: 119Chain 3: 120Chain 4: 121Chain 4: Gradient evaluation took 0.000109 seconds 122Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 1.09 seconds. 123Chain 4: Adjust your expectations accordingly! 124Chain 4: 125Chain 4: 126Chain 1: 127Chain 1: Elapsed Time: 0.23286 seconds (Warm-up) 128Chain 1: 0.032903 seconds (Sampling) 129Chain 1: 0.265763 seconds (Total) 130Chain 1: 131Chain 2: 132Chain 2: Elapsed Time: 0.196139 seconds (Warm-up) 133Chain 2: 0.032946 seconds (Sampling) 134Chain 2: 0.229085 seconds (Total) 135Chain 2: 136Chain 3: 137Chain 3: Elapsed Time: 0.15298 seconds (Warm-up) 138Chain 3: 0.042238 seconds (Sampling) 139Chain 3: 0.195218 seconds (Total) 140Chain 3: 141Chain 4: 142Chain 4: Elapsed Time: 0.109015 seconds (Warm-up) 143Chain 4: 0.041119 seconds (Sampling) 144Chain 4: 0.150134 seconds (Total) 145Chain 4: 146Chain 1: 147Chain 1: Gradient evaluation took 7.4e-05 seconds 148Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.74 seconds. 149Chain 1: Adjust your expectations accordingly! 150Chain 1: 151Chain 1: 152Chain 2: 153Chain 2: Gradient evaluation took 7.9e-05 seconds 154Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.79 seconds. 155Chain 2: Adjust your expectations accordingly! 156Chain 2: 157Chain 2: 158Chain 3: 159Chain 3: Gradient evaluation took 0.000109 seconds 160Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.09 seconds. 161Chain 3: Adjust your expectations accordingly! 162Chain 3: 163Chain 3: 164Chain 4: 165Chain 4: Gradient evaluation took 0.000101 seconds 166Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 1.01 seconds. 167Chain 4: Adjust your expectations accordingly! 168Chain 4: 169Chain 4: 170Chain 1: 171Chain 1: Elapsed Time: 0.304209 seconds (Warm-up) 172Chain 1: 0.038004 seconds (Sampling) 173Chain 1: 0.342213 seconds (Total) 174Chain 1: 175Chain 2: 176Chain 2: Elapsed Time: 0.274087 seconds (Warm-up) 177Chain 2: 0.034831 seconds (Sampling) 178Chain 2: 0.308918 seconds (Total) 179Chain 2: 180Chain 3: 181Chain 3: Elapsed Time: 0.231719 seconds (Warm-up) 182Chain 3: 0.038131 seconds (Sampling) 183Chain 3: 0.26985 seconds (Total) 184Chain 3: 185Chain 4: 186Chain 4: Elapsed Time: 0.177718 seconds (Warm-up) 187Chain 4: 0.038546 seconds (Sampling) 188Chain 4: 0.216264 seconds (Total) 189Chain 4: 190Chain 1: 191Chain 1: Gradient evaluation took 0.000117 seconds 192Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.17 seconds. 193Chain 1: Adjust your expectations accordingly! 194Chain 1: 195Chain 1: 196Chain 2: 197Chain 2: Gradient evaluation took 7.3e-05 seconds 198Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.73 seconds. 199Chain 2: Adjust your expectations accordingly! 200Chain 2: 201Chain 2: 202Chain 3: 203Chain 3: Gradient evaluation took 7.2e-05 seconds 204Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.72 seconds. 205Chain 3: Adjust your expectations accordingly! 206Chain 3: 207Chain 3: 208Chain 4: 209Chain 4: Gradient evaluation took 7.3e-05 seconds 210Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.73 seconds. 211Chain 4: Adjust your expectations accordingly! 212Chain 4: 213Chain 4: 214Chain 1: 215Chain 1: Elapsed Time: 0.322041 seconds (Warm-up) 216Chain 1: 0.048995 seconds (Sampling) 217Chain 1: 0.371036 seconds (Total) 218Chain 1: 219Chain 2: 220Chain 2: Elapsed Time: 0.293325 seconds (Warm-up) 221Chain 2: 0.032541 seconds (Sampling) 222Chain 2: 0.325866 seconds (Total) 223Chain 2: 224Chain 3: 225Chain 3: Elapsed Time: 0.264383 seconds (Warm-up) 226Chain 3: 0.04051 seconds (Sampling) 227Chain 3: 0.304893 seconds (Total) 228Chain 3: 229Chain 4: 230Chain 4: Elapsed Time: 0.220301 seconds (Warm-up) 231Chain 4: 0.040218 seconds (Sampling) 232Chain 4: 0.260519 seconds (Total) 233Chain 4: 234Chain 1: 235Chain 1: Gradient evaluation took 4.9e-05 seconds 236Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.49 seconds. 237Chain 1: Adjust your expectations accordingly! 238Chain 1: 239Chain 1: 240Chain 2: 241Chain 2: Gradient evaluation took 5.1e-05 seconds 242Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.51 seconds. 243Chain 2: Adjust your expectations accordingly! 244Chain 2: 245Chain 2: 246Chain 3: 247Chain 3: Gradient evaluation took 3.9e-05 seconds 248Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds. 249Chain 3: Adjust your expectations accordingly! 250Chain 3: 251Chain 3: 252Chain 4: 253Chain 4: Gradient evaluation took 3.9e-05 seconds 254Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds. 255Chain 4: Adjust your expectations accordingly! 256Chain 4: 257Chain 4: 258Chain 1: 259Chain 1: Elapsed Time: 0.306918 seconds (Warm-up) 260Chain 1: 0.048194 seconds (Sampling) 261Chain 1: 0.355112 seconds (Total) 262Chain 1: 263Chain 2: 264Chain 2: Elapsed Time: 0.299256 seconds (Warm-up) 265Chain 2: 0.052776 seconds (Sampling) 266Chain 2: 0.352032 seconds (Total) 267Chain 2: 268Chain 3: 269Chain 3: Elapsed Time: 0.300093 seconds (Warm-up) 270Chain 3: 0.052132 seconds (Sampling) 271Chain 3: 0.352225 seconds (Total) 272Chain 3: 273Chain 4: 274Chain 4: Elapsed Time: 0.278897 seconds (Warm-up) 275Chain 4: 0.053532 seconds (Sampling) 276Chain 4: 0.332429 seconds (Total) 277Chain 4: 278Chain 1: 279Chain 1: Gradient evaluation took 9.4e-05 seconds 280Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.94 seconds. 281Chain 1: Adjust your expectations accordingly! 282Chain 1: 283Chain 1: 284Chain 2: 285Chain 2: Gradient evaluation took 8.4e-05 seconds 286Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.84 seconds. 287Chain 2: Adjust your expectations accordingly! 288Chain 2: 289Chain 2: 290Chain 3: 291Chain 3: Gradient evaluation took 7.1e-05 seconds 292Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.71 seconds. 293Chain 3: Adjust your expectations accordingly! 294Chain 3: 295Chain 3: 296Chain 4: 297Chain 4: Gradient evaluation took 6.2e-05 seconds 298Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.62 seconds. 299Chain 4: Adjust your expectations accordingly! 300Chain 4: 301Chain 4: 302Chain 1: 303Chain 1: Elapsed Time: 0.437519 seconds (Warm-up) 304Chain 1: 0.04052 seconds (Sampling) 305Chain 1: 0.478039 seconds (Total) 306Chain 1: 307Chain 3: 308Chain 3: Elapsed Time: 0.3479 seconds (Warm-up) 309Chain 3: 0.033007 seconds (Sampling) 310Chain 3: 0.380907 seconds (Total) 311Chain 3: 312Chain 2: 313Chain 2: Elapsed Time: 0.412403 seconds (Warm-up) 314Chain 2: 0.052948 seconds (Sampling) 315Chain 2: 0.465351 seconds (Total) 316Chain 2: 317Chain 4: 318Chain 4: Elapsed Time: 0.315601 seconds (Warm-up) 319Chain 4: 0.038006 seconds (Sampling) 320Chain 4: 0.353607 seconds (Total) 321Chain 4: 322Chain 1: 323Chain 1: Gradient evaluation took 8.8e-05 seconds 324Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.88 seconds. 325Chain 1: Adjust your expectations accordingly! 326Chain 1: 327Chain 1: 328Chain 2: 329Chain 2: Gradient evaluation took 5.8e-05 seconds 330Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.58 seconds. 331Chain 2: Adjust your expectations accordingly! 332Chain 2: 333Chain 2: 334Chain 3: 335Chain 3: Gradient evaluation took 5.9e-05 seconds 336Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.59 seconds. 337Chain 3: Adjust your expectations accordingly! 338Chain 3: 339Chain 3: 340Chain 4: 341Chain 4: Gradient evaluation took 4.4e-05 seconds 342Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.44 seconds. 343Chain 4: Adjust your expectations accordingly! 344Chain 4: 345Chain 4: 346Chain 1: 347Chain 1: Elapsed Time: 0.387624 seconds (Warm-up) 348Chain 1: 0.033915 seconds (Sampling) 349Chain 1: 0.421539 seconds (Total) 350Chain 1: 351Chain 2: 352Chain 2: Elapsed Time: 0.385989 seconds (Warm-up) 353Chain 2: 0.044089 seconds (Sampling) 354Chain 2: 0.430078 seconds (Total) 355Chain 2: 356Chain 3: 357Chain 3: Elapsed Time: 0.409957 seconds (Warm-up) 358Chain 3: 0.039642 seconds (Sampling) 359Chain 3: 0.449599 seconds (Total) 360Chain 3: 361Chain 4: 362Chain 4: Elapsed Time: 0.363549 seconds (Warm-up) 363Chain 4: 0.042624 seconds (Sampling) 364Chain 4: 0.406173 seconds (Total) 365Chain 4: 366Chain 1: 367Chain 1: Gradient evaluation took 7.8e-05 seconds 368Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.78 seconds. 369Chain 1: Adjust your expectations accordingly! 370Chain 1: 371Chain 1: 372Chain 2: 373Chain 2: Gradient evaluation took 5.1e-05 seconds 374Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.51 seconds. 375Chain 2: Adjust your expectations accordingly! 376Chain 2: 377Chain 2: 378Chain 3: 379Chain 3: Gradient evaluation took 4.7e-05 seconds 380Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.47 seconds. 381Chain 3: Adjust your expectations accordingly! 382Chain 3: 383Chain 3: 384Chain 4: 385Chain 4: Gradient evaluation took 5.1e-05 seconds 386Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.51 seconds. 387Chain 4: Adjust your expectations accordingly! 388Chain 4: 389Chain 4: 390Chain 2: 391Chain 2: Elapsed Time: 0.355205 seconds (Warm-up) 392Chain 2: 0.051537 seconds (Sampling) 393Chain 2: 0.406742 seconds (Total) 394Chain 2: 395Chain 1: 396Chain 1: Elapsed Time: 0.394264 seconds (Warm-up) 397Chain 1: 0.037432 seconds (Sampling) 398Chain 1: 0.431696 seconds (Total) 399Chain 1: 400Chain 3: 401Chain 3: Elapsed Time: 0.383287 seconds (Warm-up) 402Chain 3: 0.036322 seconds (Sampling) 403Chain 3: 0.419609 seconds (Total) 404Chain 3: 405Chain 4: 406Chain 4: Elapsed Time: 0.335487 seconds (Warm-up) 407Chain 4: 0.048271 seconds (Sampling) 408Chain 4: 0.383758 seconds (Total) 409Chain 4: 410Chain 1: 411Chain 1: Gradient evaluation took 5e-05 seconds 412Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.5 seconds. 413Chain 1: Adjust your expectations accordingly! 414Chain 1: 415Chain 1: 416Chain 2: 417Chain 2: Gradient evaluation took 5.5e-05 seconds 418Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.55 seconds. 419Chain 2: Adjust your expectations accordingly! 420Chain 2: 421Chain 2: 422Chain 3: 423Chain 3: Gradient evaluation took 6.3e-05 seconds 424Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.63 seconds. 425Chain 3: Adjust your expectations accordingly! 426Chain 3: 427Chain 3: 428Chain 4: 429Chain 4: Gradient evaluation took 5.1e-05 seconds 430Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.51 seconds. 431Chain 4: Adjust your expectations accordingly! 432Chain 4: 433Chain 4: 434Chain 1: 435Chain 1: Elapsed Time: 0.454863 seconds (Warm-up) 436Chain 1: 0.037816 seconds (Sampling) 437Chain 1: 0.492679 seconds (Total) 438Chain 1: 439Chain 2: 440Chain 2: Elapsed Time: 0.415499 seconds (Warm-up) 441Chain 2: 0.037093 seconds (Sampling) 442Chain 2: 0.452592 seconds (Total) 443Chain 2: 444Chain 3: 445Chain 3: Elapsed Time: 0.363295 seconds (Warm-up) 446Chain 3: 0.061002 seconds (Sampling) 447Chain 3: 0.424297 seconds (Total) 448Chain 3: 449Chain 4: 450Chain 4: Elapsed Time: 0.462541 seconds (Warm-up) 451Chain 4: 0.043532 seconds (Sampling) 452Chain 4: 0.506073 seconds (Total) 453Chain 4: 454Warning messages: 4551: There were 470 divergent transitions after warmup. Increasing adapt_delta above 0.95 may help. See 456http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 4572: There were 1 chains where the estimated Bayesian Fraction of Missing Information was low. See 458http://mc-stan.org/misc/warnings.html#bfmi-low 4593: Examine the pairs() plot to diagnose sampling problems 4604614: The largest R-hat is 1.05, indicating chains have not mixed. 462Running the chains for more iterations may help. See 463http://mc-stan.org/misc/warnings.html#r-hat 4645: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. 465Running the chains for more iterations may help. See 466http://mc-stan.org/misc/warnings.html#bulk-ess 4676: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. 468Running the chains for more iterations may help. See 469http://mc-stan.org/misc/warnings.html#tail-ess 1nSampleEff %\u0026gt;% tibble(nWarmup=warmTrial) 1\u001b[90m# A tibble: 10 x 2\u001b[39m 2.[,\u0026#34;a[1]\u0026#34;] [,\u0026#34;b[1]\u0026#34;] [,\u0026#34;sigma\u0026#34;] nWarmup 3\u001b[3m\u001b[90m\u0026lt;dbl\u0026gt;\u001b[39m\u001b[23m \u001b[3m\u001b[90m\u0026lt;dbl\u0026gt;\u001b[39m\u001b[23m \u001b[3m\u001b[90m\u0026lt;dbl\u0026gt;\u001b[39m\u001b[23m \u001b[3m\u001b[90m\u0026lt;int\u0026gt;\u001b[39m\u001b[23m 4\u001b[90m 1\u001b[39m \u001b[4m1\u001b[24m051. 399. 45.8 10 5\u001b[90m 2\u001b[39m \u001b[4m3\u001b[24m101. \u001b[4m3\u001b[24m085. \u001b[4m3\u001b[24m135. \u001b[4m1\u001b[24m120 6\u001b[90m 3\u001b[39m \u001b[4m3\u001b[24m515. \u001b[4m3\u001b[24m529. \u001b[4m3\u001b[24m214. \u001b[4m2\u001b[24m230 7\u001b[90m 4\u001b[39m \u001b[4m3\u001b[24m122. \u001b[4m3\u001b[24m277. \u001b[4m3\u001b[24m522. \u001b[4m3\u001b[24m340 8\u001b[90m 5\u001b[39m \u001b[4m3\u001b[24m145. \u001b[4m3\u001b[24m382. \u001b[4m3\u001b[24m322. \u001b[4m4\u001b[24m450 9\u001b[90m 6\u001b[39m \u001b[4m3\u001b[24m378. \u001b[4m3\u001b[24m193. \u001b[4m3\u001b[24m701. \u001b[4m5\u001b[24m560 10\u001b[90m 7\u001b[39m \u001b[4m3\u001b[24m299. \u001b[4m3\u001b[24m539. \u001b[4m3\u001b[24m149. \u001b[4m6\u001b[24m670 11\u001b[90m 8\u001b[39m \u001b[4m3\u001b[24m570. \u001b[4m3\u001b[24m050. \u001b[4m3\u001b[24m079. \u001b[4m7\u001b[24m780 12\u001b[90m 9\u001b[39m \u001b[4m3\u001b[24m247. \u001b[4m3\u001b[24m148. \u001b[4m3\u001b[24m340. \u001b[4m8\u001b[24m890 13\u001b[90m10\u001b[39m \u001b[4m3\u001b[24m159. \u001b[4m2\u001b[24m929. \u001b[4m2\u001b[24m960. \u001b[4m1\u001b[24m\u001b[4m0\u001b[24m000      Exponential Model\n1for(i in 1:length(warmTrial)){ 2tmp\u0026lt;-ulam(m91exp,chains=4,cores=4,refresh=-1,warmup=warmTrial[i],iter=1000+warmTrial[i]) 3nSampleEffExp[i,]\u0026lt;-precis(tmp,2)$n_eff 4} 12Chain 1: 3Chain 1: Gradient evaluation took 3e-05 seconds 4Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds. 5Chain 1: Adjust your expectations accordingly! 6Chain 1: 7Chain 1: 8Chain 1: WARNING: No variance estimation is 9Chain 1: performed for num_warmup \u0026lt; 20 10Chain 1: 11Chain 2: 12Chain 2: Gradient evaluation took 2.8e-05 seconds 13Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. 14Chain 2: Adjust your expectations accordingly! 15Chain 2: 16Chain 2: 17Chain 2: WARNING: No variance estimation is 18Chain 2: performed for num_warmup \u0026lt; 20 19Chain 2: 20Chain 3: 21Chain 3: Gradient evaluation took 2.7e-05 seconds 22Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. 23Chain 3: Adjust your expectations accordingly! 24Chain 3: 25Chain 3: 26Chain 3: WARNING: No variance estimation is 27Chain 3: performed for num_warmup \u0026lt; 20 28Chain 3: 29Chain 4: 30Chain 4: Gradient evaluation took 2.5e-05 seconds 31Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds. 32Chain 4: Adjust your expectations accordingly! 33Chain 4: 34Chain 4: 35Chain 4: WARNING: No variance estimation is 36Chain 4: performed for num_warmup \u0026lt; 20 37Chain 4: 38Chain 1: 39Chain 1: Elapsed Time: 0.000825 seconds (Warm-up) 40Chain 1: 0.074412 seconds (Sampling) 41Chain 1: 0.075237 seconds (Total) 42Chain 1: 43Chain 3: 44Chain 3: Elapsed Time: 0.000433 seconds (Warm-up) 45Chain 3: 0.05124 seconds (Sampling) 46Chain 3: 0.051673 seconds (Total) 47Chain 3: 48Chain 4: 49Chain 4: Elapsed Time: 0.001827 seconds (Warm-up) 50Chain 4: 0.059028 seconds (Sampling) 51Chain 4: 0.060855 seconds (Total) 52Chain 4: 53Chain 2: 54Chain 2: Elapsed Time: 0.005092 seconds (Warm-up) 55Chain 2: 0.111894 seconds (Sampling) 56Chain 2: 0.116986 seconds (Total) 57Chain 2: 58Chain 1: 59Chain 1: Gradient evaluation took 2.7e-05 seconds 60Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. 61Chain 1: Adjust your expectations accordingly! 62Chain 1: 63Chain 1: 64Chain 2: 65Chain 2: Gradient evaluation took 2.7e-05 seconds 66Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. 67Chain 2: Adjust your expectations accordingly! 68Chain 2: 69Chain 2: 70Chain 3: 71Chain 3: Gradient evaluation took 2.7e-05 seconds 72Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. 73Chain 3: Adjust your expectations accordingly! 74Chain 3: 75Chain 3: 76Chain 4: 77Chain 4: Gradient evaluation took 3.3e-05 seconds 78Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.33 seconds. 79Chain 4: Adjust your expectations accordingly! 80Chain 4: 81Chain 4: 82Chain 1: 83Chain 1: Elapsed Time: 0.048625 seconds (Warm-up) 84Chain 1: 0.03552 seconds (Sampling) 85Chain 1: 0.084145 seconds (Total) 86Chain 1: 87Chain 2: 88Chain 2: Elapsed Time: 0.045476 seconds (Warm-up) 89Chain 2: 0.03068 seconds (Sampling) 90Chain 2: 0.076156 seconds (Total) 91Chain 2: 92Chain 3: 93Chain 3: Elapsed Time: 0.059723 seconds (Warm-up) 94Chain 3: 0.034697 seconds (Sampling) 95Chain 3: 0.09442 seconds (Total) 96Chain 3: 97Chain 4: 98Chain 4: Elapsed Time: 0.055658 seconds (Warm-up) 99Chain 4: 0.030508 seconds (Sampling) 100Chain 4: 0.086166 seconds (Total) 101Chain 4: 102Chain 1: 103Chain 1: Gradient evaluation took 3.7e-05 seconds 104Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds. 105Chain 1: Adjust your expectations accordingly! 106Chain 1: 107Chain 1: 108Chain 2: 109Chain 2: Gradient evaluation took 2.8e-05 seconds 110Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. 111Chain 2: Adjust your expectations accordingly! 112Chain 2: 113Chain 2: 114Chain 3: 115Chain 3: Gradient evaluation took 2.6e-05 seconds 116Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds. 117Chain 3: Adjust your expectations accordingly! 118Chain 3: 119Chain 3: 120Chain 4: 121Chain 4: Gradient evaluation took 2.6e-05 seconds 122Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds. 123Chain 4: Adjust your expectations accordingly! 124Chain 4: 125Chain 4: 126Chain 1: 127Chain 1: Elapsed Time: 0.10462 seconds (Warm-up) 128Chain 1: 0.035886 seconds (Sampling) 129Chain 1: 0.140506 seconds (Total) 130Chain 1: 131Chain 3: 132Chain 3: Elapsed Time: 0.08377 seconds (Warm-up) 133Chain 3: 0.036938 seconds (Sampling) 134Chain 3: 0.120708 seconds (Total) 135Chain 3: 136Chain 2: 137Chain 2: Elapsed Time: 0.096442 seconds (Warm-up) 138Chain 2: 0.0419 seconds (Sampling) 139Chain 2: 0.138342 seconds (Total) 140Chain 2: 141Chain 4: 142Chain 4: Elapsed Time: 0.091513 seconds (Warm-up) 143Chain 4: 0.062068 seconds (Sampling) 144Chain 4: 0.153581 seconds (Total) 145Chain 4: 146Chain 1: 147Chain 1: Gradient evaluation took 3.1e-05 seconds 148Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. 149Chain 1: Adjust your expectations accordingly! 150Chain 1: 151Chain 1: 152Chain 2: 153Chain 2: Gradient evaluation took 3.1e-05 seconds 154Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. 155Chain 2: Adjust your expectations accordingly! 156Chain 2: 157Chain 2: 158Chain 3: 159Chain 3: Gradient evaluation took 2.6e-05 seconds 160Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds. 161Chain 3: Adjust your expectations accordingly! 162Chain 3: 163Chain 3: 164Chain 4: 165Chain 4: Gradient evaluation took 2.4e-05 seconds 166Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds. 167Chain 4: Adjust your expectations accordingly! 168Chain 4: 169Chain 4: 170Chain 1: 171Chain 1: Elapsed Time: 0.12662 seconds (Warm-up) 172Chain 1: 0.033816 seconds (Sampling) 173Chain 1: 0.160436 seconds (Total) 174Chain 1: 175Chain 2: 176Chain 2: Elapsed Time: 0.119929 seconds (Warm-up) 177Chain 2: 0.048545 seconds (Sampling) 178Chain 2: 0.168474 seconds (Total) 179Chain 2: 180Chain 4: 181Chain 4: Elapsed Time: 0.118087 seconds (Warm-up) 182Chain 4: 0.040127 seconds (Sampling) 183Chain 4: 0.158214 seconds (Total) 184Chain 4: 185Chain 3: 186Chain 3: Elapsed Time: 0.131859 seconds (Warm-up) 187Chain 3: 0.053882 seconds (Sampling) 188Chain 3: 0.185741 seconds (Total) 189Chain 3: 190Chain 1: 191Chain 1: Gradient evaluation took 2.8e-05 seconds 192Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. 193Chain 1: Adjust your expectations accordingly! 194Chain 1: 195Chain 1: 196Chain 2: 197Chain 2: Gradient evaluation took 3.1e-05 seconds 198Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. 199Chain 2: Adjust your expectations accordingly! 200Chain 2: 201Chain 2: 202Chain 3: 203Chain 3: Gradient evaluation took 3.8e-05 seconds 204Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.38 seconds. 205Chain 3: Adjust your expectations accordingly! 206Chain 3: 207Chain 3: 208Chain 4: 209Chain 4: Gradient evaluation took 2.8e-05 seconds 210Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. 211Chain 4: Adjust your expectations accordingly! 212Chain 4: 213Chain 4: 214Chain 1: 215Chain 1: Elapsed Time: 0.178377 seconds (Warm-up) 216Chain 1: 0.051505 seconds (Sampling) 217Chain 1: 0.229882 seconds (Total) 218Chain 1: 219Chain 2: 220Chain 2: Elapsed Time: 0.19769 seconds (Warm-up) 221Chain 2: 0.028853 seconds (Sampling) 222Chain 2: 0.226543 seconds (Total) 223Chain 2: 224Chain 3: 225Chain 3: Elapsed Time: 0.209976 seconds (Warm-up) 226Chain 3: 0.049889 seconds (Sampling) 227Chain 3: 0.259865 seconds (Total) 228Chain 3: 229Chain 4: 230Chain 4: Elapsed Time: 0.233406 seconds (Warm-up) 231Chain 4: 0.03065 seconds (Sampling) 232Chain 4: 0.264056 seconds (Total) 233Chain 4: 234Chain 1: 235Chain 1: Gradient evaluation took 3.1e-05 seconds 236Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. 237Chain 1: Adjust your expectations accordingly! 238Chain 1: 239Chain 1: 240Chain 2: 241Chain 2: Gradient evaluation took 3e-05 seconds 242Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds. 243Chain 2: Adjust your expectations accordingly! 244Chain 2: 245Chain 2: 246Chain 3: 247Chain 3: Gradient evaluation took 2.9e-05 seconds 248Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.29 seconds. 249Chain 3: Adjust your expectations accordingly! 250Chain 3: 251Chain 3: 252Chain 4: 253Chain 4: Gradient evaluation took 3.3e-05 seconds 254Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.33 seconds. 255Chain 4: Adjust your expectations accordingly! 256Chain 4: 257Chain 4: 258Chain 1: 259Chain 1: Elapsed Time: 0.226122 seconds (Warm-up) 260Chain 1: 0.031202 seconds (Sampling) 261Chain 1: 0.257324 seconds (Total) 262Chain 1: 263Chain 2: 264Chain 2: Elapsed Time: 0.229103 seconds (Warm-up) 265Chain 2: 0.028798 seconds (Sampling) 266Chain 2: 0.257901 seconds (Total) 267Chain 2: 268Chain 3: 269Chain 3: Elapsed Time: 0.238441 seconds (Warm-up) 270Chain 3: 0.03459 seconds (Sampling) 271Chain 3: 0.273031 seconds (Total) 272Chain 3: 273Chain 4: 274Chain 4: Elapsed Time: 0.228212 seconds (Warm-up) 275Chain 4: 0.036574 seconds (Sampling) 276Chain 4: 0.264786 seconds (Total) 277Chain 4: 278Chain 1: 279Chain 1: Gradient evaluation took 3e-05 seconds 280Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds. 281Chain 1: Adjust your expectations accordingly! 282Chain 1: 283Chain 1: 284Chain 2: 285Chain 2: Gradient evaluation took 2.7e-05 seconds 286Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. 287Chain 2: Adjust your expectations accordingly! 288Chain 2: 289Chain 2: 290Chain 3: 291Chain 3: Gradient evaluation took 2.8e-05 seconds 292Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. 293Chain 3: Adjust your expectations accordingly! 294Chain 3: 295Chain 3: 296Chain 4: 297Chain 4: Gradient evaluation took 2.4e-05 seconds 298Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds. 299Chain 4: Adjust your expectations accordingly! 300Chain 4: 301Chain 4: 302Chain 1: 303Chain 1: Elapsed Time: 0.285094 seconds (Warm-up) 304Chain 1: 0.048246 seconds (Sampling) 305Chain 1: 0.33334 seconds (Total) 306Chain 1: 307Chain 4: 308Chain 4: Elapsed Time: 0.263814 seconds (Warm-up) 309Chain 4: 0.034411 seconds (Sampling) 310Chain 4: 0.298225 seconds (Total) 311Chain 4: 312Chain 2: 313Chain 2: Elapsed Time: 0.305613 seconds (Warm-up) 314Chain 2: 0.039627 seconds (Sampling) 315Chain 2: 0.34524 seconds (Total) 316Chain 2: 317Chain 3: 318Chain 3: Elapsed Time: 0.314974 seconds (Warm-up) 319Chain 3: 0.040995 seconds (Sampling) 320Chain 3: 0.355969 seconds (Total) 321Chain 3: 322Chain 1: 323Chain 1: Gradient evaluation took 3e-05 seconds 324Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds. 325Chain 1: Adjust your expectations accordingly! 326Chain 1: 327Chain 1: 328Chain 2: 329Chain 2: Gradient evaluation took 2.8e-05 seconds 330Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. 331Chain 2: Adjust your expectations accordingly! 332Chain 2: 333Chain 2: 334Chain 3: 335Chain 3: Gradient evaluation took 3e-05 seconds 336Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds. 337Chain 3: Adjust your expectations accordingly! 338Chain 3: 339Chain 3: 340Chain 4: 341Chain 4: Gradient evaluation took 2.7e-05 seconds 342Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. 343Chain 4: Adjust your expectations accordingly! 344Chain 4: 345Chain 4: 346Chain 1: 347Chain 1: Elapsed Time: 0.293873 seconds (Warm-up) 348Chain 1: 0.028447 seconds (Sampling) 349Chain 1: 0.32232 seconds (Total) 350Chain 1: 351Chain 3: 352Chain 3: Elapsed Time: 0.288431 seconds (Warm-up) 353Chain 3: 0.04518 seconds (Sampling) 354Chain 3: 0.333611 seconds (Total) 355Chain 3: 356Chain 2: 357Chain 2: Elapsed Time: 0.36752 seconds (Warm-up) 358Chain 2: 0.027453 seconds (Sampling) 359Chain 2: 0.394973 seconds (Total) 360Chain 2: 361Chain 4: 362Chain 4: Elapsed Time: 0.383786 seconds (Warm-up) 363Chain 4: 0.032154 seconds (Sampling) 364Chain 4: 0.41594 seconds (Total) 365Chain 4: 366Chain 1: 367Chain 1: Gradient evaluation took 3.1e-05 seconds 368Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. 369Chain 1: Adjust your expectations accordingly! 370Chain 1: 371Chain 1: 372Chain 2: 373Chain 2: Gradient evaluation took 2.7e-05 seconds 374Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. 375Chain 2: Adjust your expectations accordingly! 376Chain 2: 377Chain 2: 378Chain 3: 379Chain 3: Gradient evaluation took 2.7e-05 seconds 380Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. 381Chain 3: Adjust your expectations accordingly! 382Chain 3: 383Chain 3: 384Chain 4: 385Chain 4: Gradient evaluation took 3.1e-05 seconds 386Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. 387Chain 4: Adjust your expectations accordingly! 388Chain 4: 389Chain 4: 390Chain 1: 391Chain 1: Elapsed Time: 0.354637 seconds (Warm-up) 392Chain 1: 0.027996 seconds (Sampling) 393Chain 1: 0.382633 seconds (Total) 394Chain 1: 395Chain 2: 396Chain 2: Elapsed Time: 0.339298 seconds (Warm-up) 397Chain 2: 0.030313 seconds (Sampling) 398Chain 2: 0.369611 seconds (Total) 399Chain 2: 400Chain 4: 401Chain 4: Elapsed Time: 0.319312 seconds (Warm-up) 402Chain 4: 0.027904 seconds (Sampling) 403Chain 4: 0.347216 seconds (Total) 404Chain 4: 405Chain 3: 406Chain 3: Elapsed Time: 0.333814 seconds (Warm-up) 407Chain 3: 0.032747 seconds (Sampling) 408Chain 3: 0.366561 seconds (Total) 409Chain 3: 410Chain 1: 411Chain 1: Gradient evaluation took 3.8e-05 seconds 412Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.38 seconds. 413Chain 1: Adjust your expectations accordingly! 414Chain 1: 415Chain 1: 416Chain 2: 417Chain 2: Gradient evaluation took 3.7e-05 seconds 418Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds. 419Chain 2: Adjust your expectations accordingly! 420Chain 2: 421Chain 2: 422Chain 3: 423Chain 3: Gradient evaluation took 2.6e-05 seconds 424Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds. 425Chain 3: Adjust your expectations accordingly! 426Chain 3: 427Chain 3: 428Chain 4: 429Chain 4: Gradient evaluation took 3.1e-05 seconds 430Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. 431Chain 4: Adjust your expectations accordingly! 432Chain 4: 433Chain 4: 434Chain 1: 435Chain 1: Elapsed Time: 0.343002 seconds (Warm-up) 436Chain 1: 0.028223 seconds (Sampling) 437Chain 1: 0.371225 seconds (Total) 438Chain 1: 439Chain 3: 440Chain 3: Elapsed Time: 0.305501 seconds (Warm-up) 441Chain 3: 0.027824 seconds (Sampling) 442Chain 3: 0.333325 seconds (Total) 443Chain 3: 444Chain 2: 445Chain 2: Elapsed Time: 0.358116 seconds (Warm-up) 446Chain 2: 0.02975 seconds (Sampling) 447Chain 2: 0.387866 seconds (Total) 448Chain 2: 449Chain 4: 450Chain 4: Elapsed Time: 0.330673 seconds (Warm-up) 451Chain 4: 0.030407 seconds (Sampling) 452Chain 4: 0.36108 seconds (Total) 453Chain 4: 454Warning messages: 4551: There were 14 divergent transitions after warmup. Increasing adapt_delta above 0.95 may help. See 456http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 4572: Examine the pairs() plot to diagnose sampling problems 1nSampleEffExp %\u0026gt;% tibble(nWarmup=warmTrial) 1\u001b[90m# A tibble: 10 x 2\u001b[39m 2.[,1] [,2] [,3] nWarmup 3\u001b[3m\u001b[90m\u0026lt;dbl\u0026gt;\u001b[39m\u001b[23m \u001b[3m\u001b[90m\u0026lt;dbl\u0026gt;\u001b[39m\u001b[23m \u001b[3m\u001b[90m\u0026lt;dbl\u0026gt;\u001b[39m\u001b[23m \u001b[3m\u001b[90m\u0026lt;int\u0026gt;\u001b[39m\u001b[23m 4\u001b[90m 1\u001b[39m \u001b[4m3\u001b[24m063. \u001b[4m1\u001b[24m265. \u001b[4m1\u001b[24m045. 10 5\u001b[90m 2\u001b[39m \u001b[4m3\u001b[24m238. \u001b[4m2\u001b[24m909. \u001b[4m3\u001b[24m425. \u001b[4m1\u001b[24m120 6\u001b[90m 3\u001b[39m \u001b[4m3\u001b[24m268. \u001b[4m2\u001b[24m949. \u001b[4m2\u001b[24m966. \u001b[4m2\u001b[24m230 7\u001b[90m 4\u001b[39m \u001b[4m3\u001b[24m123. \u001b[4m3\u001b[24m257. \u001b[4m3\u001b[24m218. \u001b[4m3\u001b[24m340 8\u001b[90m 5\u001b[39m \u001b[4m3\u001b[24m227. \u001b[4m3\u001b[24m449. \u001b[4m3\u001b[24m554. \u001b[4m4\u001b[24m450 9\u001b[90m 6\u001b[39m \u001b[4m3\u001b[24m494. \u001b[4m3\u001b[24m743. \u001b[4m3\u001b[24m343. \u001b[4m5\u001b[24m560 10\u001b[90m 7\u001b[39m \u001b[4m3\u001b[24m200. \u001b[4m3\u001b[24m012. \u001b[4m2\u001b[24m879. \u001b[4m6\u001b[24m670 11\u001b[90m 8\u001b[39m \u001b[4m3\u001b[24m210. \u001b[4m3\u001b[24m207. \u001b[4m3\u001b[24m125. \u001b[4m7\u001b[24m780 12\u001b[90m 9\u001b[39m \u001b[4m2\u001b[24m919. \u001b[4m3\u001b[24m329. \u001b[4m3\u001b[24m060. \u001b[4m8\u001b[24m890 13\u001b[90m10\u001b[39m \u001b[4m2\u001b[24m951. \u001b[4m3\u001b[24m461. \u001b[4m3\u001b[24m078. \u001b[4m1\u001b[24m\u001b[4m0\u001b[24m000 It is important to note that the divergent transitions are probably why the number of effective samples decrease in the last two rows.\n     Results\nWe can see that the number of effective samples increases almost constantly. This is probably due to correlations in the chain, which are removed during the warmup period.\n  Chapter XI: God Spiked The Integers Easy Questions (Ch11) 11E1 If an event has probability \\(0.35\\), what are the log-odds of this event?\nSolution 1log(0.35/(1-0.35)) 1[1] -0.6190392 11E2 If an event has log-odds \\(3.2\\), what is the probability of this event?\nSolution 1logistic(3.2) 1[1] 0.9608343 11E3 Suppose that a coefficient in a logistic regression has value \\(1.7\\). What does this imply about the proportional change in odds of the outcome?\nSolution 1exp(1.7) 1[1] 5.473947 Note that this is not really the change in the variable, but the proportional odds.\nHOLD 11E4 Why do Poisson regressions sometimes require the use of an offset? Provide an example.\nSolution The Poisson distribution is often understood as a limiting distribution of the Binomial where \\(λ=np\\) as \\(n→∞\\) and \\(p→0\\). The single parameter thus expresses the expected value, but is often used to encode different time-steps as well. Essentially, the distribution assumes a constant rate in time or space, and thus the change in exposure, is expressed by the offset, which is the logarithm of the exposure.\nFor any case where samples are drawn from populations which have different aggregation time periods but are still within the purview of a Poisson distribution, the offset is a natural way of expressing these.\nTo leverage the example of the book, when constructing a model to account for the fact that one Monastery calculates their averages on a weekly basis, while the other averages by day, this constraint should be modeled by having differing offsets.\nQuestions of Medium Complexity (Ch11) HOLD 11M2 If a coefficient in a Poisson regression has values \\(1.7\\), what does this imply about the change in the outcome?\nSolution 1exp(1.7) 1[1] 5.473947 The coefficient in a Poisson regression implies that the proportional change in the count will be ~5.474 when the predictor variable increase by one unit.\nHOLD 11M3 Explain why the logit link is appropriate for a binomial generalized linear model.\nSolution The logit link essentially connects a parameter constrained between zero and one and the real space. The logit function is defined as:\n\\[\\mathrm{logit}(pᵢ)=\\log{\\frac{pᵢ}{1-pᵢ}}\\]\nWhere \\(pᵢ\\) is a probability mass. This link makes sense for a GLM since the predicted value is a probability distribution parameter, and we would like to obtain this from a linear model which spans the entire set of real numbers.\n1curve(logit,from=-0.5,to=1.5)  The link function maps a parameter onto a linear model.\nHOLD 11M4 Explain why the log is appropriate for a Poisson generalized linear model.\nSolution The log function ensures that the parameter cannot take values which are less than zero. This is a natural consequence of the function definition.\n1curve(log,from=-0.5,to=100000)  The log link assumes that the parameter value is the exponentiation of the linear model.\nThis makes sense for a Poisson GLM as the Poisson distribution does not accept negative values.\nHOLD 11M5 What would it imply to use a logit link for the mean of a Poisson generalized linear model? Can you think of a real research problem for which this would make sense?\nSolution We should write this out more explicitly.\nThis implies that the mean μ lies between zero and one. Since the Poisson distribution is defined by a single parameter, this does limit the model outputs. The premise of a Poisson regression problem is that the GLM models a count with an unknown maximum, so it does seem to be a very severe restriction.\nTo my mind this is feasible for constrained problems, where the Poisson distribution is to be followed but only within a particular range for some reason, and when the Binomial (of which the Poisson is a special case), decreases too slowly.\nIt was mentioned on the class forums, that the COVID-19 problem was modeled with a two parameter generalized link function, i.e. \\(\\log{\\frac{p}{S-p}}\\) which essentially constrains the model to have Poisson dynamics but with an output mean between 0 and S.\nHOLD 11M6 State the constraints for which the binomial and Poisson distributions have maximum entropy. Are the constraints different at all for binomial and Poisson? Why or why not?\nSolution The Binomial distribution is defined to be the maximum entropy distributions are:\n Discrete binary outcomes Constant probability (or expectation)  This is defined by the number of outcomes (n) as well as the probability (p). The experiment is essentially reduced to a series of independent and identical Bernoulli trials with only two outcomes. The Poisson distribution is derived as a limiting form of the Binomial, where \\(n→∞\\) and \\(p→0\\). Since this does not change the underlying constraints, this is still a maximum entropy distribution.\nHOLD 11M8 Revisit the data(Kline) islands example. This time drop Hawaii from the sample and refit the models. What changes do you observe?\nSolution 1data(Kline) 2kDat\u0026lt;-Kline 3kDat\u0026lt;-kDat %\u0026gt;% dplyr::mutate(cid=ifelse(contact==\u0026#34;high\u0026#34;,2,1), 4stdPop=standardize(log(population))) %\u0026gt;% filter(culture!=\u0026#34;Hawaii\u0026#34;) 5datList\u0026lt;-list( 6totTools=kDat$total_tools, 7stdPop=kDat$stdPop, 8cid=as.integer(kDat$cid) 9) We can now fit this.\n1m11m10res\u0026lt;-ulam( 2alist( 3totTools ~ dpois(lambda), 4log(lambda)\u0026lt;-a[cid]+b[cid]*stdPop, 5a[cid] ~ dnorm(3,0.5), 6b[cid] ~ dnorm(0,0.2) 7),data=datList, chains=4, cores=4 8) 1m11m10res %\u0026gt;% precis(2) 1mean sd 5.5% 94.5% n_eff Rhat4 2a[1] 3.18 0.12 2.99 3.37 1621 1 3a[2] 3.61 0.08 3.48 3.73 1962 1 4b[1] 0.19 0.13 -0.01 0.39 1639 1 5b[2] 0.19 0.16 -0.06 0.44 1830 1 We see that the slopes are now the same, which makes sense since in this data-set Hawaii was the only outlier.\nChapter XII: Monsters and Mixtures Easy Questions (Ch12) HOLD 12E4 Over-dispersion is common in count data. Give an example of a natural process that might produce over-dispersed counts. Can you also give an example of a process that might produce /under-/dispersed counts?\nSolution    Over-dispersion\nOver dispersion is essentially the occurrence of greater variability than accounted for based on the statistical model. The presence of over-dispersion is typically due to heterogeneity in populations. This heterogeneity may arise from simple aggregation issues like in the case considered in the text, of Monasteries which accumulate data weekly or daily, in-spite of following the same Poisson model.\n     Under-dispersion\nUnder dispersion is essentially the occurrence of less variability than accounted for based on the statistical model. The clearest example of under-dispersion is from the draws of an MCMC sampler. The number of effective samples is typically lower than the number of samples, as the data is highly correlated (autocorrelated) as the sampler draws sequential samples. For a count model, if a hidden rate limiting variable exists and has not been accounted for, then the variation in counts is lowered, and will show up as under-dispersion.\n  Hard Questions (Ch12) 12H1 In 2014, a paper was published that was entitle \u0026ldquo;Female hurricanes are deadlier than male hurricanes.\u0026rdquo; As the title suggests, the paper claimed that hurricanes with female names have caused greater loss of life, and the explanation given is that people unconsciously rate female hurricanes as less dangerous and so are less likely to evacuate. Statisticians severely criticized the paper after publication. Here, you\u0026rsquo;ll explore the complete data used in the paper and consider the hypothesis that hurricanes with female names are deadlier. Load the data with:\n1library(rethinking) 2data(Hurricanes) Acquaint yourself with the columns by inspecting the help ?Hurricanes. In this problem, you\u0026rsquo;ll focus on predicting deaths using feminity as a predictor. You can use quap or ulam. Compare the model to an intercept-only Poisson model of deaths. How strong is the association between feminity of name and deaths? Which storms does the model fit (retrodict) well? Which storms does it fit poorly?\nSolution Since I have no understanding of hurricanes except that it is unlikely to have too much of an effect. I will run through some sample priors. Presumably, most hurricanes do not kill over a thousand people. Furthermore, a-priori, I would not like to assume that femininity is positive or negative, so I will instead encode a belief that it shouldn\u0026rsquo;t matter much either way, ergo a Gaussian.\n1N\u0026lt;-100 2a\u0026lt;-rnorm(N,1,0.5) 3bF\u0026lt;-rnorm(N,0.5,2) 4seqF\u0026lt;-seq(from=-2,to=2,length.out=100) 5plot(NULL,xlim=c(-2,2),ylim=c(0,1000),xlab=\u0026#34;Femininity\u0026#34;,ylab=\u0026#34;deaths\u0026#34;) 6for(i in 1:N) lines(seqF,exp(a[i]+bF[i]*seqF),col=grau())  This seems to be reasonable to me. It does have a bit of an unreasonable focus on 0, but it does also seem to mostly hug the x-axis in a way indicating my prior belief that it should not matter all that much. There is enough diversity in the priors to allow for stronger trends, but they are by and large unlikely.\nNow we can actually use these in a model.\n1data(Hurricanes) 2hurDat\u0026lt;-Hurricanes %\u0026gt;% as.data.frame 3hurDat\u0026lt;-hurDat %\u0026gt;% dplyr::mutate(femStd=standardize(femininity)) 4datListH\u0026lt;-list(deaths=hurDat$deaths,femStd=hurDat$femStd) 5m12h1norm\u0026lt;-ulam(alist(deaths ~ dpois(lambda), 6log(lambda) \u0026lt;- a+bF*femStd, 7a ~ dnorm(1,0.5), 8bF ~ dnorm(0.5,2) 9),data=datListH, chains=4, cores=4,log_lik = TRUE) 1SAMPLING FOR MODEL \u0026#39;bd16fb771b491de48a3f8ce09fc68301\u0026#39; NOW (CHAIN 1). 23SAMPLING FOR MODEL \u0026#39;bd16fb771b491de48a3f8ce09fc68301\u0026#39; NOW (CHAIN 2). 4Chain 2: 5Chain 1: 6Chain Chain 12: : Gradient evaluation took 5.1e-05 secondsGradient evaluation took 3.9e-05 seconds 78Chain Chain 12: : 1000 transitions using 10 leapfrog steps per transition would take 0.51 seconds.1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds. 910Chain Chain 12: : Adjust your expectations accordingly!Adjust your expectations accordingly! 1112Chain Chain 12: : 1314Chain Chain 12: : 1516Chain Chain 21: Iteration: 1 / 1000 [ 0%] (Warmup) 17: Iteration: 1 / 1000 [ 0%] (Warmup) 18Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) 19Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) 2021SAMPLING FOR MODEL \u0026#39;bd16fb771b491de48a3f8ce09fc68301\u0026#39; NOW (CHAIN 3). 22Chain 3: 23Chain 3: Gradient evaluation took 4.3e-05 seconds 24Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.43 seconds. 25Chain 3: Adjust your expectations accordingly! 26Chain 3: 27Chain 3: 28Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) 29Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) 30Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) 31Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) 32Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) 3334SAMPLING FOR MODEL \u0026#39;bd16fb771b491de48a3f8ce09fc68301\u0026#39; NOW (CHAIN 4). 35Chain 4: 36Chain 4: Gradient evaluation took 3.1e-05 seconds 37Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. 38Chain 4: Adjust your expectations accordingly! 39Chain 4: 40Chain 4: 41Chain 4: Iteration: 1 / 1000 [ 0%] (Warmup) 42Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) 43Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) 44Chain 4: Iteration: 100 / 1000 [ 10%] (Warmup) 45Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) 46Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) 47Chain 4: Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) 48Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) 49Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) 50Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) 51Chain 3: Iteration: 300 / 1000 [ 30%] (Warmup) 52Iteration: 200 / 1000 [ 20%] (Warmup) 53Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) 54Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) 55Chain 3: Iteration: 400 / 1000 [ 40%] (Warmup) 56Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) 57Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) 58Chain 4: Iteration: 300 / 1000 [ 30%] (Warmup) 59Chain 3: Iteration: 500 / 1000 [ 50%] (Warmup) 60Chain 3: Iteration: 501 / 1000 [ 50%] (Sampling) 61Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) 62Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) 63Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) 64Chain 4: Iteration: 400 / 1000 [ 40%] (Warmup) 65Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) 66Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) 67Chain 4: Iteration: 500 / 1000 [ 50%] (Warmup) 68Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) 69Chain 4: Iteration: 501 / 1000 [ 50%] (Sampling) 70Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) 71Chain 1: 72Chain 1: Elapsed Time: 0.055115 seconds (Warm-up) 73Chain 1: 0.042775 seconds (Sampling) 74Chain 1: 0.09789 seconds (Total) 75Chain 1: 76Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) 77Chain 4: Iteration: 600 / 1000 [ 60%] (Sampling) 78Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) 79Chain 2: 80Chain 2: Elapsed Time: 0.050636 seconds (Warm-up) 81Chain 2: 0.049372 seconds (Sampling) 82Chain 2: 0.100008 seconds (Total) 83Chain 2: 84Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) 85Chain 4: Iteration: 700 / 1000 [ 70%] (Sampling) 86Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) 87Chain 3: 88Chain 3: Elapsed Time: 0.056256 seconds (Warm-up) 89Chain 3: 0.039069 seconds (Sampling) 90Chain 3: 0.095325 seconds (Total) 91Chain 3: 92Chain 4: Iteration: 800 / 1000 [ 80%] (Sampling) 93Chain 4: Iteration: 900 / 1000 [ 90%] (Sampling) 94Chain 4: Iteration: 1000 / 1000 [100%] (Sampling) 95Chain 4: 96Chain 4: Elapsed Time: 0.054509 seconds (Warm-up) 97Chain 4: 0.036572 seconds (Sampling) 98Chain 4: 0.091081 seconds (Total) 99Chain 4: 1m12h1norm %\u0026gt;% precis 1mean sd 5.5% 94.5% n_eff Rhat4 2a 3.00 0.02 2.96 3.04 1334 1 3bF 0.24 0.02 0.20 0.28 1361 1 There are several points to be noted:\n The number of effective samples is far lower than the number simulated (4000) The bounds are quite tight for the values  All told, we can see that the model is quite certain of the values, but given the low number of effective samples it would make sense to run this model longer.\nFurthermore, the model infers that from our data, there is a positive correlation (quite a high one) between femininity and deaths.\nThis is best seen by actually visualizing the results.\n1m12h1norm %\u0026gt;% pairs  We should check the posterior as well.\n1k\u0026lt;-PSIS(m12h1norm,pointwise=TRUE)$k 2plot(hurDat$femStd,hurDat$deaths,xlab=\u0026#34;Standardized Femininity\u0026#34;,ylab=\u0026#34;Deaths\u0026#34;,col=rangi2,pch=hurDat$female, lwd=2,cex=1+normalize(k)) 3## Axis for predictions 4ns\u0026lt;-500 5femininity\u0026lt;-seq(from=min(hurDat$femStd), to=max(hurDat$femStd),length.out = ns) 6## Female 7lambda\u0026lt;-link(m12h1norm,data=data.frame(femStd=femininity)) 8lmu\u0026lt;-apply(lambda,2,mean) 9lci\u0026lt;-apply(lambda,2,PI) 10lines(femininity,lmu,lty=2,lwd=1.5) 11shade(lci,femininity,xpd=TRUE)  We can see that the model prediction does not actually handle the data very well, in that it is evident the model simply cannot account for the high death rate values. We have plotted the 89% interval as well (the default for PI).\nWe can also inspect the expect PSISk values.\n1m12h1norm %\u0026gt;% PSISk %\u0026gt;% summary 1Some Pareto k values are very high (\u0026gt;1). Set pointwise=TRUE to inspect individual points. 2Min. 1st Qu. Median Mean 3rd Qu. Max. 3-0.1800 -0.0500 0.0600 0.1521 0.1500 2.5800 Clearly, there are some points with high leverage.\nFinally we can plot the posterior distribution.\n1posterior\u0026lt;-m12h1norm %\u0026gt;% extract.samples 2fem\u0026lt;-sample(hurDat$femStd,3) 3for(i in 1:10){ 4curve(dgamma(x,exp(posterior$a[i]+posterior$bF[i]*(fem[1]))), from=0,to=100, col=\u0026#34;red\u0026#34;,ylab=\u0026#34;Density\u0026#34;,xlab=\u0026#34;Average deaths\u0026#34;,add=ifelse(i==1,FALSE,TRUE)) 5curve(dgamma(x,exp(posterior$a[i]+posterior$bF[i]*(fem[2]))), from=0,to=100, col=\u0026#34;blue\u0026#34;,add=TRUE) 6curve(dgamma(x,exp(posterior$a[i]+posterior$bF[i]*(fem[3]))), from=0,to=100, col=\u0026#34;black\u0026#34;,add=TRUE)} 7legend(\u0026#34;topright\u0026#34;, 8legend=c(sprintf(\u0026#34;Femininity=%.3f\u0026#34;,fem[1]), sprintf(\u0026#34;Femininity=%.3f\u0026#34;,fem[2]), sprintf(\u0026#34;Femininity=%.3f\u0026#34;,fem[3])), 9col=c(\u0026#34;red\u0026#34;,\u0026#34;blue\u0026#34;,\u0026#34;black\u0026#34;), 10pch=19 11)  As is to be expected, for each value of femininity, we have one family of gamma distributions.\nHOLD 12H2 Counts are nearly always over-dispersed relative to Poisson. So fit a gamma-Poisson (aka negative-binomial) model to predict deaths using feminity. Show that the over-dispersed model no longer shows as precise a positive association between feminity and deaths, with an $89$% interval that overlaps zero. Can you explain why the association diminished in strength?\nSolution Recall that the gamma-Poisson has two parameters, one for the rate, and the other for the dispersion of rates. Larger values of the dispersion imply that the distribution is more similar to a pure Poisson process. For ensuring meaningful comparisons, we will keep the same priors as before. We will need a scale parameter, but we will postulate a simple exponential prior for that.\n1data(Hurricanes) 2hurDat\u0026lt;-Hurricanes %\u0026gt;% as.data.frame 3hurDat\u0026lt;-hurDat %\u0026gt;% dplyr::mutate(femStd=standardize(femininity)) 4datListH\u0026lt;-list(deaths=hurDat$deaths,femStd=hurDat$femStd) 5m12h2norm\u0026lt;-ulam(alist(deaths ~ dgampois(lambda,scale), 6log(lambda) \u0026lt;- a+bF*femStd, 7a ~ dnorm(1,0.5), 8bF ~ dnorm(0.5,2), 9scale ~ dexp(1) 10),data=datListH, chains=4, cores=4,log_lik = TRUE) 1SAMPLING FOR MODEL \u0026#39;5dc94bd836781d34a208695cf643c56c\u0026#39; NOW (CHAIN 1). 2Chain 1: 3Chain 1: Gradient evaluation took 0.00012 seconds 4Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.2 seconds. 5Chain 1: Adjust your expectations accordingly! 6Chain 1: 7Chain 1: 89SAMPLING FOR MODEL \u0026#39;5dc94bd836781d34a208695cf643c56c\u0026#39; NOW (CHAIN 2). 10Chain 2: 11Chain 2: Gradient evaluation took 9.6e-05 seconds 12Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.96 seconds. 13Chain 2: Adjust your expectations accordingly! 14Chain 2: 15Chain 2: 16Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) 17Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup) 1819SAMPLING FOR MODEL \u0026#39;5dc94bd836781d34a208695cf643c56c\u0026#39; NOW (CHAIN 3). 20Chain 3: 21Chain 3: Gradient evaluation took 9.1e-05 seconds 22Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.91 seconds. 23Chain 3: Adjust your expectations accordingly! 24Chain 3: 25Chain 3: 26Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) 2728SAMPLING FOR MODEL \u0026#39;5dc94bd836781d34a208695cf643c56c\u0026#39; NOW (CHAIN 4). 29Chain 4: 30Chain 4: Gradient evaluation took 8.2e-05 seconds 31Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.82 seconds. 32Chain 4: Adjust your expectations accordingly! 33Chain 4: 34Chain 4: 35Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) 36Chain 4: Iteration: 1 / 1000 [ 0%] (Warmup) 37Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) 38Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) 39Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) 40Chain 4: Iteration: 100 / 1000 [ 10%] (Warmup) 41Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) 42Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) 43Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) 44Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) 45Chain 4: Iteration: 200 / 1000 [ 20%] (Warmup) 46Chain 3: Iteration: 300 / 1000 [ 30%] (Warmup) 47Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) 48Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) 49Chain 3: Iteration: 400 / 1000 [ 40%] (Warmup) 50Chain 4: Iteration: 300 / 1000 [ 30%] (Warmup) 51Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) 52Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) 53Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) 54Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) 55Chain 3: Iteration: 500 / 1000 [ 50%] (Warmup) 56Chain 3: Iteration: 501 / 1000 [ 50%] (Sampling) 57Chain 4: Iteration: 400 / 1000 [ 40%] (Warmup) 58Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) 59Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) 60Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) 61Chain 4: Iteration: 500 / 1000 [ 50%] (Warmup) 62Chain 4: Iteration: 501 / 1000 [ 50%] (Sampling) 63Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) 64Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) 65Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) 66Chain 4: Iteration: 600 / 1000 [ 60%] (Sampling) 67Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) 68Chain 4: Iteration: 700 / 1000 [ 70%] (Sampling) 69Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) 70Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) 71Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) 72Chain 4: Iteration: 800 / 1000 [ 80%] (Sampling) 73Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) 74Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) 75Chain 2: 76Chain 2: Elapsed Time: 0.119773 seconds (Warm-up) 77Chain 2: 0.091705 seconds (Sampling) 78Chain 2: 0.211478 seconds (Total) 79Chain 2: 80Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) 81Chain 4: Iteration: 900 / 1000 [ 90%] (Sampling) 82Chain 4: Iteration: 1000 / 1000 [100%] (Sampling) 83Chain 4: 84Chain 4: Elapsed Time: 0.122953 seconds (Warm-up) 85Chain 4: 0.082244 seconds (Sampling) 86Chain 4: 0.205197 seconds (Total) 87Chain 4: 88Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) 89Chain 3: 90Chain 3: Elapsed Time: 0.115457 seconds (Warm-up) 91Chain 3: 0.114422 seconds (Sampling) 92Chain 3: 0.229879 seconds (Total) 93Chain 3: 94Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) 95Chain 1: 96Chain 1: Elapsed Time: 0.110931 seconds (Warm-up) 97Chain 1: 0.129622 seconds (Sampling) 98Chain 1: 0.240553 seconds (Total) 99Chain 1: 1m12h2norm %\u0026gt;% precis 2m12h1norm %\u0026gt;% precis 1mean sd 5.5% 94.5% n_eff Rhat4 2a 2.86 0.14 2.64 3.08 1705 1 3bF 0.21 0.14 -0.01 0.44 1946 1 4scale 0.45 0.06 0.35 0.55 1957 1 56mean sd 5.5% 94.5% n_eff Rhat4 7a 3.00 0.02 2.96 3.04 1334 1 8bF 0.24 0.02 0.20 0.28 1361 1 We note that the effective number of samples in the second model are greater, which implies that this model is less prone to correlations. We can quantify this with the WAIC as well.\n1WAIC(m12h2norm) %\u0026gt;% rbind(WAIC(m12h1norm)) %\u0026gt;% tibble(model=c(\u0026#34;Gamma-Poisson\u0026#34;,\u0026#34;Poisson\u0026#34;)) %\u0026gt;% toOrg 1| WAIC | lppd | penalty | std_err | model | 2|-----------------+-------------------+------------------+------------------+---------------| 3| 710.78471929582 | -351.457619486557 | 3.93474016135277 | 34.6128979470592 | Gamma-Poisson | 4| 4427.5667952452 | -2080.92835360918 | 132.855044013418 | 1009.13483879188 | Poisson | The WAIC values show that the gamma-Poisson model is less likely to over-fit.\nWe would like to see the models together.\n1m12h1norm %\u0026gt;% precis(pars=c(\u0026#34;a\u0026#34;,\u0026#34;bF\u0026#34;)) %\u0026gt;% plot(col=\u0026#34;blue\u0026#34;) 2m12h2norm %\u0026gt;% precis(pars=c(\u0026#34;a\u0026#34;,\u0026#34;bF\u0026#34;)) %\u0026gt;% plot(add=TRUE,col=\u0026#34;red\u0026#34;)  We can see that there is little to no difference in the means, though the intervals seem wider than before. This is more clear in the coeftab plot.\n1plot(coeftab(m12h1norm,m12h2norm))  An important consequence of this is that the model is no longer completely sure that there is any effect of femininity on the death count, as can be seen from the wider uncertainty interval, which includes 0.\nWe can also visualize the model with pairs.\n1pairs(m12h2norm)  1k\u0026lt;-PSIS(m12h2norm,pointwise=TRUE)$k 2plot(hurDat$femStd,hurDat$deaths,xlab=\u0026#34;Standardized Femininity\u0026#34;,ylab=\u0026#34;Deaths\u0026#34;,col=rangi2,pch=hurDat$female, lwd=2,cex=1+normalize(k)) 3## Axis for predictions 4ns\u0026lt;-500 5femininity\u0026lt;-seq(from=min(hurDat$femStd),to=max(hurDat$femStd),length.out = ns) 6## Gamma Poisson 7lambda\u0026lt;-link(m12h2norm,data=data.frame(femStd=femininity)) 8lmu\u0026lt;-apply(lambda,2,mean) 9lci\u0026lt;-apply(lambda,2,PI) 10shade(lci,femininity,xpd=TRUE,col=\u0026#34;red\u0026#34;) 11lines(femininity,lmu,lty=2,lwd=1.5,col=\u0026#34;white\u0026#34;) 12## Poisson 13lambda\u0026lt;-link(m12h1norm,data=data.frame(femStd=femininity)) 14lmu\u0026lt;-apply(lambda,2,mean) 15lci\u0026lt;-apply(lambda,2,PI) 16shade(lci,femininity,xpd=TRUE,col=\u0026#34;blue\u0026#34;) 17lines(femininity,lmu,lty=2,lwd=1.5,col=\u0026#34;white\u0026#34;)  Clearly, the uncertainty of the newer model is much greater, even though the predictions do not differ much. Unfortunately, both models fail to account for storms with high death counts.\nWe would also like to plot the predicted distributions.\n1posterior\u0026lt;-m12h2norm %\u0026gt;% extract.samples 2fem\u0026lt;-sample(hurDat$femStd,2) 3for(i in 1:100){ 4curve(dgamma2(x,exp(posterior$a[i]+posterior$bF[i]*(fem[1])),posterior$scale[i]), from=0,to=100,col=\u0026#34;red\u0026#34;,ylab=\u0026#34;Density\u0026#34;,xlab=\u0026#34;Average deaths\u0026#34;,add=ifelse(i==1,FALSE,TRUE)) 5curve(dgamma2(x,exp(posterior$a[i]+posterior$bF[i]*(fem[2])),posterior$scale[i]), from=0,to=100,col=\u0026#34;blue\u0026#34;,add=TRUE) 6} 7legend(\u0026#34;topright\u0026#34;, 8legend=c(sprintf(\u0026#34;Femininity=%.3f\u0026#34;,fem[1]), sprintf(\u0026#34;Femininity=%.3f\u0026#34;,fem[2])), 9col=c(\u0026#34;red\u0026#34;,\u0026#34;blue\u0026#34;), 10pch=19 11)  This clearly has more spread than the previous predictions. By definition, the dispersion term tends to spread the distribution out, with higher values of the dispersion corresponding to a \u0026ldquo;true\u0026rdquo; Poisson distribution.\nA: Colophon To ensure that this document is fully reproducible at a later date, we will record the session info.\n1devtools::session_info() 1─ Session info ─────────────────────────────────────────────────────────────── 2setting value 3version R version 4.0.0 (2020-04-24) 4os Arch Linux 5system x86_64, linux-gnu 6ui X11 7language (EN) 8collate en_US.UTF-8 9ctype en_US.UTF-8 10tz Iceland 11date 2020-06-21 1213─ Packages ─────────────────────────────────────────────────────────────────── 14package * version date lib source 15arrayhelpers 1.1-0 2020-02-04 [167] CRAN (R 4.0.0) 16assertthat 0.2.1 2019-03-21 [34] CRAN (R 4.0.0) 17backports 1.1.6 2020-04-05 [68] CRAN (R 4.0.0) 18boot 1.3-24 2019-12-20 [5] CRAN (R 4.0.0) 19broom 0.5.6 2020-04-20 [67] CRAN (R 4.0.0) 20callr 3.4.3 2020-03-28 [87] CRAN (R 4.0.0) 21cellranger 1.1.0 2016-07-27 [55] CRAN (R 4.0.0) 22cli 2.0.2 2020-02-28 [33] CRAN (R 4.0.0) 23coda 0.19-3 2019-07-05 [169] CRAN (R 4.0.0) 24colorspace 1.4-1 2019-03-18 [97] CRAN (R 4.0.0) 25crayon 1.3.4 2017-09-16 [35] CRAN (R 4.0.0) 26curl 4.3 2019-12-02 [26] CRAN (R 4.0.0) 27dagitty * 0.2-2 2016-08-26 [244] CRAN (R 4.0.0) 28data.table * 1.12.8 2019-12-09 [27] CRAN (R 4.0.0) 29DBI 1.1.0 2019-12-15 [77] CRAN (R 4.0.0) 30dbplyr 1.4.3 2020-04-19 [76] CRAN (R 4.0.0) 31desc 1.2.0 2018-05-01 [84] CRAN (R 4.0.0) 32devtools * 2.3.0 2020-04-10 [219] CRAN (R 4.0.0) 33digest 0.6.25 2020-02-23 [42] CRAN (R 4.0.0) 34dplyr * 0.8.5 2020-03-07 [69] CRAN (R 4.0.0) 35ellipsis 0.3.0 2019-09-20 [30] CRAN (R 4.0.0) 36evaluate 0.14 2019-05-28 [82] CRAN (R 4.0.0) 37fansi 0.4.1 2020-01-08 [36] CRAN (R 4.0.0) 38forcats * 0.5.0 2020-03-01 [29] CRAN (R 4.0.0) 39fs 1.4.1 2020-04-04 [109] CRAN (R 4.0.0) 40generics 0.0.2 2018-11-29 [71] CRAN (R 4.0.0) 41ggplot2 * 3.3.0 2020-03-05 [78] CRAN (R 4.0.0) 42glue * 1.4.0 2020-04-03 [37] CRAN (R 4.0.0) 43gridExtra 2.3 2017-09-09 [123] CRAN (R 4.0.0) 44gtable 0.3.0 2019-03-25 [79] CRAN (R 4.0.0) 45haven 2.2.0 2019-11-08 [28] CRAN (R 4.0.0) 46hms 0.5.3 2020-01-08 [44] CRAN (R 4.0.0) 47htmltools 0.4.0 2019-10-04 [112] CRAN (R 4.0.0) 48httr 1.4.1 2019-08-05 [100] CRAN (R 4.0.0) 49inline 0.3.15 2018-05-18 [162] CRAN (R 4.0.0) 50jsonlite 1.6.1 2020-02-02 [101] CRAN (R 4.0.0) 51kableExtra * 1.1.0 2019-03-16 [212] CRAN (R 4.0.0) 52knitr 1.28 2020-02-06 [113] CRAN (R 4.0.0) 53latex2exp * 0.4.0 2015-11-30 [211] CRAN (R 4.0.0) 54lattice 0.20-41 2020-04-02 [6] CRAN (R 4.0.0) 55lifecycle 0.2.0 2020-03-06 [38] CRAN (R 4.0.0) 56loo 2.2.0 2019-12-19 [163] CRAN (R 4.0.0) 57lubridate 1.7.8 2020-04-06 [106] CRAN (R 4.0.0) 58magrittr 1.5 2014-11-22 [21] CRAN (R 4.0.0) 59MASS 7.3-51.5 2019-12-20 [7] CRAN (R 4.0.0) 60matrixStats 0.56.0 2020-03-13 [164] CRAN (R 4.0.0) 61memoise 1.1.0 2017-04-21 [229] CRAN (R 4.0.0) 62modelr 0.1.6 2020-02-22 [107] CRAN (R 4.0.0) 63munsell 0.5.0 2018-06-12 [96] CRAN (R 4.0.0) 64mvtnorm 1.1-0 2020-02-24 [243] CRAN (R 4.0.0) 65nlme 3.1-147 2020-04-13 [11] CRAN (R 4.0.0) 66orgutils * 0.4-1 2017-03-21 [209] CRAN (R 4.0.0) 67pillar 1.4.3 2019-12-20 [39] CRAN (R 4.0.0) 68pkgbuild 1.0.6 2019-10-09 [86] CRAN (R 4.0.0) 69pkgconfig 2.0.3 2019-09-22 [43] CRAN (R 4.0.0) 70pkgload 1.0.2 2018-10-29 [83] CRAN (R 4.0.0) 71plyr 1.8.6 2020-03-03 [73] CRAN (R 4.0.0) 72prettyunits 1.1.1 2020-01-24 [58] CRAN (R 4.0.0) 73printr * 0.1 2017-05-19 [214] CRAN (R 4.0.0) 74processx 3.4.2 2020-02-09 [88] CRAN (R 4.0.0) 75ps 1.3.2 2020-02-13 [89] CRAN (R 4.0.0) 76purrr * 0.3.4 2020-04-17 [50] CRAN (R 4.0.0) 77R6 2.4.1 2019-11-12 [48] CRAN (R 4.0.0) 78Rcpp 1.0.4.6 2020-04-09 [10] CRAN (R 4.0.0) 79readr * 1.3.1 2018-12-21 [45] CRAN (R 4.0.0) 80readxl 1.3.1 2019-03-13 [54] CRAN (R 4.0.0) 81remotes 2.1.1 2020-02-15 [233] CRAN (R 4.0.0) 82reprex 0.3.0 2019-05-16 [108] CRAN (R 4.0.0) 83rethinking * 2.01 2020-06-06 [242] local 84rlang 0.4.5 2020-03-01 [31] CRAN (R 4.0.0) 85rmarkdown 2.1 2020-01-20 [110] CRAN (R 4.0.0) 86rprojroot 1.3-2 2018-01-03 [85] CRAN (R 4.0.0) 87rstan * 2.19.3 2020-02-11 [161] CRAN (R 4.0.0) 88rstudioapi 0.11 2020-02-07 [91] CRAN (R 4.0.0) 89rvest 0.3.5 2019-11-08 [120] CRAN (R 4.0.0) 90scales 1.1.0 2019-11-18 [93] CRAN (R 4.0.0) 91sessioninfo 1.1.1 2018-11-05 [231] CRAN (R 4.0.0) 92shape 1.4.4 2018-02-07 [193] CRAN (R 4.0.0) 93StanHeaders * 2.19.2 2020-02-11 [165] CRAN (R 4.0.0) 94stringi 1.4.6 2020-02-17 [52] CRAN (R 4.0.0) 95stringr * 1.4.0 2019-02-10 [74] CRAN (R 4.0.0) 96svUnit 1.0.3 2020-04-20 [168] CRAN (R 4.0.0) 97testthat 2.3.2 2020-03-02 [81] CRAN (R 4.0.0) 98textutils 0.2-0 2020-01-07 [210] CRAN (R 4.0.0) 99tibble * 3.0.1 2020-04-20 [32] CRAN (R 4.0.0) 100tidybayes * 2.0.3 2020-04-04 [166] CRAN (R 4.0.0) 101tidybayes.rethinking * 2.0.3.9000 2020-06-07 [246] local 102tidyr * 1.0.2 2020-01-24 [75] CRAN (R 4.0.0) 103tidyselect 1.0.0 2020-01-27 [49] CRAN (R 4.0.0) 104tidyverse * 1.3.0 2019-11-21 [66] CRAN (R 4.0.0) 105usethis * 1.6.0 2020-04-09 [238] CRAN (R 4.0.0) 106V8 3.0.2 2020-03-14 [245] CRAN (R 4.0.0) 107vctrs 0.2.4 2020-03-10 [41] CRAN (R 4.0.0) 108viridisLite 0.3.0 2018-02-01 [99] CRAN (R 4.0.0) 109webshot 0.5.2 2019-11-22 [213] CRAN (R 4.0.0) 110withr 2.2.0 2020-04-20 [90] CRAN (R 4.0.0) 111xfun 0.13 2020-04-13 [116] CRAN (R 4.0.0) 112xml2 1.3.2 2020-04-23 [122] CRAN (R 4.0.0) 113114[1] /nix/store/xzd8h53xkyvfm3kvj5ab6znp685wi04w-r-car-3.0-7/library 115[2] /nix/store/mhr8zw9bmxarc3n821b83i0gz2j9zlrq-r-abind-1.4-5/library 116[3] /nix/store/hp86nhr0787vib3l8mkw0gf9nxwb45im-r-carData-3.0-3/library 117[4] /nix/store/vhw7s2h5ds6sp110z2yvilchv8j9jch5-r-lme4-1.1-23/library 118[5] /nix/store/987n8g0zy9sjvfvnsck1bkkcknw05yvb-r-boot-1.3-24/library 119[6] /nix/store/jxxxxyz4c1k5g3drd35gsrbjdg028d11-r-lattice-0.20-41/library 120[7] /nix/store/q9zfm5h53m8rd08xcsdcwaag31k4z1pf-r-MASS-7.3-51.5/library 121[8] /nix/store/kjkm50sr144yvrhl5axfgykbiy13pbmg-r-Matrix-1.2-18/library 122[9] /nix/store/8786z5lgy8h3akfjgj3yq5yq4s17rhjy-r-minqa-1.2.4/library 123[10] /nix/store/93wv3j0z1nzqp6fjsm9v7v8bf8d1xkm2-r-Rcpp-1.0.4.6/library 124[11] /nix/store/akfw6zsmawmz8lmjkww0rnqrazm4mqp0-r-nlme-3.1-147/library 125[12] /nix/store/rxs0d9bbn8qhw7wmkfb21yk5abp6lpq1-r-nloptr-1.2.2.1/library 126[13] /nix/store/8n0jfiqn4275i58qgld0dv8zdaihdzrk-r-RcppEigen-0.3.3.7.0/library 127[14] /nix/store/8vxrma33rhc96260zsi1jiw7dy3v2mm4-r-statmod-1.4.34/library 128[15] /nix/store/2y46pb5x9lh8m0hdmzajnx7sc1bk9ihl-r-maptools-0.9-9/library 129[16] /nix/store/iwf9nxx1v883wlv0p88q947hpz5lhfh7-r-foreign-0.8-78/library 130[17] /nix/store/rl9sjqply6rjbnz5k792ghm62ybv76px-r-sp-1.4-1/library 131[18] /nix/store/ws4bkzyv2vj5pyn1hgwyy6nlp48arz0n-r-mgcv-1.8-31/library 132[19] /nix/store/307dzxrmnqk4p86560a02r64x1fhhmxb-r-nnet-7.3-13/library 133[20] /nix/store/g2zpzkdb9hzkza1wpcbrk58119v1wyaf-r-pbkrtest-0.4-8.6/library 134[21] /nix/store/p0l503fr8960vld70w6ilmknxs5qwq77-r-magrittr-1.5/library 135[22] /nix/store/rmjpcaw3i446kwnjgcxcaid0yac36cj2-r-quantreg-5.55/library 136[23] /nix/store/10mzmnvc5jjgk2xzasia522pk60a30qz-r-MatrixModels-0.4-1/library 137[24] /nix/store/6qwdzvmnnmhjwdnvg2zmvv6wafd1vf91-r-SparseM-1.78/library 138[25] /nix/store/aa9c39a3yiqkh1h7pbngjlbr7czvc7yi-r-rio-0.5.16/library 139[26] /nix/store/2fx4vqlybgwp5rhhy6pssqx7h1a927fn-r-curl-4.3/library 140[27] /nix/store/k4m3fn1kqvvvn8y33kd57gq49hr3ar8y-r-data.table-1.12.8/library 141[28] /nix/store/651hfjylqzmsf565wyx474vyjny771gy-r-haven-2.2.0/library 142[29] /nix/store/a3rnz28irmqvmj8axj5x5j1am2c3gzs4-r-forcats-0.5.0/library 143[30] /nix/store/j8v4gzib137q2cml31hvvfkrc0f60pp5-r-ellipsis-0.3.0/library 144[31] /nix/store/xaswqlnamf4k8vwx0x3wav3l0x60sag0-r-rlang-0.4.5/library 145[32] /nix/store/dqm3xpix2jwhhhr67s6fgrwbw7hizap7-r-tibble-3.0.1/library 146[33] /nix/store/v7xfsq6d97wpn6m0hjrac78w5xawbr8a-r-cli-2.0.2/library 147[34] /nix/store/fikjasr98klhk9cf44x4lhi57vh3pmkg-r-assertthat-0.2.1/library 148[35] /nix/store/3fya6cd38vsqdj0gjb7bcsy00sirlyw1-r-crayon-1.3.4/library 149[36] /nix/store/payqi9bwh216rwhaq07jgc26l4fv1zsb-r-fansi-0.4.1/library 150[37] /nix/store/h6a61ghws7yrdxlg412xl1im37z5r28i-r-glue-1.4.0/library 151[38] /nix/store/y8mjbia1wbnq26dkigr0p3xxwrbzsc2r-r-lifecycle-0.2.0/library 152[39] /nix/store/kwaghh12cnifgvcbvlv2anx0hd5f4ild-r-pillar-1.4.3/library 153[40] /nix/store/k1phn8j10nni7gzvcgp0vc25dby6bb77-r-utf8-1.1.4/library 154[41] /nix/store/k3b77y8v7zsshpp1ccs8jwk2i2g4rm9a-r-vctrs-0.2.4/library 155[42] /nix/store/iibjmbh7vj0d0bfafz98yn29ymg43gkw-r-digest-0.6.25/library 156[43] /nix/store/aqsj4k3pgm80qk4jjg7sh3ac28n6alv0-r-pkgconfig-2.0.3/library 157[44] /nix/store/i7c5v8s4hd9rlqah3bbvy06yywjqwdgk-r-hms-0.5.3/library 158[45] /nix/store/2fyrk58cmcbrxid66rbwjli7y114lvrm-r-readr-1.3.1/library 159[46] /nix/store/163xq2g5nblqgh7qhvzb6mvgg6qdrirj-r-BH-1.72.0-3/library 160[47] /nix/store/dr27b6k49prwgrjs0v30b6mf5lxa36pk-r-clipr-0.7.0/library 161[48] /nix/store/bghvqg9mcaj2jkbwpy0di6c563v24acz-r-R6-2.4.1/library 162[49] /nix/store/nq8jdq7nlg9xns4xpgyj6sqv8p4ny1wz-r-tidyselect-1.0.0/library 163[50] /nix/store/zlwhf75qld7vmwx3d4bdws057ld4mqbp-r-purrr-0.3.4/library 164[51] /nix/store/0gbmmnbpqlr69l573ymkcx8154fvlaca-r-openxlsx-4.1.4/library 165[52] /nix/store/1m1q4rmwx56dvx9rdzfsfq0jpw3hw0yx-r-stringi-1.4.6/library 166[53] /nix/store/mhy5vnvbsl4q7dcinwx3vqlyywxphbfd-r-zip-2.0.4/library 167[54] /nix/store/88sp7f7q577i6l5jjanqiv5ak6nv5357-r-readxl-1.3.1/library 168[55] /nix/store/6q9zwivzalhmzdracc8ma932wirq8rl5-r-cellranger-1.1.0/library 169[56] /nix/store/jh2n6k2ancdzqych5ix8n4rq9w514qq9-r-rematch-1.0.1/library 170[57] /nix/store/22xjqikqd6q556absb5224sbx6q0kp0c-r-progress-1.2.2/library 171[58] /nix/store/9vp32wa1qvv6lkq6p70qlli5whrxzfbi-r-prettyunits-1.1.1/library 172[59] /nix/store/r9rhqb6fsk75shihmb7nagqb51pqwp0y-r-class-7.3-16/library 173[60] /nix/store/z1kad071y43wij1ml9lpghh7jbimmcli-r-cluster-2.1.0/library 174[61] /nix/store/i8wr965caf6j1rxs2dsvpzhlh4hyyb4y-r-codetools-0.2-16/library 175[62] /nix/store/8iglq3zr68a39hzswvzxqi2ffhpw9p51-r-KernSmooth-2.23-16/library 176[63] /nix/store/n3k50zv40i40drpdf8npbmy2y08gkr6w-r-rpart-4.1-15/library 177[64] /nix/store/b4r6adzcvpm8ivflsmis7ja7q4r5hkjy-r-spatial-7.3-11/library 178[65] /nix/store/zqg6hmrncl8ax3vn7z5drf4csddwnhcx-r-survival-3.1-12/library 179[66] /nix/store/4anrihkx11h8mzb269xdyi84yp5v7grl-r-tidyverse-1.3.0/library 180[67] /nix/store/945haq0w8nfm9ib7r0nfngn5lk2i15ix-r-broom-0.5.6/library 181[68] /nix/store/52viqxzrmxl7dk0zji293g5b0b9grwh8-r-backports-1.1.6/library 182[69] /nix/store/zp1k42sw2glqy51w4hnzsjs8rgi8xzx2-r-dplyr-0.8.5/library 183[70] /nix/store/mkjd98mnshch2pwnj6h31czclqdaph3f-r-plogr-0.2.0/library 184[71] /nix/store/kflrzax6y5pwfqwzgfvqz433a3q3hnhn-r-generics-0.0.2/library 185[72] /nix/store/xi1n5h5w17c33y6ax3dfhg2hgzjl9bxz-r-reshape2-1.4.4/library 186[73] /nix/store/vn63z92zkpbaxmmhzpb6mq2fvg0xa26h-r-plyr-1.8.6/library 187[74] /nix/store/wmpyxss67bj44rin7hlnr9qabx66p5hj-r-stringr-1.4.0/library 188[75] /nix/store/330qbgbvllwz3h0i2qidrlk50y0mbgph-r-tidyr-1.0.2/library 189[76] /nix/store/cx3x4pqb65l1mhss65780hbzv9jdrzl6-r-dbplyr-1.4.3/library 190[77] /nix/store/gsj49bp3hpw9jlli3894c49amddryqsq-r-DBI-1.1.0/library 191[78] /nix/store/kvymhwp4gac0343c2yi1qvdpavx4gdn2-r-ggplot2-3.3.0/library 192[79] /nix/store/knv51jvpairvibrkkq48b6f1l2pa1cv8-r-gtable-0.3.0/library 193[80] /nix/store/158dx0ddv20ikwag2860nlg9p3hbh1zc-r-isoband-0.2.1/library 194[81] /nix/store/fprs9rp1jlhxzj7fp6l79akyf8k3p7zd-r-testthat-2.3.2/library 195[82] /nix/store/0pmlnkyn0ir3k9bvxihi1r06jyl64w3i-r-evaluate-0.14/library 196[83] /nix/store/7210bjjqn5cjndxn5isnd4vip00xhkhy-r-pkgload-1.0.2/library 197[84] /nix/store/9a12ybd74b7dns40gcfs061wv7913qjy-r-desc-1.2.0/library 198[85] /nix/store/na9pb1apa787zp7vvyz1kzym0ywjwbj0-r-rprojroot-1.3-2/library 199[86] /nix/store/pa2n7bh61qxyarn5i2ynd62k6knb1np1-r-pkgbuild-1.0.6/library 200[87] /nix/store/1hxm1m7h4272zxk9bpsaq46mvnl0dbss-r-callr-3.4.3/library 201[88] /nix/store/bigvyk6ipglbiil93zkf442nv4y3xa1x-r-processx-3.4.2/library 202[89] /nix/store/370lr0wf7qlq0m72xnmasg2iahkp2n52-r-ps-1.3.2/library 203[90] /nix/store/rr72q61d8mkd42zc5fhcd2rqjghvc141-r-withr-2.2.0/library 204[91] /nix/store/9gw77p7fmz89fa8wi1d9rvril6hd4sxy-r-rstudioapi-0.11/library 205[92] /nix/store/9x4v4pbrgmykbz2801h77yz2l0nmm5nb-r-praise-1.0.0/library 206[93] /nix/store/pf8ssb0dliw5bzsncl227agc8przb7ic-r-scales-1.1.0/library 207[94] /nix/store/095z4wgjrxn63ixvyzrj1fm1rdv6ci95-r-farver-2.0.3/library 208[95] /nix/store/5aczj4s7i9prf5i32ik5ac5baqvjwdb1-r-labeling-0.3/library 209[96] /nix/store/wch26phipzz9gxd4vbr4fynh7v28349j-r-munsell-0.5.0/library 210[97] /nix/store/3w8fh756mszhsjx5fwgwydcpn8vkwady-r-colorspace-1.4-1/library 211[98] /nix/store/8cmaj81v2vm4f8p59ylbnsby8adkbmhd-r-RColorBrewer-1.1-2/library 212[99] /nix/store/h4x4ygax7gpz6f0c2v0xacr62080qwb8-r-viridisLite-0.3.0/library 213[100] /nix/store/qhx0i2nn5syb6vygdn8fdxgl7k56yj81-r-httr-1.4.1/library 214[101] /nix/store/lxnb4aniv02i4jhdvz02aaql1kznbpxb-r-jsonlite-1.6.1/library 215[102] /nix/store/13dcry4gad3vfwqzqb0ii4n06ybrxybr-r-mime-0.9/library 216[103] /nix/store/2can5l8gscc92a3bqlak8hfcg96v5hvf-r-openssl-1.4.1/library 217[104] /nix/store/piwsgxdz5w2ak8c6fcq0lc978qbxwdp1-r-askpass-1.1/library 218[105] /nix/store/3sj5h6dwa1l27d2hvdchclygk0pgffsr-r-sys-3.3/library 219[106] /nix/store/2z0p88g0c03gigl2ip60dlsfkdv1k30h-r-lubridate-1.7.8/library 220[107] /nix/store/1pkmj8nqjg2iinrkg2w0zkwq0ldc01za-r-modelr-0.1.6/library 221[108] /nix/store/bswkzvn8lczwbyw3y7n0p0qp2q472s0g-r-reprex-0.3.0/library 222[109] /nix/store/yid22gad8z49q52d225vfba2m4cgj2lx-r-fs-1.4.1/library 223[110] /nix/store/d185qiqaplm5br9fk1pf29y0srlabw83-r-rmarkdown-2.1/library 224[111] /nix/store/iszqviydsdj31c3ww095ndqy1ld3cibs-r-base64enc-0.1-3/library 225[112] /nix/store/i89wfw4cr0fz3wbd7cg44fk4dwz8b6h1-r-htmltools-0.4.0/library 226[113] /nix/store/qrl28laqwmhpwg3dpcf4nca8alv0px0g-r-knitr-1.28/library 227[114] /nix/store/jffaxc4a3bbf2g6ip0gdcya73dmg53mb-r-highr-0.8/library 228[115] /nix/store/717srph13qpnbzmgsvhx25q8pl51ivpj-r-markdown-1.1/library 229[116] /nix/store/mxqmyq3ybdfyc6p0anhfy2kfw0iz5k4n-r-xfun-0.13/library 230[117] /nix/store/b8g6hadva0359l6j1aq4dbvxlqf1acxc-r-yaml-2.2.1/library 231[118] /nix/store/rrl05vpv7cw58zi0k9ykm7m4rjb9gjv3-r-tinytex-0.22/library 232[119] /nix/store/2ziq8nzah6xy3dgmxgim9h2wszz1f89f-r-whisker-0.4/library 233[120] /nix/store/540wbw4p1g2qmnmbfk0rhvwvfnf657sj-r-rvest-0.3.5/library 234[121] /nix/store/n3prn77gd9sf3z4whqp86kghr55bf5w8-r-selectr-0.4-2/library 235[122] /nix/store/gv28yjk5isnglq087y7767xw64qa40cw-r-xml2-1.3.2/library 236[123] /nix/store/693czdcvkp6glyir0mi8cqvdc643whvc-r-gridExtra-2.3/library 237[124] /nix/store/3sykinp7lyy70dgzr0fxjb195nw864dv-r-future-1.17.0/library 238[125] /nix/store/bqi2l53jfxncks6diy0hr34bw8f86rvk-r-globals-0.12.5/library 239[126] /nix/store/dydyl209klklzh69w9q89f2dym9xycnp-r-listenv-0.8.0/library 240[127] /nix/store/lni0bi36r4swldkx7g4hql7gfz9b121b-r-gganimate-1.0.5/library 241[128] /nix/store/hh92jxs79kx7vxrxr6j6vin1icscl4k7-r-tweenr-1.0.1/library 242[129] /nix/store/0npx3srjnqgh7bib80xscjqvfyzjvimq-r-GGally-1.5.0/library 243[130] /nix/store/x5nzxklmacj6l162g7kg6ln9p25r3f17-r-reshape-0.8.8/library 244[131] /nix/store/q29z7ckdyhfmg1zlzrrg1nrm36ax756j-r-ggfortify-0.4.9/library 245[132] /nix/store/1rvm1w9iv2c5n22p4drbjq8lr9wa2q2r-r-cowplot-1.0.0/library 246[133] /nix/store/rp8jhnasaw1vbv5ny5zx0mw30zgcp796-r-ggrepel-0.8.2/library 247[134] /nix/store/wb7y931mm8nsj7w9xin83bvbaq8wvi4d-r-corrplot-0.84/library 248[135] /nix/store/gdzcqivfvgdrsz247v5kmnnw1v6p9c1p-r-rpart.plot-3.0.8/library 249[136] /nix/store/6yqg37108r0v22476cm2kv0536wyilki-r-caret-6.0-86/library 250[137] /nix/store/6fjdgcwgisiqz451sg5fszxnn9z8vxg6-r-foreach-1.5.0/library 251[138] /nix/store/c3ph5i341gk7jdinrkkqf6y631xli424-r-iterators-1.0.12/library 252[139] /nix/store/sjm1rxshlpakpxbrynfhsjnnp1sjvc3r-r-ModelMetrics-1.2.2.2/library 253[140] /nix/store/vgk4m131d057xglmrrb9rijhzdr2qhhp-r-pROC-1.16.2/library 254[141] /nix/store/bv1kvy1wc2jx3v55rzn3cg2qjbv7r8zp-r-recipes-0.1.10/library 255[142] /nix/store/001h42q4za01gli7avjxhq7shpv73n9k-r-gower-0.2.1/library 256[143] /nix/store/ssffpl6ydffqyn9phscnccxnj71chnzg-r-ipred-0.9-9/library 257[144] /nix/store/baliqip8m6p0ylqhqcgqak29d8ghral1-r-prodlim-2019.11.13/library 258[145] /nix/store/j4n2wsv98asw83qiffg6a74dymk8r2hl-r-lava-1.6.7/library 259[146] /nix/store/hf5wq5kpsf6p9slglq5iav09s4by0y5i-r-numDeriv-2016.8-1.1/library 260[147] /nix/store/s58hm38078mx4gyqffvv09zn575xn648-r-SQUAREM-2020.2/library 261[148] /nix/store/g63ydzd53586pvr9kdgk8kf5szq5f2bc-r-timeDate-3043.102/library 262[149] /nix/store/0jkarmlf1kjv4g8a3svkc7jfarpp77ny-r-mlr3-0.2.0/library 263[150] /nix/store/g1m0n1w7by213v773iyn7vnxr25pkf56-r-checkmate-2.0.0/library 264[151] /nix/store/fc2ah8cz2sj6j2jk7zldvjmsjn1yakpn-r-lgr-0.3.4/library 265[152] /nix/store/0i2hs088j1s0a6i61124my6vnzq8l27m-r-mlbench-2.1-1/library 266[153] /nix/store/vzcs6k21pqrli3ispqnvj5qwkv14srf5-r-mlr3measures-0.1.3/library 267[154] /nix/store/h2yqqaia46bk3b1d1a7bq35zf09p1b1a-r-mlr3misc-0.2.0/library 268[155] /nix/store/c9mrkc928cmsvvnib50l0jb8lsz59nyk-r-paradox-0.2.0/library 269[156] /nix/store/vqpbdipi4p4advl2vxrn765mmgcrabvk-r-uuid-0.1-4/library 270[157] /nix/store/xpclynxnfq4h9218gk4y62nmgyyga6zl-r-mlr3viz-0.1.1/library 271[158] /nix/store/7w6pld5vir3p9bybay67kq0qwl0gnx17-r-mlr3learners-0.2.0/library 272[159] /nix/store/ca50rp6ha5s51qmhb1gjlj62r19xfzxs-r-mlr3pipelines-0.1.3/library 273[160] /nix/store/9hg0xap4pir64mhbgq8r8cgrfjn8aiz5-r-mlr3filters-0.2.0/library 274[161] /nix/store/jgqcmfix0xxm3y90m8wy3xkgmqf2b996-r-rstan-2.19.3/library 275[162] /nix/store/mvv1gjyrrpvf47fn7a8x722wdwrf5azk-r-inline-0.3.15/library 276[163] /nix/store/zmkw51x4w4d1v1awcws0xihj4hnxfr09-r-loo-2.2.0/library 277[164] /nix/store/30xxalfwzxl05bbfvj5sy8k3ysys6z5y-r-matrixStats-0.56.0/library 278[165] /nix/store/fhkww2l0izx87bjnf0pl9ydl1wprp0xv-r-StanHeaders-2.19.2/library 279[166] /nix/store/aflck5pzxa8ym5q1dxchx5hisfmfghkr-r-tidybayes-2.0.3/library 280[167] /nix/store/jhlbhiv4fg0wsbxwjz8igc4hcg79vw94-r-arrayhelpers-1.1-0/library 281[168] /nix/store/fv089zrnvicnavbi08hnzqpi9g1z4inj-r-svUnit-1.0.3/library 282[169] /nix/store/xci2rgjizx1fyb33818jx5s1bgn8v8k6-r-coda-0.19-3/library 283[170] /nix/store/dch9asd38yldz0sdn8nsgk9ivjrkbhva-r-HDInterval-0.2.0/library 284[171] /nix/store/rs8dri2m5cqdmpiw187rvl4yhjn0jg2v-r-e1071-1.7-3/library 285[172] /nix/store/qs1zyh3sbvccgnqjzas3br6pak399zgc-r-pvclust-2.2-0/library 286[173] /nix/store/sh3zxvdazp7rkjn1iczrag1h2358ifm1-r-forecast-8.12/library 287[174] /nix/store/h67kaxqr2ppdpyj77wg5hm684jypznji-r-fracdiff-1.5-1/library 288[175] /nix/store/fh0z465ligbpqyam5l1fwiijc7334kbk-r-lmtest-0.9-37/library 289[176] /nix/store/0lnsbwfg0axr80h137q52pa50cllbjpf-r-zoo-1.8-7/library 290[177] /nix/store/p7k4s3ivf83dp2kcxr1cr0wlc1rfk6jx-r-RcppArmadillo-0.9.860.2.0/library 291[178] /nix/store/ssnxv5x6zid2w11v8k5yvnyxis6n1qfk-r-tseries-0.10-47/library 292[179] /nix/store/zrbskjwaz0bzz4v76j044d771m24g6h8-r-quadprog-1.5-8/library 293[180] /nix/store/2x3w5sjalrfm6hf1dxd951j8y94nh765-r-quantmod-0.4.17/library 294[181] /nix/store/7g55xshf49s9379ijm1zi1qnh1vbsifq-r-TTR-0.23-6/library 295[182] /nix/store/6ilyzph46q6ijyanq4p7f0ccyni0d7j0-r-xts-0.12-0/library 296[183] /nix/store/17xhqghcnqha7pwbf98dxsq1729slqd5-r-urca-1.3-0/library 297[184] /nix/store/722lyn0k8y27pj1alik56r4vpjnncd9z-r-swdft-1.0.0/library 298[185] /nix/store/36n0zgy10fsqcq76n0qmdwjxrwh7pn9n-r-xgboost-1.0.0.2/library 299[186] /nix/store/ac0ar7lf75qx84xsdjv6j02rkdgnhybz-r-ranger-0.12.1/library 300[187] /nix/store/i1ighkq42x10dirqmzgbx2mhbnz1ynkb-r-DALEX-1.2.0/library 301[188] /nix/store/28fqnhsfng1bkphl0wvr7lg5y3p6va46-r-iBreakDown-1.2.0/library 302[189] /nix/store/dpym77x9qc2ksr4mwjm3pb9ar1kvwhdl-r-ingredients-1.2.0/library 303[190] /nix/store/sp4d281w6dpr31as0xdjqizdx8hhb01q-r-DALEXtra-0.2.1/library 304[191] /nix/store/ckhp9kpmjcs0wxb113pxn25c2wip2d0n-r-ggdendro-0.1-20/library 305[192] /nix/store/f3k7dxj1dsmqri2gn0svq4c9fvvl9g7q-r-glmnet-3.0-2/library 306[193] /nix/store/l6ccj6mwkqybjvh6dr8qzalygp0i7jyb-r-shape-1.4.4/library 307[194] /nix/store/418mqfwlafh6984xld8lzhl7rv29qw68-r-reticulate-1.15/library 308[195] /nix/store/qwh982mgxd2mzrgbjk14irqbasywa1jk-r-rappdirs-0.3.1/library 309[196] /nix/store/6sxs76abll23c6372h6nf101wi8fcr4c-r-FactoMineR-2.3/library 310[197] /nix/store/39d2va10ydgyzddwr07xwdx11fwk191i-r-ellipse-0.4.1/library 311[198] /nix/store/4lxym5nxdn8hb7l8a566n5vg9paqcfi2-r-flashClust-1.01-2/library 312[199] /nix/store/wp161zbjjs41fq4kn4k3m244c7b8l2l2-r-leaps-3.1/library 313[200] /nix/store/irghsaplrpb3hg3y7j831bbklf2cqs6d-r-scatterplot3d-0.3-41/library 314[201] /nix/store/09ahkf50g1q9isxanbdykqgcdrp8mxl1-r-factoextra-1.0.7/library 315[202] /nix/store/zi9bq7amsgc6w2x7fvd62g9qxz69vjfm-r-dendextend-1.13.4/library 316[203] /nix/store/wcywb7ydglzlxg57jf354x31nmy63923-r-viridis-0.5.1/library 317[204] /nix/store/pvnpg4vdvv93pmwrlgmy51ihrb68j55f-r-ggpubr-0.2.5/library 318[205] /nix/store/qpapsc4l9pylzfhc72ha9d82hcbac41z-r-ggsci-2.9/library 319[206] /nix/store/h0zg4x3bmkc82ggx8h4q595ffckcqgx5-r-ggsignif-0.6.0/library 320[207] /nix/store/vn5svgbf8vsgv8iy8fdzlj0izp279q15-r-polynom-1.4-0/library 321[208] /nix/store/mc1mlsjx5h3gc8nkl7jlpd4vg145nk1z-r-lindia-0.9/library 322[209] /nix/store/z1k4c8lhabp9niwfg1xylg58pf99ld9r-r-orgutils-0.4-1/library 323[210] /nix/store/ybj4538v74wx4f1l064m0qn589vyjmzg-r-textutils-0.2-0/library 324[211] /nix/store/hhm5j0wvzjc0bfd53170bw8w7mij2wnh-r-latex2exp-0.4.0/library 325[212] /nix/store/njlv5mkxgjyx3x8p984nr84dwa2v1iqp-r-kableExtra-1.1.0/library 326[213] /nix/store/lf2sb84ylh259m421ljbj731a4prjhsl-r-webshot-0.5.2/library 327[214] /nix/store/n6b8ap54b78h8l70kyx9nvayp44rnfzf-r-printr-0.1/library 328[215] /nix/store/02g1v6d3ly8zylpckigwk6w3l1mx2i9d-r-microbenchmark-1.4-7/library 329[216] /nix/store/ri6qm0fp8cyx2qnysxjv2wsk0nndl1x9-r-webchem-0.5.0/library 330[217] /nix/store/cg95rqc1gmaqxf5kxja3cz8m5w4vl76l-r-RCurl-1.98-1.2/library 331[218] /nix/store/qbpinv148778fzdz8372x8gp34hspvy1-r-bitops-1.0-6/library 332[219] /nix/store/1g0lbrx6si76k282sxr9cj0mgknrw0lx-r-devtools-2.3.0/library 333[220] /nix/store/hnvww0128czlx6w8aipjn0zs7nvmvak9-r-covr-3.5.0/library 334[221] /nix/store/p4nv59przmb14sxi49jwqarkv0l40jsp-r-rex-1.2.0/library 335[222] /nix/store/vnysmc3vkgkligwah1zh9l4sahr533a8-r-lazyeval-0.2.2/library 336[223] /nix/store/d638w33ahybsa3sqr52fafvxs2b7w9x3-r-DT-0.13/library 337[224] /nix/store/35nqc34wy2nhd9bl7lv6wriw0l3cghsw-r-crosstalk-1.1.0.1/library 338[225] /nix/store/03838i63x5irvgmpgwj67ah0wi56k9d7-r-htmlwidgets-1.5.1/library 339[226] /nix/store/l4640jxlsjzqhw63c18fziar5vc0xyhk-r-promises-1.1.0/library 340[227] /nix/store/rxrb8p3dxzsg10v7yqaq5pi3y3gk6nqh-r-later-1.0.0/library 341[228] /nix/store/giprr32bl6k18b9n4qjckpf102flarly-r-git2r-0.26.1/library 342[229] /nix/store/bbkpkf44b13ig1pkz7af32kw5dzp12vb-r-memoise-1.1.0/library 343[230] /nix/store/m31vzssnfzapsapl7f8v4m15003lcc8r-r-rcmdcheck-1.3.3/library 344[231] /nix/store/hbiylknhxsin9hp9zaa6dwc2c9ai1mqx-r-sessioninfo-1.1.1/library 345[232] /nix/store/8vwlbx3s345gjccrkiqa6h1bm9wq4s9q-r-xopen-1.0.0/library 346[233] /nix/store/mjnwnlv60cn56ap0rrzvrkqlh5qisszx-r-remotes-2.1.1/library 347[234] /nix/store/1rq4zyzqymml7cc11q89rl5g514ml9na-r-roxygen2-7.1.0/library 348[235] /nix/store/2658mrn1hpkq0fv629rvags91qg65pbn-r-brew-1.0-6/library 349[236] /nix/store/nvjalws9lzva4pd4nz1z2131xsb9b5p6-r-commonmark-1.7/library 350[237] /nix/store/qx900vivd9s2zjrxc6868s92ljfwj5dv-r-rversions-2.0.1/library 351[238] /nix/store/1drg446wilq5fjnxkglxnnv8pbp1hllg-r-usethis-1.6.0/library 352[239] /nix/store/p3f3wa41d304zbs5cwvw7vy4j17zd6nq-r-gh-1.1.0/library 353[240] /nix/store/769g7jh93da8w15ad0wsbn2aqziwwx56-r-ini-0.3.1/library 354[241] /nix/store/p7kifw1l6z2zg68a71s4sdbfj8gdmnv5-r-rematch2-2.1.1/library 355[242] /nix/store/6zhdqip9ld9vl6pvifqcf4gsqy2f5wix-r-rethinking/library 356[243] /nix/store/496p28klmflihdkc83c8p1cywg85mgk4-r-mvtnorm-1.1-0/library 357[244] /nix/store/xb1zn7ab4nka7h1vm678ginzfwg4w9wf-r-dagitty-0.2-2/library 358[245] /nix/store/3zj4dkjbdwgf3mdsl9nf9jkicpz1nwgc-r-V8-3.0.2/library 359[246] /nix/store/qiqsh62w69b5xgj2i4wjamibzxxji0mf-r-tidybayes.rethinking/library 360[247] /nix/store/4j6byy1klyk4hm2k6g3657682cf3wxcj-R-4.0.0/lib/R/library   Summer of 2020\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/sr2-ch9-ch11-ch12/","tags":["solutions","R","SR2"],"title":"  \"SR2 :: Solutions for Chapters {9,11,12}\"\n  "},{"categories":["programming"],"contents":" Setup details are described here, and the meta-post about these solutions is here.\n Materials The summmer course1 is based off of the second edition of Statistical Rethinking by Richard McElreath. This post covers the following exercise questions:\n Chapter 5  E{1,2,3,4} M{1,2,3,5}   Chapter 6  E{1,2,3,4} M{1,2,3}   Chapter 7  E{1,2,3,4} M{1,2,3,4,5,6}    Packages A colophon with details is provided at the end, but the following packages and theme parameters are used throughout.\n1libsUsed\u0026lt;-c(\u0026#34;tidyverse\u0026#34;,\u0026#34;tidybayes\u0026#34;,\u0026#34;orgutils\u0026#34;,\u0026#34;dagitty\u0026#34;, 2\u0026#34;rethinking\u0026#34;,\u0026#34;tidybayes.rethinking\u0026#34;, 3\u0026#34;ggplot2\u0026#34;,\u0026#34;kableExtra\u0026#34;,\u0026#34;dplyr\u0026#34;,\u0026#34;glue\u0026#34;, 4\u0026#34;latex2exp\u0026#34;,\u0026#34;data.table\u0026#34;,\u0026#34;printr\u0026#34;,\u0026#34;devtools\u0026#34;) 5invisible(lapply(libsUsed, library, character.only = TRUE)); 6theme_set(theme_grey(base_size=24)) Chapter V: The Many Variables \u0026amp; The Spurious Waffles Easy Questions (Ch5) 5E1 Which of the linear models below are multiple linear regressions?\n \\(μᵢ=α+βxᵢ\\) \\(μᵢ=βₓxᵢ+β_{z}zᵢ\\) \\(μᵢ=α+β(xᵢ-zᵢ)\\) \\(μᵢ=α+βₓxᵢ+β_{z}zᵢ\\)  Solution A multiple regression problem is one with more than one predictor and corresponding coefficients in an additive (hence \u0026ldquo;linear\u0026rdquo;) manner. By this logic, we can analyze the options as follows:\n Has one predictor variable, \\(x\\) thus is not a multiple regression Is a multiple linear regression since there are two independent variables, \\(x\\) and \\(z\\) Is not a multiple regression model, since only the difference of \\(x\\) and \\(z\\) enters the model (with slope \\(\\beta\\)) This is a multiple linear regression problem, since there are two predictor variables \\(x\\) and \\(z\\)  Thus options two and four are correct.\n5E2 Write down a multiple regression to evaluate the claim: Animal diversity is linearly related to latitude, but only after controlling for plant diversity. You just need to write down the model definition.\nSolution Without any further information, we can simply write a model for diversity as:\n\\[D_{A}\\sim\\mathrm{Log-Normal}(μᵢ,σ)\\] \\[μᵢ=α+β_{L}Lᵢ+β_{D_P}D_{Pᵢ}\\]\nWhere:\n \\(D_{A}\\) is the animal diversity \\(D_{P}\\) is the plant diversity \\(L\\) is the latitude  We have used a log-normal distribution for the animal diversity, since negative values for diversity are meaningless. This arises from the understanding that the diversity is on an ordinal scale with classes. The linear model posits a linear model which has two predictors, the latitude and plant diversity. Thus this model allows for \u0026ldquo;control\u0026rdquo; of the plant diversity.\nFurther details would be relegated to the choice of priors instead of the model.\n5E3 Write down a multiple regression to evaluate the claim: Neither amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree. Write down the model definition and indicate which side of zero each slope parameter should be on.\nSolution Without considering priors, we would like to write a linear model with two variables, funding and the lab size. To allow for extensions later regarding the type of funding, we will use \u0026ldquo;money\u0026rdquo; and \u0026ldquo;time\u0026rdquo; as inputs for the model. Again, since the time to a PhD cannot be negative, we will posit a log-normal distribution.\n\\[Tᵢ∼\\mathrm{Log-Normal}(μᵢ,σ)\\] \\[μᵢ=α+β_{M}M_{i}+β_{S}Sᵢ\\]\nWhere:\n \\(Tᵢ\\) is the time to completion \\(M\\) corresponds to money \\(S\\) corresponds to the size of the lab  Since we are told that the variables considered jointly have a positive association with the time, we note that the slope parameters for both should be positive.\n5E4 Suppose you have a single categorical predictor with 4 levels (unique values), labeled A,B,C and D. Let \\(Aᵢ\\) be an indicator variable that is \\(1\\) where case \\(i\\) is in category A. Also suppose \\(Bᵢ\\), \\(Cᵢ\\) and \\(Dᵢ\\) for the other categories. Now which of the following linear models are inferentially equivalent ways to include the categorical variable in a regression? Models are inferentially equivalent when it\u0026rsquo;s possible to compute one posterior distribution from the posterior distribution of another model.\n \\(μᵢ=α+β_{A}Aᵢ+β_{B}Bᵢ+β_{D}Dᵢ\\) \\(μᵢ=α+β_{A}Aᵢ+β_{B}Bᵢ+β_{C}Cᵢ+β_{D}Dᵢ\\) \\(μᵢ=α+β_{A}Aᵢ+β_{C}Cᵢ+β_{D}Dᵢ\\) \\(μᵢ=α_{A}Aᵢ+α_{B}Bᵢ+α_{C}Cᵢ+α_{D}Dᵢ\\) \\(μᵢ=α(1-Bᵢ-Cᵢ-Dᵢ)+α_{B}Bᵢ+α_{C}Cᵢ+α_{D}Dᵢ\\)  Solution Without the priors, it is difficult to infer much from these models. For the rest of the answer to make sense, we can assume indifferent priors, and enough data to overwhelm our priors (i.e., they are weakly informative).\nAll the models listed have an intercept term, and several variables. We will therefore only consider the number of independent variables and their nature.\n   Model Variables     (1) \\(μᵢ=α+β_{A}Aᵢ+β_{B}Bᵢ+β_{D}Dᵢ\\) 4   (2) \\(μᵢ=α+β_{A}Aᵢ+β_{B}Bᵢ+β_{C}Cᵢ+β_{D}Dᵢ\\) 5   (3) \\(μᵢ=α+β_{A}Aᵢ+β_{C}Cᵢ+β_{D}Dᵢ\\) 4   (4) \\(μᵢ=α_{A}Aᵢ+α_{B}Bᵢ+α_{C}Cᵢ+α_{D}Dᵢ\\) 4   (5) \\(μᵢ=α(1-Bᵢ-Cᵢ-Dᵢ)+α_{B}Bᵢ+α_{C}Cᵢ+α_{D}Dᵢ\\) 4    Thus we can infer that of the models, after fitting, only option two will have inferences which cannot be computed from the others.\nQuestions of Medium Complexity (Ch5) HOLD 5M1 Invent your own example of a spurious correlation. An outcome variable should be correlated with both predictor variables. But when both predictors are entered in the same model, the correlation between the outcome and one of the predictors should mostly vanish (or at least be greatly reduced).\nSolution For this example, consider the total potential energy of a molecular system. We will recall that this can be written as follows: \\[ E_{total}=E_{electrostatics}+E_{1B}+E_{2B} + \\cdots\\] Where the \\(B\\) terms indicate the correction terms. When predicting the total energy, if the electrostatic energy is a function of the atomic descriptors, and is entered in a model, then it masks the effect of the correction terms which also rely on the atomic descriptors. This means that correction terms to the total energy can also be thought of as a correction to the electrostatics, thus following the pattern of the divorce rate and waffles example in the chapter.\nTo put this is more context, let us introduce more explicit variables.\n\\[ E_{T}=E_{Elec}(\\theta)+E_{1B}(\\theta)+E_{2B}(\\theta)+\\cdots\\]\nIn this setting it is clear to see that the masking of variables is artificially induced.\nAnother possible example is from textcite:wainerMostDangerousEquation, where the utility of having smaller schools is a function of school size and the average number of achievements. The school size also affects the average number of achievements, as well as the actual utility. This then implies that there is a spurious correlation which does not exist when the variances are taken into account.\nHOLD 5M2 Invent your own example of a masked relationship. An outcome variable should be correlated with both predictor variables, but in opposite directions. And the two predictor variables should be correlated with one another.\nSolution Let us consider a simple case of student completion rate based on the influences of college tuition and faculty members. Assuming that college tuition is negatively correlated, and the number of faculty is positively correlated. However, since there are more wealthy people who can afford college, a chosen sample may show a spurious where examining either variable shows a weak correlation with completion rate, due to the positive association in the wealthy population.\nIt is important to note that masked relationships usually arise when the population is incorrectly sampled.\n5M3 It is sometimes observed that the best predictor of fire risk is the presence of firefighters-States and localities with many firefighters also have more fires. Presumably firefighters do not cause fires. Nevertheless, this is not a spurious correlation. Instead fires cause firefighters. Consider the same reversal of causal inferences in the context of the divorce and marriage data. How might a high divorce rate cause a higher marriage rate? Can you think of a way to evaluate this relationship, using multiple regression?\nSolution The example given simply allows for the inference that areas with a higher incidence of fires do tend to allocated more money and resources to prevent them, hence the observed larger number of firefighters. Similarly, a reversal of the divorce and marriage data might be focused on the possibility that divorcees tend to get married more often than other singles. However, to understand this further, more categorical variables would be required, though this information might also be best represented by a time series of life events. We can posit the following:\n\\[M\\sim\\mathrm{Normal}(μᵢ,σ)\\] \\[μᵢ=α+β_{L}Lᵢ+β_{R}Rᵢ\\]\nWhere:\n \\(M\\) is the marriage rate \\(L\\) is the probability of being married based on \u0026ldquo;love\u0026rdquo; \\(R\\) is the variable accounting for remarriage  5M5 One way to reason through multiple causation hypotheses is to imagine detailed mechanisms through which predictor variables may influence outcomes. For example, it is sometimes argued that the price of gasoline (predictor variable) is positively associated with lower obesity rates (outcome variable). However, there are at least two important mechanisms by which the price of gas could reduce obesity. First, it could lead to less driving and therefore more exercise. Second, it could lead to less driving, which leads to less eating out, which leads to less consumption of huge restaurant meals. Can you outline one or more multiple regressions that address these two mechanisms? Assume you can have any predictor data you need.\nSolution We adopt the following notation:\n \\(P\\) is price (predictor) \\(O\\) is obesity (outcome) \\(D\\) is for driving \\(E\\) for eating out \\(E_{x}\\) for exercise  Let us try to put this in the form of a DAG.\n1dag5m5\u0026lt;- dagitty(\u0026#34;dag{ 2P -\u0026gt; D -\u0026gt; E -\u0026gt; O 3P -\u0026gt; D -\u0026gt; Ex -\u0026gt; O 4}\u0026#34;) 1dag5m5 %\u0026gt;% graphLayout %\u0026gt;% plot  We should note that it seems straightforward, but it is nice to check as well.\n1dag5m5 %\u0026gt;% adjustmentSets(exposure=\u0026#34;P\u0026#34;,outcome=\u0026#34;O\u0026#34;) %\u0026gt;% print 12{} Now we can start working our way through the set of regressions by the most basic walk through the DAG.\n   Path One\n \\(D(P)\\) decreases \\(E_{x}(D)\\) increases \\(O(E_{x})\\) decreases       Path Two\n \\(D(P)\\) decreases \\(E(D)\\) decreases \\(O(E)\\) decreases    Chapter VI: The Haunted DAG \u0026amp; The Causal Terror Easy Questions (Ch6) 6E1 List three mechanisms by which multiple regression can produce false inferences about causal effects.\nSolution As per chapter five and six, we have three mechanisms:\n Confounding Where there exists an additional variable which influences exposure and outcome values Multicollinearity Strong associations between two or more predictor variables, which will cause the posterior distribution to suggest that none of variables are associated with the outcome even if they all actually are Post-treatment variables This is a form of included variable bias Collider Bias Conditioning on collider variables creates statistical but not causal associations between its causes  HOLD 6E2 For one of the mechanisms in the previous problem, provide an example of your choice, perhaps from your own research.\nSolution One of the core tenets of the field of computational chemistry is the act of fitting empirical potential models to more accurate potential data (or even experiments).\n Multicollinearity When dealing with decreasing effects, then using strongly correlated variables (like distance and effective distance measures like centeroid densities) cause the overall model to suggest that none of the measures are useful Post-treatment variables Often while finding minima and saddle points on a potential energy surface, adding information of the existing minima values will impede training a model which actually fits to the whole potential energy surface instead of being concentrated around the known minima  6E3 List the four elemental confounds. Can you explain the conditional dependencies of each?\nSolution The four elemental confounds are enumerated in Figure 1.\n\n Figure 1: The four elemental confounds\n  In symbolic notation, we can express this as:\n   Confound Symbolic Form Conditional Independencies     Forks \\(X←Z→Y\\) \\(Y⫫ X\\vert Z\\)   Pipes \\(X → Z → Y\\) \\(Y⫫ X\\vert Z\\)   Colliders \\(X→Z←Y\\) \\(Y \\not⫫ X\\vert Z\\)   Descendants See Figure 1 Weakly conditions on parent    6E4 How is a biased sample like conditioning on a collider? Think of the example at the open of the chapter.\nSolution Recall that the biased sample in the introduction to the chapter was:\n It seems like the most newsworthy scientific studies are the least trustworthy. The more likely it is to kill you, if true, the less likely it is to be true. The more boring the topic, the more rigorous the results. How could this widely believed negative correlation exist? There doesn’t seem to be any reason for studies of topics that people care about to produce less reliable results. Maybe popular topics attract more and worse researchers, like flies drawn to the smell of honey?\n Note that this can also be expressed as a collider in a causal DAG as:\n\\[\\mathrm{newsworthiness}→\\mathrm{acceptance}←\\mathrm{trustworthiness}\\]\nThe idea is that a proposal will be accepted if either the newsworthiness or the trustworthiness is high. There is thus on average a negative association between these criteria among the selected set of proposals.\nIn essence the association in the sub-samples is not the same as the total sample, and this causes wrong inferences on the total sample set, when conditioning on collider variables.\nQuestions of Medium Complexity (Ch6) 6M1 Modify the DAG on page \\(186\\) to include the variable \\(V\\), an unobserved cause of \\(C\\) and \\(Y:C\\gets V \\to Y\\). Reanalyze the DAG. How many paths connect \\(X\\) to \\(Y\\)? Which must be closed? Which variables should you condition on now?\nSolution Let us outline this DAG.\n1dag6m1\u0026lt;- dagitty(\u0026#34;dag{ 2U [unobserved] 3V [unobserved] 4X -\u0026gt; Y 5X \u0026lt;- U -\u0026gt; B \u0026lt;- C -\u0026gt; Y 6U \u0026lt;- A -\u0026gt; C 7C \u0026lt;- V -\u0026gt; Y 8}\u0026#34;) 9coordinates(dag6m1)\u0026lt;-list( 10x=c(X=0,Y=2,U=0,A=1,B=1,C=2,V=2.5), 11y=c(X=2,Y=2,U=1,A=0.2,B=1.5,C=1,V=1.5) 12) We can visualize this with:\n1dag6m1 %\u0026gt;% drawdag  The paths between \\(X\\) and \\(Y\\) are:\n \\(X→Y\\) \\(X←U→A←C→Y\\) \\(X←U→A←C←V→Y\\) \\(X←U→B←C→Y\\) \\(X←U→B←C←V→Y\\)  We can leverage dagitty to check which paths should be closed.\n1dag6m1 %\u0026gt;% adjustmentSets(exposure=\u0026#34;X\u0026#34;,outcome=\u0026#34;Y\u0026#34;) %\u0026gt;% print 1{ A } Logically, conditioning on \\(A\\) to close non-causal paths makes sense as it consistent with the understanding that only (1) is a causal path, and the rest will confound paths.\n6M2 Sometimes in order to avoid multicollinearity, people inspect pairwise correlations among predictors before including them in a model. This is a bad procedure, because what matters is the conditional association, not the association before the variables are included in the model. To highlight this, consider the DAG \\(X\\to Z\\to Y\\). Simulate data from this DAG so that the correlation between \\(X\\) and \\(Z\\) is very large. Then include both in a model prediction \\(Y\\). Do you observe any multicollinearity? Why or why not? What is different from the legs example in the chapter?\nSolution The DAG under consideration is: \\[ X\\to Z\\to Y \\] We will simulate data first.\n1N\u0026lt;-5000 2X\u0026lt;-N %\u0026gt;% rnorm(mean=0,sd=1) 3Z\u0026lt;-N %\u0026gt;% rnorm(mean=X,sd=0.5) 4Y\u0026lt;-N %\u0026gt;% rnorm(mean=Z,sd=1) 5cor(X,Z) %\u0026gt;% print 12[1] 0.9987166 The variables \\(X\\) and \\(Z\\) are highly correlated. We can check with a regression model for this.\n1m6m2\u0026lt;-quap( 2alist( 3Y ~ dnorm(mu,sigma), 4mu\u0026lt;-a+bX*X+bZ*Z, 5c(a,bX,bZ)~dnorm(0,1), 6sigma~dexp(1) 7), data=list(X=X,Y=Y,Z=Z) 8) The regression fit is essentially.\n1m6m2 %\u0026gt;% precis 1mean sd 5.5% 94.5% 2a -0.01 0.01 -0.03 0.01 3bX 0.06 0.03 0.00 0.11 4bZ 0.95 0.03 0.90 1.00 5sigma 1.02 0.01 1.00 1.04 The fit shows how \\(X\\) is not a useful variable, due to the addition of \\(Z\\), which is a post-treatment variable, and thus should not have been included. In effect, we also realize from this that multicollinearity is a data-driven property, and has no interpretation outside specific model instances.\n6M3 Learning to analyze DAGs requires practice. For each of the four DAGs below, state which variables, if any, you must adjust for (condition on) to estimate the total causal influence of \\(X\\) on \\(Y\\).\n Solution We can leverage the dagitty package as well to figure out which variables should be conditioned on.\n1dag6m3a\u0026lt;- dagitty(\u0026#34;dag{ 2X -\u0026gt; Y 3X \u0026lt;- Z -\u0026gt; Y 4X \u0026lt;- Z \u0026lt;- A -\u0026gt; Y 5}\u0026#34;) 6dag6m3b\u0026lt;- dagitty(\u0026#34;dag{ 7X -\u0026gt; Y 8X -\u0026gt; Z -\u0026gt; Y 9X -\u0026gt; Z \u0026lt;- A -\u0026gt; Y 10}\u0026#34;) 11dag6m3c\u0026lt;- dagitty(\u0026#34;dag{ 12X -\u0026gt; Y 13X -\u0026gt; Z \u0026lt;- Y 14X \u0026lt;- A -\u0026gt; Z \u0026lt;- Y 15}\u0026#34;) 16dag6m3d\u0026lt;- dagitty(\u0026#34;dag{ 17X -\u0026gt; Y 18X -\u0026gt; Z -\u0026gt; Y 19X \u0026lt;- A -\u0026gt; Z -\u0026gt; Y 20}\u0026#34;) 1dag6m3a %\u0026gt;% adjustmentSets(exposure=\u0026#34;X\u0026#34;,outcome=\u0026#34;Y\u0026#34;) %\u0026gt;% print 2dag6m3b %\u0026gt;% adjustmentSets(exposure=\u0026#34;X\u0026#34;,outcome=\u0026#34;Y\u0026#34;) %\u0026gt;% print 3dag6m3c %\u0026gt;% adjustmentSets(exposure=\u0026#34;X\u0026#34;,outcome=\u0026#34;Y\u0026#34;) %\u0026gt;% print 4dag6m3d %\u0026gt;% adjustmentSets(exposure=\u0026#34;X\u0026#34;,outcome=\u0026#34;Y\u0026#34;) %\u0026gt;% print 1{ Z } 23{} 45{} 67{ A } Clearly the upper left and lower right DAGs need to be conditioned on Z and A respectively to close non-causal paths.\nWe can further rationalize this as follows:\n Upper Left \\(X\\gets Z\\to Y\\) and \\(X\\gets Z \\gets A \\to Y\\) are open, non-causal paths which need to be closed Upper Right \\(Z\\) is a collider which ensures that only causal paths are open Lower Left There is a collider \\(Z\\) which ensures that the non-causal paths are closed Lower Right This figure is more complicated, so we will consider all the paths, i.e. \\(X \\to Y\\), \\(X \\to Z \\to Y\\), \\(X\\gets A \\to Z\\to Y\\), and we clearly need to condition on either \\(A\\) or \\(Z\\). \\(Z\\) is also part of a causal path, so only \\(A\\) is to be conditioned on  A more canonical way to do this is to enumerate all paths for every option, but dagitty is more elegant.\n1dag6m3d %\u0026gt;% graphLayout %\u0026gt;% plot Chapter VII: Ulysses\u0026rsquo; Compass Easy Questions (Ch7) HOLD 7E1 State the three motivating criteria that define information entropy. Try to express each in your own words.\nSolution The motivating criteria for defining informational entropy or \u0026ldquo;uncertainity\u0026rdquo; are:\n Continuity It is preferable to have a continuous function to define our informational criteria, since we can always discretize a continuous function (by binning) later, but a discrete function does not have a full range of values which can correspond to all the possible models. As a metric then, it is preferable to have a minimum and maximum bound, but define it such that it is continuous for representing arbitrary models Positive and Monotonic The monotonicity constraint is simply to ensure that as the number of events increases, given no other changes in the system, the uncertainity will increase. Since the function is already continuous, the incerasing nature is really by construction. It should be noted that a monotonously decreasing function would also satisfy the motivating criteria, but will change the interpretation completely Additivity As mentioned for continuity, it is possible always to bin continuous functions or discretize it. Similarly, it is desirable to keep the amount of uncertainity constant and add or subtract values to redefine categories  7E2 Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads \\(70\\%\\) of the time. What is the entropy of this coin?\nSolution We can simulate this system easily.\n1p\u0026lt;-c(0.7,0.3) 2-sum(p*log(p)) %\u0026gt;% print 1[1] 0.6108643 7E3 Suppose a four-sided die is loaded such that, when tossed onto a table, it shows \u0026ldquo;1\u0026rdquo; \\(20\\%\\), \u0026ldquo;2\u0026rdquo;, \\(25\\%\\), and \u0026ldquo;4\u0026rdquo; \\(30\\%\\) of the time. What is the entropy of this die?\nSolution 1p\u0026lt;-c(0.2,0.25,0.25,0.3) 2-sum(p*log(p)) %\u0026gt;% print 1[1] 1.376227 7E4 Suppose another four-sided die is loaded such that it never shows \u0026ldquo;4\u0026rdquo;. The other three sides show equally often. What is the entropy of this die?\nSolution We will not consider impossible events in our simulation.\n1p\u0026lt;- c(1/3,1/3,1/3) 2-sum(p*log(p)) %\u0026gt;% print 1[1] 1.098612 Questions of Medium Complexity (Ch7) HOLD 7M1 Write down and compare the definitions of AIC and WAIC. Which of these criteria is most general? Which assumptions are required to transform the more general criterion into a less general one?\nSolution We know that AIC or \u0026ldquo;Akaike Information Criterion\u0026rdquo; is defined as:\nWhere \\(k\\) is the number of parameters in the model.\nThe WAIC or \u0026ldquo;Widely Applicable Information Criterion\u0026rdquo; is given by: \\[\\mathrm{WAIC}=-2\\left(\\sum_{i}\\log\\Pr(y_{i})-\\sum_{i}V(y_{i})\\right)\\]\nWAIC is more general than the AIC. WAIC and AIC will be approximately equivalent when the priors are effectively flat or when there is enough data to render the priors redundant. This is because the WAIC makes no assumptions about the shape of the posterior, while AIC is an approximation depending on:\n A flat prior (or one overwhelmed by the likelihood) A posterior distribution which is approximately a multivariate Gaussian Sample size \\(N\\) with more parameters (\\(p\\))  Furthermore, we note that the AIC simply estimates that the penalty term is twice the number of parameters, while the WAIC fits uses the lppd or the sum of variances of each log-likelihood.\nHOLD 7M2 Explain the difference between model selection and model comparison. What information is lost under model selection?\nSolution Model selection involves choosing one model over the others. Ideally this occurs after appropriate model comparision. However, the chapter does mention that it is common to use heuristics like \u0026ldquo;stargazing\u0026rdquo; which uses frequentist tools to estimate which variables are important, then choose a model (or causal salad)a which has the highest number of significant variables.\nModel comparision in theory should be based off entropic measures for the information used. The models should be trained on the same data-set for the metrics to be meaningful.\nModel selection loses information regarding the uncertainity quantifications of the models which do not necessarily have the (relatively) optimal values of the metric used for comparision. This is important, especially since models which are parameterized for prediction, often perform better without being useful for causal analysis.\n7M3 When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure.\nSolution When using an information criterion, it is important to understand that different values define different \u0026ldquo;small worlds\u0026rdquo;.\nThis is why when working on gauging the information criterion, which work on the basis of the accumulated deviance values, having a varying number of training values will effectively be comparing apples and oranges. Each training data-set essentially fits one model, and comparing models trained on different data-sets (even subsets of the same data) will not lead to a fundamentally sound comparison.\nWe also know that in general, fewer data-points will have fewer deviance terms, and therefore artificially seem to be better.\nWe will prove this with an artificial data-set.\n1ySmallDat \u0026lt;- rnorm(100) 2yLargeDat \u0026lt;- rnorm(1000) 3m7m3S \u0026lt;- quap( 4alist( 5y ~ dnorm(mu,1), 6mu ~ dnorm(0,sigma) 7), data=list(y=ySmallDat,sigma=1) 8) 9m7m3L \u0026lt;- quap( 10alist( 11y ~ dnorm(mu,1), 12mu ~ dnorm(0,sigma) 13),data=list(y=yLargeDat,sigma=1) 14) 1WAIC(m7m3S) %\u0026gt;% rbind(WAIC(m7m3L)) %\u0026gt;% mutate(numSamples=c(100,1000)) %\u0026gt;% toOrg    WAIC lppd penalty std_err numSamples     278.876677095335 -138.576629006818 0.861709540849766 11.1055429875975 100   2898.5831283182 -1448.20174278015 1.08982137894866 49.5298847525459 1000    We see that apparently, the model with fewer data-points is superior, but from the discussion above, as well as by construction, we know that the models are the same, so the effect is clearly spurious, and caused by training on different data-sets.\n7M4 What happens to the effective number of parameters as measured by PSIS or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure.\nSolution Since a strength of a prior is directly related to the process of regularization, it is clear that as a prior becomes more concentrated, the model tends to be more critical of new data, and therefore the effective number of parameters will drop proportionately. Another approach to the same problem is to understand that the prior encodes our previous beliefs which in effect represents additional data which the model a-priori has been trained with.\nWe can test this simply by re-using the models we defined for 7M3.\n1yDat \u0026lt;- rnorm(5) 2sigL\u0026lt;-1000 3sigS\u0026lt;-1 4m7m4S \u0026lt;- quap( 5alist( 6y ~ dnorm(mu,1), 7mu ~ dnorm(0,sigma) 8), data=list(y=yDat,sigma=sigS) 9) 10m7m4L \u0026lt;- quap( 11alist( 12y ~ dnorm(mu,1), 13mu ~ dnorm(0,sigma) 14),data=list(y=yDat,sigma=sigL) 15) Recall that the WAIC is defined by:\n\\[ WAIC = -2(lppd-pWAIC) \\]\nWhere pWAIC is the effective number of parameters. So we note that:\n\\[ pWAIC=lppd-0.5*WAIC \\]\nThis is reported by WAIC as the penalty parameter.\n1WAIC(m7m4S) %\u0026gt;% rbind(WAIC(m7m4L)) %\u0026gt;% mutate(sigma=c(sigS,sigL)) %\u0026gt;% toOrg    WAIC lppd penalty std_err sigma     16.4098404638955 -7.31440324407321 0.890516987874561 2.31086161575483 1   16.9915093637752 -7.3011838990595 1.19457078282808 2.5268591022024 1000    Though the effect is not too strong, it is clear that having a denser prior (a.k.a smaller sigma) has a smaller number of effective paramters, as expected.\nHOLD 7M5 Provide an informal explanation of why informative priors reduce overfitting.\nSolution Overfitting is easier to understand in the context of data-compression. Essentially, when overfitting occurs, the data is represented in a different encoding, instead of being compressed.\nWe can also look at the overfitting process to be a trade off between simply fitting to every data-point (low bias, high variance) and being completely oblivious to the data (high bias, low variance). In another sense, overfitting occurs when the model is \u0026ldquo;overly eager\u0026rdquo; to learn from the data.\nGiven this understanding, informative priors essentially regularize the model, by ensuring that the likelihood is closer to the posterior, and hence prevents the model from \u0026ldquo;learning\u0026rdquo; from data-points which are not actually relevant to the prior.\nThis implies that overfitting reduces the model by lowering the sensitivity of the model to a sample, which implicitly implies that the data contains points which are not actually a feature of the process which will generate future data.\nHOLD 7M6 Provide an informal explanation of why overly informative priors result in underfitting.\nSolution Underfitting occurs when the model is insensitive to newer samples of the data. In classical terms, this means that the model has a very high bias, and typically has a correspondingly low variance.\nWith the understanding that priors cause regularization, which enforces sparsity of features, it is easier to see that very strong priors ensure that the model is overly sparse and incapable of picking up relevant trends in the training data.\nOverly informative priors, essentially imply that the model has \u0026ldquo;seen\u0026rdquo; a large amount of data previously, which then means that it is less sensitive to newer samples of data. This means that features present in the training data which are relevant to future data will be ignored in favor of the prior predictions.\nA: Colophon To ensure that this document is fully reproducible at a later date, we will record the session info.\n1devtools::session_info() 1- Session info --------------------------------------------------------------- 2setting value 3version R version 4.0.0 (2020-04-24) 4os Arch Linux 5system x86_64, linux-gnu 6ui X11 7language (EN) 8collate C 9ctype C 10tz Iceland 11date 2020-06-13 1213- Packages ------------------------------------------------------------------- 14package * version date lib source 15arrayhelpers 1.1-0 2020-02-04 [167] CRAN (R 4.0.0) 16assertthat 0.2.1 2019-03-21 [34] CRAN (R 4.0.0) 17backports 1.1.6 2020-04-05 [68] CRAN (R 4.0.0) 18boot 1.3-24 2019-12-20 [5] CRAN (R 4.0.0) 19broom 0.5.6 2020-04-20 [67] CRAN (R 4.0.0) 20callr 3.4.3 2020-03-28 [87] CRAN (R 4.0.0) 21cellranger 1.1.0 2016-07-27 [55] CRAN (R 4.0.0) 22cli 2.0.2 2020-02-28 [33] CRAN (R 4.0.0) 23coda 0.19-3 2019-07-05 [169] CRAN (R 4.0.0) 24colorspace 1.4-1 2019-03-18 [97] CRAN (R 4.0.0) 25crayon 1.3.4 2017-09-16 [35] CRAN (R 4.0.0) 26curl 4.3 2019-12-02 [26] CRAN (R 4.0.0) 27dagitty * 0.2-2 2016-08-26 [244] CRAN (R 4.0.0) 28data.table * 1.12.8 2019-12-09 [27] CRAN (R 4.0.0) 29DBI 1.1.0 2019-12-15 [77] CRAN (R 4.0.0) 30dbplyr 1.4.3 2020-04-19 [76] CRAN (R 4.0.0) 31desc 1.2.0 2018-05-01 [84] CRAN (R 4.0.0) 32devtools * 2.3.0 2020-04-10 [219] CRAN (R 4.0.0) 33digest 0.6.25 2020-02-23 [42] CRAN (R 4.0.0) 34dplyr * 0.8.5 2020-03-07 [69] CRAN (R 4.0.0) 35ellipsis 0.3.0 2019-09-20 [30] CRAN (R 4.0.0) 36evaluate 0.14 2019-05-28 [82] CRAN (R 4.0.0) 37fansi 0.4.1 2020-01-08 [36] CRAN (R 4.0.0) 38forcats * 0.5.0 2020-03-01 [29] CRAN (R 4.0.0) 39fs 1.4.1 2020-04-04 [109] CRAN (R 4.0.0) 40generics 0.0.2 2018-11-29 [71] CRAN (R 4.0.0) 41ggplot2 * 3.3.0 2020-03-05 [78] CRAN (R 4.0.0) 42glue * 1.4.0 2020-04-03 [37] CRAN (R 4.0.0) 43gridExtra 2.3 2017-09-09 [123] CRAN (R 4.0.0) 44gtable 0.3.0 2019-03-25 [79] CRAN (R 4.0.0) 45haven 2.2.0 2019-11-08 [28] CRAN (R 4.0.0) 46hms 0.5.3 2020-01-08 [44] CRAN (R 4.0.0) 47htmltools 0.4.0 2019-10-04 [112] CRAN (R 4.0.0) 48httr 1.4.1 2019-08-05 [100] CRAN (R 4.0.0) 49inline 0.3.15 2018-05-18 [162] CRAN (R 4.0.0) 50jsonlite 1.6.1 2020-02-02 [101] CRAN (R 4.0.0) 51kableExtra * 1.1.0 2019-03-16 [212] CRAN (R 4.0.0) 52knitr 1.28 2020-02-06 [113] CRAN (R 4.0.0) 53latex2exp * 0.4.0 2015-11-30 [211] CRAN (R 4.0.0) 54lattice 0.20-41 2020-04-02 [6] CRAN (R 4.0.0) 55lifecycle 0.2.0 2020-03-06 [38] CRAN (R 4.0.0) 56loo 2.2.0 2019-12-19 [163] CRAN (R 4.0.0) 57lubridate 1.7.8 2020-04-06 [106] CRAN (R 4.0.0) 58magrittr 1.5 2014-11-22 [21] CRAN (R 4.0.0) 59MASS 7.3-51.5 2019-12-20 [7] CRAN (R 4.0.0) 60matrixStats 0.56.0 2020-03-13 [164] CRAN (R 4.0.0) 61memoise 1.1.0 2017-04-21 [229] CRAN (R 4.0.0) 62modelr 0.1.6 2020-02-22 [107] CRAN (R 4.0.0) 63munsell 0.5.0 2018-06-12 [96] CRAN (R 4.0.0) 64mvtnorm 1.1-0 2020-02-24 [243] CRAN (R 4.0.0) 65nlme 3.1-147 2020-04-13 [11] CRAN (R 4.0.0) 66orgutils * 0.4-1 2017-03-21 [209] CRAN (R 4.0.0) 67pillar 1.4.3 2019-12-20 [39] CRAN (R 4.0.0) 68pkgbuild 1.0.6 2019-10-09 [86] CRAN (R 4.0.0) 69pkgconfig 2.0.3 2019-09-22 [43] CRAN (R 4.0.0) 70pkgload 1.0.2 2018-10-29 [83] CRAN (R 4.0.0) 71plyr 1.8.6 2020-03-03 [73] CRAN (R 4.0.0) 72prettyunits 1.1.1 2020-01-24 [58] CRAN (R 4.0.0) 73printr * 0.1 2017-05-19 [214] CRAN (R 4.0.0) 74processx 3.4.2 2020-02-09 [88] CRAN (R 4.0.0) 75ps 1.3.2 2020-02-13 [89] CRAN (R 4.0.0) 76purrr * 0.3.4 2020-04-17 [50] CRAN (R 4.0.0) 77R6 2.4.1 2019-11-12 [48] CRAN (R 4.0.0) 78Rcpp 1.0.4.6 2020-04-09 [10] CRAN (R 4.0.0) 79readr * 1.3.1 2018-12-21 [45] CRAN (R 4.0.0) 80readxl 1.3.1 2019-03-13 [54] CRAN (R 4.0.0) 81remotes 2.1.1 2020-02-15 [233] CRAN (R 4.0.0) 82reprex 0.3.0 2019-05-16 [108] CRAN (R 4.0.0) 83rethinking * 2.01 2020-06-06 [242] local 84rlang 0.4.5 2020-03-01 [31] CRAN (R 4.0.0) 85rmarkdown 2.1 2020-01-20 [110] CRAN (R 4.0.0) 86rprojroot 1.3-2 2018-01-03 [85] CRAN (R 4.0.0) 87rstan * 2.19.3 2020-02-11 [161] CRAN (R 4.0.0) 88rstudioapi 0.11 2020-02-07 [91] CRAN (R 4.0.0) 89rvest 0.3.5 2019-11-08 [120] CRAN (R 4.0.0) 90scales 1.1.0 2019-11-18 [93] CRAN (R 4.0.0) 91sessioninfo 1.1.1 2018-11-05 [231] CRAN (R 4.0.0) 92shape 1.4.4 2018-02-07 [193] CRAN (R 4.0.0) 93StanHeaders * 2.19.2 2020-02-11 [165] CRAN (R 4.0.0) 94stringi 1.4.6 2020-02-17 [52] CRAN (R 4.0.0) 95stringr * 1.4.0 2019-02-10 [74] CRAN (R 4.0.0) 96svUnit 1.0.3 2020-04-20 [168] CRAN (R 4.0.0) 97testthat 2.3.2 2020-03-02 [81] CRAN (R 4.0.0) 98textutils 0.2-0 2020-01-07 [210] CRAN (R 4.0.0) 99tibble * 3.0.1 2020-04-20 [32] CRAN (R 4.0.0) 100tidybayes * 2.0.3 2020-04-04 [166] CRAN (R 4.0.0) 101tidybayes.rethinking * 2.0.3.9000 2020-06-07 [246] local 102tidyr * 1.0.2 2020-01-24 [75] CRAN (R 4.0.0) 103tidyselect 1.0.0 2020-01-27 [49] CRAN (R 4.0.0) 104tidyverse * 1.3.0 2019-11-21 [66] CRAN (R 4.0.0) 105usethis * 1.6.0 2020-04-09 [238] CRAN (R 4.0.0) 106V8 3.0.2 2020-03-14 [245] CRAN (R 4.0.0) 107vctrs 0.2.4 2020-03-10 [41] CRAN (R 4.0.0) 108viridisLite 0.3.0 2018-02-01 [99] CRAN (R 4.0.0) 109webshot 0.5.2 2019-11-22 [213] CRAN (R 4.0.0) 110withr 2.2.0 2020-04-20 [90] CRAN (R 4.0.0) 111xfun 0.13 2020-04-13 [116] CRAN (R 4.0.0) 112xml2 1.3.2 2020-04-23 [122] CRAN (R 4.0.0) 113114[1] /nix/store/xzd8h53xkyvfm3kvj5ab6znp685wi04w-r-car-3.0-7/library 115[2] /nix/store/mhr8zw9bmxarc3n821b83i0gz2j9zlrq-r-abind-1.4-5/library 116[3] /nix/store/hp86nhr0787vib3l8mkw0gf9nxwb45im-r-carData-3.0-3/library 117[4] /nix/store/vhw7s2h5ds6sp110z2yvilchv8j9jch5-r-lme4-1.1-23/library 118[5] /nix/store/987n8g0zy9sjvfvnsck1bkkcknw05yvb-r-boot-1.3-24/library 119[6] /nix/store/jxxxxyz4c1k5g3drd35gsrbjdg028d11-r-lattice-0.20-41/library 120[7] /nix/store/q9zfm5h53m8rd08xcsdcwaag31k4z1pf-r-MASS-7.3-51.5/library 121[8] /nix/store/kjkm50sr144yvrhl5axfgykbiy13pbmg-r-Matrix-1.2-18/library 122[9] /nix/store/8786z5lgy8h3akfjgj3yq5yq4s17rhjy-r-minqa-1.2.4/library 123[10] /nix/store/93wv3j0z1nzqp6fjsm9v7v8bf8d1xkm2-r-Rcpp-1.0.4.6/library 124[11] /nix/store/akfw6zsmawmz8lmjkww0rnqrazm4mqp0-r-nlme-3.1-147/library 125[12] /nix/store/rxs0d9bbn8qhw7wmkfb21yk5abp6lpq1-r-nloptr-1.2.2.1/library 126[13] /nix/store/8n0jfiqn4275i58qgld0dv8zdaihdzrk-r-RcppEigen-0.3.3.7.0/library 127[14] /nix/store/8vxrma33rhc96260zsi1jiw7dy3v2mm4-r-statmod-1.4.34/library 128[15] /nix/store/2y46pb5x9lh8m0hdmzajnx7sc1bk9ihl-r-maptools-0.9-9/library 129[16] /nix/store/iwf9nxx1v883wlv0p88q947hpz5lhfh7-r-foreign-0.8-78/library 130[17] /nix/store/rl9sjqply6rjbnz5k792ghm62ybv76px-r-sp-1.4-1/library 131[18] /nix/store/ws4bkzyv2vj5pyn1hgwyy6nlp48arz0n-r-mgcv-1.8-31/library 132[19] /nix/store/307dzxrmnqk4p86560a02r64x1fhhmxb-r-nnet-7.3-13/library 133[20] /nix/store/g2zpzkdb9hzkza1wpcbrk58119v1wyaf-r-pbkrtest-0.4-8.6/library 134[21] /nix/store/p0l503fr8960vld70w6ilmknxs5qwq77-r-magrittr-1.5/library 135[22] /nix/store/rmjpcaw3i446kwnjgcxcaid0yac36cj2-r-quantreg-5.55/library 136[23] /nix/store/10mzmnvc5jjgk2xzasia522pk60a30qz-r-MatrixModels-0.4-1/library 137[24] /nix/store/6qwdzvmnnmhjwdnvg2zmvv6wafd1vf91-r-SparseM-1.78/library 138[25] /nix/store/aa9c39a3yiqkh1h7pbngjlbr7czvc7yi-r-rio-0.5.16/library 139[26] /nix/store/2fx4vqlybgwp5rhhy6pssqx7h1a927fn-r-curl-4.3/library 140[27] /nix/store/k4m3fn1kqvvvn8y33kd57gq49hr3ar8y-r-data.table-1.12.8/library 141[28] /nix/store/651hfjylqzmsf565wyx474vyjny771gy-r-haven-2.2.0/library 142[29] /nix/store/a3rnz28irmqvmj8axj5x5j1am2c3gzs4-r-forcats-0.5.0/library 143[30] /nix/store/j8v4gzib137q2cml31hvvfkrc0f60pp5-r-ellipsis-0.3.0/library 144[31] /nix/store/xaswqlnamf4k8vwx0x3wav3l0x60sag0-r-rlang-0.4.5/library 145[32] /nix/store/dqm3xpix2jwhhhr67s6fgrwbw7hizap7-r-tibble-3.0.1/library 146[33] /nix/store/v7xfsq6d97wpn6m0hjrac78w5xawbr8a-r-cli-2.0.2/library 147[34] /nix/store/fikjasr98klhk9cf44x4lhi57vh3pmkg-r-assertthat-0.2.1/library 148[35] /nix/store/3fya6cd38vsqdj0gjb7bcsy00sirlyw1-r-crayon-1.3.4/library 149[36] /nix/store/payqi9bwh216rwhaq07jgc26l4fv1zsb-r-fansi-0.4.1/library 150[37] /nix/store/h6a61ghws7yrdxlg412xl1im37z5r28i-r-glue-1.4.0/library 151[38] /nix/store/y8mjbia1wbnq26dkigr0p3xxwrbzsc2r-r-lifecycle-0.2.0/library 152[39] /nix/store/kwaghh12cnifgvcbvlv2anx0hd5f4ild-r-pillar-1.4.3/library 153[40] /nix/store/k1phn8j10nni7gzvcgp0vc25dby6bb77-r-utf8-1.1.4/library 154[41] /nix/store/k3b77y8v7zsshpp1ccs8jwk2i2g4rm9a-r-vctrs-0.2.4/library 155[42] /nix/store/iibjmbh7vj0d0bfafz98yn29ymg43gkw-r-digest-0.6.25/library 156[43] /nix/store/aqsj4k3pgm80qk4jjg7sh3ac28n6alv0-r-pkgconfig-2.0.3/library 157[44] /nix/store/i7c5v8s4hd9rlqah3bbvy06yywjqwdgk-r-hms-0.5.3/library 158[45] /nix/store/2fyrk58cmcbrxid66rbwjli7y114lvrm-r-readr-1.3.1/library 159[46] /nix/store/163xq2g5nblqgh7qhvzb6mvgg6qdrirj-r-BH-1.72.0-3/library 160[47] /nix/store/dr27b6k49prwgrjs0v30b6mf5lxa36pk-r-clipr-0.7.0/library 161[48] /nix/store/bghvqg9mcaj2jkbwpy0di6c563v24acz-r-R6-2.4.1/library 162[49] /nix/store/nq8jdq7nlg9xns4xpgyj6sqv8p4ny1wz-r-tidyselect-1.0.0/library 163[50] /nix/store/zlwhf75qld7vmwx3d4bdws057ld4mqbp-r-purrr-0.3.4/library 164[51] /nix/store/0gbmmnbpqlr69l573ymkcx8154fvlaca-r-openxlsx-4.1.4/library 165[52] /nix/store/1m1q4rmwx56dvx9rdzfsfq0jpw3hw0yx-r-stringi-1.4.6/library 166[53] /nix/store/mhy5vnvbsl4q7dcinwx3vqlyywxphbfd-r-zip-2.0.4/library 167[54] /nix/store/88sp7f7q577i6l5jjanqiv5ak6nv5357-r-readxl-1.3.1/library 168[55] /nix/store/6q9zwivzalhmzdracc8ma932wirq8rl5-r-cellranger-1.1.0/library 169[56] /nix/store/jh2n6k2ancdzqych5ix8n4rq9w514qq9-r-rematch-1.0.1/library 170[57] /nix/store/22xjqikqd6q556absb5224sbx6q0kp0c-r-progress-1.2.2/library 171[58] /nix/store/9vp32wa1qvv6lkq6p70qlli5whrxzfbi-r-prettyunits-1.1.1/library 172[59] /nix/store/r9rhqb6fsk75shihmb7nagqb51pqwp0y-r-class-7.3-16/library 173[60] /nix/store/z1kad071y43wij1ml9lpghh7jbimmcli-r-cluster-2.1.0/library 174[61] /nix/store/i8wr965caf6j1rxs2dsvpzhlh4hyyb4y-r-codetools-0.2-16/library 175[62] /nix/store/8iglq3zr68a39hzswvzxqi2ffhpw9p51-r-KernSmooth-2.23-16/library 176[63] /nix/store/n3k50zv40i40drpdf8npbmy2y08gkr6w-r-rpart-4.1-15/library 177[64] /nix/store/b4r6adzcvpm8ivflsmis7ja7q4r5hkjy-r-spatial-7.3-11/library 178[65] /nix/store/zqg6hmrncl8ax3vn7z5drf4csddwnhcx-r-survival-3.1-12/library 179[66] /nix/store/4anrihkx11h8mzb269xdyi84yp5v7grl-r-tidyverse-1.3.0/library 180[67] /nix/store/945haq0w8nfm9ib7r0nfngn5lk2i15ix-r-broom-0.5.6/library 181[68] /nix/store/52viqxzrmxl7dk0zji293g5b0b9grwh8-r-backports-1.1.6/library 182[69] /nix/store/zp1k42sw2glqy51w4hnzsjs8rgi8xzx2-r-dplyr-0.8.5/library 183[70] /nix/store/mkjd98mnshch2pwnj6h31czclqdaph3f-r-plogr-0.2.0/library 184[71] /nix/store/kflrzax6y5pwfqwzgfvqz433a3q3hnhn-r-generics-0.0.2/library 185[72] /nix/store/xi1n5h5w17c33y6ax3dfhg2hgzjl9bxz-r-reshape2-1.4.4/library 186[73] /nix/store/vn63z92zkpbaxmmhzpb6mq2fvg0xa26h-r-plyr-1.8.6/library 187[74] /nix/store/wmpyxss67bj44rin7hlnr9qabx66p5hj-r-stringr-1.4.0/library 188[75] /nix/store/330qbgbvllwz3h0i2qidrlk50y0mbgph-r-tidyr-1.0.2/library 189[76] /nix/store/cx3x4pqb65l1mhss65780hbzv9jdrzl6-r-dbplyr-1.4.3/library 190[77] /nix/store/gsj49bp3hpw9jlli3894c49amddryqsq-r-DBI-1.1.0/library 191[78] /nix/store/kvymhwp4gac0343c2yi1qvdpavx4gdn2-r-ggplot2-3.3.0/library 192[79] /nix/store/knv51jvpairvibrkkq48b6f1l2pa1cv8-r-gtable-0.3.0/library 193[80] /nix/store/158dx0ddv20ikwag2860nlg9p3hbh1zc-r-isoband-0.2.1/library 194[81] /nix/store/fprs9rp1jlhxzj7fp6l79akyf8k3p7zd-r-testthat-2.3.2/library 195[82] /nix/store/0pmlnkyn0ir3k9bvxihi1r06jyl64w3i-r-evaluate-0.14/library 196[83] /nix/store/7210bjjqn5cjndxn5isnd4vip00xhkhy-r-pkgload-1.0.2/library 197[84] /nix/store/9a12ybd74b7dns40gcfs061wv7913qjy-r-desc-1.2.0/library 198[85] /nix/store/na9pb1apa787zp7vvyz1kzym0ywjwbj0-r-rprojroot-1.3-2/library 199[86] /nix/store/pa2n7bh61qxyarn5i2ynd62k6knb1np1-r-pkgbuild-1.0.6/library 200[87] /nix/store/1hxm1m7h4272zxk9bpsaq46mvnl0dbss-r-callr-3.4.3/library 201[88] /nix/store/bigvyk6ipglbiil93zkf442nv4y3xa1x-r-processx-3.4.2/library 202[89] /nix/store/370lr0wf7qlq0m72xnmasg2iahkp2n52-r-ps-1.3.2/library 203[90] /nix/store/rr72q61d8mkd42zc5fhcd2rqjghvc141-r-withr-2.2.0/library 204[91] /nix/store/9gw77p7fmz89fa8wi1d9rvril6hd4sxy-r-rstudioapi-0.11/library 205[92] /nix/store/9x4v4pbrgmykbz2801h77yz2l0nmm5nb-r-praise-1.0.0/library 206[93] /nix/store/pf8ssb0dliw5bzsncl227agc8przb7ic-r-scales-1.1.0/library 207[94] /nix/store/095z4wgjrxn63ixvyzrj1fm1rdv6ci95-r-farver-2.0.3/library 208[95] /nix/store/5aczj4s7i9prf5i32ik5ac5baqvjwdb1-r-labeling-0.3/library 209[96] /nix/store/wch26phipzz9gxd4vbr4fynh7v28349j-r-munsell-0.5.0/library 210[97] /nix/store/3w8fh756mszhsjx5fwgwydcpn8vkwady-r-colorspace-1.4-1/library 211[98] /nix/store/8cmaj81v2vm4f8p59ylbnsby8adkbmhd-r-RColorBrewer-1.1-2/library 212[99] /nix/store/h4x4ygax7gpz6f0c2v0xacr62080qwb8-r-viridisLite-0.3.0/library 213[100] /nix/store/qhx0i2nn5syb6vygdn8fdxgl7k56yj81-r-httr-1.4.1/library 214[101] /nix/store/lxnb4aniv02i4jhdvz02aaql1kznbpxb-r-jsonlite-1.6.1/library 215[102] /nix/store/13dcry4gad3vfwqzqb0ii4n06ybrxybr-r-mime-0.9/library 216[103] /nix/store/2can5l8gscc92a3bqlak8hfcg96v5hvf-r-openssl-1.4.1/library 217[104] /nix/store/piwsgxdz5w2ak8c6fcq0lc978qbxwdp1-r-askpass-1.1/library 218[105] /nix/store/3sj5h6dwa1l27d2hvdchclygk0pgffsr-r-sys-3.3/library 219[106] /nix/store/2z0p88g0c03gigl2ip60dlsfkdv1k30h-r-lubridate-1.7.8/library 220[107] /nix/store/1pkmj8nqjg2iinrkg2w0zkwq0ldc01za-r-modelr-0.1.6/library 221[108] /nix/store/bswkzvn8lczwbyw3y7n0p0qp2q472s0g-r-reprex-0.3.0/library 222[109] /nix/store/yid22gad8z49q52d225vfba2m4cgj2lx-r-fs-1.4.1/library 223[110] /nix/store/d185qiqaplm5br9fk1pf29y0srlabw83-r-rmarkdown-2.1/library 224[111] /nix/store/iszqviydsdj31c3ww095ndqy1ld3cibs-r-base64enc-0.1-3/library 225[112] /nix/store/i89wfw4cr0fz3wbd7cg44fk4dwz8b6h1-r-htmltools-0.4.0/library 226[113] /nix/store/qrl28laqwmhpwg3dpcf4nca8alv0px0g-r-knitr-1.28/library 227[114] /nix/store/jffaxc4a3bbf2g6ip0gdcya73dmg53mb-r-highr-0.8/library 228[115] /nix/store/717srph13qpnbzmgsvhx25q8pl51ivpj-r-markdown-1.1/library 229[116] /nix/store/mxqmyq3ybdfyc6p0anhfy2kfw0iz5k4n-r-xfun-0.13/library 230[117] /nix/store/b8g6hadva0359l6j1aq4dbvxlqf1acxc-r-yaml-2.2.1/library 231[118] /nix/store/rrl05vpv7cw58zi0k9ykm7m4rjb9gjv3-r-tinytex-0.22/library 232[119] /nix/store/2ziq8nzah6xy3dgmxgim9h2wszz1f89f-r-whisker-0.4/library 233[120] /nix/store/540wbw4p1g2qmnmbfk0rhvwvfnf657sj-r-rvest-0.3.5/library 234[121] /nix/store/n3prn77gd9sf3z4whqp86kghr55bf5w8-r-selectr-0.4-2/library 235[122] /nix/store/gv28yjk5isnglq087y7767xw64qa40cw-r-xml2-1.3.2/library 236[123] /nix/store/693czdcvkp6glyir0mi8cqvdc643whvc-r-gridExtra-2.3/library 237[124] /nix/store/3sykinp7lyy70dgzr0fxjb195nw864dv-r-future-1.17.0/library 238[125] /nix/store/bqi2l53jfxncks6diy0hr34bw8f86rvk-r-globals-0.12.5/library 239[126] /nix/store/dydyl209klklzh69w9q89f2dym9xycnp-r-listenv-0.8.0/library 240[127] /nix/store/lni0bi36r4swldkx7g4hql7gfz9b121b-r-gganimate-1.0.5/library 241[128] /nix/store/hh92jxs79kx7vxrxr6j6vin1icscl4k7-r-tweenr-1.0.1/library 242[129] /nix/store/0npx3srjnqgh7bib80xscjqvfyzjvimq-r-GGally-1.5.0/library 243[130] /nix/store/x5nzxklmacj6l162g7kg6ln9p25r3f17-r-reshape-0.8.8/library 244[131] /nix/store/q29z7ckdyhfmg1zlzrrg1nrm36ax756j-r-ggfortify-0.4.9/library 245[132] /nix/store/1rvm1w9iv2c5n22p4drbjq8lr9wa2q2r-r-cowplot-1.0.0/library 246[133] /nix/store/rp8jhnasaw1vbv5ny5zx0mw30zgcp796-r-ggrepel-0.8.2/library 247[134] /nix/store/wb7y931mm8nsj7w9xin83bvbaq8wvi4d-r-corrplot-0.84/library 248[135] /nix/store/gdzcqivfvgdrsz247v5kmnnw1v6p9c1p-r-rpart.plot-3.0.8/library 249[136] /nix/store/6yqg37108r0v22476cm2kv0536wyilki-r-caret-6.0-86/library 250[137] /nix/store/6fjdgcwgisiqz451sg5fszxnn9z8vxg6-r-foreach-1.5.0/library 251[138] /nix/store/c3ph5i341gk7jdinrkkqf6y631xli424-r-iterators-1.0.12/library 252[139] /nix/store/sjm1rxshlpakpxbrynfhsjnnp1sjvc3r-r-ModelMetrics-1.2.2.2/library 253[140] /nix/store/vgk4m131d057xglmrrb9rijhzdr2qhhp-r-pROC-1.16.2/library 254[141] /nix/store/bv1kvy1wc2jx3v55rzn3cg2qjbv7r8zp-r-recipes-0.1.10/library 255[142] /nix/store/001h42q4za01gli7avjxhq7shpv73n9k-r-gower-0.2.1/library 256[143] /nix/store/ssffpl6ydffqyn9phscnccxnj71chnzg-r-ipred-0.9-9/library 257[144] /nix/store/baliqip8m6p0ylqhqcgqak29d8ghral1-r-prodlim-2019.11.13/library 258[145] /nix/store/j4n2wsv98asw83qiffg6a74dymk8r2hl-r-lava-1.6.7/library 259[146] /nix/store/hf5wq5kpsf6p9slglq5iav09s4by0y5i-r-numDeriv-2016.8-1.1/library 260[147] /nix/store/s58hm38078mx4gyqffvv09zn575xn648-r-SQUAREM-2020.2/library 261[148] /nix/store/g63ydzd53586pvr9kdgk8kf5szq5f2bc-r-timeDate-3043.102/library 262[149] /nix/store/0jkarmlf1kjv4g8a3svkc7jfarpp77ny-r-mlr3-0.2.0/library 263[150] /nix/store/g1m0n1w7by213v773iyn7vnxr25pkf56-r-checkmate-2.0.0/library 264[151] /nix/store/fc2ah8cz2sj6j2jk7zldvjmsjn1yakpn-r-lgr-0.3.4/library 265[152] /nix/store/0i2hs088j1s0a6i61124my6vnzq8l27m-r-mlbench-2.1-1/library 266[153] /nix/store/vzcs6k21pqrli3ispqnvj5qwkv14srf5-r-mlr3measures-0.1.3/library 267[154] /nix/store/h2yqqaia46bk3b1d1a7bq35zf09p1b1a-r-mlr3misc-0.2.0/library 268[155] /nix/store/c9mrkc928cmsvvnib50l0jb8lsz59nyk-r-paradox-0.2.0/library 269[156] /nix/store/vqpbdipi4p4advl2vxrn765mmgcrabvk-r-uuid-0.1-4/library 270[157] /nix/store/xpclynxnfq4h9218gk4y62nmgyyga6zl-r-mlr3viz-0.1.1/library 271[158] /nix/store/7w6pld5vir3p9bybay67kq0qwl0gnx17-r-mlr3learners-0.2.0/library 272[159] /nix/store/ca50rp6ha5s51qmhb1gjlj62r19xfzxs-r-mlr3pipelines-0.1.3/library 273[160] /nix/store/9hg0xap4pir64mhbgq8r8cgrfjn8aiz5-r-mlr3filters-0.2.0/library 274[161] /nix/store/jgqcmfix0xxm3y90m8wy3xkgmqf2b996-r-rstan-2.19.3/library 275[162] /nix/store/mvv1gjyrrpvf47fn7a8x722wdwrf5azk-r-inline-0.3.15/library 276[163] /nix/store/zmkw51x4w4d1v1awcws0xihj4hnxfr09-r-loo-2.2.0/library 277[164] /nix/store/30xxalfwzxl05bbfvj5sy8k3ysys6z5y-r-matrixStats-0.56.0/library 278[165] /nix/store/fhkww2l0izx87bjnf0pl9ydl1wprp0xv-r-StanHeaders-2.19.2/library 279[166] /nix/store/aflck5pzxa8ym5q1dxchx5hisfmfghkr-r-tidybayes-2.0.3/library 280[167] /nix/store/jhlbhiv4fg0wsbxwjz8igc4hcg79vw94-r-arrayhelpers-1.1-0/library 281[168] /nix/store/fv089zrnvicnavbi08hnzqpi9g1z4inj-r-svUnit-1.0.3/library 282[169] /nix/store/xci2rgjizx1fyb33818jx5s1bgn8v8k6-r-coda-0.19-3/library 283[170] /nix/store/dch9asd38yldz0sdn8nsgk9ivjrkbhva-r-HDInterval-0.2.0/library 284[171] /nix/store/rs8dri2m5cqdmpiw187rvl4yhjn0jg2v-r-e1071-1.7-3/library 285[172] /nix/store/qs1zyh3sbvccgnqjzas3br6pak399zgc-r-pvclust-2.2-0/library 286[173] /nix/store/sh3zxvdazp7rkjn1iczrag1h2358ifm1-r-forecast-8.12/library 287[174] /nix/store/h67kaxqr2ppdpyj77wg5hm684jypznji-r-fracdiff-1.5-1/library 288[175] /nix/store/fh0z465ligbpqyam5l1fwiijc7334kbk-r-lmtest-0.9-37/library 289[176] /nix/store/0lnsbwfg0axr80h137q52pa50cllbjpf-r-zoo-1.8-7/library 290[177] /nix/store/p7k4s3ivf83dp2kcxr1cr0wlc1rfk6jx-r-RcppArmadillo-0.9.860.2.0/library 291[178] /nix/store/ssnxv5x6zid2w11v8k5yvnyxis6n1qfk-r-tseries-0.10-47/library 292[179] /nix/store/zrbskjwaz0bzz4v76j044d771m24g6h8-r-quadprog-1.5-8/library 293[180] /nix/store/2x3w5sjalrfm6hf1dxd951j8y94nh765-r-quantmod-0.4.17/library 294[181] /nix/store/7g55xshf49s9379ijm1zi1qnh1vbsifq-r-TTR-0.23-6/library 295[182] /nix/store/6ilyzph46q6ijyanq4p7f0ccyni0d7j0-r-xts-0.12-0/library 296[183] /nix/store/17xhqghcnqha7pwbf98dxsq1729slqd5-r-urca-1.3-0/library 297[184] /nix/store/722lyn0k8y27pj1alik56r4vpjnncd9z-r-swdft-1.0.0/library 298[185] /nix/store/36n0zgy10fsqcq76n0qmdwjxrwh7pn9n-r-xgboost-1.0.0.2/library 299[186] /nix/store/ac0ar7lf75qx84xsdjv6j02rkdgnhybz-r-ranger-0.12.1/library 300[187] /nix/store/i1ighkq42x10dirqmzgbx2mhbnz1ynkb-r-DALEX-1.2.0/library 301[188] /nix/store/28fqnhsfng1bkphl0wvr7lg5y3p6va46-r-iBreakDown-1.2.0/library 302[189] /nix/store/dpym77x9qc2ksr4mwjm3pb9ar1kvwhdl-r-ingredients-1.2.0/library 303[190] /nix/store/sp4d281w6dpr31as0xdjqizdx8hhb01q-r-DALEXtra-0.2.1/library 304[191] /nix/store/ckhp9kpmjcs0wxb113pxn25c2wip2d0n-r-ggdendro-0.1-20/library 305[192] /nix/store/f3k7dxj1dsmqri2gn0svq4c9fvvl9g7q-r-glmnet-3.0-2/library 306[193] /nix/store/l6ccj6mwkqybjvh6dr8qzalygp0i7jyb-r-shape-1.4.4/library 307[194] /nix/store/418mqfwlafh6984xld8lzhl7rv29qw68-r-reticulate-1.15/library 308[195] /nix/store/qwh982mgxd2mzrgbjk14irqbasywa1jk-r-rappdirs-0.3.1/library 309[196] /nix/store/6sxs76abll23c6372h6nf101wi8fcr4c-r-FactoMineR-2.3/library 310[197] /nix/store/39d2va10ydgyzddwr07xwdx11fwk191i-r-ellipse-0.4.1/library 311[198] /nix/store/4lxym5nxdn8hb7l8a566n5vg9paqcfi2-r-flashClust-1.01-2/library 312[199] /nix/store/wp161zbjjs41fq4kn4k3m244c7b8l2l2-r-leaps-3.1/library 313[200] /nix/store/irghsaplrpb3hg3y7j831bbklf2cqs6d-r-scatterplot3d-0.3-41/library 314[201] /nix/store/09ahkf50g1q9isxanbdykqgcdrp8mxl1-r-factoextra-1.0.7/library 315[202] /nix/store/zi9bq7amsgc6w2x7fvd62g9qxz69vjfm-r-dendextend-1.13.4/library 316[203] /nix/store/wcywb7ydglzlxg57jf354x31nmy63923-r-viridis-0.5.1/library 317[204] /nix/store/pvnpg4vdvv93pmwrlgmy51ihrb68j55f-r-ggpubr-0.2.5/library 318[205] /nix/store/qpapsc4l9pylzfhc72ha9d82hcbac41z-r-ggsci-2.9/library 319[206] /nix/store/h0zg4x3bmkc82ggx8h4q595ffckcqgx5-r-ggsignif-0.6.0/library 320[207] /nix/store/vn5svgbf8vsgv8iy8fdzlj0izp279q15-r-polynom-1.4-0/library 321[208] /nix/store/mc1mlsjx5h3gc8nkl7jlpd4vg145nk1z-r-lindia-0.9/library 322[209] /nix/store/z1k4c8lhabp9niwfg1xylg58pf99ld9r-r-orgutils-0.4-1/library 323[210] /nix/store/ybj4538v74wx4f1l064m0qn589vyjmzg-r-textutils-0.2-0/library 324[211] /nix/store/hhm5j0wvzjc0bfd53170bw8w7mij2wnh-r-latex2exp-0.4.0/library 325[212] /nix/store/njlv5mkxgjyx3x8p984nr84dwa2v1iqp-r-kableExtra-1.1.0/library 326[213] /nix/store/lf2sb84ylh259m421ljbj731a4prjhsl-r-webshot-0.5.2/library 327[214] /nix/store/n6b8ap54b78h8l70kyx9nvayp44rnfzf-r-printr-0.1/library 328[215] /nix/store/02g1v6d3ly8zylpckigwk6w3l1mx2i9d-r-microbenchmark-1.4-7/library 329[216] /nix/store/ri6qm0fp8cyx2qnysxjv2wsk0nndl1x9-r-webchem-0.5.0/library 330[217] /nix/store/cg95rqc1gmaqxf5kxja3cz8m5w4vl76l-r-RCurl-1.98-1.2/library 331[218] /nix/store/qbpinv148778fzdz8372x8gp34hspvy1-r-bitops-1.0-6/library 332[219] /nix/store/1g0lbrx6si76k282sxr9cj0mgknrw0lx-r-devtools-2.3.0/library 333[220] /nix/store/hnvww0128czlx6w8aipjn0zs7nvmvak9-r-covr-3.5.0/library 334[221] /nix/store/p4nv59przmb14sxi49jwqarkv0l40jsp-r-rex-1.2.0/library 335[222] /nix/store/vnysmc3vkgkligwah1zh9l4sahr533a8-r-lazyeval-0.2.2/library 336[223] /nix/store/d638w33ahybsa3sqr52fafvxs2b7w9x3-r-DT-0.13/library 337[224] /nix/store/35nqc34wy2nhd9bl7lv6wriw0l3cghsw-r-crosstalk-1.1.0.1/library 338[225] /nix/store/03838i63x5irvgmpgwj67ah0wi56k9d7-r-htmlwidgets-1.5.1/library 339[226] /nix/store/l4640jxlsjzqhw63c18fziar5vc0xyhk-r-promises-1.1.0/library 340[227] /nix/store/rxrb8p3dxzsg10v7yqaq5pi3y3gk6nqh-r-later-1.0.0/library 341[228] /nix/store/giprr32bl6k18b9n4qjckpf102flarly-r-git2r-0.26.1/library 342[229] /nix/store/bbkpkf44b13ig1pkz7af32kw5dzp12vb-r-memoise-1.1.0/library 343[230] /nix/store/m31vzssnfzapsapl7f8v4m15003lcc8r-r-rcmdcheck-1.3.3/library 344[231] /nix/store/hbiylknhxsin9hp9zaa6dwc2c9ai1mqx-r-sessioninfo-1.1.1/library 345[232] /nix/store/8vwlbx3s345gjccrkiqa6h1bm9wq4s9q-r-xopen-1.0.0/library 346[233] /nix/store/mjnwnlv60cn56ap0rrzvrkqlh5qisszx-r-remotes-2.1.1/library 347[234] /nix/store/1rq4zyzqymml7cc11q89rl5g514ml9na-r-roxygen2-7.1.0/library 348[235] /nix/store/2658mrn1hpkq0fv629rvags91qg65pbn-r-brew-1.0-6/library 349[236] /nix/store/nvjalws9lzva4pd4nz1z2131xsb9b5p6-r-commonmark-1.7/library 350[237] /nix/store/qx900vivd9s2zjrxc6868s92ljfwj5dv-r-rversions-2.0.1/library 351[238] /nix/store/1drg446wilq5fjnxkglxnnv8pbp1hllg-r-usethis-1.6.0/library 352[239] /nix/store/p3f3wa41d304zbs5cwvw7vy4j17zd6nq-r-gh-1.1.0/library 353[240] /nix/store/769g7jh93da8w15ad0wsbn2aqziwwx56-r-ini-0.3.1/library 354[241] /nix/store/p7kifw1l6z2zg68a71s4sdbfj8gdmnv5-r-rematch2-2.1.1/library 355[242] /nix/store/6zhdqip9ld9vl6pvifqcf4gsqy2f5wix-r-rethinking/library 356[243] /nix/store/496p28klmflihdkc83c8p1cywg85mgk4-r-mvtnorm-1.1-0/library 357[244] /nix/store/xb1zn7ab4nka7h1vm678ginzfwg4w9wf-r-dagitty-0.2-2/library 358[245] /nix/store/3zj4dkjbdwgf3mdsl9nf9jkicpz1nwgc-r-V8-3.0.2/library 359[246] /nix/store/qiqsh62w69b5xgj2i4wjamibzxxji0mf-r-tidybayes.rethinking/library 360[247] /nix/store/4j6byy1klyk4hm2k6g3657682cf3wxcj-R-4.0.0/lib/R/library   Summer of 2020\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/sr2-ch5-ch6-ch7/","tags":["solutions","R","SR2"],"title":"  \"SR2 :: Solutions for Chapters {5,6,7}\"\n  "},{"categories":["programming"],"contents":" Setup details are described here, and the meta-post about these solutions is here.\n Materials The summmer course1 is based off of the second edition of Statistical Rethinking by Richard McElreath. This post covers the following exercise questions:\n Chapter 2  Easy {1,2,3,4} Medium {1,2,4}   Chapter 3  Easy {1,2,3,4,5} Medium {1,2,3,4,6}   Chapter 4  Easy {1,2,3,4,5} Medium {1,2,3,4,5,6,7}    Packages 1libsUsed\u0026lt;-c(\u0026#34;tidyverse\u0026#34;,\u0026#34;tidybayes\u0026#34;,\u0026#34;orgutils\u0026#34;, 2\u0026#34;rethinking\u0026#34;,\u0026#34;tidybayes.rethinking\u0026#34;, 3\u0026#34;ggplot2\u0026#34;,\u0026#34;kableExtra\u0026#34;,\u0026#34;dplyr\u0026#34;,\u0026#34;glue\u0026#34;, 4\u0026#34;latex2exp\u0026#34;,\u0026#34;data.table\u0026#34;,\u0026#34;printr\u0026#34;) 5invisible(lapply(libsUsed, library, character.only = TRUE)); We also set the following theme parameters for the plots.\n1theme_set(theme_grey(base_size=24)) Chapter II: The Golem of Prague Easy Questions (Ch2) 2E1 Which of the expressions below correspond to the statement: /the probability of rain on Monday?\n Pr(rain) Pr(rain|Monday) Pr(Monday|rain) Pr(rain, Monday)/Pr(Monday)  Solution We can read each of these sentences as follows:\n Pr(rain) Probability of rain Pr(rain|Monday) Probability of rain given that it is Monday, or probability that it rains on Monday Pr(Monday|rain) Probability of being a Monday given that it rains Pr(rain,Monday)/Pr(Monday) Compound statement  We further note that we can express the joint probability of 4. to be equivalent to the probability written in 2.\nHence the correct solutions are options 2 and 4.\n2E2 Which of the following statements corresponds to the expression: Pr(Monday|rain)?\n The probability of rain on Monday. The probability of rain, given that it is Monday. The probability that it is Monday, given that it is raining. The probability that it is Monday and that it is raining.  Solution Using the same logic as described previously we note that the sentences correspond to the following formulations:\n Pr(rain,Monday) The probability of rain on Monday. Pr(rain|Monday) The probability of rain, given that it is Monday. Pr(Monday|rain) The probability that it is Monday, given that it is raining. Pr(Monday,rain) The probability that it is Monday and that it is raining. Hence only option 3 is correct.\n  2E3 Which of the expressions below correspond to the statement: the probability that it is Monday, given that it is raining?\n Pr(Monday|rain) Pr(rain|Monday) Pr(rain|Monday)Pr(Monday) Pr(rain|Monday)Pr(Monday)/Pr(rain) Pr(Monday|rain)Pr(rain)/Pr(Monday)  Solution We will simplify these slightly.\nHence the correct solutions are options 1 and 4.\n2E4 The Bayesian statistician Bruno de Finetti (1906\u0026ndash;1985) began his 1973 book on probability theory with the declaration: \u0026ldquo;PROBABILITY DOES NOT EXIST.\u0026rdquo; The capitals appeared in the original, so I imagine de Finetti wanted us to shout this statement. What he meant is that probability is a device for describing uncertainty from the perspective of an observer with limited knowledge; it has no objective reality. Discuss the globe tossing example from the chapter, in light of this statement. What does it mean to say \u0026ldquo;the probability of water is 0.7\u0026rdquo;?\nSolution The de Finetti school of thought subscribed to the belief that all processes were deterministic and therefore did not have any inherent probabilitic interpretation. With this framework, uncertainty did not have any physical realization, and so the random effects of a process could be accounted for entirely in terms of aleatoric and epistemic uncertainity without needing to consider the fact that in some processes (not discovered then) like quantum mechanics, random effects are part of the physical system, and are not due to a lack of information. Under this assumption, the entirity of probability is simply an numerical artifact with which the lack of information about a process could be expressed. Thus the statement \u0026ldquo;the probability of water is 0.7\u0026rdquo; would mean that the observable is 0.7, i.e., it would express the partial knowledge of the observer, and not have any bearing on the (presumably fully deterministic) underlying process which is a (presumed) exact function of angular momentum, and other exact classical properties.\nQuestions of Medium Complexity (Ch2) 2M1 Recall the globe tossing model from the chapter. Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for \\(p\\).\n W, W, W W, W, W, L L, W, W, L, W, W, W  Solution 1nPoints\u0026lt;-50 2# Grid 3pGrid\u0026lt;-seq(0,1,length.out=nPoints) 4# Prior 5prior\u0026lt;-rep(1,nPoints) 6# Likelihood for each grid point 7likelihood\u0026lt;-pGrid %\u0026gt;% dbinom(3,3,prob=.) 8noStdPosterior\u0026lt;-likelihood*prior 9# Posterior 10posterior\u0026lt;- noStdPosterior / sum(noStdPosterior) Now we can visualize this.\n1tibble(pGrid,posterior) %\u0026gt;% ggplot(aes(x=pGrid,y=posterior))+ 2geom_line(size=3)+geom_point(size=5,color=\u0026#34;red\u0026#34;)+ 3labs( 4title=\u0026#34;Globe Tosses\u0026#34;, 5subtitle=\u0026#34;2M1.1)W W W\u0026#34;, 6y=\u0026#34;Posterior\u0026#34;, 7x=\u0026#34;Grid Approximation points\u0026#34; 8)+ 9scale_x_continuous(breaks = seq(0,1,length.out=10), 10labels = seq(0,1,length.out=10) %\u0026gt;% sprintf(\u0026#34;%.2f\u0026#34;,.), 11expand = c(0, 0))+ 12theme( 13plot.title.position = \u0026#34;plot\u0026#34;, 14)  For the remaining parts we will use a more abbreviated solution.\n1tibble(pGrid, 2li2=pGrid %\u0026gt;% dbinom(3,4,prob=.), 3prior) %\u0026gt;% mutate(post2unstd=li2*prior) %\u0026gt;% mutate(posterior=post2unstd/sum(post2unstd)) %\u0026gt;% ggplot(aes(x=pGrid,y=posterior))+ 4geom_line(size=3)+geom_point(size=5,color=\u0026#34;red\u0026#34;)+ 5labs( 6title=\u0026#34;Globe Tosses\u0026#34;, 7subtitle=\u0026#34;2M1.2) W W W L\u0026#34;, 8y=\u0026#34;Posterior\u0026#34;, 9x=\u0026#34;Grid Approximation points\u0026#34; 10)+ 11scale_x_continuous(breaks = seq(0,1,length.out=10), 12labels = seq(0,1,length.out=10) %\u0026gt;% sprintf(\u0026#34;%.2f\u0026#34;,.), 13expand = c(0, 0))+ 14theme( 15plot.title.position = \u0026#34;plot\u0026#34; 16)  For the final part, note that since the observations are independent, the ordering is irrelevant.\n1tibble(pGrid, 2li2=pGrid %\u0026gt;% dbinom(5,7,prob=.), 3prior) %\u0026gt;% mutate(post2unstd=li2*prior) %\u0026gt;% mutate(posterior=post2unstd/sum(post2unstd)) %\u0026gt;% ggplot(aes(x=pGrid,y=posterior))+ 4geom_line(size=3)+geom_point(size=5,color=\u0026#34;red\u0026#34;)+ 5labs( 6title=\u0026#34;Globe Tosses\u0026#34;, 7subtitle=\u0026#34;2M1.2) L W W L W W W\u0026#34;, 8y=\u0026#34;Posterior\u0026#34;, 9x=\u0026#34;Grid Approximation points\u0026#34; 10)+ 11scale_x_continuous(breaks = seq(0,1,length.out=10), 12labels = seq(0,1,length.out=10) %\u0026gt;% sprintf(\u0026#34;%.2f\u0026#34;,.), 13expand = c(0, 0))+ 14theme( 15plot.title.position = \u0026#34;plot\u0026#34; 16)  2M2 Now assume a prior for \\(p\\) that is equal to zero when \\(p \u0026lt; 0.5\\) and is a positive constant when \\(p\\geq 0.5\\). Again compute and plot the grid approximate posterior distribution for each of the sets of observations in the problem just above.\n W, W, W W, W, W, L L, W, W, L, W, W, W  Solution We proceed in much the same way as in the previous question. We also use the vectorized ifelse instead of explicitly using a for loop.\n1nPoints\u0026lt;-50 2## Grid 3pGrid\u0026lt;-seq(0,1,length.out=nPoints) 4## Prior 5prior\u0026lt;-ifelse(pGrid\u0026lt;0.5,0,1) 1tibble(pGrid, 2li2=pGrid %\u0026gt;% dbinom(3,3,prob=.), 3prior) %\u0026gt;% mutate(post2unstd=li2*prior) %\u0026gt;% mutate(posterior=post2unstd/sum(post2unstd)) %\u0026gt;% ggplot(aes(x=pGrid,y=posterior))+ 4geom_line(size=3)+geom_point(size=5,color=\u0026#34;red\u0026#34;)+ 5labs( 6title=\u0026#34;Globe Tosses with Prior information\u0026#34;, 7subtitle=\u0026#34;2M2.1) W W W\u0026#34;, 8y=\u0026#34;Posterior\u0026#34;, 9x=\u0026#34;Grid Approximation points\u0026#34; 10)+ 11scale_x_continuous(breaks = seq(0,1,length.out=10), 12labels = seq(0,1,length.out=10) %\u0026gt;% sprintf(\u0026#34;%.2f\u0026#34;,.), 13expand = c(0, 0))+ 14theme( 15plot.title.position = \u0026#34;plot\u0026#34; 16)  1tibble(pGrid, 2li2=pGrid %\u0026gt;% dbinom(3,4,prob=.), 3prior) %\u0026gt;% mutate(post2unstd=li2*prior) %\u0026gt;% mutate(posterior=post2unstd/sum(post2unstd)) %\u0026gt;% ggplot(aes(x=pGrid,y=posterior))+ 4geom_line(size=3)+geom_point(size=5,color=\u0026#34;red\u0026#34;)+ 5labs( 6title=\u0026#34;Globe Tosses with Prior information\u0026#34;, 7subtitle=\u0026#34;2M2.2) W W W L\u0026#34;, 8y=\u0026#34;Posterior\u0026#34;, 9x=\u0026#34;Grid Approximation points\u0026#34; 10)+ 11scale_x_continuous(breaks = seq(0,1,length.out=10), 12labels = seq(0,1,length.out=10) %\u0026gt;% sprintf(\u0026#34;%.2f\u0026#34;,.), 13expand = c(0, 0))+ 14theme( 15plot.title.position = \u0026#34;plot\u0026#34; 16)  1tibble(pGrid, 2li2=pGrid %\u0026gt;% dbinom(5,7,prob=.), 3prior) %\u0026gt;% mutate(post2unstd=li2*prior) %\u0026gt;% mutate(posterior=post2unstd/sum(post2unstd)) %\u0026gt;% ggplot(aes(x=pGrid,y=posterior))+ 4geom_line(size=3)+geom_point(size=5,color=\u0026#34;red\u0026#34;)+ 5labs( 6title=\u0026#34;Globe Tosses with Prior information\u0026#34;, 7subtitle=\u0026#34;2M2.3) L W W L W W W\u0026#34;, 8y=\u0026#34;Posterior\u0026#34;, 9x=\u0026#34;Grid Approximation points\u0026#34; 10)+ 11scale_x_continuous(breaks = seq(0,1,length.out=10), 12labels = seq(0,1,length.out=10) %\u0026gt;% sprintf(\u0026#34;%.2f\u0026#34;,.), 13expand = c(0, 0))+ 14theme( 15plot.title.position = \u0026#34;plot\u0026#34; 16)  2M4 Suppose you have a deck with only three cards. Each card has two sides, and each side is either black or white. One card has two black sides. The second card has one black and one white side. The third card has two white sides. Now suppose all three cards are placed in a bag and shuffled. Someone reaches into the bag and pulls out a card and places it flat on a table. A black side is shown facing up, but you don\u0026rsquo;t know the color of the side facing down. Show that the probability that the other side is also black is 2/3. Use the counting method (Section 2 of this chapter) to approach this problem. This means counting up the ways that each card could produce the observed data (a black side facing up on the table).\nSolution Let us begin by defining what is given to us.\n There are three cards  One has a black side and a white side (cBW) One is colored black on both sides (cBB) One is colored white on both sides (cWW)    Our total probability universe is defined by all possible states defined by the enumeration of possible states for each card and their combinations. In other words, it is a universe defined by color and the number of cards.\nThe question posed is essentially, given a single observation, that is that a random draw from our universe has produced a black side (note that we already know that there is one card so it satisfies the requirements of being a valid state of the universe we are considering), what is the probability of the other side being black as well?\nHence we can express this as O: cB? and we need P(B|cB?). We will enumerate possibilities of observing the black side in our universe.\n cBW This has \\(1\\) way of producing cB? cBB This has \\(2\\) ways of producing cB? cWW This has \\(0\\) ways of producing cB?     Card Ways to cB?     cBW 1   cBB 2   cWW 0    So we see that there are \\(3\\) ways to see a black side in a single draw, and two of these come from (cBB), thus the probability of seeing another black side is \\(\\frac{2}{3}\\).\nChapter III: Sampling the Imaginary Easy Questions (Ch3) These questions are associated with the following code snippet for the globe tossing example.\n1p_grid\u0026lt;-seq(from=0, to=1, length.out=1000) 2prior\u0026lt;-rep(1,1000) 3likelihood\u0026lt;-dbinom(6,size=9,prob=p_grid) 4posterior\u0026lt;-likelihood*prior 5posterior\u0026lt;-posterior/sum(posterior) 6set.seed(100) 7samples\u0026lt;-sample(p_grid,prob=posterior,size=1e4,replace=TRUE) 3E1 How much posterior probability lies below \\(p = 0.2\\)?\nSolution We can check the number of samples as follows:\n1ifelse(samples\u0026lt;0.2,1,0) %\u0026gt;% sum(.) 1[1] 4 Now we will simply divide by the number of samples.\n1ifelse(samples\u0026lt;0.2,1,0) %\u0026gt;% sum(.)/1e4 1[1] 4e-04 More practically, the percentage of the probability density below \\(p=0.2\\) is:\n1ifelse(samples\u0026lt;0.2,1,0) %\u0026gt;% sum(.)/1e4 *100 1[1] 0.04 3E2 How much posterior probability lies above \\(p = 0.8\\)?\nSolution 1ifelse(samples\u0026gt;0.8,1,0) %\u0026gt;% sum(.)/1e4 *100 1[1] 11.16 3E3 How much posterior probability lies between \\(p = 0.2\\) and \\(p = 0.8\\)?\nSolution 1ifelse(samples\u0026gt;0.2 \u0026amp; samples\u0026lt;0.8,1,0) %\u0026gt;% sum(.)/1e4 *100 1[1] 88.8 3E4 20% of the posterior probability lies below which value of \\(p\\)?\nSolution 1samples %\u0026gt;% quantile(0.2) 120% 20.5185185 3E5 20% of the posterior probability lies above which value of \\(p\\)?\nSolution 1samples %\u0026gt;% quantile(0.8) 180% 20.7557558 Questions of Medium Complexity (Ch3) 3M1 Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.\nSolution 1nPoints\u0026lt;-1000 2## Grid 3pGrid\u0026lt;-seq(0,1,length.out=nPoints) 4## Prior 5prior\u0026lt;-rep(1,nPoints) 1tibble(pGrid, 2li2=pGrid %\u0026gt;% dbinom(8,15,prob=.), 3prior) %\u0026gt;% mutate(post2unstd=li2*prior) %\u0026gt;% mutate(posterior=post2unstd/sum(post2unstd)) %\u0026gt;% ggplot(aes(x=pGrid,y=posterior))+ 4geom_line(size=3)+geom_point(size=5,color=\u0026#34;red\u0026#34;)+ 5labs( 6title=\u0026#34;Globe Tosses with a Flat Prior\u0026#34;, 7subtitle=\u0026#34;3M1) 8W in 15 tosses\u0026#34;, 8y=\u0026#34;Posterior\u0026#34;, 9x=\u0026#34;Grid Approximation points\u0026#34; 10)+ 11scale_x_continuous(breaks = seq(0,1,length.out=10), 12labels = seq(0,1,length.out=10) %\u0026gt;% sprintf(\u0026#34;%.2f\u0026#34;,.), 13expand = c(0, 0))+ 14theme( 15plot.title.position = \u0026#34;plot\u0026#34; 16)  3M2 Draw 10,000 samples from the grid approximation from above. Then use the samples to calculate the 90% HPDI for \\(p\\).\nSolution For this, we will create a tibble of the experiment and then sample from it.\n1exp3m2\u0026lt;-tibble(pGrid, 2li2=pGrid %\u0026gt;% dbinom(8,15,prob=.), 3prior) %\u0026gt;% mutate(post2unstd=li2*prior) %\u0026gt;% mutate(posterior=post2unstd/sum(post2unstd)) 45sample(pGrid,prob=exp3m2$posterior,size=1e4,replace=TRUE) %\u0026gt;% HPDI(prob=0.9) 12|0.9 0.9| 30.3293293 0.7167167 3M3 Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in \\(p\\). What is the probability of observing 8 water in 15 tosses.\nSolution As covered in the chapter, we will sample from the posterior distribution and use that to obtain experiment instances with rbinom. Finally we will then use these instances to count our way to the probability of observing the true data.\n1waterPred\u0026lt;-sample(pGrid,prob=exp3m2$posterior,size=1e4,replace=TRUE) %\u0026gt;% rbinom(1e4,size=15,prob=.) 2ifelse(waterPred==8,1,0) %\u0026gt;% sum(.)/1e4 1[1] 0.1447 This seems like a very poor model, but the spread of the realizations is probably a better measure.\n1waterPred %\u0026gt;% summary 1Min. 1st Qu. Median Mean 3rd Qu. Max. 20.000 6.000 8.000 7.936 10.000 15.000 This does seem to indicate that at any rate the realizations are close enough to 8 for the model to be viable. A visualization of this should also be checked.\n1waterPred %\u0026gt;% simplehist  3M4 Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses\nSolution We will reuse the posterior, and recalculate our samples (corresponding to new possible instances).\n1waterPred\u0026lt;-sample(pGrid,prob=exp3m2$posterior,size=1e4,replace=TRUE) %\u0026gt;% rbinom(1e4,size=9,prob=.) 1ifelse(waterPred==6,1,0) %\u0026gt;% sum(.)/1e4 2waterPred %\u0026gt;% summary 1[1] 0.1723 23Min. 1st Qu. Median Mean 3rd Qu. Max. 40.000 4.000 5.000 4.777 6.000 9.000 Though the numerical value is not very different from the previous solution, we note that the centrality measures are much worse.\n1waterPred %\u0026gt;% simplehist  3M6 Suppose you want to estimate the Earth\u0026rsquo;s proportion of water very precisely. Specifically, you want the 99% percentile interval of the posterior distribution of p to be only 0.05 wide. This means the distance between the upper and lower bound of the interval should be 0.05. How many times will you have to toss the globe to do this?\nSolution Since the no information of the model has been provided here, we will consider reason out an approach, before making an estimate.\nThe number globe tosses corresponds to the number of observations we require. A percentile interval this narrow will essentially require a large number of samples i.e. be in the large number limit, so the choice of prior should not matter much. The width of our interval should also be related to the number of grid points we use in our approximation.\nWe will first use the true amount of water on the planet (approximately 71 percent) to generate information for the number of throws.\nLet us generate observations.\n1nThrows\u0026lt;-10000 2nWater\u0026lt;-0.71*nThrows %\u0026gt;% round We will set up a simple model for this, with an indifferent prior along with a better prior.\n1nPoints\u0026lt;-1000 2## Grid 3pGrid\u0026lt;-seq(0,1,length.out=nPoints) 4## Prior 5prior\u0026lt;-rep(1,nPoints) 6betterPrior\u0026lt;-ifelse(pGrid\u0026lt;0.5,0,1) We will define a function for generating our results since we will need to perform this a few times.\n1genModel\u0026lt;- function(nThrows){ 2nWater\u0026lt;-0.71*nThrows %\u0026gt;% round 3tibble(pGrid, 4li2=pGrid %\u0026gt;% dbinom(nWater,nThrows,prob=.), 5prior,betterPrior) %\u0026gt;% 6mutate( 7postUnifNSD=li2*prior, 8postUnif=postUnifNSD/sum(postUnifNSD), 9smplUnif=sample(pGrid, 10prob=postUnif, 11size=nrow(.), 12replace=TRUE), 13predUnif=rbinom(nrow(.),size=9,prob=smplUnif), 14postBPNSD=li2*betterPrior, 15postBP=postBPNSD/sum(postBPNSD), 16smplBP=sample(pGrid, 17prob=postBP, 18size=nrow(.), 19replace=TRUE), 20predBP=rbinom(nrow(.),size=9,prob=smplBP) 21) 22} It would be nice to look at the different priors as well.\n1genModel2(1e4) %\u0026gt;% .$predBP %\u0026gt;% simplehist  1genModel(1e6) %\u0026gt;% .$predUnif %\u0026gt;% simplehist  As can be expected, with the large number of observations, the priors barely make a difference.\nWe note that what we are looking for is a credibility width of less than 0.05 at a probability of 0.99.\n1get99Width\u0026lt;- function(x,nx) { 2piInter\u0026lt;-PI(x$postUnif,prob=0.99) 3hpdiInter\u0026lt;-HPDI(x$postUnif,prob=0.99) 4tibble(wPI=piInter[2]-piInter[1],wHPDI=hpdiInter[2]-hpdiInter[1],nSamples=nx) 5} Now we are in a position to start testing samples.\n1base100\u0026lt;-genModel(1e2) %\u0026gt;% get99Width(1e2) 2t\u0026lt;-1000 3while(min(base100$wPI) \u0026amp; min(base100$wHPDI) \u0026gt; 0.005) { 4t\u0026lt;-t*10 5base100\u0026lt;-genModel(t) %\u0026gt;% get99Width(t) %\u0026gt;% bind_rows(base100) 6} 7base100 %\u0026gt;% toOrg 12| wPI | wHPDI | nSamples | 3|---------------------+---------------------+----------| 4| 0.0464554168997065 | 0.00121611594937107 | 1e+05 | 5| 0.0735809517149284 | 0.0511200696528017 | 10000 | 6| 0.00884431494702241 | 0.0088121623437198 | 100 | There is an inherent problem with this formulation, and that is that the model confidence is based on the observations which were drawn to emulate the true distribution of water on Earth. This means that a highly tight width, should be recognized to still be highly dependent on the observed data. Let us try to obtain the same intervals with a randomized observation setup instead.\nIn order to do this we simply modify the number of water observations.\n1genModel2\u0026lt;- function(nThrows){ 2tibble(pGrid, 3li2=pGrid %\u0026gt;% dbinom(sample(1:nThrows, 1, replace=TRUE),nThrows,prob=.), 4prior,betterPrior) %\u0026gt;% 5mutate( 6postUnifNSD=li2*prior, 7postUnif=postUnifNSD/sum(postUnifNSD), 8smplUnif=sample(pGrid, 9prob=postUnif, 10size=nrow(.), 11replace=TRUE), 12predUnif=rbinom(nrow(.),size=9,prob=smplUnif), 13postBPNSD=li2*betterPrior, 14postBP=postBPNSD/sum(postBPNSD), 15smplBP=sample(pGrid, 16prob=postBP, 17size=nrow(.), 18replace=TRUE), 19predBP=rbinom(nrow(.),size=9,prob=smplBP) 20) 21} We can now test these the same way.\n1base100\u0026lt;-genModel2(100) %\u0026gt;% get99Width(100) 2t\u0026lt;-100 3while(min(base100$wPI) \u0026amp; min(base100$wHPDI) \u0026gt; 0.05) { 4t\u0026lt;-t+100 5base100\u0026lt;-genModel2(t) %\u0026gt;% get99Width(t) %\u0026gt;% bind_rows(base100) 6} 7base100 %\u0026gt;% toOrg 12| wPI | wHPDI | nSamples | 3|---------------------+---------------------+----------| 4| 0.00853874459759265 | 0.00851020493831213 | 100 | Chapter IV: Geocentric Models Easy Questions (Ch4) 4E1 In the model definition below, which line is the likelihood?\nSolution The likelihood is defined by the first line, that is, \\(y_i \\sim\\mathrm{Normal}(\\mu, \\sigma)\\)\n4E2 In the model definition above, how many parameters are in the posterior distribution?\nSolution The model has two parameters for the posterior distribution, \\(\\mu\\) and \\(\\sigma\\).\n4E3 Using the model definition above, write down the appropriate form of Bayes\u0026rsquo; theorem that includes the proper likelihood and priors.\nSolution The appropriate form of Bayes\u0026rsquo; theorem in this case is:\n\\[ \\mathrm(Pr)(\\mu,\\sigma|y)=\\frac{\\mathrm{Normal}(y|\\mu,\\sigma)\\mathrm{Normal}(\\mu|0,10)\\mathrm{Exponential}(\\sigma|1)}{\\int\\int \\mathrm{Normal}(y|\\mu,\\sigma)\\mathrm{Normal}(\\mu|0,10)\\mathrm{Exponential}(\\sigma|1)d\\mu d\\sigma} \\]\n4E4 In the model definition below, which line is the linear model?\nSolution The second line is the linear model in the definition, that is: \\[\\mu_i = \\alpha + \\beta x_i\\]\n4E5 In the model definition just above, how many parameters are in the posterior distribution?\nSolution The model defined has three independent parameters for the posterior distribution, which are \\(\\alpha\\), \\(\\beta\\) and \\(\\sigma\\). Though \\(\\mu\\) is a parameter, it is defined in terms of \\(\\alpha\\), \\(\\beta\\) and \\(x\\) so will not be considered to be a parameter for the posterior.\nQuestions of Medium Complexity (Ch4) 4M1 For the model definition below, simulate observed \\(y\\) values from the prior (not the posterior).\nSolution Sampling from the prior involves averaging over the prior distributions of \\(\\mu\\) and \\(\\sigma\\).\n1muPrior\u0026lt;-rnorm(1e4,0,10) 2sigmaPrior\u0026lt;-rexp(1e4,1) 3hSim\u0026lt;-rnorm(1e4,muPrior,sigmaPrior) We can visualize this as well.\n1hSim %\u0026gt;% qplot(binwidth=0.8)  4M2 Translate the model just above into a quap formula.\nSolution 1ex4m2\u0026lt;-alist( 2y~dnorm(mu,sigma), 3mu~dnorm(0,10), 4sigma~dexp(1) 5) 4M3 Translate the quap model formula below into a mathematical model definition\nSolution The model defined can be expressed mathematically as:\n4M4 A sample of students is measured for height each year for 3y ears. After the third year,you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors.\nSolution Let us first declare a model.\nWhere the double subscript is meant to indicate that the data is obtained both per year and per student. That is, we have:\n \\(y_{ij}\\) is the height of each student each year \\(x_{j}\\) is the \u0026ldquo;reduced\u0026rdquo; year, that is the difference between a particular year and the average sample year  The prior distributions are not too complicated. Since the distribution of males and females in the student population is missing, as is information on the age distribution, we will set a conservative lower value of 120 centimeters, based on the understanding that the students have at-least 3 years in school, so they are growing, so we assume students grow around 8 centimeters every year.\nThe distribution for beta is a log-normal distribution to ensure that we do not sample negative values which might arise from sampling a Gaussian. This is due to the assumption that the students grow every year.\nFollowing the procedures of the chapter, we will visualize realizations from the prior.\n1nDraws\u0026lt;-50 2a\u0026lt;-rnorm(nDraws,100,8) 3b\u0026lt;-rlnorm(nDraws,0,2) 4sigmaPrior\u0026lt;-rexp(nDraws,1) We will express our yearly function:\n1genYr\u0026lt;- function(x,yr) {rnorm(1,a[x]+b[x]*(yr-1.5),sigmaPrior[x])} Now we will assume a class size of 35 for our priors.\n1testDat\u0026lt;-tibble(student=1:36) 2testDat$year1\u0026lt;-sapply(testDat$student,FUN=genYr,yr=1) 3testDat$year2\u0026lt;-sapply(testDat$student,FUN=genYr,yr=2) 4testDat$year3\u0026lt;-sapply(testDat$student,FUN=genYr,yr=3) We will now look at this.\n1testDat %\u0026gt;% pivot_longer(-student,names_to = \u0026#34;year\u0026#34;,values_to = \u0026#34;height\u0026#34;) %\u0026gt;% ggplot(aes(x=student,y=height,color=year))+ 2geom_line(size=4,alpha=0.4)+ 3geom_point(size=4,colour=\u0026#34;blue\u0026#34;)+ 4labs( 5title=\u0026#34;Student Height variation\u0026#34;, 6subtitle=\u0026#34;By year\u0026#34;, 7y=\u0026#34;Height\u0026#34;, 8x=\u0026#34;Student\u0026#34; 9)+ 10theme( 11plot.title.position = \u0026#34;plot\u0026#34;, 12)  1testDat %\u0026gt;% pivot_longer(-student,names_to = \u0026#34;year\u0026#34;,values_to = \u0026#34;height\u0026#34;) %\u0026gt;% ggplot(aes(x=year,y=height,color=student))+ 2geom_line(size=4,alpha=0.4)+ 3geom_point(size=4,colour=\u0026#34;blue\u0026#34;)+ 4labs( 5title=\u0026#34;Student Height variation\u0026#34;, 6subtitle=\u0026#34;By year\u0026#34;, 7y=\u0026#34;Height\u0026#34;, 8x=\u0026#34;Student\u0026#34; 9)+ 10theme( 11plot.title.position = \u0026#34;plot\u0026#34;, 12)  This seems to be a pretty reasonable model, all things considered.\n4M5 Now suppose I remind you that every student got taller each year. Does this information lead you to change your choice of priors? How?\nSolution Since we have incorporated a LogNormal term, we do not need to change our choice of prior. However, the parameters of our previous model did include some heights decreasing, so we will modify the LogNormal distribution on beta. Recall that we would like to see around 7 centimeters of growth, so our mean should be exp(1+2/2) which is around 7.\nSimulating draws as before.\n1nDraws\u0026lt;-50 2a\u0026lt;-rnorm(nDraws,100,8) 3b\u0026lt;-rlnorm(nDraws,1,2) 4sigmaPrior\u0026lt;-rexp(nDraws,1) 5genYr\u0026lt;- function(x,yr) {rnorm(1,a[x]+b[x]*(yr-1.5),sigmaPrior[x])} 6testDat\u0026lt;-tibble(student=1:36) 7testDat$year1\u0026lt;-sapply(testDat$student,FUN=genYr,yr=1) 8testDat$year2\u0026lt;-sapply(testDat$student,FUN=genYr,yr=2) 9testDat$year3\u0026lt;-sapply(testDat$student,FUN=genYr,yr=3) 1testDat %\u0026gt;% pivot_longer(-student,names_to = \u0026#34;year\u0026#34;,values_to = \u0026#34;height\u0026#34;) %\u0026gt;% ggplot(aes(x=student,y=height,color=year))+ 2geom_line(size=4,alpha=0.4)+ 3geom_point(size=4,colour=\u0026#34;blue\u0026#34;)+ 4labs( 5title=\u0026#34;Student Height variation\u0026#34;, 6subtitle=\u0026#34;By year with better priors\u0026#34;, 7y=\u0026#34;Height\u0026#34;, 8x=\u0026#34;Student\u0026#34; 9)+ 10theme( 11plot.title.position = \u0026#34;plot\u0026#34;, 12)  4M6 Now suppose I tell you that the variance among heights for students of the same age is never more than 64cm. How does this lead you to revise your priors?\nSolution This information will change the variance term in our model. We will incorporate this into our model by using a uniform distribution for sigma. The new model is then:\nSimulating draws as before.\n1nDraws\u0026lt;-50 2a\u0026lt;-rnorm(nDraws,100,8) 3b\u0026lt;-rlnorm(nDraws,1,2) 4sigmaPrior\u0026lt;-runif(nDraws,0,8) 5genYr\u0026lt;- function(x,yr) {rnorm(1,a[x]+b[x]*(yr-1.5),sigmaPrior[x])} 6testDat\u0026lt;-tibble(student=1:36) 7testDat$year1\u0026lt;-sapply(testDat$student,FUN=genYr,yr=1) 8testDat$year2\u0026lt;-sapply(testDat$student,FUN=genYr,yr=2) 9testDat$year3\u0026lt;-sapply(testDat$student,FUN=genYr,yr=3) We will now look at this.\n1testDat %\u0026gt;% pivot_longer(-student,names_to = \u0026#34;year\u0026#34;,values_to = \u0026#34;height\u0026#34;) %\u0026gt;% ggplot(aes(x=year,y=height,color=student))+ 2geom_line(size=4,alpha=0.4)+ 3geom_point(size=4,colour=\u0026#34;blue\u0026#34;)+ 4labs( 5title=\u0026#34;Student Height variation\u0026#34;, 6subtitle=\u0026#34;By year with additional information\u0026#34;, 7y=\u0026#34;Height\u0026#34;, 8x=\u0026#34;Student\u0026#34; 9)+ 10theme( 11plot.title.position = \u0026#34;plot\u0026#34;, 12)  1testDat %\u0026gt;% pivot_longer(-student,names_to = \u0026#34;year\u0026#34;,values_to = \u0026#34;height\u0026#34;) %\u0026gt;% mutate(year=year %\u0026gt;% as.factor %\u0026gt;% as.numeric) %\u0026gt;% filter(student==3) %\u0026gt;% ggplot(aes(x=year,y=height,color=student))+ 2geom_line(size=4,alpha=0.4)+ 3geom_point(size=4,colour=\u0026#34;blue\u0026#34;)+ 4labs( 5title=\u0026#34;Student Height variation with additional information\u0026#34;, 6subtitle=\u0026#34;By year\u0026#34;, 7y=\u0026#34;Height\u0026#34;, 8x=\u0026#34;year\u0026#34; 9)+ 10theme( 11plot.title.position = \u0026#34;plot\u0026#34;, 12) This is now much more reasonable.\n4M7 Refit model m4.3 from the chapter, but omit the mean weight xbar this time. Compare the new model\u0026rsquo;s posterior to that of the original model. In particular, look at the covariance among the parameters. What is different? Then compare the posterior predictions of both models.\nSolution Let us re-create the original data model first.\n1data(Howell1) 2howDat\u0026lt;-Howell1 3howDat\u0026lt;-howDat %\u0026gt;% filter(age\u0026gt;=18) Recall that the original model was given by:\n1xbar\u0026lt;-mean(howDat$weight) 2m43\u0026lt;-quap( 3alist( 4height ~ dnorm(mu,sigma), 5mu\u0026lt;-a+b*(weight-xbar), 6a ~ dnorm(178,20), 7b ~ dlnorm(0,1), 8sigma ~ dunif(0,50) 9),data=howDat 10) 11m43 %\u0026gt;% precis %\u0026gt;% toOrg    row.names mean sd 5.5% 94.5%     a 154.601366268055 0.270307670586024 154.169362403256 155.033370132855   b 0.90328084058895 0.041923632631821 0.83627877851613 0.970282902661771   sigma 5.07188106954248 0.191154797986941 4.76637878273641 5.37738335634854    Now we can incorporate the new information we have.\n1m43b\u0026lt;-quap( 2alist( 3height ~ dnorm(mu,sigma), 4mu\u0026lt;-a+b*weight, 5a ~ dnorm(178,20), 6b ~ dlnorm(0,1), 7sigma ~ dunif(0,50) 8),data=howDat 9) 10m43b %\u0026gt;% precis %\u0026gt;% toOrg    row.names mean sd 5.5% 94.5%     a 114.515009160638 1.89397291870405 111.488074634765 117.54194368651   b 0.891112682994587 0.0416750085549524 0.824507970215838 0.957717395773337   sigma 5.06263423083996 0.190299406826185 4.75849902431897 5.36676943736095    Thus we note that the slope is the same, while the intercept changes. Now we need to check the covariances. We will convert from the variable covariance scale to the correlation matrix.\n1m43b %\u0026gt;% vcov %\u0026gt;% cov2cor %\u0026gt;% round(.,2) 1a b sigma 2a 1.00 -0.99 0.02 3b -0.99 1.00 -0.02 4sigma 0.02 -0.02 1.00 1m43 %\u0026gt;% vcov %\u0026gt;% cov2cor %\u0026gt;% round(.,2) 1a b sigma 2a 1 0 0 3b 0 1 0 4sigma 0 0 1 The contrast between the two is very clear from the correlation matrices, the new model has an almost perfect negatively correlated intercept and slope.\n1m43 %\u0026gt;% pairs  1m43b %\u0026gt;% pairs  This indicates that the scaling of the variables leads to a completely different model in terms of the parameter relationships, in-spite of the fact that the models have almost the same posterior predictions.\n  Summer of 2020\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/sr2-ch2-ch3-ch4/","tags":["solutions","R","SR2"],"title":"  \"SR2 :: Solutions for Chapters {2,3,4}\"\n  "},{"categories":["programming"],"contents":"Background  I recently read this post written by the now deceased Prof. David MacKay 1 It should be read widely, however, given that it is distributed as a ps.gz which is then a .ps file, and thus probably inaccessible to many of the people who should read it, I decided to rework it for native online consumption (there is also a pdf) THIS IS NOT MY CONTENT 2 Now, enjoy the post  Everyone Should Get an A Imagine a University – call it Camwick – where all students arrive with straight A grades. They are successful, enthusiastic, and curious. By the time they leave, only one third still receive straight As. The other two thirds get lower grades, do not enjoy their studies, and are not fun to teach. Is Camwick University a success? Camwick could point to its excellent teaching assessment scores and argue that it is ‘adding value’: students emerge knowing more. Future employers love the University’s policy of assigning grades – the University ranks its students, saving companies the bother of assessing job applicants themselves. But should a University be a sorting service? Isn’t something wrong with an institution that takes in mainly A-quality input and turns out less than half A-quality output? If a University fails to turn out as much A-quality enthusiasts as come in, is it in fact a place of intellectual destruction, throwing away the potential of the majority of its students? What are the roots of this destruction?\nExams I would recommend that Camwick consider abolishing traditional exams. In the current system, Camwick teaches Anna, Bob, and Charlie, who are all smart, then examines them; Anna comes \u0026rsquo;top\u0026rsquo;, Bob \u0026lsquo;second\u0026rsquo; and Charlie \u0026rsquo;third\u0026rsquo;. Perhaps Charlie, given a little more time, would have figured out the material, but he wasn\u0026rsquo;t quite ready when the exam arrived - perhaps because other courses consumed his attention.\nBob\u0026rsquo;s response to his \u0026lsquo;failure\u0026rsquo; is to adopt strategies of tlittle educational value: he parrot learns, he crams, and he asks lecturers to tell him what\u0026rsquo;s going to be on the exam. The exams become the focus of attention, even though the purpose of Bob\u0026rsquo;s going to the University was learning.\nCharlie\u0026rsquo;s response is to give up on doing \u0026lsquo;well\u0026rsquo;, and coast through University, no longer understanding everything. He loses self-worth and resents the University for making him feel bad.\nSome courses at Camwick assign grades using continuous assessment instead of exams. But continuous assessment has the same effect as exams on Bob and Charlie. So course grades based on continuous assessment should be abolished at the same time as exams.\n Figure 1: Everyone can get an A, regardless of learning rate, if their education is not halted by exams. Traditional system on the left, with an educational system on the right.\n  If Camwick had no exams, the focus of attention would have to be elsewhere. How about education, for example? Students could spend their time at Camwick exploring subjects that interest them, and attending classes that offer something they want to know about, free from the stress and misdirection of the exam system. Lecturers would at all times be friends rather than adversaries. [When I was an undergraduate at Cambridge, I asked a physis lecturer to clarify topic \\(N\\), which I felt had not been covered clearly. His response: \u0026ldquo;That\u0026rsquo;s what I love about \\(N\\): some students get it, some don\u0026rsquo;t - so we get beautiful bell shaped curves in the exam.\u0026rdquo;]\nOf course the extreme suggestion of abolishing all exams will not go down well: \u0026ldquo;What about standards?\u0026rdquo; \u0026ldquo;How can we get funding if we do not\u0026rdquo; \u0026ldquo;How do we award degrees that people will respect?\u0026rdquo; Traditionalists might say that students appreciate exams for the targets and feedback. Well, there\u0026rsquo;s nothing to stop us giving students targets or feedback. We can provide events just like exams, if students want them - self-administered tests, for example, would allow students to check how well they have assimilated all the material in a course. Other systems of targets and feedback that students enjoy include project work, problem-based learning, and portfolio-based assessment.\nAs a compromise, let\u0026rsquo;s modify our proposal a little: Camwick should become a place where the only achievable grade is an A. I\u0026rsquo;m not recommending that we simply give everyone an A. It\u0026rsquo;s a crime to let standards slip. When I say everyone should get an A, I mean that everyone should be allowed to get to an A.\nThink back to Alice, Bob, and Charlie. Alice grasped most of the material in the course and achieved an A. Given a little more time and little less stress, Bob and Charlie could probably have grasped it all too, and become equally strong masters of the material. What good does it do Bob and Charlie to record the fact that they were a little slower than alice? Wouldn\u0026rsquo;t it have been better, educationally, to give Bob and Charlie a little more time and help, so that they achieved the same A standard?\nDoes a bus-driver-training school rank its graduating drivers? No, it ensures that all attain the standard required of a bus-driver. Would you like to be treated by a C-grade doctor? No, everyone wants an A-grade doctor! So doctors and drivers are (I hope!) trained and trained and not let out until they are A-grade in standard. Why should other professions be treated differently?\nFigure 1a shows the command of the material of each student as a function of time in the traditional system. A traditional exam interrupts the learning process, and Bob and Charlie are recorded as having achieved a lower standard. Figure 1b shows the same students in an exam-free system, assuming they learn at the same rate as in the old system. Each student takes a different time to achieve full command of the course material. Every student has the satisfaction of achieving full command of the material.\n Figure 2: Everyone can get an A, regardless of learning rate, if their education is not halted by exams. Traditional system on the left, with an educational system on the right.\n  The difference between the two systems is also striking if we assume that students start the course at different levels of ability. In Figure 2, albert comes from a privileged background and already knows half the course material when he arrives. Brenda and Catharine arrive at a lower educational level. Brenda and Catharine are actually faster learners than Albert, but, as Figure 2a shows, the traditional exam system rewards Albert with the A grade (\u0026lsquo;congratulations, you started first!\u0026rsquo;), and brands Brenda and Catharine failures. In the \u0026lsquo;Only A-grades\u0026rsquo; system, everyone attains an A-grade in due course; and Albert isn\u0026rsquo;t actually first to finish.\nThe information about \u0026lsquo;who finished when\u0026rsquo; could in principle be retained in order to provide some sort of student-ranking service to employers, but I would strongly urge the destruction of all such records. Only the achieving of an A grade should be recorded, nothing else. Why?\n Because being ranked creates stress. Because students who are competing with each other for ranks may be reluctant to help each other learn. In contrast, in the \u0026lsquo;Only A-grades\u0026rsquo; system, the top students lose nothing if they help their peers; indeed, they may gain in several ways: peer-teaching strengthens the students\u0026rsquo; grasp on material, and often speeds up the whole class. Evidence that a student is a quick learner may well make itself evident in her transcript without rankings being made: Alice, covering material quickly, will have time to take extra courses. So in one year she\u0026rsquo;ll accumulate a slightly fatter sheaf of A-grade qualifications. What value are rankings? If future employers want students to be formally evaluated, they can pay for an evaluation service. Why ruin a great institution? The very best students might like grades too, as they enjoy being congratulated. But the \u0026lsquo;only A-grades\u0026rsquo; system will congratualte them too.  These ideas are not new, nor are they unprecedented. In many German Universities, first- and second-year courses have no grades, no obligatory coursework, and no obligatory exams. End-of-course exams are provided only as a service to students, to help them find out if they have indeed grasped the material and are ready progress to the next stage.\nIn practice, how should we organize courses so that everyone reaches 100% mastery? For Bob and Charlie\u0026rsquo;s benefit, the average pace probably has to be reduced. Figure 3 shows one way of organizing the material in stages, so that a class is kept together. Whenever Alice has completed the material in a stage, she can spend time on other interests, or can help other members of the class.\n Figure 3: Possible course plan. This scheme assumes that the students have rates of progress ranging from A (fastest) to C (slowest). Every two weeks, a consolidation period is inserted to ensure that C has assimilated all the learning objectives. Alice can use the consolidation period to pursue others interests or act as a peer-teacher.\n  Camwick staff who say \u0026ldquo;we can\u0026rsquo;t possibly cover a full degree course if we reduce the pace!\u0026rdquo; should bear in mind that, had Bob and charlie gone to a less prestigious University, they probably would have got first-class degrees. How can this paradox - going slowing and arriving at almost the same time - be explained? I suspect an important factor is this: struggling students get ever slower if we pile on new material before they have assimilated the old. For example, 2ⁿᵈ-year Lagrangian dynamics is difficult to absorb if one hasn\u0026rsquo;t grasped 1ˢᵗ-year Newtonian dynamics. So the steady linear progress assumed in Figures 1 to 3 is a poor model of Carlie. The more Charlie is left behind, the slower he learns. This means that the true difference in pace between Alice and Charlie need not be very big. If Charlie gets lost and left behind, we are wasting everyone\u0026rsquo;s time by having him sit in classes where new material is presented. A stitch in time saves nine (Figure 4).\n Figure 4: A stitch in time saves nine. Curve C shows Charlie\u0026rsquo;s progress in a course taught at the pace that is ideal for Alice. The more Charlie is left behind, the slower he learns. By the end of the course, there is a big gap between A and C. Curve C′ shows Charlie\u0026rsquo;s progress in a course taught at the pace that is ideal for him. Just a small decrease in class pace allows the big gap between Alice and Charlie to be eliminated.\n  Teaching methods must be modified to ensure that everyone in the class benefits. I advocate interactive teaching: students are asked questions and encouraged to ask questions and to be active participants in their own learning. It\u0026rsquo;s not enough to ask a question and let one person in the class (Alice!) answer it. The whole class must have the chance to think, puzzle and discuss; the teacher must ascertain the level of understanding of the whole class. In large classes, I find Mazur\u0026rsquo;s voting method works well: a lecture is centered on two or three carefully chosen questions with multiple-choice answers. Students discuss a question with their neighbors, then all vote. The vote informs the lecturer whether previous material has been understood. Diversity of votes can seed a useful discussion.\nTo conclude, here are a few further advantages of the educational approach advocated here:\n Happy, curious, and self-motivated students are fun to teach. At present, British students have little choice of university teaching and assessment style: all universities give out grades. Shouldn\u0026rsquo;t we offer them a choice? Some students would like the chance to go to a place with high standards where only A-grades are awarded. If some universities adopt student-centered educational policies and stop ranking students, perhaps these attitudes will spread to schools, with consequent benefits to pupils, and in due course, to universities. Dumbed-down A levels could be replaced by educational programmes that ensure that everyone attains their maximum potential and feels happy about it. Happy graduates who get A grades are likely to become grateful alumni donors.    Also known for the fabulous free book called Information Theory, Inference, and Learning Algorithms\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n If you have a good reason why this should not be distributed here in this manner, please contact me and I will do the needful\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/mackay-all-a/","tags":["academics","teaching","evaluation","ideas"],"title":"  \"Everyone Should Get an A - David MacKay\"\n  "},{"categories":["programming"],"contents":"Chapter VII - Moving Beyond Linearity All the questions are as per the ISL seventh printing of the First edition1.\nCommon 1libsUsed\u0026lt;-c(\u0026#34;dplyr\u0026#34;,\u0026#34;ggplot2\u0026#34;,\u0026#34;tidyverse\u0026#34;, 2\u0026#34;ISLR\u0026#34;,\u0026#34;caret\u0026#34;,\u0026#34;MASS\u0026#34;, \u0026#34;gridExtra\u0026#34;, 3\u0026#34;pls\u0026#34;,\u0026#34;latex2exp\u0026#34;,\u0026#34;data.table\u0026#34;) 4invisible(lapply(libsUsed, library, character.only = TRUE)) Question 7.6 - Page 299 In this exercise, you will further analyze the Wage data set considered throughout this chapter.\n(a) Perform polynomial regression to predict wage using age. Use cross-validation to select the optimal degree d for the polynomial. What degree was chosen, and how does this compare to the results of hypothesis testing using ANOVA? Make a plot of the resulting polynomial fit to the data.\n(b) Fit a step function to predict wage using age, and perform cross-validation to choose the optimal number of cuts. Make a plot of the fit obtained. In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.\nAnswer Lets get the data.\n1set.seed(1984) 2wageDat\u0026lt;-ISLR::Wage 3wageDat %\u0026gt;% str %\u0026gt;% print 1## \u0026#39;data.frame\u0026#39;: 3000 obs. of 11 variables: 2## $ year : int 2006 2004 2003 2003 2005 2008 2009 2008 2006 2004 ... 3## $ age : int 18 24 45 43 50 54 44 30 41 52 ... 4## $ maritl : Factor w/ 5 levels \u0026#34;1. Never Married\u0026#34;,..: 1 1 2 2 4 2 2 1 1 2 ... 5## $ race : Factor w/ 4 levels \u0026#34;1. White\u0026#34;,\u0026#34;2. Black\u0026#34;,..: 1 1 1 3 1 1 4 3 2 1 ... 6## $ education : Factor w/ 5 levels \u0026#34;1. \u0026lt; HS Grad\u0026#34;,..: 1 4 3 4 2 4 3 3 3 2 ... 7## $ region : Factor w/ 9 levels \u0026#34;1. New England\u0026#34;,..: 2 2 2 2 2 2 2 2 2 2 ... 8## $ jobclass : Factor w/ 2 levels \u0026#34;1. Industrial\u0026#34;,..: 1 2 1 2 2 2 1 2 2 2 ... 9## $ health : Factor w/ 2 levels \u0026#34;1. \u0026lt;=Good\u0026#34;,\u0026#34;2. \u0026gt;=Very Good\u0026#34;: 1 2 1 2 1 2 2 1 2 2 ... 10## $ health_ins: Factor w/ 2 levels \u0026#34;1. Yes\u0026#34;,\u0026#34;2. No\u0026#34;: 2 2 1 1 1 1 1 1 1 1 ... 11## $ logwage : num 4.32 4.26 4.88 5.04 4.32 ... 12## $ wage : num 75 70.5 131 154.7 75 ... 13## NULL 1wageDat %\u0026gt;% summary %\u0026gt;% print 1## year age maritl race 2## Min. :2003 Min. :18.00 1. Never Married: 648 1. White:2480 3## 1st Qu.:2004 1st Qu.:33.75 2. Married :2074 2. Black: 293 4## Median :2006 Median :42.00 3. Widowed : 19 3. Asian: 190 5## Mean :2006 Mean :42.41 4. Divorced : 204 4. Other: 37 6## 3rd Qu.:2008 3rd Qu.:51.00 5. Separated : 55 7## Max. :2009 Max. :80.00 8## 9## education region jobclass 10## 1. \u0026lt; HS Grad :268 2. Middle Atlantic :3000 1. Industrial :1544 11## 2. HS Grad :971 1. New England : 0 2. Information:1456 12## 3. Some College :650 3. East North Central: 0 13## 4. College Grad :685 4. West North Central: 0 14## 5. Advanced Degree:426 5. South Atlantic : 0 15## 6. East South Central: 0 16## (Other) : 0 17## health health_ins logwage wage 18## 1. \u0026lt;=Good : 858 1. Yes:2083 Min. :3.000 Min. : 20.09 19## 2. \u0026gt;=Very Good:2142 2. No : 917 1st Qu.:4.447 1st Qu.: 85.38 20## Median :4.653 Median :104.92 21## Mean :4.654 Mean :111.70 22## 3rd Qu.:4.857 3rd Qu.:128.68 23## Max. :5.763 Max. :318.34 24## 1wageDat %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) %\u0026gt;% print 1## year age maritl race education region jobclass 2## 7 61 5 4 5 1 2 3## health health_ins logwage wage 4## 2 2 508 508 1library(boot) 1## 2## Attaching package: \u0026#39;boot\u0026#39; 1## The following object is masked from \u0026#39;package:lattice\u0026#39;: 2## 3## melanoma a) Polynomial regression 1all.deltas = rep(NA, 10) 2for (i in 1:10) { 3glm.fit = glm(wage~poly(age, i), data=Wage) 4all.deltas[i] = cv.glm(Wage, glm.fit, K=10)$delta[2] 5} 6plot(1:10, all.deltas, xlab=\u0026#34;Degree\u0026#34;, ylab=\u0026#34;CV error\u0026#34;, type=\u0026#34;l\u0026#34;, pch=20, lwd=2, ylim=c(1590, 1700)) 7min.point = min(all.deltas) 8sd.points = sd(all.deltas) 9abline(h=min.point + 0.2 * sd.points, col=\u0026#34;red\u0026#34;, lty=\u0026#34;dashed\u0026#34;) 10abline(h=min.point - 0.2 * sd.points, col=\u0026#34;red\u0026#34;, lty=\u0026#34;dashed\u0026#34;) 11legend(\u0026#34;topright\u0026#34;, \u0026#34;0.2-standard deviation lines\u0026#34;, lty=\u0026#34;dashed\u0026#34;, col=\u0026#34;red\u0026#34;)  1# ANOVA 2fits=list() 3for (i in 1:10) { 4fits[[i]]=glm(wage~poly(age,i),data=wageDat) 5} 6anova(fits[[1]],fits[[2]],fits[[3]],fits[[4]],fits[[5]], 7fits[[6]],fits[[7]],fits[[8]],fits[[9]],fits[[10]]) 1## Analysis of Deviance Table 2## 3## Model 1: wage ~ poly(age, i) 4## Model 2: wage ~ poly(age, i) 5## Model 3: wage ~ poly(age, i) 6## Model 4: wage ~ poly(age, i) 7## Model 5: wage ~ poly(age, i) 8## Model 6: wage ~ poly(age, i) 9## Model 7: wage ~ poly(age, i) 10## Model 8: wage ~ poly(age, i) 11## Model 9: wage ~ poly(age, i) 12## Model 10: wage ~ poly(age, i) 13## Resid. Df Resid. Dev Df Deviance 14## 1 2998 5022216 15## 2 2997 4793430 1 228786 16## 3 2996 4777674 1 15756 17## 4 2995 4771604 1 6070 18## 5 2994 4770322 1 1283 19## 6 2993 4766389 1 3932 20## 7 2992 4763834 1 2555 21## 8 2991 4763707 1 127 22## 9 2990 4756703 1 7004 23## 10 2989 4756701 1 3  The 4th degree looks the best at the moment   1# 3rd or 4th degrees look best based on ANOVA test 2# let\u0026#39;s go with 4th degree fit 3plot(wage~age, data=wageDat, col=\u0026#34;darkgrey\u0026#34;) 4agelims = range(wageDat$age) 5age.grid = seq(from=agelims[1], to=agelims[2]) 6lm.fit = lm(wage~poly(age, 4), data=wageDat) 7lm.pred = predict(lm.fit, data.frame(age=age.grid)) 8lines(age.grid, lm.pred, col=\u0026#34;blue\u0026#34;, lwd=2)  b) Step function and cross-validation 1# cross-validation 2cv.error \u0026lt;- rep(0,9) 3for (i in 2:10) { 4wageDat$age.cut \u0026lt;- cut(wageDat$age,i) 5glm.fit \u0026lt;- glm(wage~age.cut, data=wageDat) 6cv.error[i-1] \u0026lt;- cv.glm(wageDat, glm.fit, K=10)$delta[1] # [1]:std, [2]:bias-corrected 7} 8cv.error 1## [1] 1732.337 1682.978 1636.736 1635.600 1624.174 1610.688 1604.081 1612.005 2## [9] 1607.022 1cv.error 1## [1] 1732.337 1682.978 1636.736 1635.600 1624.174 1610.688 1604.081 1612.005 2## [9] 1607.022 1plot(2:10, cv.error, type=\u0026#34;b\u0026#34;)  1cut.fit \u0026lt;- glm(wage~cut(age,8), data=wageDat) 2preds \u0026lt;- predict(cut.fit, newdata=list(age=age.grid), se=TRUE) 3se.bands \u0026lt;- preds$fit + cbind(2*preds$se.fit, -2*preds$se.fit) 4plot(wageDat$age, wageDat$wage, xlim=agelims, cex=0.5, col=\u0026#34;darkgrey\u0026#34;) 5title(\u0026#34;Fit with 8 Age Bands\u0026#34;) 6lines(age.grid, preds$fit, lwd=2, col=\u0026#34;blue\u0026#34;) 7matlines(age.grid, se.bands, lwd=1, col=\u0026#34;blue\u0026#34;, lty=3)  Question 7.8 - Page 299 Fit some of the non-linear models investigated in this chapter to the Auto data set. Is there evidence for non-linear relationships in this data set? Create some informative plots to justify your answer.\nAnswer 1autoDat\u0026lt;-ISLR::Auto 1autoDat %\u0026gt;% pivot_longer(-c(mpg,name),names_to=\u0026#34;Params\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=mpg,y=Value)) + 2geom_point() + 3facet_wrap(~ Params, scales = \u0026#34;free_y\u0026#34;)  Very clearly there is a lot of non-linearity in the mpg data, especially for acceleration, weight, displacement, horsepower.\n1rss = rep(NA, 10) 2fits = list() 3for (d in 1:10) { 4fits[[d]] = lm(mpg ~ poly(displacement, d), data = autoDat) 5rss[d] = deviance(fits[[d]]) 6} 7rss %\u0026gt;% print 1## [1] 8378.822 7412.263 7392.322 7391.722 7380.838 7270.746 7089.716 6917.401 2## [9] 6737.801 6610.190 1anova(fits[[1]],fits[[2]],fits[[3]],fits[[4]],fits[[5]], 2fits[[6]],fits[[7]],fits[[8]],fits[[9]],fits[[10]]) 1## Analysis of Variance Table 2## 3## Model 1: mpg ~ poly(displacement, d) 4## Model 2: mpg ~ poly(displacement, d) 5## Model 3: mpg ~ poly(displacement, d) 6## Model 4: mpg ~ poly(displacement, d) 7## Model 5: mpg ~ poly(displacement, d) 8## Model 6: mpg ~ poly(displacement, d) 9## Model 7: mpg ~ poly(displacement, d) 10## Model 8: mpg ~ poly(displacement, d) 11## Model 9: mpg ~ poly(displacement, d) 12## Model 10: mpg ~ poly(displacement, d) 13## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) 14## 1 390 8378.8 15## 2 389 7412.3 1 966.56 55.7108 5.756e-13 *** 16## 3 388 7392.3 1 19.94 1.1494 0.284364 17## 4 387 7391.7 1 0.60 0.0346 0.852549 18## 5 386 7380.8 1 10.88 0.6273 0.428823 19## 6 385 7270.7 1 110.09 6.3455 0.012177 * 20## 7 384 7089.7 1 181.03 10.4343 0.001344 ** 21## 8 383 6917.4 1 172.31 9.9319 0.001753 ** 22## 9 382 6737.8 1 179.60 10.3518 0.001404 ** 23## 10 381 6610.2 1 127.61 7.3553 0.006990 ** 24## --- 25## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Confirming our visual indications, we see that the second degree models work well.\n1library(glmnet) 1## Loading required package: Matrix 1## 2## Attaching package: \u0026#39;Matrix\u0026#39; 1## The following objects are masked from \u0026#39;package:tidyr\u0026#39;: 2## 3## expand, pack, unpack 1## Loaded glmnet 3.0-2 1library(boot) 1cv.errs = rep(NA, 15) 2for (d in 1:15) { 3fit = glm(mpg ~ poly(displacement, d), data = Auto) 4cv.errs[d] = cv.glm(Auto, fit, K = 15)$delta[2] 5} 6which.min(cv.errs) 1## [1] 10 Strangely, we seem to have ended up with a ten variable model here.\n1# Step functions 2cv.errs = rep(NA, 10) 3for (c in 2:10) { 4Auto$dis.cut = cut(Auto$displacement, c) 5fit = glm(mpg ~ dis.cut, data = Auto) 6cv.errs[c] = cv.glm(Auto, fit, K = 10)$delta[2] 7} 8which.min(cv.errs) %\u0026gt;% print 1## [1] 9 1library(splines) 2cv.errs = rep(NA, 10) 3for (df in 3:10) { 4fit = glm(mpg ~ ns(displacement, df = df), data = Auto) 5cv.errs[df] = cv.glm(Auto, fit, K = 10)$delta[2] 6} 7which.min(cv.errs) %\u0026gt;% print 1## [1] 10 1library(gam) 1## Loading required package: foreach 1## 2## Attaching package: \u0026#39;foreach\u0026#39; 1## The following objects are masked from \u0026#39;package:purrr\u0026#39;: 2## 3## accumulate, when 1## Loaded gam 1.16.1 1# GAMs 2fit = gam(mpg ~ s(displacement, 4) + s(horsepower, 4), data = Auto) 1## Warning in model.matrix.default(mt, mf, contrasts): non-list contrasts argument 2## ignored 1summary(fit) 1## 2## Call: gam(formula = mpg ~ s(displacement, 4) + s(horsepower, 4), data = Auto) 3## Deviance Residuals: 4## Min 1Q Median 3Q Max 5## -11.2982 -2.1592 -0.4394 2.1247 17.0946 6## 7## (Dispersion Parameter for gaussian family taken to be 15.3543) 8## 9## Null Deviance: 23818.99 on 391 degrees of freedom 10## Residual Deviance: 5880.697 on 382.9999 degrees of freedom 11## AIC: 2194.05 12## 13## Number of Local Scoring Iterations: 2 14## 15## Anova for Parametric Effects 16## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) 17## s(displacement, 4) 1 15254.9 15254.9 993.524 \u0026lt; 2e-16 *** 18## s(horsepower, 4) 1 1038.4 1038.4 67.632 3.1e-15 *** 19## Residuals 383 5880.7 15.4 20## --- 21## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 22## 23## Anova for Nonparametric Effects 24## Npar Df Npar F Pr(F) 25## (Intercept) 26## s(displacement, 4) 3 13.613 1.863e-08 *** 27## s(horsepower, 4) 3 15.606 1.349e-09 *** 28## --- 29## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Question 7.9 - Pages 299-300 This question uses the variables dis (the weighted mean of distances to five Boston employment centers) and nox (nitrogen oxides concentration in parts per 10 million) from the Boston data. We will treat dis as the predictor and nox as the response.\n(a) Use the poly() function to fit a cubic polynomial regression to predict nox using dis. Report the regression output, and plot the resulting data and polynomial fits.\n(b) Plot the polynomial fits for a range of different polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares.\n(c) Perform cross-validation or another approach to select the optimal degree for the polynomial, and explain your results.\n(d) Use the bs() function to fit a regression spline to predict nox using dis. Report the output for the fit using four degrees of freedom. How did you choose the knots? Plot the resulting fit.\n(e) Now fit a regression spline for a range of degrees of freedom, and plot the resulting fits and report the resulting RSS. Describe the results obtained.\n(f) Perform cross-validation or another approach in order to select the best degrees of freedom for a regression spline on this data. Describe your results.\nAnswer 1boston\u0026lt;-MASS::Boston 2boston %\u0026gt;% str %\u0026gt;% print 1## \u0026#39;data.frame\u0026#39;: 506 obs. of 14 variables: 2## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... 3## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... 4## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... 5## $ chas : int 0 0 0 0 0 0 0 0 0 0 ... 6## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... 7## $ rm : num 6.58 6.42 7.18 7 7.15 ... 8## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... 9## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... 10## $ rad : int 1 2 2 3 3 3 5 5 5 5 ... 11## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... 12## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... 13## $ black : num 397 397 393 395 397 ... 14## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... 15## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... 16## NULL 1boston %\u0026gt;% summary %\u0026gt;% print 1## crim zn indus chas 2## Min. : 0.00632 Min. : 0.00 Min. : 0.46 Min. :0.00000 3## 1st Qu.: 0.08204 1st Qu.: 0.00 1st Qu.: 5.19 1st Qu.:0.00000 4## Median : 0.25651 Median : 0.00 Median : 9.69 Median :0.00000 5## Mean : 3.61352 Mean : 11.36 Mean :11.14 Mean :0.06917 6## 3rd Qu.: 3.67708 3rd Qu.: 12.50 3rd Qu.:18.10 3rd Qu.:0.00000 7## Max. :88.97620 Max. :100.00 Max. :27.74 Max. :1.00000 8## nox rm age dis 9## Min. :0.3850 Min. :3.561 Min. : 2.90 Min. : 1.130 10## 1st Qu.:0.4490 1st Qu.:5.886 1st Qu.: 45.02 1st Qu.: 2.100 11## Median :0.5380 Median :6.208 Median : 77.50 Median : 3.207 12## Mean :0.5547 Mean :6.285 Mean : 68.57 Mean : 3.795 13## 3rd Qu.:0.6240 3rd Qu.:6.623 3rd Qu.: 94.08 3rd Qu.: 5.188 14## Max. :0.8710 Max. :8.780 Max. :100.00 Max. :12.127 15## rad tax ptratio black 16## Min. : 1.000 Min. :187.0 Min. :12.60 Min. : 0.32 17## 1st Qu.: 4.000 1st Qu.:279.0 1st Qu.:17.40 1st Qu.:375.38 18## Median : 5.000 Median :330.0 Median :19.05 Median :391.44 19## Mean : 9.549 Mean :408.2 Mean :18.46 Mean :356.67 20## 3rd Qu.:24.000 3rd Qu.:666.0 3rd Qu.:20.20 3rd Qu.:396.23 21## Max. :24.000 Max. :711.0 Max. :22.00 Max. :396.90 22## lstat medv 23## Min. : 1.73 Min. : 5.00 24## 1st Qu.: 6.95 1st Qu.:17.02 25## Median :11.36 Median :21.20 26## Mean :12.65 Mean :22.53 27## 3rd Qu.:16.95 3rd Qu.:25.00 28## Max. :37.97 Max. :50.00 1boston %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) %\u0026gt;% print 1## crim zn indus chas nox rm age dis rad tax 2## 504 26 76 2 81 446 356 412 9 66 3## ptratio black lstat medv 4## 46 357 455 229 a) Polynomial 1fit.03 \u0026lt;- lm(nox~poly(dis,3), data=boston) 2dislims \u0026lt;- range(boston$dis) 3dis.grid \u0026lt;- seq(dislims[1], dislims[2], 0.1) 4preds \u0026lt;- predict(fit.03, newdata=list(dis=dis.grid), se=TRUE) 5se.bands \u0026lt;- preds$fit + cbind(2*preds$se.fit, -2*preds$se.fit) 6par(mfrow=c(1,1), mar=c(4.5,4.5,1,1), oma=c(0,0,4,0)) 7plot(boston$dis, boston$nox, xlim=dislims, cex=0.5, col=\u0026#34;darkgrey\u0026#34;) 8title(\u0026#34;Degree 3 Polynomial Fit\u0026#34;) 9lines(dis.grid, preds$fit, lwd=2, col=\u0026#34;blue\u0026#34;) 10matlines(dis.grid, se.bands, lwd=1, col=\u0026#34;blue\u0026#34;, lty=3)  1summary(fit.03) 1## 2## Call: 3## lm(formula = nox ~ poly(dis, 3), data = boston) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -0.121130 -0.040619 -0.009738 0.023385 0.194904 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 0.554695 0.002759 201.021 \u0026lt; 2e-16 *** 12## poly(dis, 3)1 -2.003096 0.062071 -32.271 \u0026lt; 2e-16 *** 13## poly(dis, 3)2 0.856330 0.062071 13.796 \u0026lt; 2e-16 *** 14## poly(dis, 3)3 -0.318049 0.062071 -5.124 4.27e-07 *** 15## --- 16## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 17## 18## Residual standard error: 0.06207 on 502 degrees of freedom 19## Multiple R-squared: 0.7148, Adjusted R-squared: 0.7131 20## F-statistic: 419.3 on 3 and 502 DF, p-value: \u0026lt; 2.2e-16 b) Multiple Polynomials 1rss.error \u0026lt;- rep(0,10) 2for (i in 1:10) { 3lm.fit \u0026lt;- lm(nox~poly(dis,i), data=boston) 4rss.error[i] \u0026lt;- sum(lm.fit$residuals^2) 5} 6rss.error 1## [1] 2.768563 2.035262 1.934107 1.932981 1.915290 1.878257 1.849484 1.835630 2## [9] 1.833331 1.832171 1plot(rss.error, type=\u0026#34;b\u0026#34;)  c) Cross validation and polynomial selection 1require(boot) 2set.seed(1) 3cv.error \u0026lt;- rep(0,10) 4for (i in 1:10) { 5glm.fit \u0026lt;- glm(nox~poly(dis,i), data=boston) 6cv.error[i] \u0026lt;- cv.glm(boston, glm.fit, K=10)$delta[1] # [1]:std, [2]:bias-corrected 7} 8cv.error 1## [1] 0.005558263 0.004085706 0.003876521 0.003863342 0.004237452 0.005686862 2## [7] 0.010278897 0.006810868 0.033308607 0.004075599 1plot(cv.error, type=\u0026#34;b\u0026#34;)   I feel like the second degree fit would be the most reasonable, though the fourth degree seems to be doing well.  d) Regression spline 1fit.sp \u0026lt;- lm(nox~bs(dis, df=4), data=boston) 2pred \u0026lt;- predict(fit.sp, newdata=list(dis=dis.grid), se=T) 3plot(boston$dis, boston$nox, col=\u0026#34;gray\u0026#34;) 4lines(dis.grid, pred$fit, lwd=2) 5lines(dis.grid, pred$fit+2*pred$se, lty=\u0026#34;dashed\u0026#34;) 6lines(dis.grid, pred$fit-2*pred$se, lty=\u0026#34;dashed\u0026#34;)  1# set df to select knots at uniform quantiles of `dis` 2attr(bs(boston$dis,df=4),\u0026#34;knots\u0026#34;) # only 1 knot at 50th percentile 1## 50% 2## 3.20745 e) Range of regression splines 1rss.error \u0026lt;- rep(0,7) 2for (i in 4:10) { 3fit.sp \u0026lt;- lm(nox~bs(dis, df=i), data=boston) 4rss.error[i-3] \u0026lt;- sum(fit.sp$residuals^2) 5} 6rss.error 1## [1] 1.922775 1.840173 1.833966 1.829884 1.816995 1.825653 1.792535 1plot(4:10, rss.error, type=\u0026#34;b\u0026#34;)   As the model gains more degrees of freedom, it tends to over fit to the training data better  f) Cross validation for best spline 1cv.error \u0026lt;- rep(0,7) 2for (i in 4:10) { 3glm.fit \u0026lt;- glm(nox~bs(dis, df=i), data=boston) 4cv.error[i-3] \u0026lt;- cv.glm(boston, glm.fit, K=10)$delta[1] 5} 1## Warning in bs(dis, degree = 3L, knots = c(`50%` = 3.1523), Boundary.knots = 2## c(1.1296, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned 3## bases 45## Warning in bs(dis, degree = 3L, knots = c(`50%` = 3.1523), Boundary.knots = 6## c(1.1296, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned 7## bases 1## Warning in bs(dis, degree = 3L, knots = c(`50%` = 3.2157), Boundary.knots = 2## c(1.137, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`50%` = 3.2157), Boundary.knots = 5## c(1.137, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`33.33333%` = 2.35953333333333, : some 2## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`33.33333%` = 2.35953333333333, : some 5## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`33.33333%` = 2.38403333333333, : some 2## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`33.33333%` = 2.38403333333333, : some 5## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`25%` = 2.07945, `50%` = 3.1323, : 2## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`25%` = 2.07945, `50%` = 3.1323, : 5## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`25%` = 2.1103, `50%` = 3.2797, : some 2## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`25%` = 2.1103, `50%` = 3.2797, : some 5## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`20%` = 1.9682, `40%` = 2.7147, : some 2## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`20%` = 1.9682, `40%` = 2.7147, : some 5## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`20%` = 1.95434, `40%` = 2.59666, : 2## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`20%` = 1.95434, `40%` = 2.59666, : 5## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`16.66667%` = 1.82203333333333, : some 2## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`16.66667%` = 1.82203333333333, : some 5## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`16.66667%` = 1.8226, `33.33333%` = 2## 2.3817, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`16.66667%` = 1.8226, `33.33333%` = 5## 2.3817, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`14.28571%` = 1.7936, `28.57143%` 2## = 2.16972857142857, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill- 3## conditioned bases 45## Warning in bs(dis, degree = 3L, knots = c(`14.28571%` = 1.7936, `28.57143%` 6## = 2.16972857142857, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill- 7## conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`12.5%` = 1.754625, `25%` = 2.10215, : 2## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`12.5%` = 1.754625, `25%` = 2.10215, : 5## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1## Warning in bs(dis, degree = 3L, knots = c(`12.5%` = 1.751575, `25%` = 2.08755, : 2## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 34## Warning in bs(dis, degree = 3L, knots = c(`12.5%` = 1.751575, `25%` = 2.08755, : 5## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases 1cv.error 1## [1] 0.003898810 0.003694675 0.003732665 0.003766202 0.003716389 0.003723126 2## [7] 0.003727358 1plot(4:10, cv.error, type=\u0026#34;b\u0026#34;)   A fifth degree polynomial is clearly indicated  Question 10 - Page 300 This question relates to the College data set.\n(a) Split the data into a training set and a test set. Using out-of-state tuition as the response and the other variables as the predictors, perform forward stepwise selection on the training set in order to identify a satisfactory model that uses just a subset of the predictors.\n(b) Fit a GAM on the training data, using out-of-state tuition as the response and the features selected in the previous step as the predictors. Plot the results, and explain your ﬁndings.\n(c) Evaluate the model obtained on the test set, and explain the results obtained.\n(d) For which variables, if any, is there evidence of a non-linear relationship with the response?\nAnswer 1colDat\u0026lt;-ISLR::College 2colDat %\u0026gt;% str %\u0026gt;% print 1## \u0026#39;data.frame\u0026#39;: 777 obs. of 18 variables: 2## $ Private : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 2 2 2 2 2 2 2 2 ... 3## $ Apps : num 1660 2186 1428 417 193 ... 4## $ Accept : num 1232 1924 1097 349 146 ... 5## $ Enroll : num 721 512 336 137 55 158 103 489 227 172 ... 6## $ Top10perc : num 23 16 22 60 16 38 17 37 30 21 ... 7## $ Top25perc : num 52 29 50 89 44 62 45 68 63 44 ... 8## $ F.Undergrad: num 2885 2683 1036 510 249 ... 9## $ P.Undergrad: num 537 1227 99 63 869 ... 10## $ Outstate : num 7440 12280 11250 12960 7560 ... 11## $ Room.Board : num 3300 6450 3750 5450 4120 ... 12## $ Books : num 450 750 400 450 800 500 500 450 300 660 ... 13## $ Personal : num 2200 1500 1165 875 1500 ... 14## $ PhD : num 70 29 53 92 76 67 90 89 79 40 ... 15## $ Terminal : num 78 30 66 97 72 73 93 100 84 41 ... 16## $ S.F.Ratio : num 18.1 12.2 12.9 7.7 11.9 9.4 11.5 13.7 11.3 11.5 ... 17## $ perc.alumni: num 12 16 30 37 2 11 26 37 23 15 ... 18## $ Expend : num 7041 10527 8735 19016 10922 ... 19## $ Grad.Rate : num 60 56 54 59 15 55 63 73 80 52 ... 20## NULL 1colDat %\u0026gt;% summary %\u0026gt;% print 1## Private Apps Accept Enroll Top10perc 2## No :212 Min. : 81 Min. : 72 Min. : 35 Min. : 1.00 3## Yes:565 1st Qu.: 776 1st Qu.: 604 1st Qu.: 242 1st Qu.:15.00 4## Median : 1558 Median : 1110 Median : 434 Median :23.00 5## Mean : 3002 Mean : 2019 Mean : 780 Mean :27.56 6## 3rd Qu.: 3624 3rd Qu.: 2424 3rd Qu.: 902 3rd Qu.:35.00 7## Max. :48094 Max. :26330 Max. :6392 Max. :96.00 8## Top25perc F.Undergrad P.Undergrad Outstate 9## Min. : 9.0 Min. : 139 Min. : 1.0 Min. : 2340 10## 1st Qu.: 41.0 1st Qu.: 992 1st Qu.: 95.0 1st Qu.: 7320 11## Median : 54.0 Median : 1707 Median : 353.0 Median : 9990 12## Mean : 55.8 Mean : 3700 Mean : 855.3 Mean :10441 13## 3rd Qu.: 69.0 3rd Qu.: 4005 3rd Qu.: 967.0 3rd Qu.:12925 14## Max. :100.0 Max. :31643 Max. :21836.0 Max. :21700 15## Room.Board Books Personal PhD 16## Min. :1780 Min. : 96.0 Min. : 250 Min. : 8.00 17## 1st Qu.:3597 1st Qu.: 470.0 1st Qu.: 850 1st Qu.: 62.00 18## Median :4200 Median : 500.0 Median :1200 Median : 75.00 19## Mean :4358 Mean : 549.4 Mean :1341 Mean : 72.66 20## 3rd Qu.:5050 3rd Qu.: 600.0 3rd Qu.:1700 3rd Qu.: 85.00 21## Max. :8124 Max. :2340.0 Max. :6800 Max. :103.00 22## Terminal S.F.Ratio perc.alumni Expend 23## Min. : 24.0 Min. : 2.50 Min. : 0.00 Min. : 3186 24## 1st Qu.: 71.0 1st Qu.:11.50 1st Qu.:13.00 1st Qu.: 6751 25## Median : 82.0 Median :13.60 Median :21.00 Median : 8377 26## Mean : 79.7 Mean :14.09 Mean :22.74 Mean : 9660 27## 3rd Qu.: 92.0 3rd Qu.:16.50 3rd Qu.:31.00 3rd Qu.:10830 28## Max. :100.0 Max. :39.80 Max. :64.00 Max. :56233 29## Grad.Rate 30## Min. : 10.00 31## 1st Qu.: 53.00 32## Median : 65.00 33## Mean : 65.46 34## 3rd Qu.: 78.00 35## Max. :118.00 1colDat %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) %\u0026gt;% print 1## Private Apps Accept Enroll Top10perc Top25perc 2## 2 711 693 581 82 89 3## F.Undergrad P.Undergrad Outstate Room.Board Books Personal 4## 714 566 640 553 122 294 5## PhD Terminal S.F.Ratio perc.alumni Expend Grad.Rate 6## 78 65 173 61 744 81 1plotLEAP=function(leapObj){ 2par(mfrow = c(2,2)) 3bar2=which.max(leapObj$adjr2) 4bbic=which.min(leapObj$bic) 5bcp=which.min(leapObj$cp) 6plot(leapObj$rss,xlab=\u0026#34;Number of variables\u0026#34;,ylab=\u0026#34;RSS\u0026#34;,type=\u0026#34;b\u0026#34;) 7plot(leapObj$adjr2,xlab=\u0026#34;Number of variables\u0026#34;,ylab=TeX(\u0026#34;Adjusted R^2\u0026#34;),type=\u0026#34;b\u0026#34;) 8points(bar2,leapObj$adjr2[bar2],col=\u0026#34;green\u0026#34;,cex=2,pch=20) 9plot(leapObj$bic,xlab=\u0026#34;Number of variables\u0026#34;,ylab=TeX(\u0026#34;BIC\u0026#34;),type=\u0026#34;b\u0026#34;) 10points(bbic,leapObj$bic[bbic],col=\u0026#34;blue\u0026#34;,cex=2,pch=20) 11plot(leapObj$cp,xlab=\u0026#34;Number of variables\u0026#34;,ylab=TeX(\u0026#34;C_p\u0026#34;),type=\u0026#34;b\u0026#34;) 12points(bcp,leapObj$cp[bcp],col=\u0026#34;red\u0026#34;,cex=2,pch=20) 13} a) Train test 1train_ind = sample(colDat %\u0026gt;% nrow,100) 2test_ind = setdiff(seq_len(colDat %\u0026gt;% nrow), train_ind) Best subset selection 1train_set\u0026lt;-colDat[train_ind,] 2test_set\u0026lt;-colDat[-train_ind,] 1library(leaps) 1modelFit\u0026lt;-regsubsets(Outstate~.,data=colDat,nvmax=20) 2modelFit %\u0026gt;% summary %\u0026gt;% print 1## Subset selection object 2## Call: regsubsets.formula(Outstate ~ ., data = colDat, nvmax = 20) 3## 17 Variables (and intercept) 4## Forced in Forced out 5## PrivateYes FALSE FALSE 6## Apps FALSE FALSE 7## Accept FALSE FALSE 8## Enroll FALSE FALSE 9## Top10perc FALSE FALSE 10## Top25perc FALSE FALSE 11## F.Undergrad FALSE FALSE 12## P.Undergrad FALSE FALSE 13## Room.Board FALSE FALSE 14## Books FALSE FALSE 15## Personal FALSE FALSE 16## PhD FALSE FALSE 17## Terminal FALSE FALSE 18## S.F.Ratio FALSE FALSE 19## perc.alumni FALSE FALSE 20## Expend FALSE FALSE 21## Grad.Rate FALSE FALSE 22## 1 subsets of each size up to 17 23## Selection Algorithm: exhaustive 24## PrivateYes Apps Accept Enroll Top10perc Top25perc F.Undergrad 25## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 26## 2 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 27## 3 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 28## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 29## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 30## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 31## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 32## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 33## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 34## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 35## 11 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 36## 12 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 37## 13 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 38## 14 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 39## 15 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 40## 16 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 41## 17 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 42## P.Undergrad Room.Board Books Personal PhD Terminal S.F.Ratio 43## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 44## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 45## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 46## 4 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 47## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 48## 6 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 49## 7 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 50## 8 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 51## 9 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 52## 10 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 53## 11 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 54## 12 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 55## 13 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 56## 14 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 57## 15 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 58## 16 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 59## 17 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 60## perc.alumni Expend Grad.Rate 61## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 62## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 63## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 64## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 65## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 66## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 67## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 68## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 69## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 70## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 71## 11 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 72## 12 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 73## 13 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 74## 14 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 75## 15 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 76## 16 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 77## 17 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; We might want to take a look at these.\n1par(mfrow=c(2,2)) 2plot(modelFit) 3plot(modelFit,scale=\u0026#39;Cp\u0026#39;) 4plot(modelFit,scale=\u0026#39;r2\u0026#39;) 5plot(modelFit,scale=\u0026#39;adjr2\u0026#39;)  1plotLEAP(modelFit %\u0026gt;% summary)   So we like 14 variables, namely   1coefficients(modelFit,id=14) %\u0026gt;% print 1## (Intercept) PrivateYes Apps Accept Enroll 2## -1.817040e+03 2.256946e+03 -2.999022e-01 8.023519e-01 -5.372545e-01 3## Top10perc F.Undergrad Room.Board Personal PhD 4## 2.365529e+01 -9.569936e-02 8.741819e-01 -2.478418e-01 1.269506e+01 5## Terminal S.F.Ratio perc.alumni Expend Grad.Rate 6## 2.297296e+01 -4.700560e+01 4.195006e+01 2.003912e-01 2.383197e+01  But five seems like a better bet.   1coefficients(modelFit,id=5) 1## (Intercept) PrivateYes Room.Board PhD perc.alumni 2## -2864.6325619 2936.7416766 1.0677573 40.5334088 61.3147684 3## Expend 4## 0.2253945 b) GAM 1library(gam) 1fit = gam(Outstate ~ Private+s(Apps,3)+Accept+Enroll+ 2Top10perc+F.Undergrad+Room.Board+ 3Personal+PhD+Terminal+S.F.Ratio+ 4perc.alumni+Expend+Grad.Rate 5, data = colDat) 1## Warning in model.matrix.default(mt, mf, contrasts): non-list contrasts argument 2## ignored 1fit2 = gam(Outstate ~ Private+s(Room.Board,2)+s(PhD,3)+s(perc.alumni)+Expend 2, data = colDat) 1## Warning in model.matrix.default(mt, mf, contrasts): non-list contrasts argument 2## ignored 1par(mfrow=c(2,2)) 2plot(fit,se=TRUE) 1par(mfrow=c(2,2)) 2plot(fit2,se=TRUE)  c) Evaluate 1pred \u0026lt;- predict(fit, test_set) 2mse.error \u0026lt;- mean((test_set$Outstate - pred)^2) 3mse.error %\u0026gt;% print 1## [1] 3691891 1gam.tss = mean((test_set$Outstate - mean(test_set$Outstate))^2) 2test.rss = 1 - mse.error/gam.tss 3test.rss %\u0026gt;% print 1## [1] 0.7731239 1pred2 \u0026lt;- predict(fit2, test_set) 2mse.error2 \u0026lt;- mean((test_set$Outstate - pred2)^2) 3mse.error2 %\u0026gt;% print 1## [1] 4121902 1gam.tss2 = mean((test_set$Outstate - mean(test_set$Outstate))^2) 2test.rss2 = 1 - mse.error2/gam.tss2 3test.rss2 %\u0026gt;% print 1## [1] 0.7466987 This is pretty good model, all told.\nd) Summary 1summary(fit) %\u0026gt;% print 1## 2## Call: gam(formula = Outstate ~ Private + s(Apps, 3) + Accept + Enroll + 3## Top10perc + F.Undergrad + Room.Board + Personal + PhD + Terminal + 4## S.F.Ratio + perc.alumni + Expend + Grad.Rate, data = colDat) 5## Deviance Residuals: 6## Min 1Q Median 3Q Max 7## -6641.083 -1262.806 -5.698 1270.911 9965.901 8## 9## (Dispersion Parameter for gaussian family taken to be 3749048) 10## 11## Null Deviance: 12559297426 on 776 degrees of freedom 12## Residual Deviance: 2849276343 on 760 degrees of freedom 13## AIC: 13985.3 14## 15## Number of Local Scoring Iterations: 2 16## 17## Anova for Parametric Effects 18## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) 19## Private 1 4034912907 4034912907 1076.250 \u0026lt; 2.2e-16 *** 20## s(Apps, 3) 1 1344548030 1344548030 358.637 \u0026lt; 2.2e-16 *** 21## Accept 1 90544274 90544274 24.151 1.091e-06 *** 22## Enroll 1 144471570 144471570 38.535 8.838e-10 *** 23## Top10perc 1 1802244831 1802244831 480.721 \u0026lt; 2.2e-16 *** 24## F.Undergrad 1 45230645 45230645 12.065 0.0005430 *** 25## Room.Board 1 1110285773 1110285773 296.151 \u0026lt; 2.2e-16 *** 26## Personal 1 47886988 47886988 12.773 0.0003738 *** 27## PhD 1 220249039 220249039 58.748 5.476e-14 *** 28## Terminal 1 66366007 66366007 17.702 2.892e-05 *** 29## S.F.Ratio 1 190811028 190811028 50.896 2.274e-12 *** 30## perc.alumni 1 225293653 225293653 60.094 2.904e-14 *** 31## Expend 1 258162295 258162295 68.861 4.805e-16 *** 32## Grad.Rate 1 57947219 57947219 15.457 9.214e-05 *** 33## Residuals 760 2849276343 3749048 34## --- 35## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 36## 37## Anova for Nonparametric Effects 38## Npar Df Npar F Pr(F) 39## (Intercept) 40## Private 41## s(Apps, 3) 2 8.571 0.0002085 *** 42## Accept 43## Enroll 44## Top10perc 45## F.Undergrad 46## Room.Board 47## Personal 48## PhD 49## Terminal 50## S.F.Ratio 51## perc.alumni 52## Expend 53## Grad.Rate 54## --- 55## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 1summary(fit2) %\u0026gt;% print 1## 2## Call: gam(formula = Outstate ~ Private + s(Room.Board, 2) + s(PhD, 3## 3) + s(perc.alumni) + Expend, data = colDat) 4## Deviance Residuals: 5## Min 1Q Median 3Q Max 6## -8676.030 -1345.678 -8.409 1265.524 9590.459 7## 8## (Dispersion Parameter for gaussian family taken to be 4175193) 9## 10## Null Deviance: 12559297426 on 776 degrees of freedom 11## Residual Deviance: 3194023899 on 765.0002 degrees of freedom 12## AIC: 14064.05 13## 14## Number of Local Scoring Iterations: 2 15## 16## Anova for Parametric Effects 17## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) 18## Private 1 3751107814 3751107814 898.43 \u0026lt; 2.2e-16 *** 19## s(Room.Board, 2) 1 2913770756 2913770756 697.88 \u0026lt; 2.2e-16 *** 20## s(PhD, 3) 1 1149711330 1149711330 275.37 \u0026lt; 2.2e-16 *** 21## s(perc.alumni) 1 556759894 556759894 133.35 \u0026lt; 2.2e-16 *** 22## Expend 1 554812125 554812125 132.88 \u0026lt; 2.2e-16 *** 23## Residuals 765 3194023899 4175193 24## --- 25## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 26## 27## Anova for Nonparametric Effects 28## Npar Df Npar F Pr(F) 29## (Intercept) 30## Private 31## s(Room.Board, 2) 1 4.9853 0.0258517 * 32## s(PhD, 3) 2 9.1614 0.0001171 *** 33## s(perc.alumni) 3 0.8726 0.4548496 34## Expend 35## --- 36## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1   James, G., Witten, D., Hastie, T., \u0026amp; Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Berlin, Germany: Springer Science \u0026amp; Business Media.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/islr-ch7/","tags":["solutions","R","ISLR"],"title":"  \"ISLR :: Moving Beyond Linearity\"\n  "},{"categories":["programming"],"contents":"Chapter VI - Linear Model Selection and Regularization All the questions are as per the ISL seventh printing of the First edition1.\nCommon Instead of using the standard functions, we will leverage the mlr3 package2.\n1#install.packages(\u0026#34;mlr3\u0026#34;,\u0026#34;data.table\u0026#34;,\u0026#34;mlr3viz\u0026#34;,\u0026#34;mlr3learners\u0026#34;) Actually for R version 3.6.2, the steps to get it working were a bit more involved.\nLoad ISLR and other libraries.\n1libsUsed\u0026lt;-c(\u0026#34;dplyr\u0026#34;,\u0026#34;ggplot2\u0026#34;,\u0026#34;tidyverse\u0026#34;, 2\u0026#34;ISLR\u0026#34;,\u0026#34;caret\u0026#34;,\u0026#34;MASS\u0026#34;, \u0026#34;gridExtra\u0026#34;, 3\u0026#34;pls\u0026#34;,\u0026#34;latex2exp\u0026#34;,\u0026#34;data.table\u0026#34;) 4invisible(lapply(libsUsed, library, character.only = TRUE)) Question 6.8 - Page 262 In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.\n(a) Use the =rnorm()=function to generate a predictor \\(X\\) of length \\(n = 100\\), as well as a noise vector \\(\\eta\\) of length \\(n = 100\\).\n(b) Generate a response vector \\(Y\\) of length \\(n = 100\\) according to the model \\[Y = \\beta_0 + \\beta_1X + \\beta2X^2 + \\beta_3X^3 + \\eta\\], where \\(\\beta_{0}\\) , \\(\\beta_{1}\\), \\(\\beta_{2}\\), and \\(\\beta_{3}\\) are constants of your choice.\n(c) Use the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors \\(X\\), \\(X^{2}\\), \u0026hellip;, \\(X^{10}\\). What is the best model obtained according to \\(C_p\\) , BIC, and adjusted \\(R^2\\) ? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both \\(X\\) and \\(Y\\).\n(d) Repeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)?\n(e) Now fit a lasso model to the simulated data, again using \\(X\\), \\(X^{2}\\), \u0026hellip;, \\(X^{10}\\) as predictors. Use cross-validation to select the optimal value of \\(\\lambda\\). Create plots of the cross-validation error as a function of \\(\\lambda\\). Report the resulting coefficient estimates, and discuss the results obtained.\n(f) Now generate a response vector Y according to the model \\[Y = \\beta_{0} + \\beta_{7}X^{7} + \\eta,\\] and perform best subset selection and the lasso. Discuss the results obtained.\nAnswer a) Generate model 1set.seed(1984) 2x\u0026lt;-rnorm(100) 3noise\u0026lt;-rnorm(100) b) Response vector 1beta=c(43,5,3,6) 2y\u0026lt;-beta[1] + beta[2]*x + beta[3]*x^2 + beta[4]*x^3 + noise 3qplot(x,y)  c) Subset selection Since the question requires it, we will be using the leaps libraries.\n1library(leaps) 2df\u0026lt;-data.frame(y=y,x=x) 3sets=regsubsets(y~poly(x,10,raw=T),data=df,nvmax=10) 4sets %\u0026gt;% summary 1## Subset selection object 2## Call: regsubsets.formula(y ~ poly(x, 10, raw = T), data = df, nvmax = 10) 3## 10 Variables (and intercept) 4## Forced in Forced out 5## poly(x, 10, raw = T)1 FALSE FALSE 6## poly(x, 10, raw = T)2 FALSE FALSE 7## poly(x, 10, raw = T)3 FALSE FALSE 8## poly(x, 10, raw = T)4 FALSE FALSE 9## poly(x, 10, raw = T)5 FALSE FALSE 10## poly(x, 10, raw = T)6 FALSE FALSE 11## poly(x, 10, raw = T)7 FALSE FALSE 12## poly(x, 10, raw = T)8 FALSE FALSE 13## poly(x, 10, raw = T)9 FALSE FALSE 14## poly(x, 10, raw = T)10 FALSE FALSE 15## 1 subsets of each size up to 10 16## Selection Algorithm: exhaustive 17## poly(x, 10, raw = T)1 poly(x, 10, raw = T)2 poly(x, 10, raw = T)3 18## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 19## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 20## 3 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 21## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 22## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 23## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 24## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 25## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 26## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 27## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 28## poly(x, 10, raw = T)4 poly(x, 10, raw = T)5 poly(x, 10, raw = T)6 29## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 30## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 31## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 32## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 33## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 34## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 35## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 36## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 37## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 38## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 39## poly(x, 10, raw = T)7 poly(x, 10, raw = T)8 poly(x, 10, raw = T)9 40## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 41## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 42## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 43## 4 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 44## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 45## 6 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 46## 7 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 47## 8 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 48## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 49## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 50## poly(x, 10, raw = T)10 51## 1 ( 1 ) \u0026#34; \u0026#34; 52## 2 ( 1 ) \u0026#34; \u0026#34; 53## 3 ( 1 ) \u0026#34; \u0026#34; 54## 4 ( 1 ) \u0026#34; \u0026#34; 55## 5 ( 1 ) \u0026#34;*\u0026#34; 56## 6 ( 1 ) \u0026#34;*\u0026#34; 57## 7 ( 1 ) \u0026#34;*\u0026#34; 58## 8 ( 1 ) \u0026#34;*\u0026#34; 59## 9 ( 1 ) \u0026#34;*\u0026#34; 60## 10 ( 1 ) \u0026#34;*\u0026#34; We also want the best parameters.\n1summarySet\u0026lt;-summary(sets) 2which.min(summarySet$cp) %\u0026gt;% print 1## [1] 3 1which.min(summarySet$bic) %\u0026gt;% print 1## [1] 3 1which.max(summarySet$adjr2) %\u0026gt;% print 1## [1] 7 We might want to see this as a plot.\n1plot(summarySet$cp, xlab = \u0026#34;Subset Size\u0026#34;, ylab = \u0026#34;Cp\u0026#34;, pch = 20, type = \u0026#34;l\u0026#34;) 2points(3,summarySet$cp[3],pch=4,col=\u0026#39;red\u0026#39;,lwd=7)  1plot(summarySet$bic, xlab = \u0026#34;Subset Size\u0026#34;, ylab = \u0026#34;BIC\u0026#34;, pch = 20, type = \u0026#34;l\u0026#34;) 2points(3,summarySet$bic[3],pch=4,col=\u0026#39;red\u0026#39;,lwd=7)  1plot(summarySet$adjr2, xlab = \u0026#34;Subset Size\u0026#34;, ylab = \u0026#34;Adjusted R2\u0026#34;, pch = 20, type = \u0026#34;l\u0026#34;) 2points(3,summarySet$adjr2[3],pch=4,col=\u0026#39;red\u0026#39;,lwd=7)  Lets check the coefficients.\n1coefficients(sets,id=3) %\u0026gt;% print 1## (Intercept) poly(x, 10, raw = T)1 poly(x, 10, raw = T)2 2## 42.895657 5.108094 3.034408 3## poly(x, 10, raw = T)3 4## 5.989367 1beta %\u0026gt;% print 1## [1] 43 5 3 6 We see that we actually have a pretty good set of coefficients.\nd) Forward and backward stepwise models 1modelX\u0026lt;-poly(x,10,raw=T) 2forwardFit\u0026lt;-regsubsets(y~modelX,data=df,nvmax=10,method=\u0026#34;forward\u0026#34;) 3forwardFit %\u0026gt;% summary %\u0026gt;% print 1## Subset selection object 2## Call: regsubsets.formula(y ~ modelX, data = df, nvmax = 10, method = \u0026#34;forward\u0026#34;) 3## 10 Variables (and intercept) 4## Forced in Forced out 5## modelX1 FALSE FALSE 6## modelX2 FALSE FALSE 7## modelX3 FALSE FALSE 8## modelX4 FALSE FALSE 9## modelX5 FALSE FALSE 10## modelX6 FALSE FALSE 11## modelX7 FALSE FALSE 12## modelX8 FALSE FALSE 13## modelX9 FALSE FALSE 14## modelX10 FALSE FALSE 15## 1 subsets of each size up to 10 16## Selection Algorithm: forward 17## modelX1 modelX2 modelX3 modelX4 modelX5 modelX6 modelX7 modelX8 18## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 19## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 20## 3 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 21## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 22## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 23## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 24## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 25## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 26## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 27## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 28## modelX9 modelX10 29## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 30## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 31## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 32## 4 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 33## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 34## 6 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 35## 7 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 36## 8 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 37## 9 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 38## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; We might want to take a look at these.\n1par(mfrow=c(2,2)) 2plot(forwardFit) 3plot(forwardFit,scale=\u0026#39;Cp\u0026#39;) 4plot(forwardFit,scale=\u0026#39;r2\u0026#39;) 5plot(forwardFit,scale=\u0026#39;adjr2\u0026#39;)  I find these not as fun to look at, so we will do better.\n1plotLEAP=function(leapObj){ 2par(mfrow = c(2,2)) 3bar2=which.max(leapObj$adjr2) 4bbic=which.min(leapObj$bic) 5bcp=which.min(leapObj$cp) 6plot(leapObj$rss,xlab=\u0026#34;Number of variables\u0026#34;,ylab=\u0026#34;RSS\u0026#34;,type=\u0026#34;b\u0026#34;) 7plot(leapObj$adjr2,xlab=\u0026#34;Number of variables\u0026#34;,ylab=TeX(\u0026#34;Adjusted R^2\u0026#34;),type=\u0026#34;b\u0026#34;) 8points(bar2,leapObj$adjr2[bar2],col=\u0026#34;green\u0026#34;,cex=2,pch=20) 9plot(leapObj$bic,xlab=\u0026#34;Number of variables\u0026#34;,ylab=TeX(\u0026#34;BIC\u0026#34;),type=\u0026#34;b\u0026#34;) 10points(bbic,leapObj$bic[bbic],col=\u0026#34;blue\u0026#34;,cex=2,pch=20) 11plot(leapObj$cp,xlab=\u0026#34;Number of variables\u0026#34;,ylab=TeX(\u0026#34;C_p\u0026#34;),type=\u0026#34;b\u0026#34;) 12points(bcp,leapObj$cp[bcp],col=\u0026#34;red\u0026#34;,cex=2,pch=20) 13} 1plotLEAP(forwardFit %\u0026gt;% summary)  Lets check the backward selection as well.\n1modelX\u0026lt;-poly(x,10,raw=T) 2backwardFit\u0026lt;-regsubsets(y~modelX,data=df,nvmax=10,method=\u0026#34;backward\u0026#34;) 3backwardFit %\u0026gt;% summary %\u0026gt;% print 1## Subset selection object 2## Call: regsubsets.formula(y ~ modelX, data = df, nvmax = 10, method = \u0026#34;backward\u0026#34;) 3## 10 Variables (and intercept) 4## Forced in Forced out 5## modelX1 FALSE FALSE 6## modelX2 FALSE FALSE 7## modelX3 FALSE FALSE 8## modelX4 FALSE FALSE 9## modelX5 FALSE FALSE 10## modelX6 FALSE FALSE 11## modelX7 FALSE FALSE 12## modelX8 FALSE FALSE 13## modelX9 FALSE FALSE 14## modelX10 FALSE FALSE 15## 1 subsets of each size up to 10 16## Selection Algorithm: backward 17## modelX1 modelX2 modelX3 modelX4 modelX5 modelX6 modelX7 modelX8 18## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 19## 2 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 20## 3 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 21## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 22## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 23## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 24## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 25## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 26## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 27## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 28## modelX9 modelX10 29## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 30## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 31## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 32## 4 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 33## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; 34## 6 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 35## 7 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 36## 8 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 37## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 38## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; We might want to take a look at these.\n1par(mfrow=c(2,2)) 2plot(backwardFit) 3plot(backwardFit,scale=\u0026#39;Cp\u0026#39;) 4plot(backwardFit,scale=\u0026#39;r2\u0026#39;) 5plot(backwardFit,scale=\u0026#39;adjr2\u0026#39;)  1plotLEAP(backwardFit %\u0026gt;% summary)  In spite of some slight variations, overall all methods converge to the same best set of parameters, that of the third model.\ne) LASSO and Cross Validation For this, instead of using glmnet directly, we will use caret.\n1df\u0026lt;-df %\u0026gt;% mutate(x2=x^2,x3=x^3, 2x4=x^4,x5=x^5, 3x6=x^6,x7=x^7, 4x8=x^8,x9=x^9, 5x10=x^10) 1lambda\u0026lt;-10^seq(-3, 3, length = 100) 2lassoCaret= train(y~.,data=df,method=\u0026#34;glmnet\u0026#34;,tuneGrid=expand.grid(alpha=1,lambda=lambda)) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1lassoCaret %\u0026gt;% print 1## glmnet 2## 3## 100 samples 4## 10 predictor 5## 6## No pre-processing 7## Resampling: Bootstrapped (25 reps) 8## Summary of sample sizes: 100, 100, 100, 100, 100, 100, ... 9## Resampling results across tuning parameters: 10## 11## lambda RMSE Rsquared MAE 12## 1.000000e-03 1.009696 0.9965632 0.8051425 13## 1.149757e-03 1.009696 0.9965632 0.8051425 14## 1.321941e-03 1.009696 0.9965632 0.8051425 15## 1.519911e-03 1.009696 0.9965632 0.8051425 16## 1.747528e-03 1.009696 0.9965632 0.8051425 17## 2.009233e-03 1.009696 0.9965632 0.8051425 18## 2.310130e-03 1.009696 0.9965632 0.8051425 19## 2.656088e-03 1.009696 0.9965632 0.8051425 20## 3.053856e-03 1.009696 0.9965632 0.8051425 21## 3.511192e-03 1.009696 0.9965632 0.8051425 22## 4.037017e-03 1.009696 0.9965632 0.8051425 23## 4.641589e-03 1.009696 0.9965632 0.8051425 24## 5.336699e-03 1.009696 0.9965632 0.8051425 25## 6.135907e-03 1.009696 0.9965632 0.8051425 26## 7.054802e-03 1.009696 0.9965632 0.8051425 27## 8.111308e-03 1.009696 0.9965632 0.8051425 28## 9.326033e-03 1.009696 0.9965632 0.8051425 29## 1.072267e-02 1.009696 0.9965632 0.8051425 30## 1.232847e-02 1.009696 0.9965632 0.8051425 31## 1.417474e-02 1.009696 0.9965632 0.8051425 32## 1.629751e-02 1.009696 0.9965632 0.8051425 33## 1.873817e-02 1.009696 0.9965632 0.8051425 34## 2.154435e-02 1.009696 0.9965632 0.8051425 35## 2.477076e-02 1.009696 0.9965632 0.8051425 36## 2.848036e-02 1.009696 0.9965632 0.8051425 37## 3.274549e-02 1.009696 0.9965632 0.8051425 38## 3.764936e-02 1.009696 0.9965632 0.8051425 39## 4.328761e-02 1.009696 0.9965632 0.8051425 40## 4.977024e-02 1.009696 0.9965632 0.8051425 41## 5.722368e-02 1.009696 0.9965632 0.8051425 42## 6.579332e-02 1.009696 0.9965632 0.8051425 43## 7.564633e-02 1.009637 0.9965632 0.8050666 44## 8.697490e-02 1.009216 0.9965637 0.8047862 45## 1.000000e-01 1.008901 0.9965636 0.8046468 46## 1.149757e-01 1.009470 0.9965616 0.8054790 47## 1.321941e-01 1.011206 0.9965561 0.8074253 48## 1.519911e-01 1.014475 0.9965476 0.8104930 49## 1.747528e-01 1.019202 0.9965383 0.8147296 50## 2.009233e-01 1.025943 0.9965259 0.8203974 51## 2.310130e-01 1.035374 0.9965094 0.8284187 52## 2.656088e-01 1.048294 0.9964878 0.8393282 53## 3.053856e-01 1.065717 0.9964592 0.8530952 54## 3.511192e-01 1.088903 0.9964215 0.8701072 55## 4.037017e-01 1.119433 0.9963715 0.8918217 56## 4.641589e-01 1.158919 0.9963053 0.9193677 57## 5.336699e-01 1.209841 0.9962136 0.9532842 58## 6.135907e-01 1.275467 0.9960778 0.9957151 59## 7.054802e-01 1.357247 0.9958966 1.0471169 60## 8.111308e-01 1.457886 0.9956561 1.1087362 61## 9.326033e-01 1.580743 0.9953362 1.1818188 62## 1.072267e+00 1.729330 0.9949070 1.2696235 63## 1.232847e+00 1.907599 0.9943306 1.3758463 64## 1.417474e+00 2.120178 0.9935518 1.5059031 65## 1.629751e+00 2.369642 0.9924954 1.6673393 66## 1.873817e+00 2.662906 0.9910539 1.8621728 67## 2.154435e+00 3.007271 0.9890638 2.0978907 68## 2.477076e+00 3.409377 0.9863097 2.3788439 69## 2.848036e+00 3.864727 0.9825900 2.7053428 70## 3.274549e+00 4.350785 0.9778541 3.0659309 71## 3.764936e+00 4.847045 0.9724311 3.4403210 72## 4.328761e+00 5.369017 0.9668240 3.8351441 73## 4.977024e+00 5.919492 0.9626812 4.2512694 74## 5.722368e+00 6.562134 0.9580843 4.7389049 75## 6.579332e+00 7.307112 0.9534537 5.2945905 76## 7.564633e+00 8.132296 0.9500300 5.8774541 77## 8.697490e+00 9.067321 0.9486589 6.4760997 78## 1.000000e+01 10.167822 0.9483195 7.1226569 79## 1.149757e+01 11.473284 0.9482975 7.8556639 80## 1.321941e+01 13.002703 0.9482975 8.6990451 81## 1.519911e+01 14.727852 0.9454119 9.6414650 82## 1.747528e+01 16.325210 0.9426796 10.5303097 83## 2.009233e+01 17.740599 0.9357286 11.3560865 84## 2.310130e+01 18.585795 0.9227167 11.8799668 85## 2.656088e+01 18.939596 0.9080584 12.1336575 86## 3.053856e+01 19.123568 0.9109065 12.2733471 87## 3.511192e+01 19.197966 NaN 12.3308613 88## 4.037017e+01 19.197966 NaN 12.3308613 89## 4.641589e+01 19.197966 NaN 12.3308613 90## 5.336699e+01 19.197966 NaN 12.3308613 91## 6.135907e+01 19.197966 NaN 12.3308613 92## 7.054802e+01 19.197966 NaN 12.3308613 93## 8.111308e+01 19.197966 NaN 12.3308613 94## 9.326033e+01 19.197966 NaN 12.3308613 95## 1.072267e+02 19.197966 NaN 12.3308613 96## 1.232847e+02 19.197966 NaN 12.3308613 97## 1.417474e+02 19.197966 NaN 12.3308613 98## 1.629751e+02 19.197966 NaN 12.3308613 99## 1.873817e+02 19.197966 NaN 12.3308613 100## 2.154435e+02 19.197966 NaN 12.3308613 101## 2.477076e+02 19.197966 NaN 12.3308613 102## 2.848036e+02 19.197966 NaN 12.3308613 103## 3.274549e+02 19.197966 NaN 12.3308613 104## 3.764936e+02 19.197966 NaN 12.3308613 105## 4.328761e+02 19.197966 NaN 12.3308613 106## 4.977024e+02 19.197966 NaN 12.3308613 107## 5.722368e+02 19.197966 NaN 12.3308613 108## 6.579332e+02 19.197966 NaN 12.3308613 109## 7.564633e+02 19.197966 NaN 12.3308613 110## 8.697490e+02 19.197966 NaN 12.3308613 111## 1.000000e+03 19.197966 NaN 12.3308613 112## 113## Tuning parameter \u0026#39;alpha\u0026#39; was held constant at a value of 1 114## RMSE was used to select the optimal model using the smallest value. 115## The final values used for the model were alpha = 1 and lambda = 0.1. 1lassoCaret %\u0026gt;% ggplot  1lassoCaret %\u0026gt;% varImp %\u0026gt;% ggplot  1library(glmnet) 1## Loading required package: Matrix 1## 2## Attaching package: \u0026#39;Matrix\u0026#39; 1## The following objects are masked from \u0026#39;package:tidyr\u0026#39;: 2## 3## expand, pack, unpack 1## Loaded glmnet 3.0-2 1library(boot) 1## 2## Attaching package: \u0026#39;boot\u0026#39; 1## The following object is masked from \u0026#39;package:lattice\u0026#39;: 2## 3## melanoma 1lasso.mod \u0026lt;- cv.glmnet(as.matrix(df[-1]), y, alpha=1) 2lambda \u0026lt;- lasso.mod$lambda.min 3plot(lasso.mod)  1predict(lasso.mod, s=lambda, type=\u0026#34;coefficients\u0026#34;) 1## 11 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) 42.975240 4## x 5.005023 5## x2 2.947540 6## x3 5.989105 7## x4 . 8## x5 . 9## x6 . 10## x7 . 11## x8 . 12## x9 . 13## x10 . Clearly, the only important variables are \\(x\\), \\(x^2\\) and \\(x^3\\).\nf) New model Our new model requires a newly expanded set of betas as well.\n1y2\u0026lt;-beta[1]+23*x^7+noise 1modelX\u0026lt;-poly(x,10,raw=T) 2newDF\u0026lt;-data.frame(x=as.matrix(modelX),y=y2) 3newSub\u0026lt;-regsubsets(y2~.,data=newDF,nvmax=10) 4newSub %\u0026gt;% summary 1## Subset selection object 2## Call: regsubsets.formula(y2 ~ ., data = newDF, nvmax = 10) 3## 11 Variables (and intercept) 4## Forced in Forced out 5## x.1 FALSE FALSE 6## x.2 FALSE FALSE 7## x.3 FALSE FALSE 8## x.4 FALSE FALSE 9## x.5 FALSE FALSE 10## x.6 FALSE FALSE 11## x.7 FALSE FALSE 12## x.8 FALSE FALSE 13## x.9 FALSE FALSE 14## x.10 FALSE FALSE 15## y FALSE FALSE 16## 1 subsets of each size up to 10 17## Selection Algorithm: exhaustive 18## x.1 x.2 x.3 x.4 x.5 x.6 x.7 x.8 x.9 x.10 y 19## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 20## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 21## 3 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 22## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 23## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 24## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 25## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 26## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 27## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 28## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 1plotLEAP(newSub %\u0026gt;% summary)  Or in its more native look,\n1par(mfrow=c(2,2)) 2plot(newSub) 3plot(newSub,scale=\u0026#39;Cp\u0026#39;) 4plot(newSub,scale=\u0026#39;r2\u0026#39;) 5plot(newSub,scale=\u0026#39;adjr2\u0026#39;)  1library(glmnet) 2library(boot) 3lasso.mod2 \u0026lt;- cv.glmnet(as.matrix(newDF[-1]), y, alpha=1) 4lambda2 \u0026lt;- lasso.mod2$lambda.min 5plot(lasso.mod2)  1predict(lasso.mod2, s=lambda, type=\u0026#34;coefficients\u0026#34;) 1## 11 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) 42.67982691 4## x.2 3.22521396 5## x.3 8.56699146 6## x.4 . 7## x.5 -0.10229572 8## x.6 . 9## x.7 -0.03184905 10## x.8 . 11## x.9 . 12## x.10 . 13## y . 1lambda\u0026lt;-10^seq(-3, 3, length = 100) 2lassocaret2= train(y~.,data=newDF,method=\u0026#34;glmnet\u0026#34;,tuneGrid=expand.grid(alpha=1,lambda=lambda)) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1lassocaret2 %\u0026gt;% print 1## glmnet 2## 3## 100 samples 4## 10 predictor 5## 6## No pre-processing 7## Resampling: Bootstrapped (25 reps) 8## Summary of sample sizes: 100, 100, 100, 100, 100, 100, ... 9## Resampling results across tuning parameters: 10## 11## lambda RMSE Rsquared MAE 12## 1.000000e-03 40.03231 0.9999955 14.48774 13## 1.149757e-03 40.03231 0.9999955 14.48774 14## 1.321941e-03 40.03231 0.9999955 14.48774 15## 1.519911e-03 40.03231 0.9999955 14.48774 16## 1.747528e-03 40.03231 0.9999955 14.48774 17## 2.009233e-03 40.03231 0.9999955 14.48774 18## 2.310130e-03 40.03231 0.9999955 14.48774 19## 2.656088e-03 40.03231 0.9999955 14.48774 20## 3.053856e-03 40.03231 0.9999955 14.48774 21## 3.511192e-03 40.03231 0.9999955 14.48774 22## 4.037017e-03 40.03231 0.9999955 14.48774 23## 4.641589e-03 40.03231 0.9999955 14.48774 24## 5.336699e-03 40.03231 0.9999955 14.48774 25## 6.135907e-03 40.03231 0.9999955 14.48774 26## 7.054802e-03 40.03231 0.9999955 14.48774 27## 8.111308e-03 40.03231 0.9999955 14.48774 28## 9.326033e-03 40.03231 0.9999955 14.48774 29## 1.072267e-02 40.03231 0.9999955 14.48774 30## 1.232847e-02 40.03231 0.9999955 14.48774 31## 1.417474e-02 40.03231 0.9999955 14.48774 32## 1.629751e-02 40.03231 0.9999955 14.48774 33## 1.873817e-02 40.03231 0.9999955 14.48774 34## 2.154435e-02 40.03231 0.9999955 14.48774 35## 2.477076e-02 40.03231 0.9999955 14.48774 36## 2.848036e-02 40.03231 0.9999955 14.48774 37## 3.274549e-02 40.03231 0.9999955 14.48774 38## 3.764936e-02 40.03231 0.9999955 14.48774 39## 4.328761e-02 40.03231 0.9999955 14.48774 40## 4.977024e-02 40.03231 0.9999955 14.48774 41## 5.722368e-02 40.03231 0.9999955 14.48774 42## 6.579332e-02 40.03231 0.9999955 14.48774 43## 7.564633e-02 40.03231 0.9999955 14.48774 44## 8.697490e-02 40.03231 0.9999955 14.48774 45## 1.000000e-01 40.03231 0.9999955 14.48774 46## 1.149757e-01 40.03231 0.9999955 14.48774 47## 1.321941e-01 40.03231 0.9999955 14.48774 48## 1.519911e-01 40.03231 0.9999955 14.48774 49## 1.747528e-01 40.03231 0.9999955 14.48774 50## 2.009233e-01 40.03231 0.9999955 14.48774 51## 2.310130e-01 40.03231 0.9999955 14.48774 52## 2.656088e-01 40.03231 0.9999955 14.48774 53## 3.053856e-01 40.03231 0.9999955 14.48774 54## 3.511192e-01 40.03231 0.9999955 14.48774 55## 4.037017e-01 40.03231 0.9999955 14.48774 56## 4.641589e-01 40.03231 0.9999955 14.48774 57## 5.336699e-01 40.03231 0.9999955 14.48774 58## 6.135907e-01 40.03231 0.9999955 14.48774 59## 7.054802e-01 40.03231 0.9999955 14.48774 60## 8.111308e-01 40.03231 0.9999955 14.48774 61## 9.326033e-01 40.03231 0.9999955 14.48774 62## 1.072267e+00 40.03231 0.9999955 14.48774 63## 1.232847e+00 40.03231 0.9999955 14.48774 64## 1.417474e+00 40.03231 0.9999955 14.48774 65## 1.629751e+00 40.03231 0.9999955 14.48774 66## 1.873817e+00 40.03231 0.9999955 14.48774 67## 2.154435e+00 40.03231 0.9999955 14.48774 68## 2.477076e+00 40.03231 0.9999955 14.48774 69## 2.848036e+00 40.03231 0.9999955 14.48774 70## 3.274549e+00 40.03231 0.9999955 14.48774 71## 3.764936e+00 40.03231 0.9999955 14.48774 72## 4.328761e+00 40.03231 0.9999955 14.48774 73## 4.977024e+00 40.03231 0.9999955 14.48774 74## 5.722368e+00 40.03231 0.9999955 14.48774 75## 6.579332e+00 40.03231 0.9999955 14.48774 76## 7.564633e+00 40.43005 0.9999955 14.59881 77## 8.697490e+00 41.25214 0.9999955 14.81913 78## 1.000000e+01 42.30446 0.9999955 15.09937 79## 1.149757e+01 43.59429 0.9999955 15.44307 80## 1.321941e+01 45.43633 0.9999955 15.93255 81## 1.519911e+01 47.55425 0.9999955 16.49605 82## 1.747528e+01 49.98935 0.9999955 17.14447 83## 2.009233e+01 52.90533 0.9999955 17.91650 84## 2.310130e+01 57.57589 0.9999955 19.10125 85## 2.656088e+01 63.25484 0.9999955 20.53147 86## 3.053856e+01 70.51580 0.9999955 22.36400 87## 3.511192e+01 78.93391 0.9999955 24.49105 88## 4.037017e+01 88.61274 0.9999955 26.93830 89## 4.641589e+01 99.97831 0.9999955 29.83601 90## 5.336699e+01 113.48225 0.9999955 33.39320 91## 6.135907e+01 129.17536 0.9999955 37.58303 92## 7.054802e+01 147.76452 0.9999957 42.74333 93## 8.111308e+01 169.60027 0.9999961 48.98043 94## 9.326033e+01 194.94266 0.9999965 56.29001 95## 1.072267e+02 224.07631 0.9999969 64.70026 96## 1.232847e+02 257.56092 0.9999971 74.36989 97## 1.417474e+02 296.13382 0.9999971 85.51504 98## 1.629751e+02 340.49129 0.9999971 98.33212 99## 1.873817e+02 391.49185 0.9999971 113.06864 100## 2.154435e+02 450.13031 0.9999971 130.01206 101## 2.477076e+02 509.28329 0.9999970 147.15405 102## 2.848036e+02 564.17558 0.9999969 163.34475 103## 3.274549e+02 618.84080 0.9999969 179.85589 104## 3.764936e+02 681.69265 0.9999969 198.83969 105## 4.328761e+02 741.14452 0.9999967 217.28049 106## 4.977024e+02 807.25385 0.9999967 237.88938 107## 5.722368e+02 883.26360 0.9999967 261.58461 108## 6.579332e+02 970.65640 0.9999967 288.82836 109## 7.564633e+02 1037.84801 0.9999960 312.54099 110## 8.697490e+02 1088.92551 0.9999960 334.04769 111## 1.000000e+03 1131.46176 0.9999955 354.62317 112## 113## Tuning parameter \u0026#39;alpha\u0026#39; was held constant at a value of 1 114## RMSE was used to select the optimal model using the smallest value. 115## The final values used for the model were alpha = 1 and lambda = 6.579332. 1lassocaret2 %\u0026gt;% ggplot  1lassocaret2 %\u0026gt;% varImp %\u0026gt;% ggplot  Clearly, the LASSO model has correctly reduced the model down to the correct single variable form, though best subset seems to suggest using more predictors, their coefficients are low enough to recognize that they are noise.\nQuestion 6.9 - Page 263 In this exercise, we will predict the number of applications received using the other variables in the College data set.\n(a) Split the data set into a training set and a test set.\n(b) Fit a linear model using least squares on the training set, and report the test error obtained.\n(c) Fit a ridge regression model on the training set, with \\(\\lambda\\) chosen by cross-validation. Report the test error obtained.\n(d) Fit a lasso model on the training set, with \\(\\lambda\\) chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.\n(e) Fit a PCR model on the training set, with \\(M\\) chosen by crossvalidation. Report the test error obtained, along with the value of \\(M\\) selected by cross-validation.\n(f) Fit a PLS model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation.\n(g) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?\nAnswer We will use the caret package, since at the moment, mlr3 does not have learners for PCR and PLS.\n1colDat\u0026lt;-ISLR::College 2colDat %\u0026gt;% summary %\u0026gt;% print 1## Private Apps Accept Enroll Top10perc 2## No :212 Min. : 81 Min. : 72 Min. : 35 Min. : 1.00 3## Yes:565 1st Qu.: 776 1st Qu.: 604 1st Qu.: 242 1st Qu.:15.00 4## Median : 1558 Median : 1110 Median : 434 Median :23.00 5## Mean : 3002 Mean : 2019 Mean : 780 Mean :27.56 6## 3rd Qu.: 3624 3rd Qu.: 2424 3rd Qu.: 902 3rd Qu.:35.00 7## Max. :48094 Max. :26330 Max. :6392 Max. :96.00 8## Top25perc F.Undergrad P.Undergrad Outstate 9## Min. : 9.0 Min. : 139 Min. : 1.0 Min. : 2340 10## 1st Qu.: 41.0 1st Qu.: 992 1st Qu.: 95.0 1st Qu.: 7320 11## Median : 54.0 Median : 1707 Median : 353.0 Median : 9990 12## Mean : 55.8 Mean : 3700 Mean : 855.3 Mean :10441 13## 3rd Qu.: 69.0 3rd Qu.: 4005 3rd Qu.: 967.0 3rd Qu.:12925 14## Max. :100.0 Max. :31643 Max. :21836.0 Max. :21700 15## Room.Board Books Personal PhD 16## Min. :1780 Min. : 96.0 Min. : 250 Min. : 8.00 17## 1st Qu.:3597 1st Qu.: 470.0 1st Qu.: 850 1st Qu.: 62.00 18## Median :4200 Median : 500.0 Median :1200 Median : 75.00 19## Mean :4358 Mean : 549.4 Mean :1341 Mean : 72.66 20## 3rd Qu.:5050 3rd Qu.: 600.0 3rd Qu.:1700 3rd Qu.: 85.00 21## Max. :8124 Max. :2340.0 Max. :6800 Max. :103.00 22## Terminal S.F.Ratio perc.alumni Expend 23## Min. : 24.0 Min. : 2.50 Min. : 0.00 Min. : 3186 24## 1st Qu.: 71.0 1st Qu.:11.50 1st Qu.:13.00 1st Qu.: 6751 25## Median : 82.0 Median :13.60 Median :21.00 Median : 8377 26## Mean : 79.7 Mean :14.09 Mean :22.74 Mean : 9660 27## 3rd Qu.: 92.0 3rd Qu.:16.50 3rd Qu.:31.00 3rd Qu.:10830 28## Max. :100.0 Max. :39.80 Max. :64.00 Max. :56233 29## Grad.Rate 30## Min. : 10.00 31## 1st Qu.: 53.00 32## Median : 65.00 33## Mean : 65.46 34## 3rd Qu.: 78.00 35## Max. :118.00 1colDat %\u0026gt;% str %\u0026gt;% print 1## \u0026#39;data.frame\u0026#39;: 777 obs. of 18 variables: 2## $ Private : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 2 2 2 2 2 2 2 2 ... 3## $ Apps : num 1660 2186 1428 417 193 ... 4## $ Accept : num 1232 1924 1097 349 146 ... 5## $ Enroll : num 721 512 336 137 55 158 103 489 227 172 ... 6## $ Top10perc : num 23 16 22 60 16 38 17 37 30 21 ... 7## $ Top25perc : num 52 29 50 89 44 62 45 68 63 44 ... 8## $ F.Undergrad: num 2885 2683 1036 510 249 ... 9## $ P.Undergrad: num 537 1227 99 63 869 ... 10## $ Outstate : num 7440 12280 11250 12960 7560 ... 11## $ Room.Board : num 3300 6450 3750 5450 4120 ... 12## $ Books : num 450 750 400 450 800 500 500 450 300 660 ... 13## $ Personal : num 2200 1500 1165 875 1500 ... 14## $ PhD : num 70 29 53 92 76 67 90 89 79 40 ... 15## $ Terminal : num 78 30 66 97 72 73 93 100 84 41 ... 16## $ S.F.Ratio : num 18.1 12.2 12.9 7.7 11.9 9.4 11.5 13.7 11.3 11.5 ... 17## $ perc.alumni: num 12 16 30 37 2 11 26 37 23 15 ... 18## $ Expend : num 7041 10527 8735 19016 10922 ... 19## $ Grad.Rate : num 60 56 54 59 15 55 63 73 80 52 ... 20## NULL 1colDat %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) %\u0026gt;% print 1## Private Apps Accept Enroll Top10perc Top25perc 2## 2 711 693 581 82 89 3## F.Undergrad P.Undergrad Outstate Room.Board Books Personal 4## 714 566 640 553 122 294 5## PhD Terminal S.F.Ratio perc.alumni Expend Grad.Rate 6## 78 65 173 61 744 81 Clearly, there are no psuedo-factors which might have been converted at this stage.\na) Train-Test split 1train_ind\u0026lt;-createDataPartition(colDat$Apps,p=0.8,times=1,list=FALSE) 2train_set\u0026lt;-colDat[train_ind,] 3test_set\u0026lt;-colDat[-train_ind,] b) Linear least squares 1linCol\u0026lt;-train(Apps~.,data=train_set,method=\u0026#34;lm\u0026#34;) 2linCol %\u0026gt;% summary 1## 2## Call: 3## lm(formula = .outcome ~ ., data = dat) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -5145.6 -414.8 -20.3 340.5 7526.8 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) -2.918e+02 4.506e+02 -0.648 0.517486 12## PrivateYes -5.351e+02 1.532e+02 -3.494 0.000511 *** 13## Accept 1.617e+00 4.258e-02 37.983 \u0026lt; 2e-16 *** 14## Enroll -1.012e+00 1.959e-01 -5.165 3.26e-07 *** 15## Top10perc 5.379e+01 6.221e+00 8.647 \u0026lt; 2e-16 *** 16## Top25perc -1.632e+01 5.046e+00 -3.235 0.001282 ** 17## F.Undergrad 6.836e-02 3.457e-02 1.978 0.048410 * 18## P.Undergrad 7.929e-02 3.367e-02 2.355 0.018854 * 19## Outstate -7.303e-02 2.098e-02 -3.481 0.000536 *** 20## Room.Board 1.695e-01 5.367e-02 3.159 0.001663 ** 21## Books 9.998e-02 2.578e-01 0.388 0.698328 22## Personal -3.145e-03 6.880e-02 -0.046 0.963553 23## PhD -8.926e+00 5.041e+00 -1.771 0.077112 . 24## Terminal -2.298e+00 5.608e+00 -0.410 0.682152 25## S.F.Ratio 6.038e+00 1.420e+01 0.425 0.670757 26## perc.alumni -5.085e-01 4.560e+00 -0.112 0.911249 27## Expend 4.668e-02 1.332e-02 3.505 0.000490 *** 28## Grad.Rate 9.042e+00 3.379e+00 2.676 0.007653 ** 29## --- 30## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 31## 32## Residual standard error: 1042 on 606 degrees of freedom 33## Multiple R-squared: 0.9332, Adjusted R-squared: 0.9313 34## F-statistic: 497.7 on 17 and 606 DF, p-value: \u0026lt; 2.2e-16 1linPred\u0026lt;-predict(linCol,test_set) 2linPred %\u0026gt;% postResample(obs = test_set$Apps) 1## RMSE Rsquared MAE 2## 1071.6360025 0.9017032 625.7827996 Do note that the metrics are calculated in a manner to ensure no negative values are obtained.\nc) Ridge regression with CV for λ 1L2Grid \u0026lt;- expand.grid(alpha=0, 2lambda=10^seq(from=-3,to=30,length=100)) 1ridgCol\u0026lt;-train(Apps~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L2Grid) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1ridgCol %\u0026gt;% summary %\u0026gt;% print 1## Length Class Mode 2## a0 100 -none- numeric 3## beta 1700 dgCMatrix S4 4## df 100 -none- numeric 5## dim 2 -none- numeric 6## lambda 100 -none- numeric 7## dev.ratio 100 -none- numeric 8## nulldev 1 -none- numeric 9## npasses 1 -none- numeric 10## jerr 1 -none- numeric 11## offset 1 -none- logical 12## call 5 -none- call 13## nobs 1 -none- numeric 14## lambdaOpt 1 -none- numeric 15## xNames 17 -none- character 16## problemType 1 -none- character 17## tuneValue 2 data.frame list 18## obsLevels 1 -none- logical 19## param 0 -none- list 1coef(ridgCol$finalModel, ridgCol$bestTune$lambda) %\u0026gt;% print 1## 18 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) -1.407775e+03 4## PrivateYes -5.854245e+02 5## Accept 1.042778e+00 6## Enroll 3.511219e-01 7## Top10perc 2.780211e+01 8## Top25perc 2.883536e-02 9## F.Undergrad 6.825141e-02 10## P.Undergrad 5.281320e-02 11## Outstate -2.011504e-02 12## Room.Board 2.155224e-01 13## Books 1.517585e-01 14## Personal -3.711406e-02 15## PhD -4.453155e+00 16## Terminal -3.783231e+00 17## S.F.Ratio 6.897360e+00 18## perc.alumni -9.301831e+00 19## Expend 5.601144e-02 20## Grad.Rate 1.259989e+01 1ggplot(ridgCol)  1ridgPred\u0026lt;-predict(ridgCol,test_set) 2ridgPred %\u0026gt;% postResample(obs = test_set$Apps) 1## RMSE Rsquared MAE 2## 1047.7545250 0.9051726 644.4535063 d) LASSO with CV for λ 1L1Grid \u0026lt;- expand.grid(alpha=1, # for lasso 2lambda=10^seq(from=-3,to=30,length=100)) 1lassoCol\u0026lt;-train(Apps~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L1Grid) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1lassoCol %\u0026gt;% summary %\u0026gt;% print 1## Length Class Mode 2## a0 81 -none- numeric 3## beta 1377 dgCMatrix S4 4## df 81 -none- numeric 5## dim 2 -none- numeric 6## lambda 81 -none- numeric 7## dev.ratio 81 -none- numeric 8## nulldev 1 -none- numeric 9## npasses 1 -none- numeric 10## jerr 1 -none- numeric 11## offset 1 -none- logical 12## call 5 -none- call 13## nobs 1 -none- numeric 14## lambdaOpt 1 -none- numeric 15## xNames 17 -none- character 16## problemType 1 -none- character 17## tuneValue 2 data.frame list 18## obsLevels 1 -none- logical 19## param 0 -none- list 1coef(lassoCol$finalModel, lassoCol$bestTune$lambda) %\u0026gt;% print 1## 18 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) -325.51554340 4## PrivateYes -532.28956305 5## Accept 1.60370798 6## Enroll -0.90158328 7## Top10perc 51.96610325 8## Top25perc -14.87886847 9## F.Undergrad 0.05352324 10## P.Undergrad 0.07832395 11## Outstate -0.07047302 12## Room.Board 0.16783269 13## Books 0.08836704 14## Personal . 15## PhD -8.67634519 16## Terminal -2.18494018 17## S.F.Ratio 5.25050018 18## perc.alumni -0.67848535 19## Expend 0.04597728 20## Grad.Rate 8.67569015 1ggplot(lassoCol)  1lassoPred\u0026lt;-predict(lassoCol,test_set) 2lassoPred %\u0026gt;% postResample(obs = test_set$Apps) 1## RMSE Rsquared MAE 2## 1068.9834769 0.9021268 622.7029418 e) PCR with CV for M 1mGrid \u0026lt;- expand.grid(ncomp=seq(from=1,to=20,length=10)) 1pcrCol\u0026lt;-train(Apps~.,data=train_set,method=\u0026#34;pcr\u0026#34;,tuneGrid = mGrid) 2pcrCol %\u0026gt;% summary %\u0026gt;% print 1## Data: X dimension: 624 17 2## Y dimension: 624 1 3## Fit method: svdpc 4## Number of components considered: 17 5## TRAINING: % variance explained 6## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps 7## X 48.2314 87.24 95.02 97.26 98.63 99.43 99.91 8## .outcome 0.2419 76.54 77.88 80.19 91.27 91.34 91.34 9## 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps 14 comps 10## X 99.96 100.00 100.00 100.00 100.00 100.00 100.00 11## .outcome 91.65 91.66 92.26 92.65 92.66 92.67 92.76 12## 15 comps 16 comps 17 comps 13## X 100.00 100.00 100.00 14## .outcome 93.17 93.18 93.32 15## NULL 1ggplot(pcrCol)  1pcrPred\u0026lt;-predict(pcrCol,test_set) 2pcrPred %\u0026gt;% postResample(obs = test_set$Apps) 1## RMSE Rsquared MAE 2## 1071.6360025 0.9017032 625.7827996 f) PLS with CV for M 1plsCol\u0026lt;-train(Apps~.,data=train_set,method=\u0026#34;pls\u0026#34;,tuneGrid = mGrid) 2plsCol %\u0026gt;% summary %\u0026gt;% print 1## Data: X dimension: 624 17 2## Y dimension: 624 1 3## Fit method: oscorespls 4## Number of components considered: 17 5## TRAINING: % variance explained 6## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps 7## X 39.02 56.4 91.83 96.61 98.62 99.22 99.49 8## .outcome 78.04 84.1 86.88 91.09 91.38 91.49 91.66 9## 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps 14 comps 10## X 99.96 99.99 100.00 100.00 100.00 100.00 100.00 11## .outcome 91.68 91.85 92.64 92.87 93.16 93.18 93.18 12## 15 comps 16 comps 17 comps 13## X 100.00 100.00 100.00 14## .outcome 93.18 93.19 93.32 15## NULL 1ggplot(plsCol)  1plsPred\u0026lt;-predict(plsCol,test_set) 2plsPred %\u0026gt;% postResample(obs = test_set$Apps) 1## RMSE Rsquared MAE 2## 1071.6360039 0.9017032 625.7827987 g) Comments and Comparison 1models \u0026lt;- list(ridge = ridgCol, lasso = lassoCol, pcr = pcrCol, pls=plsCol,linear=linCol) 2resamples(models) %\u0026gt;% summary 1## 2## Call: 3## summary.resamples(object = .) 4## 5## Models: ridge, lasso, pcr, pls, linear 6## Number of resamples: 25 7## 8## MAE 9## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s 10## ridge 536.9612 600.5398 623.2005 649.6713 707.4014 793.4972 0 11## lasso 573.8563 616.3883 671.9453 655.8858 691.7620 732.2155 0 12## pcr 576.1427 618.8694 650.0360 662.9040 714.8491 767.5535 0 13## pls 553.3999 607.9757 637.1985 638.6619 668.5120 735.4479 0 14## linear 556.5553 619.2395 654.1478 659.4635 686.7747 792.4912 0 15## 16## RMSE 17## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s 18## ridge 882.2646 920.5934 1000.519 1168.603 1163.377 1939.541 0 19## lasso 801.9415 990.0724 1168.234 1184.329 1302.221 1584.712 0 20## pcr 828.1370 942.2678 1131.207 1144.071 1284.178 1544.078 0 21## pls 786.7989 1038.3265 1167.764 1157.026 1274.041 1461.434 0 22## linear 798.3771 1063.3690 1134.291 1135.977 1215.115 1403.576 0 23## 24## Rsquared 25## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s 26## ridge 0.8735756 0.8962010 0.9185736 0.9136429 0.9306819 0.9474913 0 27## lasso 0.8851991 0.9132766 0.9217660 0.9191638 0.9284838 0.9398772 0 28## pcr 0.8658504 0.9080179 0.9235117 0.9146884 0.9281892 0.9471991 0 29## pls 0.8881249 0.9080786 0.9183968 0.9173632 0.9258994 0.9420894 0 30## linear 0.8840049 0.8986452 0.9222319 0.9160913 0.9296275 0.9492198 0 1resamples(models) %\u0026gt;% bwplot(scales=\u0026#34;free\u0026#34;)   Given the tighter spread of PLS, it seems more reliable than PCR Ridge is just poor in every way OLS does well, but it also has a worrying outlier LASSO appears to be doing alright as well We also have kept track of the performance on the test_set  We might want to see the variable significance values as well.\n1lgp\u0026lt;-linCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;OLS Variable Importance\u0026#34;) 2rgp\u0026lt;-ridgCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Ridge Variable Importance\u0026#34;) 3lsgp\u0026lt;-lassoCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Lasso Variable Importance\u0026#34;) 4pcgp\u0026lt;-pcrCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;PCR Variable Importance\u0026#34;) 5plgp\u0026lt;-plsCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;PLS Variable Importance\u0026#34;) 6grid.arrange(lgp,rgp,lsgp,pcgp,plgp,ncol=3)  Question 6.10 - Pages 263-264 We have seen that as the number of features used in a model increases, the training error will necessarily decrease, but the test error may not. We will now explore this in a simulated data set.\n(a) Generate a data set with \\(p = 20\\) features, \\(n = 1,000\\) observations, and an associated quantitative response vector generated according to the model \\[Y = X\\beta + \\eta,\\] where \\(\\beta\\) has some elements that are exactly equal to zero.\n(b) Split your data set into a training set containing \\(100\\) observations and a test set containing \\(900\\) observations.\n(c) Perform best subset selection on the training set, and plot the training set MSE associated with the best model of each size.\n(d) Plot the test set MSE associated with the best model of each size.\n(e) For which model size does the test set MSE take on its minimum value? Comment on your results. If it takes on its minimum value for a model containing only an intercept or a model containing all of the features, then play around with the way that you are generating the data in (a) until you come up with a scenario in which the test set MSE is minimized for an intermediate model size.\n(f) How does the model at which the test set MSE is minimized compare to the true model used to generate the data? Comment on the coefficient values.\n(g) Create a plot displaying \\(\\sqrt{\\Sum_{j=1}^{p}(\\beta_{j}-\\hat{\\beta}_{j}^{r})^{2}}\\) for a range of values of \\(r\\), where \\(\\hat{\\beta}_{j}^{r}\\) is the $j$th coefficient estimate for the best model containing \\(r\\) coefficients. Comment on what you observe. How does this compare to the test MSE plot from (d)?\nAnswer Model creation 1p=20 2n=1000 3noise\u0026lt;-rnorm(n) 4xmat\u0026lt;-matrix(rnorm(n*p),nrow=n,ncol=p) 5beta\u0026lt;-sample(-10:34,20) 6beta[sample(1:20,4)]=0 7myY\u0026lt;-xmat %*% beta + noise 8modelDat\u0026lt;-data.frame(x=as.matrix(xmat),y=myY)  As always we will want to take a peak   1modelDat %\u0026gt;% str %\u0026gt;% print 1## \u0026#39;data.frame\u0026#39;: 1000 obs. of 21 variables: 2## $ x.1 : num -0.406 -1.375 0.858 -0.231 -0.601 ... 3## $ x.2 : num -0.129 -0.218 -0.17 0.573 -0.513 ... 4## $ x.3 : num 0.127 -0.224 1.014 0.896 0.159 ... 5## $ x.4 : num 0.499 -0.151 -0.488 -0.959 2.187 ... 6## $ x.5 : num -0.235 -0.345 -0.773 -0.346 0.773 ... 7## $ x.6 : num 0.26 -0.429 -1.183 -1.159 0.959 ... 8## $ x.7 : num 0.567 1.647 0.149 -0.593 -0.902 ... 9## $ x.8 : num -0.092 0.8391 -1.4835 0.0229 -0.1353 ... 10## $ x.9 : num -0.998 -1.043 -0.563 -0.377 0.324 ... 11## $ x.10: num -0.4401 -0.195 -0.5139 -0.0156 -0.9543 ... 12## $ x.11: num -0.147 0.829 0.165 0.101 -0.105 ... 13## $ x.12: num -0.0118 1.02 1.0794 1.3184 -2.2844 ... 14## $ x.13: num -1.683 0.487 -1.142 -0.744 -0.175 ... 15## $ x.14: num 0.228 -1.031 -2.798 -0.646 0.56 ... 16## $ x.15: num -0.718 0.508 0.637 -0.556 0.585 ... 17## $ x.16: num -1.6378 0.581 -0.9939 0.0537 -0.5854 ... 18## $ x.17: num 1.758 -0.616 1.377 -0.876 -1.174 ... 19## $ x.18: num -1.438 0.373 1.364 0.399 0.949 ... 20## $ x.19: num -0.715 -0.731 1.142 0.149 0.916 ... 21## $ x.20: num 2.774 -2.024 1.316 0.138 0.187 ... 22## $ y : num 77.5 -82.8 -38.9 -79.7 64.9 ... 23## NULL 1modelDat %\u0026gt;% summary %\u0026gt;% print 1## x.1 x.2 x.3 x.4 2## Min. :-2.79766 Min. :-3.13281 Min. :-2.71232 Min. :-4.29604 3## 1st Qu.:-0.60516 1st Qu.:-0.66759 1st Qu.:-0.60561 1st Qu.:-0.66598 4## Median : 0.04323 Median : 0.03681 Median : 0.06556 Median : 0.06589 5## Mean : 0.06879 Mean : 0.01004 Mean : 0.06443 Mean : 0.02244 6## 3rd Qu.: 0.74049 3rd Qu.: 0.68234 3rd Qu.: 0.70521 3rd Qu.: 0.71174 7## Max. : 3.50354 Max. : 3.47268 Max. : 3.02817 Max. : 3.27326 8## x.5 x.6 x.7 x.8 9## Min. :-3.228376 Min. :-4.24014 Min. :-2.98577 Min. :-3.27770 10## 1st Qu.:-0.698220 1st Qu.:-0.69448 1st Qu.:-0.59092 1st Qu.:-0.52939 11## Median :-0.058778 Median :-0.01141 Median : 0.01732 Median : 0.05703 12## Mean : 0.000126 Mean :-0.05158 Mean : 0.04767 Mean : 0.08231 13## 3rd Qu.: 0.663570 3rd Qu.: 0.64217 3rd Qu.: 0.67438 3rd Qu.: 0.72849 14## Max. : 3.036307 Max. : 3.27572 Max. : 2.72163 Max. : 3.33409 15## x.9 x.10 x.11 x.12 16## Min. :-3.08957 Min. :-3.21268 Min. :-3.00572 Min. :-3.72016 17## 1st Qu.:-0.65456 1st Qu.:-0.69401 1st Qu.:-0.68226 1st Qu.:-0.63043 18## Median :-0.04242 Median :-0.03069 Median :-0.04777 Median : 0.07079 19## Mean : 0.02049 Mean :-0.02400 Mean :-0.03729 Mean : 0.03769 20## 3rd Qu.: 0.71209 3rd Qu.: 0.61540 3rd Qu.: 0.64873 3rd Qu.: 0.67155 21## Max. : 3.23110 Max. : 2.76059 Max. : 2.87306 Max. : 3.48569 22## x.13 x.14 x.15 x.16 23## Min. :-3.20126 Min. :-3.55432 Min. :-2.857575 Min. :-3.5383 24## 1st Qu.:-0.68535 1st Qu.:-0.66752 1st Qu.:-0.658708 1st Qu.:-0.7813 25## Median :-0.01329 Median :-0.03302 Median : 0.020581 Median :-0.0740 26## Mean : 0.01094 Mean : 0.02113 Mean : 0.007976 Mean :-0.0883 27## 3rd Qu.: 0.64877 3rd Qu.: 0.74919 3rd Qu.: 0.670464 3rd Qu.: 0.5568 28## Max. : 2.78973 Max. : 3.47923 Max. : 2.891527 Max. : 3.0938 29## x.17 x.18 x.19 x.20 30## Min. :-3.28570 Min. :-4.06416 Min. :-3.0443 Min. :-4.06307 31## 1st Qu.:-0.72302 1st Qu.:-0.72507 1st Qu.:-0.6684 1st Qu.:-0.70518 32## Median :-0.02439 Median :-0.04941 Median :-0.0610 Median :-0.07697 33## Mean :-0.01459 Mean :-0.03164 Mean :-0.0414 Mean :-0.05302 34## 3rd Qu.: 0.62692 3rd Qu.: 0.68115 3rd Qu.: 0.6381 3rd Qu.: 0.58597 35## Max. : 2.86446 Max. : 3.32958 Max. : 3.1722 Max. : 3.01358 36## y 37## Min. :-199.268 38## 1st Qu.: -54.758 39## Median : -1.607 40## Mean : -1.710 41## 3rd Qu.: 49.367 42## Max. : 278.244 b) Train Test Split 1train_ind = sample(modelDat %\u0026gt;% nrow,100) 2test_ind = setdiff(seq_len(modelDat %\u0026gt;% nrow), train_set) Best subset selection 1train_set\u0026lt;-modelDat[train_ind,] 2test_set\u0026lt;-modelDat[-train_ind,] 1linCol\u0026lt;-train(y~.,data=train_set,method=\u0026#34;lm\u0026#34;) 2linCol %\u0026gt;% summary 1## 2## Call: 3## lm(formula = .outcome ~ ., data = dat) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -2.12474 -0.53970 -0.00944 0.42398 2.21086 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) -0.06052 0.09604 -0.630 0.530 12## x.1 -0.02265 0.09198 -0.246 0.806 13## x.2 28.91650 0.09879 292.719 \u0026lt;2e-16 *** 14## x.3 14.16532 0.09343 151.610 \u0026lt;2e-16 *** 15## x.4 28.16256 0.09828 286.564 \u0026lt;2e-16 *** 16## x.5 0.13742 0.09658 1.423 0.159 17## x.6 27.01497 0.08540 316.341 \u0026lt;2e-16 *** 18## x.7 31.15917 0.09003 346.092 \u0026lt;2e-16 *** 19## x.8 -9.66308 0.11095 -87.094 \u0026lt;2e-16 *** 20## x.9 0.11641 0.10768 1.081 0.283 21## x.10 19.06687 0.09662 197.344 \u0026lt;2e-16 *** 22## x.11 -9.09956 0.08627 -105.472 \u0026lt;2e-16 *** 23## x.12 -8.01933 0.10198 -78.633 \u0026lt;2e-16 *** 24## x.13 4.26852 0.09888 43.170 \u0026lt;2e-16 *** 25## x.14 20.22366 0.09853 205.247 \u0026lt;2e-16 *** 26## x.15 -0.16607 0.10466 -1.587 0.117 27## x.16 7.95594 0.11250 70.721 \u0026lt;2e-16 *** 28## x.17 10.89851 0.11157 97.684 \u0026lt;2e-16 *** 29## x.18 -1.09760 0.09391 -11.688 \u0026lt;2e-16 *** 30## x.19 22.05197 0.08697 253.553 \u0026lt;2e-16 *** 31## x.20 20.88796 0.09274 225.221 \u0026lt;2e-16 *** 32## --- 33## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 34## 35## Residual standard error: 0.8583 on 79 degrees of freedom 36## Multiple R-squared: 0.9999, Adjusted R-squared: 0.9999 37## F-statistic: 4.71e+04 on 20 and 79 DF, p-value: \u0026lt; 2.2e-16 1linPred\u0026lt;-predict(linCol,test_set) 2linPred %\u0026gt;% postResample(obs = test_set$y) 1## RMSE Rsquared MAE 2## 1.2151815 0.9997265 0.9638378 1L2Grid \u0026lt;- expand.grid(alpha=0, 2lambda=10^seq(from=-3,to=30,length=100)) 1ridgCol\u0026lt;-train(y~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L2Grid) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1ridgCol %\u0026gt;% summary %\u0026gt;% print 1## Length Class Mode 2## a0 100 -none- numeric 3## beta 2000 dgCMatrix S4 4## df 100 -none- numeric 5## dim 2 -none- numeric 6## lambda 100 -none- numeric 7## dev.ratio 100 -none- numeric 8## nulldev 1 -none- numeric 9## npasses 1 -none- numeric 10## jerr 1 -none- numeric 11## offset 1 -none- logical 12## call 5 -none- call 13## nobs 1 -none- numeric 14## lambdaOpt 1 -none- numeric 15## xNames 20 -none- character 16## problemType 1 -none- character 17## tuneValue 2 data.frame list 18## obsLevels 1 -none- logical 19## param 0 -none- list 1coef(ridgCol$finalModel, ridgCol$bestTune$lambda) %\u0026gt;% print 1## 21 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) 0.03898376 4## x.1 -0.12140945 5## x.2 27.63771674 6## x.3 13.46853844 7## x.4 26.54402352 8## x.5 -0.13838118 9## x.6 25.87706885 10## x.7 29.90687677 11## x.8 -9.42088971 12## x.9 -0.08983349 13## x.10 17.45444598 14## x.11 -8.33991071 15## x.12 -7.23653865 16## x.13 3.35145521 17## x.14 19.42178898 18## x.15 -0.02794731 19## x.16 7.63951382 20## x.17 11.08083907 21## x.18 -1.36872894 22## x.19 20.90257005 23## x.20 20.07494414 1ggplot(ridgCol)  1ridgPred\u0026lt;-predict(ridgCol,test_set) 2ridgPred %\u0026gt;% postResample(obs = test_set$y) 1## RMSE Rsquared MAE 2## 3.7554417 0.9994231 3.0184859 1L1Grid \u0026lt;- expand.grid(alpha=1, # for lasso 2lambda=10^seq(from=-3,to=30,length=100)) 1lassoCol\u0026lt;-train(y~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L1Grid) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1lassoCol %\u0026gt;% summary %\u0026gt;% print 1## Length Class Mode 2## a0 47 -none- numeric 3## beta 940 dgCMatrix S4 4## df 47 -none- numeric 5## dim 2 -none- numeric 6## lambda 47 -none- numeric 7## dev.ratio 47 -none- numeric 8## nulldev 1 -none- numeric 9## npasses 1 -none- numeric 10## jerr 1 -none- numeric 11## offset 1 -none- logical 12## call 5 -none- call 13## nobs 1 -none- numeric 14## lambdaOpt 1 -none- numeric 15## xNames 20 -none- character 16## problemType 1 -none- character 17## tuneValue 2 data.frame list 18## obsLevels 1 -none- logical 19## param 0 -none- list 1coef(lassoCol$finalModel, lassoCol$bestTune$lambda) %\u0026gt;% print 1## 21 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) 0.1158884 4## x.1 . 5## x.2 28.5897869 6## x.3 13.3637110 7## x.4 27.2558797 8## x.5 . 9## x.6 26.6625588 10## x.7 30.6841774 11## x.8 -9.1388677 12## x.9 . 13## x.10 17.9220939 14## x.11 -8.2461257 15## x.12 -7.0603651 16## x.13 3.2052101 17## x.14 19.7219890 18## x.15 . 19## x.16 7.2082509 20## x.17 10.4137411 21## x.18 -0.6693664 22## x.19 21.5357460 23## x.20 20.5226071 1ggplot(lassoCol)  1lassoPred\u0026lt;-predict(lassoCol,test_set) 2lassoPred %\u0026gt;% postResample(obs = test_set$y) 1## RMSE Rsquared MAE 2## 2.7289452 0.9992454 2.2029482 1mGrid \u0026lt;- expand.grid(ncomp=seq(from=1,to=20,length=10)) 1pcrCol\u0026lt;-train(y~.,data=train_set,method=\u0026#34;pcr\u0026#34;,tuneGrid = mGrid) 2pcrCol %\u0026gt;% summary %\u0026gt;% print 1## Data: X dimension: 100 20 2## Y dimension: 100 1 3## Fit method: svdpc 4## Number of components considered: 20 5## TRAINING: % variance explained 6## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps 7## X 10.040 18.46 26.62 34.56 41.87 48.54 54.56 8## .outcome 8.425 34.90 41.09 43.12 45.06 48.09 66.44 9## 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps 14 comps 10## X 60.33 65.37 70.01 74.46 78.72 82.32 85.67 11## .outcome 85.76 88.81 89.93 91.66 91.91 92.04 92.08 12## 15 comps 16 comps 17 comps 18 comps 19 comps 20 comps 13## X 88.85 91.94 94.59 96.73 98.51 100.00 14## .outcome 92.15 94.96 99.51 99.57 99.76 99.99 15## NULL 1ggplot(pcrCol)  1pcrPred\u0026lt;-predict(pcrCol,test_set) 2pcrPred %\u0026gt;% postResample(obs = test_set$y) 1## RMSE Rsquared MAE 2## 1.2151815 0.9997265 0.9638378 1plsCol\u0026lt;-train(y~.,data=train_set,method=\u0026#34;pls\u0026#34;,tuneGrid = mGrid) 2plsCol %\u0026gt;% summary %\u0026gt;% print 1## Data: X dimension: 100 20 2## Y dimension: 100 1 3## Fit method: oscorespls 4## Number of components considered: 20 5## TRAINING: % variance explained 6## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps 7## X 7.762 14.79 21.01 26.89 31.55 36.13 41.12 8## .outcome 92.765 98.81 99.75 99.96 99.98 99.99 99.99 9## 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps 14 comps 10## X 46.35 51.21 56.12 60.34 65.63 71.57 76.16 11## .outcome 99.99 99.99 99.99 99.99 99.99 99.99 99.99 12## 15 comps 16 comps 17 comps 18 comps 19 comps 20 comps 13## X 80.72 84.69 88.98 92.69 96.71 100.00 14## .outcome 99.99 99.99 99.99 99.99 99.99 99.99 15## NULL 1ggplot(plsCol)  1plsPred\u0026lt;-predict(plsCol,test_set) 2plsPred %\u0026gt;% postResample(obs = test_set$y) 1## RMSE Rsquared MAE 2## 1.2151815 0.9997265 0.9638378 d) Test MSE for best models  All the models have the same R² but Ridge does the worst followed by LASSO  For the rest of the question, we will consider the OLS model.\n1modelFit\u0026lt;-regsubsets(y~.,data=modelDat,nvmax=20) 2modelFit %\u0026gt;% summary %\u0026gt;% print 1## Subset selection object 2## Call: regsubsets.formula(y ~ ., data = modelDat, nvmax = 20) 3## 20 Variables (and intercept) 4## Forced in Forced out 5## x.1 FALSE FALSE 6## x.2 FALSE FALSE 7## x.3 FALSE FALSE 8## x.4 FALSE FALSE 9## x.5 FALSE FALSE 10## x.6 FALSE FALSE 11## x.7 FALSE FALSE 12## x.8 FALSE FALSE 13## x.9 FALSE FALSE 14## x.10 FALSE FALSE 15## x.11 FALSE FALSE 16## x.12 FALSE FALSE 17## x.13 FALSE FALSE 18## x.14 FALSE FALSE 19## x.15 FALSE FALSE 20## x.16 FALSE FALSE 21## x.17 FALSE FALSE 22## x.18 FALSE FALSE 23## x.19 FALSE FALSE 24## x.20 FALSE FALSE 25## 1 subsets of each size up to 20 26## Selection Algorithm: exhaustive 27## x.1 x.2 x.3 x.4 x.5 x.6 x.7 x.8 x.9 x.10 x.11 x.12 x.13 x.14 x.15 28## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 29## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 30## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 31## 4 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 32## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 33## 6 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 34## 7 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 35## 8 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 36## 9 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 37## 10 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 38## 11 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 39## 12 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 40## 13 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 41## 14 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 42## 15 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 43## 16 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 44## 17 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 45## 18 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 46## 19 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 47## 20 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 48## x.16 x.17 x.18 x.19 x.20 49## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 50## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 51## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 52## 4 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 53## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 54## 6 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 55## 7 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 56## 8 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 57## 9 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 58## 10 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 59## 11 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 60## 12 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 61## 13 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 62## 14 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 63## 15 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 64## 16 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 65## 17 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 66## 18 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 67## 19 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 68## 20 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; We might want to take a look at these.\n1par(mfrow=c(2,2)) 2plot(modelFit) 3plot(modelFit,scale=\u0026#39;Cp\u0026#39;) 4plot(modelFit,scale=\u0026#39;r2\u0026#39;) 5plot(modelFit,scale=\u0026#39;adjr2\u0026#39;)  1plotLEAP(modelFit %\u0026gt;% summary)  It would appear that 16 variables would be a good bet. We note that the lasso model did void out 4 parameters, namely x₁,x₃,x₁₃ and x₁₇.\nLets take a quick look at the various model variable significance values.\n1lgp\u0026lt;-linCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;OLS Variable Importance\u0026#34;) 2rgp\u0026lt;-ridgCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Ridge Variable Importance\u0026#34;) 3lsgp\u0026lt;-lassoCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Lasso Variable Importance\u0026#34;) 4pcgp\u0026lt;-pcrCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;PCR Variable Importance\u0026#34;) 5plgp\u0026lt;-plsCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;PLS Variable Importance\u0026#34;) 6grid.arrange(lgp,rgp,lsgp,pcgp,plgp,ncol=3,bottom=\u0026#34;Effective Importance, scaled\u0026#34;)  e) Model size The test set numeric minimum RMSE is a tie between OLS and PCR, and this was achieved for the (effective) 16 variable OLS model, as well as the 18 variable PCR model.\nf) Best model We will consider the OLS and PCR models and its parameters.\n1linCol$finalModel %\u0026gt;% print 1## 2## Call: 3## lm(formula = .outcome ~ ., data = dat) 4## 5## Coefficients: 6## (Intercept) x.1 x.2 x.3 x.4 x.5 7## -0.06052 -0.02265 28.91650 14.16532 28.16256 0.13742 8## x.6 x.7 x.8 x.9 x.10 x.11 9## 27.01497 31.15917 -9.66308 0.11641 19.06687 -9.09956 10## x.12 x.13 x.14 x.15 x.16 x.17 11## -8.01933 4.26852 20.22366 -0.16607 7.95594 10.89851 12## x.18 x.19 x.20 13## -1.09760 22.05197 20.88796 1pcrCol$bestTune %\u0026gt;% print 1## ncomp 2## 10 20 Now to compare this to the original.\n1beta %\u0026gt;% print 1## [1] 0 29 14 28 0 27 31 -10 0 19 -9 -8 4 20 0 8 11 -1 22 2## [20] 21 1t=data.frame(linCol$finalModel$coefficients[-1]) %\u0026gt;% rename(\u0026#34;Model_Coeffs\u0026#34;=1) %\u0026gt;% add_column(beta) %\u0026gt;% rename(\u0026#34;Original_Coeffs\u0026#34;=2) 2print(t) 1## Model_Coeffs Original_Coeffs 2## x.1 -0.02265289 0 3## x.2 28.91649699 29 4## x.3 14.16532050 14 5## x.4 28.16255937 28 6## x.5 0.13741621 0 7## x.6 27.01497459 27 8## x.7 31.15917172 31 9## x.8 -9.66308362 -10 10## x.9 0.11641282 0 11## x.10 19.06687041 19 12## x.11 -9.09955826 -9 13## x.12 -8.01932598 -8 14## x.13 4.26852334 4 15## x.14 20.22366153 20 16## x.15 -0.16606531 0 17## x.16 7.95593559 8 18## x.17 10.89851353 11 19## x.18 -1.09759687 -1 20## x.19 22.05196537 22 21## x.20 20.88795623 21 We see that the coefficients are pretty similar.\ng) Plotting differences 1val.errors = rep(NaN, p) 2a = rep(NaN, p) 3b = rep(NaN, p) 4x_cols = colnames(xmat, do.NULL = FALSE, prefix = \u0026#34;x.\u0026#34;) 5for (i in 1:p) { 6coefi = coef(modelFit, id = i) 7a[i] = length(coefi) - 1 ## Not counting the intercept 8b[i] = sqrt(sum((beta[x_cols %in% names(coefi)] - coefi[names(coefi) %in% x_cols])^2) + 9sum(beta[!(x_cols %in% names(coefi))])^2) ## Handling the intercept 10} 11plot(x = a, y = b, xlab = \u0026#34;Number of Coefficients\u0026#34;, ylab = \u0026#34;Relative Error\u0026#34;)  Question 6.11 - Page 264 We will now try to predict per capita crime rate in the Boston data set.\n(a) Try out some of the regression methods explored in this chapter, such as best subset selection, the lasso, ridge regression, and PCR. Present and discuss results for the approaches that you consider.\n(b) Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, crossvalidation, or some other reasonable alternative, as opposed to using training error.\n(c) Does your chosen model involve all of the features in the data set? Why or why not?\nAnswer 1boston\u0026lt;-MASS::Boston  Summarize   1boston %\u0026gt;% str %\u0026gt;% print 1## \u0026#39;data.frame\u0026#39;: 506 obs. of 14 variables: 2## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... 3## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... 4## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... 5## $ chas : int 0 0 0 0 0 0 0 0 0 0 ... 6## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... 7## $ rm : num 6.58 6.42 7.18 7 7.15 ... 8## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... 9## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... 10## $ rad : int 1 2 2 3 3 3 5 5 5 5 ... 11## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... 12## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... 13## $ black : num 397 397 393 395 397 ... 14## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... 15## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... 16## NULL 1boston %\u0026gt;% summary %\u0026gt;% print 1## crim zn indus chas 2## Min. : 0.00632 Min. : 0.00 Min. : 0.46 Min. :0.00000 3## 1st Qu.: 0.08204 1st Qu.: 0.00 1st Qu.: 5.19 1st Qu.:0.00000 4## Median : 0.25651 Median : 0.00 Median : 9.69 Median :0.00000 5## Mean : 3.61352 Mean : 11.36 Mean :11.14 Mean :0.06917 6## 3rd Qu.: 3.67708 3rd Qu.: 12.50 3rd Qu.:18.10 3rd Qu.:0.00000 7## Max. :88.97620 Max. :100.00 Max. :27.74 Max. :1.00000 8## nox rm age dis 9## Min. :0.3850 Min. :3.561 Min. : 2.90 Min. : 1.130 10## 1st Qu.:0.4490 1st Qu.:5.886 1st Qu.: 45.02 1st Qu.: 2.100 11## Median :0.5380 Median :6.208 Median : 77.50 Median : 3.207 12## Mean :0.5547 Mean :6.285 Mean : 68.57 Mean : 3.795 13## 3rd Qu.:0.6240 3rd Qu.:6.623 3rd Qu.: 94.08 3rd Qu.: 5.188 14## Max. :0.8710 Max. :8.780 Max. :100.00 Max. :12.127 15## rad tax ptratio black 16## Min. : 1.000 Min. :187.0 Min. :12.60 Min. : 0.32 17## 1st Qu.: 4.000 1st Qu.:279.0 1st Qu.:17.40 1st Qu.:375.38 18## Median : 5.000 Median :330.0 Median :19.05 Median :391.44 19## Mean : 9.549 Mean :408.2 Mean :18.46 Mean :356.67 20## 3rd Qu.:24.000 3rd Qu.:666.0 3rd Qu.:20.20 3rd Qu.:396.23 21## Max. :24.000 Max. :711.0 Max. :22.00 Max. :396.90 22## lstat medv 23## Min. : 1.73 Min. : 5.00 24## 1st Qu.: 6.95 1st Qu.:17.02 25## Median :11.36 Median :21.20 26## Mean :12.65 Mean :22.53 27## 3rd Qu.:16.95 3rd Qu.:25.00 28## Max. :37.97 Max. :50.00 1boston %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) %\u0026gt;% print 1## crim zn indus chas nox rm age dis rad tax 2## 504 26 76 2 81 446 356 412 9 66 3## ptratio black lstat medv 4## 46 357 455 229 a) Test regression models 1train_ind = sample(boston %\u0026gt;% nrow,100) 2test_ind = setdiff(seq_len(boston %\u0026gt;% nrow), train_set) 1train_set\u0026lt;-boston[train_ind,] 2test_set\u0026lt;-boston[-train_ind,] 1linCol\u0026lt;-train(crim~.,data=train_set,method=\u0026#34;lm\u0026#34;) 2linCol %\u0026gt;% summary 1## 2## Call: 3## lm(formula = .outcome ~ ., data = dat) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -6.2431 -1.0344 -0.0563 0.8187 8.1318 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 0.9339246 7.6508393 0.122 0.9031 12## zn 0.0046819 0.0157375 0.297 0.7668 13## indus 0.0276209 0.0875254 0.316 0.7531 14## chas -1.1602278 1.2386869 -0.937 0.3516 15## nox -7.5024503 5.0207818 -1.494 0.1388 16## rm 1.1240874 0.7462340 1.506 0.1356 17## age 0.0020182 0.0137404 0.147 0.8836 18## dis -0.3934753 0.2454365 -1.603 0.1126 19## rad 0.4540613 0.0791580 5.736 1.41e-07 *** 20## tax 0.0008469 0.0052593 0.161 0.8724 21## ptratio -0.2978204 0.1637629 -1.819 0.0725 . 22## black 0.0030642 0.0045281 0.677 0.5004 23## lstat 0.1322779 0.0626485 2.111 0.0376 * 24## medv -0.0841382 0.0590700 -1.424 0.1580 25## --- 26## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 27## 28## Residual standard error: 2.362 on 86 degrees of freedom 29## Multiple R-squared: 0.775, Adjusted R-squared: 0.741 30## F-statistic: 22.78 on 13 and 86 DF, p-value: \u0026lt; 2.2e-16 1linPred\u0026lt;-predict(linCol,test_set) 2linPred %\u0026gt;% postResample(obs = test_set$crim) 1## RMSE Rsquared MAE 2## 7.3794735 0.4056002 2.6774969 1L2Grid \u0026lt;- expand.grid(alpha=0, 2lambda=10^seq(from=-3,to=30,length=100)) 1ridgCol\u0026lt;-train(crim~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L2Grid) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1ridgCol %\u0026gt;% summary %\u0026gt;% print 1## Length Class Mode 2## a0 100 -none- numeric 3## beta 1300 dgCMatrix S4 4## df 100 -none- numeric 5## dim 2 -none- numeric 6## lambda 100 -none- numeric 7## dev.ratio 100 -none- numeric 8## nulldev 1 -none- numeric 9## npasses 1 -none- numeric 10## jerr 1 -none- numeric 11## offset 1 -none- logical 12## call 5 -none- call 13## nobs 1 -none- numeric 14## lambdaOpt 1 -none- numeric 15## xNames 13 -none- character 16## problemType 1 -none- character 17## tuneValue 2 data.frame list 18## obsLevels 1 -none- logical 19## param 0 -none- list 1coef(ridgCol$finalModel, ridgCol$bestTune$lambda) %\u0026gt;% print 1## 14 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) -3.881166065 4## zn 0.002597790 5## indus -0.005103517 6## chas -0.674764337 7## nox -0.053645732 8## rm 0.600064844 9## age 0.001153570 10## dis -0.179295384 11## rad 0.267082956 12## tax 0.006447932 13## ptratio -0.075885753 14## black -0.001650403 15## lstat 0.086462700 16## medv -0.027519270 1ggplot(ridgCol)  1ridgPred\u0026lt;-predict(ridgCol,test_set) 2ridgPred %\u0026gt;% postResample(obs = test_set$crim) 1## RMSE Rsquared MAE 2## 7.5065916 0.4017056 2.4777547 1L1Grid \u0026lt;- expand.grid(alpha=1, # for lasso 2lambda=10^seq(from=-3,to=30,length=100)) 1lassoCol\u0026lt;-train(crim~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L1Grid) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1lassoCol %\u0026gt;% summary %\u0026gt;% print 1## Length Class Mode 2## a0 78 -none- numeric 3## beta 1014 dgCMatrix S4 4## df 78 -none- numeric 5## dim 2 -none- numeric 6## lambda 78 -none- numeric 7## dev.ratio 78 -none- numeric 8## nulldev 1 -none- numeric 9## npasses 1 -none- numeric 10## jerr 1 -none- numeric 11## offset 1 -none- logical 12## call 5 -none- call 13## nobs 1 -none- numeric 14## lambdaOpt 1 -none- numeric 15## xNames 13 -none- character 16## problemType 1 -none- character 17## tuneValue 2 data.frame list 18## obsLevels 1 -none- logical 19## param 0 -none- list 1coef(lassoCol$finalModel, lassoCol$bestTune$lambda) %\u0026gt;% print 1## 14 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) -2.024006430 4## zn . 5## indus . 6## chas . 7## nox . 8## rm . 9## age . 10## dis -0.008506188 11## rad 0.386379255 12## tax 0.001779579 13## ptratio . 14## black . 15## lstat 0.040788606 16## medv . 1ggplot(lassoCol)  1lassoPred\u0026lt;-predict(lassoCol,test_set) 2lassoPred %\u0026gt;% postResample(obs = test_set$crim) 1## RMSE Rsquared MAE 2## 7.5868293 0.3859121 2.4892258 1mGrid \u0026lt;- expand.grid(ncomp=seq(from=1,to=20,length=10))  All the models have the same R² but Ridge does the worst followed by LASSO  For the rest of the question, we will consider the OLS model.\n1modelFit\u0026lt;-regsubsets(crim~.,data=boston,nvmax=20) 2modelFit %\u0026gt;% summary %\u0026gt;% print 1## Subset selection object 2## Call: regsubsets.formula(crim ~ ., data = boston, nvmax = 20) 3## 13 Variables (and intercept) 4## Forced in Forced out 5## zn FALSE FALSE 6## indus FALSE FALSE 7## chas FALSE FALSE 8## nox FALSE FALSE 9## rm FALSE FALSE 10## age FALSE FALSE 11## dis FALSE FALSE 12## rad FALSE FALSE 13## tax FALSE FALSE 14## ptratio FALSE FALSE 15## black FALSE FALSE 16## lstat FALSE FALSE 17## medv FALSE FALSE 18## 1 subsets of each size up to 13 19## Selection Algorithm: exhaustive 20## zn indus chas nox rm age dis rad tax ptratio black lstat medv 21## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; 22## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 23## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; 24## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 25## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 26## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 27## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; 28## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 29## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 30## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 31## 11 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 32## 12 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 33## 13 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; We might want to take a look at these.\n1par(mfrow=c(2,2)) 2plot(modelFit) 3plot(modelFit,scale=\u0026#39;Cp\u0026#39;) 4plot(modelFit,scale=\u0026#39;r2\u0026#39;) 5plot(modelFit,scale=\u0026#39;adjr2\u0026#39;)  1plotLEAP(modelFit %\u0026gt;% summary)  It would appear that 16 variables would be a good bet. We note that the lasso model did void out 4 parameters, namely x₁,x₃,x₁₃ and x₁₇.\nLets take a quick look at the various model variable significance values.\n1lgp\u0026lt;-linCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;OLS Variable Importance\u0026#34;) 2rgp\u0026lt;-ridgCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Ridge Variable Importance\u0026#34;) 3lsgp\u0026lt;-lassoCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Lasso Variable Importance\u0026#34;) 4grid.arrange(lgp,rgp,lsgp,ncol=2,bottom=\u0026#34;Effective Importance, scaled\u0026#34;)  b) Propose a model   Given the data and plots, I would probably end up using the Ridge regression model\n  Clearly, LASSO is not working very well since it seems to have taken mainly 3 variables, one of which is largely categorical (9 levels)\n  c) Model properties 1boston %\u0026gt;% str %\u0026gt;% print 1## \u0026#39;data.frame\u0026#39;: 506 obs. of 14 variables: 2## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... 3## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... 4## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... 5## $ chas : int 0 0 0 0 0 0 0 0 0 0 ... 6## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... 7## $ rm : num 6.58 6.42 7.18 7 7.15 ... 8## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... 9## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... 10## $ rad : int 1 2 2 3 3 3 5 5 5 5 ... 11## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... 12## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... 13## $ black : num 397 397 393 395 397 ... 14## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... 15## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... 16## NULL 1boston %\u0026gt;% summary %\u0026gt;% print 1## crim zn indus chas 2## Min. : 0.00632 Min. : 0.00 Min. : 0.46 Min. :0.00000 3## 1st Qu.: 0.08204 1st Qu.: 0.00 1st Qu.: 5.19 1st Qu.:0.00000 4## Median : 0.25651 Median : 0.00 Median : 9.69 Median :0.00000 5## Mean : 3.61352 Mean : 11.36 Mean :11.14 Mean :0.06917 6## 3rd Qu.: 3.67708 3rd Qu.: 12.50 3rd Qu.:18.10 3rd Qu.:0.00000 7## Max. :88.97620 Max. :100.00 Max. :27.74 Max. :1.00000 8## nox rm age dis 9## Min. :0.3850 Min. :3.561 Min. : 2.90 Min. : 1.130 10## 1st Qu.:0.4490 1st Qu.:5.886 1st Qu.: 45.02 1st Qu.: 2.100 11## Median :0.5380 Median :6.208 Median : 77.50 Median : 3.207 12## Mean :0.5547 Mean :6.285 Mean : 68.57 Mean : 3.795 13## 3rd Qu.:0.6240 3rd Qu.:6.623 3rd Qu.: 94.08 3rd Qu.: 5.188 14## Max. :0.8710 Max. :8.780 Max. :100.00 Max. :12.127 15## rad tax ptratio black 16## Min. : 1.000 Min. :187.0 Min. :12.60 Min. : 0.32 17## 1st Qu.: 4.000 1st Qu.:279.0 1st Qu.:17.40 1st Qu.:375.38 18## Median : 5.000 Median :330.0 Median :19.05 Median :391.44 19## Mean : 9.549 Mean :408.2 Mean :18.46 Mean :356.67 20## 3rd Qu.:24.000 3rd Qu.:666.0 3rd Qu.:20.20 3rd Qu.:396.23 21## Max. :24.000 Max. :711.0 Max. :22.00 Max. :396.90 22## lstat medv 23## Min. : 1.73 Min. : 5.00 24## 1st Qu.: 6.95 1st Qu.:17.02 25## Median :11.36 Median :21.20 26## Mean :12.65 Mean :22.53 27## 3rd Qu.:16.95 3rd Qu.:25.00 28## Max. :37.97 Max. :50.00 1boston %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) %\u0026gt;% print 1## crim zn indus chas nox rm age dis rad tax 2## 504 26 76 2 81 446 356 412 9 66 3## ptratio black lstat medv 4## 46 357 455 229  A good idea would be removing rad and chas from the regression   1boston\u0026lt;-boston %\u0026gt;% subset(select=-c(rad,chas)) 1train_ind = sample(boston %\u0026gt;% nrow,100) 2test_ind = setdiff(seq_len(boston %\u0026gt;% nrow), train_set) 1train_set\u0026lt;-boston[train_ind,] 2test_set\u0026lt;-boston[-train_ind,] 1L2Grid \u0026lt;- expand.grid(alpha=0, 2lambda=10^seq(from=-3,to=30,length=100)) 1ridgCol\u0026lt;-train(crim~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L2Grid) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1ridgCol %\u0026gt;% summary %\u0026gt;% print 1## Length Class Mode 2## a0 100 -none- numeric 3## beta 1100 dgCMatrix S4 4## df 100 -none- numeric 5## dim 2 -none- numeric 6## lambda 100 -none- numeric 7## dev.ratio 100 -none- numeric 8## nulldev 1 -none- numeric 9## npasses 1 -none- numeric 10## jerr 1 -none- numeric 11## offset 1 -none- logical 12## call 5 -none- call 13## nobs 1 -none- numeric 14## lambdaOpt 1 -none- numeric 15## xNames 11 -none- character 16## problemType 1 -none- character 17## tuneValue 2 data.frame list 18## obsLevels 1 -none- logical 19## param 0 -none- list 1coef(ridgCol$finalModel, ridgCol$bestTune$lambda) %\u0026gt;% print 1## 12 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) 0.23015398 4## zn 0.01595378 5## indus -0.01006683 6## nox 3.42104450 7## rm -0.05904911 8## age 0.01419585 9## dis -0.16724715 10## tax 0.01021790 11## ptratio 0.05741508 12## black -0.01414707 13## lstat 0.15519923 14## medv -0.04483670 1ggplot(ridgCol)  1ridgPred\u0026lt;-predict(ridgCol,test_set) 2ridgPred %\u0026gt;% postResample(obs = test_set$crim) 1## RMSE Rsquared MAE 2## 6.9365371 0.3619974 2.8570012 1L1Grid \u0026lt;- expand.grid(alpha=1, # for lasso 2lambda=10^seq(from=-3,to=30,length=100)) 1lassoCol\u0026lt;-train(crim~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L1Grid) 1## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : 2## There were missing values in resampled performance measures. 1lassoCol %\u0026gt;% summary %\u0026gt;% print 1## Length Class Mode 2## a0 74 -none- numeric 3## beta 814 dgCMatrix S4 4## df 74 -none- numeric 5## dim 2 -none- numeric 6## lambda 74 -none- numeric 7## dev.ratio 74 -none- numeric 8## nulldev 1 -none- numeric 9## npasses 1 -none- numeric 10## jerr 1 -none- numeric 11## offset 1 -none- logical 12## call 5 -none- call 13## nobs 1 -none- numeric 14## lambdaOpt 1 -none- numeric 15## xNames 11 -none- character 16## problemType 1 -none- character 17## tuneValue 2 data.frame list 18## obsLevels 1 -none- logical 19## param 0 -none- list 1coef(lassoCol$finalModel, lassoCol$bestTune$lambda) %\u0026gt;% print 1## 12 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; 2## 1 3## (Intercept) 0.86635621 4## zn . 5## indus . 6## nox . 7## rm . 8## age . 9## dis . 10## tax 0.01402174 11## ptratio . 12## black -0.01454496 13## lstat 0.15290845 14## medv . 1ggplot(lassoCol)  1lassoPred\u0026lt;-predict(lassoCol,test_set) 2lassoPred %\u0026gt;% postResample(obs = test_set$crim) 1## RMSE Rsquared MAE 2## 6.9630616 0.3693364 2.6767742 1rgp\u0026lt;-ridgCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Ridge Variable Importance\u0026#34;) 2lsgp\u0026lt;-lassoCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Lasso Variable Importance\u0026#34;) 3grid.arrange(rgp,lsgp,ncol=2,bottom=\u0026#34;Effective Importance, scaled\u0026#34;)  None of these models are actually any good apparently, given that we have an R² of 0.3619974 for the L2 regularization and 0.3693364 for the L1.\n  James, G., Witten, D., Hastie, T., \u0026amp; Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Berlin, Germany: Springer Science \u0026amp; Business Media.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Lang et al., (2019). mlr3: A modern object-oriented machine learning framework in R. Journal of Open Source Software, 4(44), 1903, https://doi.org/10.21105/joss.01903\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/islr-ch6/","tags":["solutions","R","ISLR"],"title":"  \"ISLR :: Linear Model Selection and Regularization\"\n  "},{"categories":["programming"],"contents":"Chapter V - Resampling Methods All the questions are as per the ISL seventh printing of the First edition1.\nCommon Instead of using the standard functions, we will leverage the mlr3 package2.\n1#install.packages(\u0026#34;mlr3\u0026#34;,\u0026#34;data.table\u0026#34;,\u0026#34;mlr3viz\u0026#34;,\u0026#34;mlr3learners\u0026#34;) Actually for R version 3.6.2, the steps to get it working were a bit more involved.\n1install.packages(\u0026#34;remotes\u0026#34;,\u0026#34;data.table\u0026#34;, 2\u0026#34;GGally\u0026#34;,\u0026#34;precerec\u0026#34;) # For plots 1library(remotes) 2remotes::install_github(\u0026#34;mlr-org/mlr3\u0026#34;) 3remotes::install_github(\u0026#34;mlr-org/mlr3viz\u0026#34;) 4remotes::install_github(\u0026#34;mlr-org/mlr3learners\u0026#34;) Load ISLR and other libraries.\n1libsUsed\u0026lt;-c(\u0026#34;dplyr\u0026#34;,\u0026#34;ggplot2\u0026#34;,\u0026#34;tidyverse\u0026#34;, 2\u0026#34;ISLR\u0026#34;,\u0026#34;caret\u0026#34;,\u0026#34;MASS\u0026#34;, 3\u0026#34;pROC\u0026#34;,\u0026#34;mlr3\u0026#34;,\u0026#34;data.table\u0026#34;, 4\u0026#34;mlr3viz\u0026#34;,\u0026#34;mlr3learners\u0026#34;) 5invisible(lapply(libsUsed, library, character.only = TRUE)) Question 5.5 - Page 198 In Chapter 4, we used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.\n(a) Fit a logistic regression model that uses income and balance to predict default.\n(b) Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:\n  Split the sample set into a training set and a validation set.\n  Fit a multiple logistic regression model using only the training observations.\n  Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than \\(0.5\\).\n  Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.\n  (c) Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained.\n(d) Now consider a logistic regression model that predicts the prob- ability of default using income , balance , and a dummy variable for student. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate.\nAnswer We will need our data.\n1defDat\u0026lt;-ISLR::Default  Very quick peek   1defDat %\u0026gt;% summary 1## default student balance income 2## No :9667 No :7056 Min. : 0.0 Min. : 772 3## Yes: 333 Yes:2944 1st Qu.: 481.7 1st Qu.:21340 4## Median : 823.6 Median :34553 5## Mean : 835.4 Mean :33517 6## 3rd Qu.:1166.3 3rd Qu.:43808 7## Max. :2654.3 Max. :73554 1defDat %\u0026gt;% str 1## \u0026#39;data.frame\u0026#39;: 10000 obs. of 4 variables: 2## $ default: Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 1 1 1 1 1 1 1 1 1 1 ... 3## $ student: Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 1 2 1 1 1 2 1 2 1 1 ... 4## $ balance: num 730 817 1074 529 786 ... 5## $ income : num 44362 12106 31767 35704 38463 ... a) Logistic Model with mlr3 Following the new approach which leverages R6 features leads us to define a classification task first. As far as I can tell, the data needs to be filtered to contain only the things we need to predict with, in this case we are required to use only income and balance so we will do so.\n1set.seed(1984) 2redDat\u0026lt;-defDat %\u0026gt;% subset(select=c(income,balance,default)) 3tskLogiFull=TaskClassif$new(id=\u0026#34;credit\u0026#34;,backend=redDat,target=\u0026#34;default\u0026#34;) 4print(tskLogiFull) 1## \u0026lt;TaskClassif:credit\u0026gt; (10000 x 3) 2## * Target: default 3## * Properties: twoclass 4## * Features (2): 5## - dbl (2): balance, income This can be visualized neatly as well.\n1autoplot(tskLogiFull)  Figure 1: MLR3 Visualizations\n  We have a pretty imbalanced data-set.\n1autoplot(tskLogiFull,type=\u0026#34;pairs\u0026#34;) 1## Registered S3 method overwritten by \u0026#39;GGally\u0026#39;: 2## method from 3## +.gg ggplot2 1## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 2## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  Figure 2: Paired mlr3 data\n  We can use any of the learners implemented, so it is a good idea to take a quick peek at them all.\n1as.data.table(mlr_learners) 1## key feature_types 2## 1: classif.debug logical,integer,numeric,character,factor,ordered 3## 2: classif.featureless logical,integer,numeric,character,factor,ordered 4## 3: classif.glmnet logical,integer,numeric 5## 4: classif.kknn logical,integer,numeric,factor,ordered 6## 5: classif.lda logical,integer,numeric,factor,ordered 7## 6: classif.log_reg logical,integer,numeric,character,factor,ordered 8## 7: classif.naive_bayes logical,integer,numeric,factor 9## 8: classif.qda logical,integer,numeric,factor,ordered 10## 9: classif.ranger logical,integer,numeric,character,factor,ordered 11## 10: classif.rpart logical,integer,numeric,factor,ordered 12## 11: classif.svm logical,integer,numeric 13## 12: classif.xgboost logical,integer,numeric 14## 13: regr.featureless logical,integer,numeric,character,factor,ordered 15## 14: regr.glmnet logical,integer,numeric 16## 15: regr.kknn logical,integer,numeric,factor,ordered 17## 16: regr.km logical,integer,numeric 18## 17: regr.lm logical,integer,numeric,factor 19## 18: regr.ranger logical,integer,numeric,character,factor,ordered 20## 19: regr.rpart logical,integer,numeric,factor,ordered 21## 20: regr.svm logical,integer,numeric 22## 21: regr.xgboost logical,integer,numeric 23## key feature_types 24## packages 25## 1: 26## 2: 27## 3: glmnet 28## 4: kknn 29## 5: MASS 30## 6: stats 31## 7: e1071 32## 8: MASS 33## 9: ranger 34## 10: rpart 35## 11: e1071 36## 12: xgboost 37## 13: stats 38## 14: glmnet 39## 15: kknn 40## 16: DiceKriging 41## 17: stats 42## 18: ranger 43## 19: rpart 44## 20: e1071 45## 21: xgboost 46## packages 47## properties 48## 1: missings,multiclass,twoclass 49## 2: importance,missings,multiclass,selected_features,twoclass 50## 3: multiclass,twoclass,weights 51## 4: multiclass,twoclass 52## 5: multiclass,twoclass,weights 53## 6: twoclass,weights 54## 7: multiclass,twoclass 55## 8: multiclass,twoclass,weights 56## 9: importance,multiclass,oob_error,twoclass,weights 57## 10: importance,missings,multiclass,selected_features,twoclass,weights 58## 11: multiclass,twoclass 59## 12: importance,missings,multiclass,twoclass,weights 60## 13: importance,missings,selected_features 61## 14: weights 62## 15: 63## 16: 64## 17: weights 65## 18: importance,oob_error,weights 66## 19: importance,missings,selected_features,weights 67## 20: 68## 21: importance,missings,weights 69## properties 70## predict_types 71## 1: response,prob 72## 2: response,prob 73## 3: response,prob 74## 4: response,prob 75## 5: response,prob 76## 6: response,prob 77## 7: response,prob 78## 8: response,prob 79## 9: response,prob 80## 10: response,prob 81## 11: response,prob 82## 12: response,prob 83## 13: response,se 84## 14: response 85## 15: response 86## 16: response,se 87## 17: response,se 88## 18: response,se 89## 19: response 90## 20: response 91## 21: response 92## predict_types We can now pick the logistic one. Note that this essentially proxies our requests down to the stats package.\n1learner = mlr_learners$get(\u0026#34;classif.log_reg\u0026#34;) Now we can final solve the question, which is to simply use the model on all our data and return the accuracy metrics.\n1trainFullCred=learner$train(tskLogiFull) 2print(learner$predict(tskLogiFull)$confusion) 1## truth 2## response No Yes 3## No 9629 225 4## Yes 38 108 1measure = msr(\u0026#34;classif.acc\u0026#34;) 2print(learner$predict(tskLogiFull)$score(measure)) 1## classif.acc 2## 0.9737 Note that this style of working with objects does not really utilize the familiar %\u0026gt;% interface.\nThe caret package still has neater default metrics so we will use that as well.\n1confusionMatrix(learner$predict(tskLogiFull)$response,defDat$default) 1## Confusion Matrix and Statistics 2## 3## Reference 4## Prediction No Yes 5## No 9629 225 6## Yes 38 108 7## 8## Accuracy : 0.9737 9## 95% CI : (0.9704, 0.9767) 10## No Information Rate : 0.9667 11## P-Value [Acc \u0026gt; NIR] : 3.067e-05 12## 13## Kappa : 0.4396 14## 15## Mcnemar\u0026#39;s Test P-Value : \u0026lt; 2.2e-16 16## 17## Sensitivity : 0.9961 18## Specificity : 0.3243 19## Pos Pred Value : 0.9772 20## Neg Pred Value : 0.7397 21## Prevalence : 0.9667 22## Detection Rate : 0.9629 23## Detection Prevalence : 0.9854 24## Balanced Accuracy : 0.6602 25## 26## \u0026#39;Positive\u0026#39; Class : No 27## 1autoplot(learner$predict(tskLogiFull))  Figure 3: Autoplot results\n  We can get some other plots as well, but we need our probabilities to be returned.\n1# For ROC curves 2lrnprob = lrn(\u0026#34;classif.log_reg\u0026#34;,predict_type=\u0026#34;prob\u0026#34;) 3lrnprob$train(tskLogiFull) 4autoplot(lrnprob$predict(tskLogiFull),type=\u0026#34;roc\u0026#34;)  Figure 4: ROC curve\n  b) Validation Sets with mlr3 Though the question seems to require a manual validation set generation and thresholding, we can simply use the defaults.\n1train_set = sample(tskLogiFull$nrow, 0.8 * tskLogiFull$nrow) 2test_set = setdiff(seq_len(tskLogiFull$nrow), train_set) 3learner$train(tskLogiFull,row_ids=train_set) 4confusionMatrix(learner$predict(tskLogiFull, row_ids=test_set)$response,defDat[-train_set,]$default) 1## Confusion Matrix and Statistics 2## 3## Reference 4## Prediction No Yes 5## No 1921 47 6## Yes 9 23 7## 8## Accuracy : 0.972 9## 95% CI : (0.9638, 0.9788) 10## No Information Rate : 0.965 11## P-Value [Acc \u0026gt; NIR] : 0.04663 12## 13## Kappa : 0.4387 14## 15## Mcnemar\u0026#39;s Test P-Value : 7.641e-07 16## 17## Sensitivity : 0.9953 18## Specificity : 0.3286 19## Pos Pred Value : 0.9761 20## Neg Pred Value : 0.7188 21## Prevalence : 0.9650 22## Detection Rate : 0.9605 23## Detection Prevalence : 0.9840 24## Balanced Accuracy : 0.6620 25## 26## \u0026#39;Positive\u0026#39; Class : No 27## For a reasonable comparison, we will demonstrate a standard approach as well. In this instance we will not use caret to ensure that our class distribution in the train and test sets are not sampled to remain the same.\n1trainNoCaret\u0026lt;-sample(nrow(defDat), size = floor(.8*nrow(defDat)), replace = F) 2glm.fit=glm(default~income+balance,data=defDat,family=binomial,subset=trainNoCaret) 3glm.probs\u0026lt;-predict(glm.fit,defDat[-trainNoCaret,],type=\u0026#34;response\u0026#34;) 4glm.preds\u0026lt;-ifelse(glm.probs \u0026lt; 0.5, \u0026#34;No\u0026#34;, \u0026#34;Yes\u0026#34;) 5confusionMatrix(glm.preds %\u0026gt;% factor,defDat[-trainNoCaret,]$default) 1## Confusion Matrix and Statistics 2## 3## Reference 4## Prediction No Yes 5## No 1930 46 6## Yes 6 18 7## 8## Accuracy : 0.974 9## 95% CI : (0.966, 0.9805) 10## No Information Rate : 0.968 11## P-Value [Acc \u0026gt; NIR] : 0.06859 12## 13## Kappa : 0.3986 14## 15## Mcnemar\u0026#39;s Test P-Value : 6.362e-08 16## 17## Sensitivity : 0.9969 18## Specificity : 0.2812 19## Pos Pred Value : 0.9767 20## Neg Pred Value : 0.7500 21## Prevalence : 0.9680 22## Detection Rate : 0.9650 23## Detection Prevalence : 0.9880 24## Balanced Accuracy : 0.6391 25## 26## \u0026#39;Positive\u0026#39; Class : No 27## Since the two approaches use different samples there is a little variation, but we can see that the accuracy is essentially the same.\nc) 3-fold cross validation As per the question, we can repeat the block above three times, or extract it into a function which takes a seed value and run that three times. Either way, here we will present the mlr3 approach to cross validation and resampling.\n1rr = resample(tskLogiFull, lrnprob, rsmp(\u0026#34;cv\u0026#34;, folds = 3)) 1## INFO [22:12:30.025] Applying learner \u0026#39;classif.log_reg\u0026#39; on task \u0026#39;credit\u0026#39; (iter 1/3) 2## INFO [22:12:30.212] Applying learner \u0026#39;classif.log_reg\u0026#39; on task \u0026#39;credit\u0026#39; (iter 2/3) 3## INFO [22:12:30.360] Applying learner \u0026#39;classif.log_reg\u0026#39; on task \u0026#39;credit\u0026#39; (iter 3/3) 1autoplot(rr,type=\u0026#34;roc\u0026#34;)  Figure 5: Resampled ROC curve\n  We might want the average as well.\n1rr$aggregate(msr(\u0026#34;classif.ce\u0026#34;)) %\u0026gt;% print 1## classif.ce 2## 0.02630035 Adding Student as a dummy variable We will stick to the mlr3 approach because it is faster.\n1redDat2\u0026lt;-defDat %\u0026gt;% mutate(student=as.numeric(defDat$student)) 2tskLogi2=TaskClassif$new(id=\u0026#34;credit\u0026#34;,backend=redDat2,target=\u0026#34;default\u0026#34;) 3print(tskLogi2) 1## \u0026lt;TaskClassif:credit\u0026gt; (10000 x 4) 2## * Target: default 3## * Properties: twoclass 4## * Features (3): 5## - dbl (3): balance, income, student 1autoplot(tskLogi2,type=\u0026#34;pairs\u0026#34;) 1## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 2## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 3## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  Figure 6: Logistic regression pairs data\n  This gives us a visual indicator and premonition that we might not be getting incredible results with our new variable in the mix, but we should still work it through.\n1confusionMatrix(lrnprob$predict(tskLogi2)$response,defDat$default) 1## Confusion Matrix and Statistics 2## 3## Reference 4## Prediction No Yes 5## No 9629 225 6## Yes 38 108 7## 8## Accuracy : 0.9737 9## 95% CI : (0.9704, 0.9767) 10## No Information Rate : 0.9667 11## P-Value [Acc \u0026gt; NIR] : 3.067e-05 12## 13## Kappa : 0.4396 14## 15## Mcnemar\u0026#39;s Test P-Value : \u0026lt; 2.2e-16 16## 17## Sensitivity : 0.9961 18## Specificity : 0.3243 19## Pos Pred Value : 0.9772 20## Neg Pred Value : 0.7397 21## Prevalence : 0.9667 22## Detection Rate : 0.9629 23## Detection Prevalence : 0.9854 24## Balanced Accuracy : 0.6602 25## 26## \u0026#39;Positive\u0026#39; Class : No 27## 1autoplot(lrnprob$predict(tskLogi2))  Figure 7: Autoplot figure\n  1lrnprob$train(tskLogi2) 2autoplot(lrnprob$predict(tskLogi2),type=\u0026#34;roc\u0026#34;)  Figure 8: ROC plot\n  Although we have slightly better accuracy with the new variable, it needs to be compared to determine if it is worth further investigation.\nWith a three-fold validation approach,\n1library(\u0026#34;gridExtra\u0026#34;) 1## 2## Attaching package: \u0026#39;gridExtra\u0026#39; 1## The following object is masked from \u0026#39;package:dplyr\u0026#39;: 2## 3## combine 1rr2 = resample(tskLogi2, lrnprob, rsmp(\u0026#34;cv\u0026#34;, folds = 3)) 1## INFO [22:12:39.670] Applying learner \u0026#39;classif.log_reg\u0026#39; on task \u0026#39;credit\u0026#39; (iter 1/3) 2## INFO [22:12:39.731] Applying learner \u0026#39;classif.log_reg\u0026#39; on task \u0026#39;credit\u0026#39; (iter 2/3) 3## INFO [22:12:39.780] Applying learner \u0026#39;classif.log_reg\u0026#39; on task \u0026#39;credit\u0026#39; (iter 3/3) 1wS\u0026lt;-autoplot(rr2) 2nS\u0026lt;-autoplot(rr) 3grid.arrange(wS,nS,ncol=2,bottom=\u0026#34;With student (left) and without (right)\u0026#34;)  Figure 9: Plot of accuracy\n  Given the results, it is fair to say that adding the student data is useful in general.\nQuestion 5.6 - Page 199 We continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set. In particular, we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways: (1) using the bootstrap, and (2) using the standard formula for computing the standard errors in the glm() function. Do not forget to set a random seed before beginning your analysis.\n(a) Using the summary() and glm() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors.\n(b) Write a function, boot.fn() , that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.\n(c) Use the boot() function together with your boot.fn() function to estimate the standard errors of the logistic regression coefficients for income and balance.\n(d) Comment on the estimated standard errors obtained using the glm() function and using your bootstrap function.\nAnswer This question is slightly more specific to the packages in the book so we will use them.\na) Fit summary 1glm.fit %\u0026gt;% summary 1## 2## Call: 3## glm(formula = default ~ income + balance, family = binomial, 4## data = defDat, subset = trainNoCaret) 5## 6## Deviance Residuals: 7## Min 1Q Median 3Q Max 8## -2.1943 -0.1488 -0.0588 -0.0217 3.7058 9## 10## Coefficients: 11## Estimate Std. Error z value Pr(\u0026gt;|z|) 12## (Intercept) -1.150e+01 4.814e-01 -23.885 \u0026lt; 2e-16 *** 13## income 2.288e-05 5.553e-06 4.121 3.78e-05 *** 14## balance 5.593e-03 2.509e-04 22.295 \u0026lt; 2e-16 *** 15## --- 16## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 17## 18## (Dispersion parameter for binomial family taken to be 1) 19## 20## Null deviance: 2354.0 on 7999 degrees of freedom 21## Residual deviance: 1283.6 on 7997 degrees of freedom 22## AIC: 1289.6 23## 24## Number of Fisher Scoring iterations: 8 b) Function 1boot.fn=function(data,subs){return(coef(glm(default~income+balance,data=data, family=binomial,subset=subs)))} 1boot.fn(defDat,train_set) %\u0026gt;% print 1## (Intercept) income balance 2## -1.136824e+01 1.846153e-05 5.576468e-03 1glm(default~income+balance,data=defDat,family=binomial,subset=train_set) %\u0026gt;% summary 1## 2## Call: 3## glm(formula = default ~ income + balance, family = binomial, 4## data = defDat, subset = train_set) 5## 6## Deviance Residuals: 7## Min 1Q Median 3Q Max 8## -2.4280 -0.1465 -0.0582 -0.0218 3.7115 9## 10## Coefficients: 11## Estimate Std. Error z value Pr(\u0026gt;|z|) 12## (Intercept) -1.137e+01 4.813e-01 -23.618 \u0026lt; 2e-16 *** 13## income 1.846e-05 5.553e-06 3.324 0.000886 *** 14## balance 5.576e-03 2.529e-04 22.046 \u0026lt; 2e-16 *** 15## --- 16## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 17## 18## (Dispersion parameter for binomial family taken to be 1) 19## 20## Null deviance: 2313.6 on 7999 degrees of freedom 21## Residual deviance: 1266.4 on 7997 degrees of freedom 22## AIC: 1272.4 23## 24## Number of Fisher Scoring iterations: 8 We see that the statistics obtained from both are the same.\nc) Bootstrap The old fashioned way. R is the resample rate, boot.fn is the statistic used.\n1library(boot) 1## 2## Attaching package: \u0026#39;boot\u0026#39; 1## The following object is masked from \u0026#39;package:lattice\u0026#39;: 2## 3## melanoma 1boot(defDat,boot.fn,R=184) %\u0026gt;% print 1## 2## ORDINARY NONPARAMETRIC BOOTSTRAP 3## 4## 5## Call: 6## boot(data = defDat, statistic = boot.fn, R = 184) 7## 8## 9## Bootstrap Statistics : 10## original bias std. error 11## t1* -1.154047e+01 -1.407368e-02 4.073453e-01 12## t2* 2.080898e-05 -6.386634e-08 4.720109e-06 13## t3* 5.647103e-03 1.350950e-05 2.111547e-04 d) Comparison  Clearly, there is not much difference in the standard error estimates  Var | Bootstrap | Summary |\n| :\u0026mdash;\u0026mdash;\u0026mdash;: | \u0026mdash;\u0026mdash;\u0026mdash; |\nIntercept | 4.428026e-01 | 4.883e-01 |\nincome | 2.797011e-06 | 5.548e-06 |\nbalance | 2.423002e-04 | 2.591e-04 |\n Question 5.8 - Page 200 We will now perform cross-validation on a simulated data set. (a) Generate a simulated data set as follows:\n1\u0026gt; set . seed (1) 2\u0026gt; y = rnorm (100) 3\u0026gt; x = rnorm (100) 4\u0026gt; y =x -2\\* x ^2+ rnorm (100) In this data set, what is n and what is p? Write out the model used to generate the data in equation form.\n(b) Create a scatterplot of \\(X\\) against \\(Y\\). Comment on what you find.\n(c) Set a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares:\n  \\(Y=\\beta_0+\\beta_1X+\\eta\\)\n  \\(Y=\\beta_0+\\beta_1X+\\beta_2X^2+\\eta\\)\n  \\(Y=\\beta_0+\\beta_1X+\\beta_2X^2+\\beta_{3}X^{3}+\\eta\\)\n  \\(Y=\\beta_0+\\beta_1X+\\beta_2X^2+\\beta_{3}X^{3}+\\beta_{4}X^{4}+\\eta\\)\n  Note you may find it helpful to use the data.frame() function to create a single data set containing both \\(X\\) and \\(Y\\).\n(d) Repeat (c) using another random seed, and report your results. Are your results the same as what you got in (c)? Why?\n(e) Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer.\n(f) Comment on the statistical significance of the coefficient esti- mates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?\nAnswer a) Modeling data 1set.seed(1) 2y \u0026lt;- rnorm(100) 3x \u0026lt;- rnorm(100) 4y \u0026lt;- x - 2*x^2 + rnorm(100) Clearly:\n Our equation is \\(y=x-2x^{2}+\\epsilon\\) where \\(epsilon\\) is normally distributed from 100 samples We have \\(n=100\\) observations \\(p=2\\) where \\(p\\) is the number of features  b) Visual inspection 1qplot(x,y)  Figure 10: Model data plot\n  We observe that the data is quadratic, as we also know from the generating function, which was a quadratic equation plus normally distributed noise.\nc) Least squares fits Not very important, but here we use the caret form.\n1pow=function(x,y){return(x^y)} 2dfDat \u0026lt;- data.frame(y,x,x2=pow(x,2),x3=pow(x,3),x4=pow(x,4)) We might have also just used poly(x,n) to skip making the data frame.\nWe will set our resampling method as follows:\n1fitControl\u0026lt;-trainControl(method=\u0026#34;LOOCV\u0026#34;) 1train(y~x,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 1 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 2.427134 0.05389864 1.878566 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE 1train(y~x+x2,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 2 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 1.042399 0.8032414 0.8029942 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE 1train(y~x+x2+x3,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 3 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 1.050041 0.8003517 0.8073024 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE 1train(y~x+x2+x3+x4,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 4 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 1.055828 0.7982111 0.8150296 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE d) Seeding effects 1set.seed(1995) 1train(y~x,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 1 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 2.427134 0.05389864 1.878566 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE 1train(y~x+x2,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 2 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 1.042399 0.8032414 0.8029942 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE 1train(y~x+x2+x3,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 3 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 1.050041 0.8003517 0.8073024 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE 1train(y~x+x2+x3+x4,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 4 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 1.055828 0.7982111 0.8150296 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE We note that there is no change on varying the seed because LOOCV is exhaustive and uses n folds for each observation.\ne) Analysis 1train(y~x,data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 1 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 2.427134 0.05389864 1.878566 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE 1train(y~poly(x,2),data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 1 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 1.042399 0.8032414 0.8029942 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE 1train(y~poly(x,3),data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 1 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 1.050041 0.8003517 0.8073024 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE 1train(y~poly(x,4),data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print 1## Linear Regression 2## 3## 100 samples 4## 1 predictor 5## 6## No pre-processing 7## Resampling: Leave-One-Out Cross-Validation 8## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... 9## Resampling results: 10## 11## RMSE Rsquared MAE 12## 1.055828 0.7982111 0.8150296 13## 14## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE Clearly the quadratic polynomial has the lowest error, which makes sense given how the data was generated.\nf) Statistical significance 1train(y~x,data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% summary %\u0026gt;% print 1## 2## Call: 3## lm(formula = .outcome ~ ., data = dat) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -7.3469 -0.9275 0.8028 1.5608 4.3974 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) -1.8185 0.2364 -7.692 1.14e-11 *** 12## x 0.2430 0.2479 0.981 0.329 13## --- 14## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 15## 16## Residual standard error: 2.362 on 98 degrees of freedom 17## Multiple R-squared: 0.009717, Adjusted R-squared: -0.0003881 18## F-statistic: 0.9616 on 1 and 98 DF, p-value: 0.3292 1train(y~poly(x,2),data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% summary %\u0026gt;% print 1## 2## Call: 3## lm(formula = .outcome ~ ., data = dat) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -2.89884 -0.53765 0.04135 0.61490 2.73607 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) -1.8277 0.1032 -17.704 \u0026lt;2e-16 *** 12## `poly(x, 2)1` 2.3164 1.0324 2.244 0.0271 * 13## `poly(x, 2)2` -21.0586 1.0324 -20.399 \u0026lt;2e-16 *** 14## --- 15## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 16## 17## Residual standard error: 1.032 on 97 degrees of freedom 18## Multiple R-squared: 0.8128, Adjusted R-squared: 0.8089 19## F-statistic: 210.6 on 2 and 97 DF, p-value: \u0026lt; 2.2e-16 1train(y~poly(x,3),data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% summary %\u0026gt;% print 1## 2## Call: 3## lm(formula = .outcome ~ ., data = dat) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -2.87250 -0.53881 0.02862 0.59383 2.74350 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) -1.8277 0.1037 -17.621 \u0026lt;2e-16 *** 12## `poly(x, 3)1` 2.3164 1.0372 2.233 0.0279 * 13## `poly(x, 3)2` -21.0586 1.0372 -20.302 \u0026lt;2e-16 *** 14## `poly(x, 3)3` -0.3048 1.0372 -0.294 0.7695 15## --- 16## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 17## 18## Residual standard error: 1.037 on 96 degrees of freedom 19## Multiple R-squared: 0.813, Adjusted R-squared: 0.8071 20## F-statistic: 139.1 on 3 and 96 DF, p-value: \u0026lt; 2.2e-16 1train(y~poly(x,4),data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% summary %\u0026gt;% print 1## 2## Call: 3## lm(formula = .outcome ~ ., data = dat) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -2.8914 -0.5244 0.0749 0.5932 2.7796 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) -1.8277 0.1041 -17.549 \u0026lt;2e-16 *** 12## `poly(x, 4)1` 2.3164 1.0415 2.224 0.0285 * 13## `poly(x, 4)2` -21.0586 1.0415 -20.220 \u0026lt;2e-16 *** 14## `poly(x, 4)3` -0.3048 1.0415 -0.293 0.7704 15## `poly(x, 4)4` -0.4926 1.0415 -0.473 0.6373 16## --- 17## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 18## 19## Residual standard error: 1.041 on 95 degrees of freedom 20## Multiple R-squared: 0.8134, Adjusted R-squared: 0.8055 21## F-statistic: 103.5 on 4 and 95 DF, p-value: \u0026lt; 2.2e-16  Clearly, the second order terms are the most significant, as expected  Question 5.9 - Page 201 We will now consider the Boston housing data set, from the MASS library.\n(a) Based on this data set, provide an estimate for the population mean of medv. Call this estimate \\(\\hat{\\mu}\\).\n(b) Provide an estimate of the standard error of \\(\\hat{\\mu}\\). Interpret this result. Hint: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations.\n(c) Now estimate the standard error of \\(\\hat{\\mu}\\) using the bootstrap. How does this compare to your answer from (b)?\n(d) Based on your bootstrap estimate from (c), provide a 95 % confidence interval for the mean of medv. Compare it to the results obtained using t.test(Boston\\$medv). Hint: You can approximate a 95 % confidence interval using the formula \\([\\hat{\\mu} − 2SE(\\hat{\\mu}), \\hat{\\mu} + 2SE(\\hat{\\mu})]\\).\n(e) Based on this data set, provide an estimate, \\(\\hat{\\mu_{med}}\\), for the median value of medv in the population.\n(f) We now would like to estimate the standard error of \\(\\hat{\\mu}\\) med. Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your findings.\n(g) Based on this data set, provide an estimate for the tenth percentile of medv in Boston suburbs. Call this quantity \\(\\hat{\\mu_{0.1}}\\). (You can use the quantile() function.)\n(h) Use the bootstrap to estimate the standard error of \\(\\hat{\\mu_{0.1}}\\). Comment on your findings.\nAnswer 1boston\u0026lt;-MASS::Boston  Reminder   1boston %\u0026gt;% summary %\u0026gt;% print 1## crim zn indus chas 2## Min. : 0.00632 Min. : 0.00 Min. : 0.46 Min. :0.00000 3## 1st Qu.: 0.08204 1st Qu.: 0.00 1st Qu.: 5.19 1st Qu.:0.00000 4## Median : 0.25651 Median : 0.00 Median : 9.69 Median :0.00000 5## Mean : 3.61352 Mean : 11.36 Mean :11.14 Mean :0.06917 6## 3rd Qu.: 3.67708 3rd Qu.: 12.50 3rd Qu.:18.10 3rd Qu.:0.00000 7## Max. :88.97620 Max. :100.00 Max. :27.74 Max. :1.00000 8## nox rm age dis 9## Min. :0.3850 Min. :3.561 Min. : 2.90 Min. : 1.130 10## 1st Qu.:0.4490 1st Qu.:5.886 1st Qu.: 45.02 1st Qu.: 2.100 11## Median :0.5380 Median :6.208 Median : 77.50 Median : 3.207 12## Mean :0.5547 Mean :6.285 Mean : 68.57 Mean : 3.795 13## 3rd Qu.:0.6240 3rd Qu.:6.623 3rd Qu.: 94.08 3rd Qu.: 5.188 14## Max. :0.8710 Max. :8.780 Max. :100.00 Max. :12.127 15## rad tax ptratio black 16## Min. : 1.000 Min. :187.0 Min. :12.60 Min. : 0.32 17## 1st Qu.: 4.000 1st Qu.:279.0 1st Qu.:17.40 1st Qu.:375.38 18## Median : 5.000 Median :330.0 Median :19.05 Median :391.44 19## Mean : 9.549 Mean :408.2 Mean :18.46 Mean :356.67 20## 3rd Qu.:24.000 3rd Qu.:666.0 3rd Qu.:20.20 3rd Qu.:396.23 21## Max. :24.000 Max. :711.0 Max. :22.00 Max. :396.90 22## lstat medv 23## Min. : 1.73 Min. : 5.00 24## 1st Qu.: 6.95 1st Qu.:17.02 25## Median :11.36 Median :21.20 26## Mean :12.65 Mean :22.53 27## 3rd Qu.:16.95 3rd Qu.:25.00 28## Max. :37.97 Max. :50.00 1boston %\u0026gt;% str %\u0026gt;% print 1## \u0026#39;data.frame\u0026#39;: 506 obs. of 14 variables: 2## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... 3## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... 4## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... 5## $ chas : int 0 0 0 0 0 0 0 0 0 0 ... 6## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... 7## $ rm : num 6.58 6.42 7.18 7 7.15 ... 8## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... 9## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... 10## $ rad : int 1 2 2 3 3 3 5 5 5 5 ... 11## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... 12## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... 13## $ black : num 397 397 393 395 397 ... 14## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... 15## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... 16## NULL a) Mean 1muhat=boston$medv %\u0026gt;% mean() 2print(muhat) 1## [1] 22.53281 b) Standard error Recall that \\(SE=\\frac{SD}{\\sqrt{N_{obs}}}\\)\n1boston$medv %\u0026gt;% sd/(nrow(boston)^0.5) %\u0026gt;% print 1## [1] 22.49444 1## [1] 0.4088611 c) Bootstrap estimate 1library(boot) 2myMean\u0026lt;-function(frame,ind){return(mean(frame[ind]))} 1boot(boston$medv,myMean,R=184) %\u0026gt;% print 1## 2## ORDINARY NONPARAMETRIC BOOTSTRAP 3## 4## 5## Call: 6## boot(data = boston$medv, statistic = myMean, R = 184) 7## 8## 9## Bootstrap Statistics : 10## original bias std. error 11## t1* 22.53281 0.03451839 0.409621 We see that the bootstrapped error over 184 samples is 0.4341499 while without it we had 0.4088611 which is similar enough.\nd) Confidence intervals with bootstrap and t.test 1boston$medv %\u0026gt;% t.test %\u0026gt;% print 1## 2## One Sample t-test 3## 4## data: . 5## t = 55.111, df = 505, p-value \u0026lt; 2.2e-16 6## alternative hypothesis: true mean is not equal to 0 7## 95 percent confidence interval: 8## 21.72953 23.33608 9## sample estimates: 10## mean of x 11## 22.53281 We can approximate this with what we already have\n1bRes=boot(boston$medv,myMean,R=184) 2seBoot\u0026lt;-bRes$t %\u0026gt;% var %\u0026gt;% sqrt 3xlow=muhat-2*(seBoot) 4xhigh=muhat+2*(seBoot) 5c(xlow,xhigh) %\u0026gt;% print 1## [1] 21.72675 23.33887 Our intervals are also pretty close to each other.\ne) Median 1boston$medv %\u0026gt;% sort %\u0026gt;% median %\u0026gt;% print 1## [1] 21.2 f) Median standard error We can reuse the logic of the myMean function defined previously.\n1myMedian=function(data,ind){return(median(data[ind]))} 1boston$medv %\u0026gt;% boot(myMedian,R=1500) %\u0026gt;% print 1## 2## ORDINARY NONPARAMETRIC BOOTSTRAP 3## 4## 5## Call: 6## boot(data = ., statistic = myMedian, R = 1500) 7## 8## 9## Bootstrap Statistics : 10## original bias std. error 11## t1* 21.2 -0.03773333 0.387315 We see that the standard error is 0.3767072.\ng) Tenth percentile 1mu0one\u0026lt;-boston$medv %\u0026gt;% quantile(c(0.1)) 2print(mu0one) 1## 10% 2## 12.75 h) Bootstrap Once again.\n1myQuant=function(data,ind){return(quantile(data[ind],0.1))} 1boston$medv %\u0026gt;% boot(myQuant,R=500) %\u0026gt;% print 1## 2## ORDINARY NONPARAMETRIC BOOTSTRAP 3## 4## 5## Call: 6## boot(data = ., statistic = myQuant, R = 500) 7## 8## 9## Bootstrap Statistics : 10## original bias std. error 11## t1* 12.75 -0.0095 0.4951415 The standard error is 0.5024526\n  James, G., Witten, D., Hastie, T., \u0026amp; Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Berlin, Germany: Springer Science \u0026amp; Business Media.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Lang et al., (2019). mlr3: A modern object-oriented machine learning framework in R. Journal of Open Source Software, 4(44), 1903, https://doi.org/10.21105/joss.01903\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/islr-ch5/","tags":["solutions","R","ISLR"],"title":"  \"ISLR :: Resampling Methods\"\n  "},{"categories":["programming"],"contents":"Chapter IV - Classification All the questions are as per the ISL seventh printing of the First edition 1.\nCommon Stuff Here I\u0026rsquo;ll load things I will be using throughout, mostly libraries.\n1libsUsed\u0026lt;-c(\u0026#34;dplyr\u0026#34;,\u0026#34;ggplot2\u0026#34;,\u0026#34;tidyverse\u0026#34;,\u0026#34;ISLR\u0026#34;,\u0026#34;caret\u0026#34;) 2invisible(lapply(libsUsed, library, character.only = TRUE)) Question 4.10 - Page 171 This question should be answered using the Weekly data set, which is part of the ISLR package. This data is similar in nature to the Smarket data from this chapter\u0026rsquo;s lab, except that it contains 1, 089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.\n(a) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?\n(b) Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?\n(c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.\n(d) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).\n(e) Repeat (d) using LDA.\n(f) Repeat (d) using QDA.\n(g) Repeat (d) using KNN with \\(K = 1\\).\n(h) Which of these methods appears to provide the best results on this data?\n(i) Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier.\nAnswer We will need the data in a variable for ease of use.\n1weeklyDat\u0026lt;-ISLR::Weekly a) Summary Statistics Text Most of this segment relies heavily on usage of dplyr and especially the %\u0026gt;% or pipe operator for readability. The use of the skimr package2 might added more descriptive statistics, but is not covered here.\nBasic Summaries 1weeklyDat %\u0026gt;% str 1# \u0026#39;data.frame\u0026#39;: 1089 obs. of 9 variables: 2# $ Year : num 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 ... 3# $ Lag1 : num 0.816 -0.27 -2.576 3.514 0.712 ... 4# $ Lag2 : num 1.572 0.816 -0.27 -2.576 3.514 ... 5# $ Lag3 : num -3.936 1.572 0.816 -0.27 -2.576 ... 6# $ Lag4 : num -0.229 -3.936 1.572 0.816 -0.27 ... 7# $ Lag5 : num -3.484 -0.229 -3.936 1.572 0.816 ... 8# $ Volume : num 0.155 0.149 0.16 0.162 0.154 ... 9# $ Today : num -0.27 -2.576 3.514 0.712 1.178 ... 10# $ Direction: Factor w/ 2 levels \u0026#34;Down\u0026#34;,\u0026#34;Up\u0026#34;: 1 1 2 2 2 1 2 2 2 1 ... We see that there is only one Factor, which makes sense.\n1weeklyDat %\u0026gt;% summary 1# Year Lag1 Lag2 Lag3 2# Min. :1990 Min. :-18.1950 Min. :-18.1950 Min. :-18.1950 3# 1st Qu.:1995 1st Qu.: -1.1540 1st Qu.: -1.1540 1st Qu.: -1.1580 4# Median :2000 Median : 0.2410 Median : 0.2410 Median : 0.2410 5# Mean :2000 Mean : 0.1506 Mean : 0.1511 Mean : 0.1472 6# 3rd Qu.:2005 3rd Qu.: 1.4050 3rd Qu.: 1.4090 3rd Qu.: 1.4090 7# Max. :2010 Max. : 12.0260 Max. : 12.0260 Max. : 12.0260 8# Lag4 Lag5 Volume Today 9# Min. :-18.1950 Min. :-18.1950 Min. :0.08747 Min. :-18.1950 10# 1st Qu.: -1.1580 1st Qu.: -1.1660 1st Qu.:0.33202 1st Qu.: -1.1540 11# Median : 0.2380 Median : 0.2340 Median :1.00268 Median : 0.2410 12# Mean : 0.1458 Mean : 0.1399 Mean :1.57462 Mean : 0.1499 13# 3rd Qu.: 1.4090 3rd Qu.: 1.4050 3rd Qu.:2.05373 3rd Qu.: 1.4050 14# Max. : 12.0260 Max. : 12.0260 Max. :9.32821 Max. : 12.0260 15# Direction 16# Down:484 17# Up :605 18# 19# 20# 21# Unique Values We might also want to know how many unique values are there in each column.\n1weeklyDat %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) 1# Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today 2# 21 1004 1005 1005 1005 1005 1089 1003 3# Direction 4# 2 We note that year has disproportionately lower values, something to keep in mind while constructing models later.\nRange The range of each variable might be useful as well, but we have to ignore the factor.\n1weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% sapply(range) 1# Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today 2# [1,] 1990 -18.195 -18.195 -18.195 -18.195 -18.195 0.087465 -18.195 3# [2,] 2010 12.026 12.026 12.026 12.026 12.026 9.328214 12.026 The most interesting thing about this is probably that the Lag variables all have the same range, also something to be kept in mind while applying transformations to the variable (if at all).\nMean and Std. Dev By now we might have a pretty good idea of how this will look, but it is still worth seeing.\n1weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% sapply(mean) 1# Year Lag1 Lag2 Lag3 Lag4 Lag5 2# 2000.0486685 0.1505849 0.1510790 0.1472048 0.1458182 0.1398926 3# Volume Today 4# 1.5746176 0.1498990 As expected, the Lag values have almost the same mean, what is a bit interesting though, is that the Today variable has roughly the same mean as the Lag variables.\n1weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% sapply(sd) 1# Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today 2# 6.033182 2.357013 2.357254 2.360502 2.360279 2.361285 1.686636 2.356927 This is largely redundant in terms of new information.\nCorrelations 1weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% cor 1# Year Lag1 Lag2 Lag3 Lag4 2# Year 1.00000000 -0.032289274 -0.03339001 -0.03000649 -0.031127923 3# Lag1 -0.03228927 1.000000000 -0.07485305 0.05863568 -0.071273876 4# Lag2 -0.03339001 -0.074853051 1.00000000 -0.07572091 0.058381535 5# Lag3 -0.03000649 0.058635682 -0.07572091 1.00000000 -0.075395865 6# Lag4 -0.03112792 -0.071273876 0.05838153 -0.07539587 1.000000000 7# Lag5 -0.03051910 -0.008183096 -0.07249948 0.06065717 -0.075675027 8# Volume 0.84194162 -0.064951313 -0.08551314 -0.06928771 -0.061074617 9# Today -0.03245989 -0.075031842 0.05916672 -0.07124364 -0.007825873 10# Lag5 Volume Today 11# Year -0.030519101 0.84194162 -0.032459894 12# Lag1 -0.008183096 -0.06495131 -0.075031842 13# Lag2 -0.072499482 -0.08551314 0.059166717 14# Lag3 0.060657175 -0.06928771 -0.071243639 15# Lag4 -0.075675027 -0.06107462 -0.007825873 16# Lag5 1.000000000 -0.05851741 0.011012698 17# Volume -0.058517414 1.00000000 -0.033077783 18# Today 0.011012698 -0.03307778 1.000000000 Useful though this is, it is kind of difficult to work with, in this form, so we might as well programmatic-ally remove strongly correlated data instead.\n1# Uses caret 2corrCols=weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% cor %\u0026gt;% findCorrelation(cutoff=0.8) 3reducedDat\u0026lt;-weeklyDat[-c(corrCols)] 4reducedDat %\u0026gt;% summary 1# Year Lag1 Lag2 Lag3 2# Min. :1990 Min. :-18.1950 Min. :-18.1950 Min. :-18.1950 3# 1st Qu.:1995 1st Qu.: -1.1540 1st Qu.: -1.1540 1st Qu.: -1.1580 4# Median :2000 Median : 0.2410 Median : 0.2410 Median : 0.2410 5# Mean :2000 Mean : 0.1506 Mean : 0.1511 Mean : 0.1472 6# 3rd Qu.:2005 3rd Qu.: 1.4050 3rd Qu.: 1.4090 3rd Qu.: 1.4090 7# Max. :2010 Max. : 12.0260 Max. : 12.0260 Max. : 12.0260 8# Lag4 Lag5 Today Direction 9# Min. :-18.1950 Min. :-18.1950 Min. :-18.1950 Down:484 10# 1st Qu.: -1.1580 1st Qu.: -1.1660 1st Qu.: -1.1540 Up :605 11# Median : 0.2380 Median : 0.2340 Median : 0.2410 12# Mean : 0.1458 Mean : 0.1399 Mean : 0.1499 13# 3rd Qu.: 1.4090 3rd Qu.: 1.4050 3rd Qu.: 1.4050 14# Max. : 12.0260 Max. : 12.0260 Max. : 12.0260 We can see that the Volume variable has been dropped, since it evidently is strongly correlated with Year. This may or may not be a useful insight, but it is good to keep in mind.\nVisualization We will be using the ggplot2 library throughout for this segment.\nLets start with some scatter plots in a one v/s all scheme, similar to the methodology described here.\n1weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% gather(-Year,key=\u0026#34;Variable\u0026#34;, value=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=Value,y=Year)) + 2geom_point() + 3facet_wrap(~Variable) + 4coord_flip()  Figure 1: One v/s all for Direction\n  That didn\u0026rsquo;t really tell us much which we didn\u0026rsquo;t already get from the cor() function, but we can go the whole hog and do this for every variable since we don\u0026rsquo;t have that many in the first place..\n1weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% pairs  Figure 2: Pairs\n  This is not especially useful, and it is doubtful if more scatter-plots will help at all, so lets move on to box plots.\n1weeklyDat %\u0026gt;% pivot_longer(-c(Direction,Volume,Today,Year),names_to=\u0026#34;Lag\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=Direction,y=Value,fill=Lag)) + 2geom_boxplot()  Figure 3: Box plots for Direction\n  1weeklyDat %\u0026gt;% pivot_longer(-c(Direction,Volume,Today,Year),names_to=\u0026#34;Lag\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=Today,y=Value,fill=Lag)) + 2geom_boxplot()  Figure 4: More box plots\n  1weeklyDat %\u0026gt;% pivot_longer(-c(Direction,Volume,Today,Year),names_to=\u0026#34;Lag\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=Lag,y=Value,fill=Direction)) + 2geom_boxplot()  Figure 5: Lag v/s all\n  This does summarize our text analysis quite well. Importantly, it tells us that the Today value is largely unrelated to the \\(4\\) Lag variables.\nA really good-looking box-plot is easy to get with the caret library:\n1weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% featurePlot( 2y = weeklyDat$Direction, 3plot = \u0026#34;box\u0026#34;, 4# Pass in options to bwplot() 5scales = list(y = list(relation=\u0026#34;free\u0026#34;), 6x = list(rot = 90)), 7auto.key = list(columns = 2))  Figure 6: Plots with caret\n  We might want to visualize our correlation matrix as well.\n1library(reshape2) 1# 2# Attaching package: \u0026#39;reshape2\u0026#39; 1# The following object is masked from \u0026#39;package:tidyr\u0026#39;: 2# 3# smiths 1weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% cor %\u0026gt;% melt %\u0026gt;% ggplot(aes(x=Var1,y=Var2,fill=value)) + 2geom_tile()  Figure 7: Heatmap of the correlation matrix\n  b) Logistic Regression - Predictor Significance Lets start with the native glm function.\n1glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=weeklyDat, family=binomial) 2summary(glm.fit) 1# 2# Call: 3# glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + 4# Volume, family = binomial, data = weeklyDat) 5# 6# Deviance Residuals: 7# Min 1Q Median 3Q Max 8# -1.6949 -1.2565 0.9913 1.0849 1.4579 9# 10# Coefficients: 11# Estimate Std. Error z value Pr(\u0026gt;|z|) 12# (Intercept) 0.26686 0.08593 3.106 0.0019 ** 13# Lag1 -0.04127 0.02641 -1.563 0.1181 14# Lag2 0.05844 0.02686 2.175 0.0296 * 15# Lag3 -0.01606 0.02666 -0.602 0.5469 16# Lag4 -0.02779 0.02646 -1.050 0.2937 17# Lag5 -0.01447 0.02638 -0.549 0.5833 18# Volume -0.02274 0.03690 -0.616 0.5377 19# --- 20# Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 21# 22# (Dispersion parameter for binomial family taken to be 1) 23# 24# Null deviance: 1496.2 on 1088 degrees of freedom 25# Residual deviance: 1486.4 on 1082 degrees of freedom 26# AIC: 1500.4 27# 28# Number of Fisher Scoring iterations: 4 Evidently, only the Lag2 value is of statistical significance.\nIt is always of importance to figure out what numeric values R will assign to our factors, and it is best not to guess.\n1contrasts(weeklyDat$Direction) 1# Up 2# Down 0 3# Up 1 c) Confusion Matrix and Metrics Essentially:\n Predict the response Create an output length vector Apply thresholding to obtain labels   1glm.probs = predict(glm.fit, type = \u0026#34;response\u0026#34;) 2glm.pred = rep(\u0026#34;Up\u0026#34;,length(glm.probs)) 3glm.pred[glm.probs\u0026lt;0.5]=\u0026#34;Down\u0026#34; 4glm.pred=factor(glm.pred) 5confusionMatrix(glm.pred,weeklyDat$Direction) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction Down Up 5# Down 54 48 6# Up 430 557 7# 8# Accuracy : 0.5611 9# 95% CI : (0.531, 0.5908) 10# No Information Rate : 0.5556 11# P-Value [Acc \u0026gt; NIR] : 0.369 12# 13# Kappa : 0.035 14# 15# Mcnemar\u0026#39;s Test P-Value : \u0026lt;2e-16 16# 17# Sensitivity : 0.11157 18# Specificity : 0.92066 19# Pos Pred Value : 0.52941 20# Neg Pred Value : 0.56434 21# Prevalence : 0.44444 22# Detection Rate : 0.04959 23# Detection Prevalence : 0.09366 24# Balanced Accuracy : 0.51612 25# 26# \u0026#39;Positive\u0026#39; Class : Down 27#  We have used the confusionMatrix function from caret (documented here) instead of displaying the results with table and then calculating precision, recall and the rest by hand.  d) Train Test Splits Although we could have used the indices and passed it to glm as the subset attribute, it is cleaner to just make subsets instead.\n1weeklyVal\u0026lt;-weeklyDat %\u0026gt;% filter(Year\u0026gt;=2009) 2weeklyTrain\u0026lt;-weeklyDat %\u0026gt;% filter(Year\u0026lt;2009) Now we can train a model on our training data.\n1glm.fit=glm(Direction~Lag2,data=weeklyTrain,family=binomial) 2summary(glm.fit) 1# 2# Call: 3# glm(formula = Direction ~ Lag2, family = binomial, data = weeklyTrain) 4# 5# Deviance Residuals: 6# Min 1Q Median 3Q Max 7# -1.536 -1.264 1.021 1.091 1.368 8# 9# Coefficients: 10# Estimate Std. Error z value Pr(\u0026gt;|z|) 11# (Intercept) 0.20326 0.06428 3.162 0.00157 ** 12# Lag2 0.05810 0.02870 2.024 0.04298 * 13# --- 14# Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 15# 16# (Dispersion parameter for binomial family taken to be 1) 17# 18# Null deviance: 1354.7 on 984 degrees of freedom 19# Residual deviance: 1350.5 on 983 degrees of freedom 20# AIC: 1354.5 21# 22# Number of Fisher Scoring iterations: 4 Having fit our model, we will test the predictions on our held out data.\n1glm.probs = predict(glm.fit,weeklyVal, type = \u0026#34;response\u0026#34;) 2glm.pred = rep(\u0026#34;Up\u0026#34;,length(glm.probs)) 3glm.pred[glm.probs\u0026lt;0.5]=\u0026#34;Down\u0026#34; 4glm.pred=factor(glm.pred) 5confusionMatrix(glm.pred,weeklyVal$Direction) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction Down Up 5# Down 9 5 6# Up 34 56 7# 8# Accuracy : 0.625 9# 95% CI : (0.5247, 0.718) 10# No Information Rate : 0.5865 11# P-Value [Acc \u0026gt; NIR] : 0.2439 12# 13# Kappa : 0.1414 14# 15# Mcnemar\u0026#39;s Test P-Value : 7.34e-06 16# 17# Sensitivity : 0.20930 18# Specificity : 0.91803 19# Pos Pred Value : 0.64286 20# Neg Pred Value : 0.62222 21# Prevalence : 0.41346 22# Detection Rate : 0.08654 23# Detection Prevalence : 0.13462 24# Balanced Accuracy : 0.56367 25# 26# \u0026#39;Positive\u0026#39; Class : Down 27# We really aren\u0026rsquo;t doing very well with this single variable model as is evident.\ne) LDA models At this stage we could use MASS to get the lda function, but it would be better to just switch to using caret. Note that the caret prediction is a label by default, so thresholding needs to be specified differently if required.\n1lda.fit=train(Direction~Lag2,data=weeklyTrain,method=\u0026#34;lda\u0026#34;) 2summary(lda.fit) 1# Length Class Mode 2# prior 2 -none- numeric 3# counts 2 -none- numeric 4# means 2 -none- numeric 5# scaling 1 -none- numeric 6# lev 2 -none- character 7# svd 1 -none- numeric 8# N 1 -none- numeric 9# call 3 -none- call 10# xNames 1 -none- character 11# problemType 1 -none- character 12# tuneValue 1 data.frame list 13# obsLevels 2 -none- character 14# param 0 -none- list 1predict(lda.fit,weeklyVal) %\u0026gt;% confusionMatrix(weeklyVal$Direction) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction Down Up 5# Down 9 5 6# Up 34 56 7# 8# Accuracy : 0.625 9# 95% CI : (0.5247, 0.718) 10# No Information Rate : 0.5865 11# P-Value [Acc \u0026gt; NIR] : 0.2439 12# 13# Kappa : 0.1414 14# 15# Mcnemar\u0026#39;s Test P-Value : 7.34e-06 16# 17# Sensitivity : 0.20930 18# Specificity : 0.91803 19# Pos Pred Value : 0.64286 20# Neg Pred Value : 0.62222 21# Prevalence : 0.41346 22# Detection Rate : 0.08654 23# Detection Prevalence : 0.13462 24# Balanced Accuracy : 0.56367 25# 26# \u0026#39;Positive\u0026#39; Class : Down 27# f) QDA models 1qda.fit=train(Direction~Lag2,data=weeklyTrain,method=\u0026#34;qda\u0026#34;) 2summary(qda.fit) 1# Length Class Mode 2# prior 2 -none- numeric 3# counts 2 -none- numeric 4# means 2 -none- numeric 5# scaling 2 -none- numeric 6# ldet 2 -none- numeric 7# lev 2 -none- character 8# N 1 -none- numeric 9# call 3 -none- call 10# xNames 1 -none- character 11# problemType 1 -none- character 12# tuneValue 1 data.frame list 13# obsLevels 2 -none- character 14# param 0 -none- list 1predict(qda.fit,weeklyVal) %\u0026gt;% confusionMatrix(weeklyVal$Direction) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction Down Up 5# Down 0 0 6# Up 43 61 7# 8# Accuracy : 0.5865 9# 95% CI : (0.4858, 0.6823) 10# No Information Rate : 0.5865 11# P-Value [Acc \u0026gt; NIR] : 0.5419 12# 13# Kappa : 0 14# 15# Mcnemar\u0026#39;s Test P-Value : 1.504e-10 16# 17# Sensitivity : 0.0000 18# Specificity : 1.0000 19# Pos Pred Value : NaN 20# Neg Pred Value : 0.5865 21# Prevalence : 0.4135 22# Detection Rate : 0.0000 23# Detection Prevalence : 0.0000 24# Balanced Accuracy : 0.5000 25# 26# \u0026#39;Positive\u0026#39; Class : Down 27# This is quite possibly the worst of the lot. As is evident, the model just predicts Up no matter what.\ng) KNN caret tends to over-zealously retrain models and find the best possible parameters. In this case that is annoying and redundant so we will use the class library. We should really scale our data before using KNN though.\n1library(class) 2set.seed(1) 3knn.pred=knn(as.matrix(weeklyTrain$Lag2),as.matrix(weeklyVal$Lag2),weeklyTrain$Direction,k=1) 4confusionMatrix(knn.pred,weeklyVal$Direction) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction Down Up 5# Down 21 30 6# Up 22 31 7# 8# Accuracy : 0.5 9# 95% CI : (0.4003, 0.5997) 10# No Information Rate : 0.5865 11# P-Value [Acc \u0026gt; NIR] : 0.9700 12# 13# Kappa : -0.0033 14# 15# Mcnemar\u0026#39;s Test P-Value : 0.3317 16# 17# Sensitivity : 0.4884 18# Specificity : 0.5082 19# Pos Pred Value : 0.4118 20# Neg Pred Value : 0.5849 21# Prevalence : 0.4135 22# Detection Rate : 0.2019 23# Detection Prevalence : 0.4904 24# Balanced Accuracy : 0.4983 25# 26# \u0026#39;Positive\u0026#39; Class : Down 27# Clearly this model is not doing very well.\nh) Model Selection We will first get the ROC curves.\n1library(pROC) 1# Type \u0026#39;citation(\u0026#34;pROC\u0026#34;)\u0026#39; for a citation. 1# 2# Attaching package: \u0026#39;pROC\u0026#39; 1# The following objects are masked from \u0026#39;package:stats\u0026#39;: 2# 3# cov, smooth, var 1knnROC\u0026lt;-roc(predictor=as.numeric(knn.pred),response=weeklyVal$Direction,levels=rev(levels(weeklyVal$Direction))) 1# Setting direction: controls \u0026lt; cases 1logiROC\u0026lt;-roc(predictor=as.numeric(predict(glm.fit,weeklyVal)),response=weeklyVal$Direction) 1# Setting levels: control = Down, case = Up 1# Setting direction: controls \u0026gt; cases 1ldaROC\u0026lt;-roc(predictor=as.numeric(predict(lda.fit,weeklyVal)),response=weeklyVal$Direction) 1# Setting levels: control = Down, case = Up 1# Setting direction: controls \u0026lt; cases 1qdaROC\u0026lt;-roc(predictor=as.numeric(predict(qda.fit,weeklyVal)),response=weeklyVal$Direction) 1# Setting levels: control = Down, case = Up 2# Setting direction: controls \u0026lt; cases Now to plot them.\n1ggroc(list(KNN=knnROC,Logistic=logiROC,LDA=ldaROC,QDA=qdaROC))  Figure 8: ROC curves for Weekly data\n  To compare models with caret it is easy to refit the logistic and knn models in the caret formulation.\n1knnCaret=train(Direction~Lag2,data=weeklyTrain,method=\u0026#34;knn\u0026#34;) However, the KNN model is the best parameter model.\n1resmod \u0026lt;- resamples(list(lda=lda.fit, qda=qda.fit, KNN=knnCaret)) 2summary(resmod) 1# 2# Call: 3# summary.resamples(object = resmod) 4# 5# Models: lda, qda, KNN 6# Number of resamples: 25 7# 8# Accuracy 9# Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s 10# lda 0.5043228 0.5344353 0.5529101 0.5500861 0.5683060 0.5846995 0 11# qda 0.5044248 0.5204360 0.5307263 0.5326785 0.5462428 0.5777778 0 12# KNN 0.4472222 0.5082873 0.5240642 0.5168327 0.5302198 0.5485714 0 13# 14# Kappa 15# Min. 1st Qu. Median Mean 3rd Qu. Max. 16# lda -0.02618939 -0.003638168 0.005796908 0.007801904 0.01635328 0.05431238 17# qda -0.06383592 -0.005606123 0.000000000 -0.003229697 0.00000000 0.03606344 18# KNN -0.11297539 0.004168597 0.024774647 0.016171229 0.04456142 0.07724439 19# NA\u0026#39;s 20# lda 0 21# qda 0 22# KNN 0 1bwplot(resmod)  Figure 9: Caret plots for comparison\n  1dotplot(resmod)  Kappa or Cohen\u0026rsquo;s Kappa is essentially classification accuracy, normalized at the baseline of random chance. It is a more useful measure to use on problems that have imbalanced classes. There\u0026rsquo;s more on model selection here.\ni) Further Tuning Do note the caret defaults.\n1fitControl \u0026lt;- trainControl(# 10-fold CV 2method = \u0026#34;repeatedcv\u0026#34;, 3number = 10, 4# repeated ten times 5repeats = 10) Logistic 1glm2.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=weeklyDat, family=binomial) 23glm2.probs = predict(glm2.fit,weeklyVal, type = \u0026#34;response\u0026#34;) 4glm2.pred = rep(\u0026#34;Up\u0026#34;,length(glm2.probs)) 5glm2.pred[glm2.probs\u0026lt;0.5]=\u0026#34;Down\u0026#34; 6glm2.pred=factor(glm2.pred) 7confusionMatrix(glm2.pred,weeklyVal$Direction) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction Down Up 5# Down 17 13 6# Up 26 48 7# 8# Accuracy : 0.625 9# 95% CI : (0.5247, 0.718) 10# No Information Rate : 0.5865 11# P-Value [Acc \u0026gt; NIR] : 0.24395 12# 13# Kappa : 0.1907 14# 15# Mcnemar\u0026#39;s Test P-Value : 0.05466 16# 17# Sensitivity : 0.3953 18# Specificity : 0.7869 19# Pos Pred Value : 0.5667 20# Neg Pred Value : 0.6486 21# Prevalence : 0.4135 22# Detection Rate : 0.1635 23# Detection Prevalence : 0.2885 24# Balanced Accuracy : 0.5911 25# 26# \u0026#39;Positive\u0026#39; Class : Down 27# QDA 1qdaCaret=train(Direction~Lag2+Lag4,data=weeklyTrain,method=\u0026#34;qda\u0026#34;,trainControl=fitControl) 1summary(qdaCaret) 1# Length Class Mode 2# prior 2 -none- numeric 3# counts 2 -none- numeric 4# means 4 -none- numeric 5# scaling 8 -none- numeric 6# ldet 2 -none- numeric 7# lev 2 -none- character 8# N 1 -none- numeric 9# call 4 -none- call 10# xNames 2 -none- character 11# problemType 1 -none- character 12# tuneValue 1 data.frame list 13# obsLevels 2 -none- character 14# param 1 -none- list 1predict(qdaCaret,weeklyVal) %\u0026gt;% confusionMatrix(weeklyVal$Direction) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction Down Up 5# Down 9 14 6# Up 34 47 7# 8# Accuracy : 0.5385 9# 95% CI : (0.438, 0.6367) 10# No Information Rate : 0.5865 11# P-Value [Acc \u0026gt; NIR] : 0.863079 12# 13# Kappa : -0.0217 14# 15# Mcnemar\u0026#39;s Test P-Value : 0.006099 16# 17# Sensitivity : 0.20930 18# Specificity : 0.77049 19# Pos Pred Value : 0.39130 20# Neg Pred Value : 0.58025 21# Prevalence : 0.41346 22# Detection Rate : 0.08654 23# Detection Prevalence : 0.22115 24# Balanced Accuracy : 0.48990 25# 26# \u0026#39;Positive\u0026#39; Class : Down 27# LDA 1ldaCaret=train(Direction~Lag2+Lag1+Year,data=weeklyTrain,method=\u0026#34;lda\u0026#34;,trainControl=fitControl) 1summary(ldaCaret) 1# Length Class Mode 2# prior 2 -none- numeric 3# counts 2 -none- numeric 4# means 6 -none- numeric 5# scaling 3 -none- numeric 6# lev 2 -none- character 7# svd 1 -none- numeric 8# N 1 -none- numeric 9# call 4 -none- call 10# xNames 3 -none- character 11# problemType 1 -none- character 12# tuneValue 1 data.frame list 13# obsLevels 2 -none- character 14# param 1 -none- list 1predict(ldaCaret,weeklyVal) %\u0026gt;% confusionMatrix(weeklyVal$Direction) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction Down Up 5# Down 20 19 6# Up 23 42 7# 8# Accuracy : 0.5962 9# 95% CI : (0.4954, 0.6913) 10# No Information Rate : 0.5865 11# P-Value [Acc \u0026gt; NIR] : 0.4626 12# 13# Kappa : 0.1558 14# 15# Mcnemar\u0026#39;s Test P-Value : 0.6434 16# 17# Sensitivity : 0.4651 18# Specificity : 0.6885 19# Pos Pred Value : 0.5128 20# Neg Pred Value : 0.6462 21# Prevalence : 0.4135 22# Detection Rate : 0.1923 23# Detection Prevalence : 0.3750 24# Balanced Accuracy : 0.5768 25# 26# \u0026#39;Positive\u0026#39; Class : Down 27# KNN Honestly, again, this should be scaled. Plot KNN with the best parameters.\n1plot(knnCaret)  Figure 10: KNN statistics\n  Evidently, the accuracy increases with an increase in the number of neighbors considered.\n1plot(knnCaret, print.thres = 0.5, type=\u0026#34;S\u0026#34;)  Figure 11: Visualizing thresholds for KNN\n  However this shows that we don\u0026rsquo;t actually get much of an increase in accuracy anyway.\nQuestion 4.11 - Pages 171-172 In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set.\n(a) Create a binary variable, mpg01 , that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.\n(b) Explore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01 ? Scatter-plots and boxplots may be useful tools to answer this question. Describe your findings.\n(c) Split the data into a training set and a test set.\n(d) Perform LDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?\n(e) Perform QDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?\n(f) Perform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?\n(g) Perform KNN on the training data, with several values of \\(K\\), in order to predict mpg01 . Use only the variables that seemed most associated with mpg01 in (b). What test errors do you obtain? Which value of \\(K\\) seems to perform the best on this data set?\nAnswer 1autoDat\u0026lt;-ISLR::Auto a) Binary Variable 1autoDat$mpg %\u0026gt;% sort() %\u0026gt;% median() 1# [1] 22.75 Now we can get a new variable from that.\n1newDat=autoDat 2newDat$mpg01 \u0026lt;- ifelse(autoDat$mpg\u0026lt;autoDat$mpg %\u0026gt;% sort() %\u0026gt;% median(),0,1) %\u0026gt;% factor() Note that the ifelse command takes a truthy function, value when false, value when true, but does not return a factor automatically so we piped it to factor to ensure it is factorial.\nb) Visual Exploration Some box-plots:\n1newDat %\u0026gt;% pivot_longer(-c(mpg01,name),names_to=\u0026#34;Params\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=mpg01,y=Value)) + 2geom_boxplot() + 3facet_wrap(~ Params, scales = \u0026#34;free_y\u0026#34;)  Figure 12: Box plots\n  With some scatter plots as well:\n1newDat %\u0026gt;% pivot_longer(-c(mpg01,name,weight),names_to=\u0026#34;Params\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=weight,y=Value,color=mpg01)) + 2geom_point() + 3facet_wrap(~ Params, scales = \u0026#34;free_y\u0026#34;)  Figure 13: Scatter plots\n  Clearly, origin, year and cylinder are essentially not very relevant numerically for the regression lines and confidence intervals.\n1newDat %\u0026gt;% select(-year,-origin,-cylinders) %\u0026gt;% pivot_longer(-c(mpg01,name,mpg),names_to=\u0026#34;Params\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=mpg,y=Value,color=mpg01)) + 2geom_point() + 3geom_smooth(method=lm) + 4facet_wrap(~ Params, scales = \u0026#34;free_y\u0026#34;)  c) Train-Test Split We can split our data very easily with caret. It is important to remember that for factors, random sampling occurs within each class to preserve the overall class distribution of the data.\n1set.seed(1984) 2trainInd \u0026lt;- createDataPartition(newDat$mpg01, # Factor, so class sampling 3p=0.7, # 70-30 train-test 4list=FALSE, # No lists 5times=1) # No bootstrap 6autoTrain\u0026lt;-newDat[trainInd,] 7autoTest\u0026lt;-newDat[-trainInd,] d) LDA with Significant Variables Whenever I see significant I think correlation, so let\u0026rsquo;s take a look at that.\n1newDat %\u0026gt;% select(-mpg01,-name) %\u0026gt;% cor 1# mpg cylinders displacement horsepower weight 2# mpg 1.0000000 -0.7776175 -0.8051269 -0.7784268 -0.8322442 3# cylinders -0.7776175 1.0000000 0.9508233 0.8429834 0.8975273 4# displacement -0.8051269 0.9508233 1.0000000 0.8972570 0.9329944 5# horsepower -0.7784268 0.8429834 0.8972570 1.0000000 0.8645377 6# weight -0.8322442 0.8975273 0.9329944 0.8645377 1.0000000 7# acceleration 0.4233285 -0.5046834 -0.5438005 -0.6891955 -0.4168392 8# year 0.5805410 -0.3456474 -0.3698552 -0.4163615 -0.3091199 9# origin 0.5652088 -0.5689316 -0.6145351 -0.4551715 -0.5850054 10# acceleration year origin 11# mpg 0.4233285 0.5805410 0.5652088 12# cylinders -0.5046834 -0.3456474 -0.5689316 13# displacement -0.5438005 -0.3698552 -0.6145351 14# horsepower -0.6891955 -0.4163615 -0.4551715 15# weight -0.4168392 -0.3091199 -0.5850054 16# acceleration 1.0000000 0.2903161 0.2127458 17# year 0.2903161 1.0000000 0.1815277 18# origin 0.2127458 0.1815277 1.0000000 1newDat %\u0026gt;% length 1# [1] 10 Now lets quickly see what it looks like with correlated values removed.\n1corrCols2=newDat %\u0026gt;% select(-mpg01,-name) %\u0026gt;% cor %\u0026gt;% findCorrelation(cutoff=0.85) 2newRed\u0026lt;-newDat[-c(corrCols2)] 3newRed %\u0026gt;% summary 1# mpg weight acceleration year origin 2# Min. : 9.00 Min. :1613 Min. : 8.00 Min. :70.00 Min. :1.000 3# 1st Qu.:17.00 1st Qu.:2225 1st Qu.:13.78 1st Qu.:73.00 1st Qu.:1.000 4# Median :22.75 Median :2804 Median :15.50 Median :76.00 Median :1.000 5# Mean :23.45 Mean :2978 Mean :15.54 Mean :75.98 Mean :1.577 6# 3rd Qu.:29.00 3rd Qu.:3615 3rd Qu.:17.02 3rd Qu.:79.00 3rd Qu.:2.000 7# Max. :46.60 Max. :5140 Max. :24.80 Max. :82.00 Max. :3.000 8# 9# name mpg01 10# amc matador : 5 0:196 11# ford pinto : 5 1:196 12# toyota corolla : 5 13# amc gremlin : 4 14# amc hornet : 4 15# chevrolet chevette: 4 16# (Other) :365 Inherent in this discussion is the fact that I consider what is correlated to mpg to be a good indicator of what will help mpg01 for obvious reasons.\nNow we can just use the columns we found with findCorrelation.\n1corrCols2 %\u0026gt;% print 1# [1] 3 4 2 1names(newDat) 1# [1] \u0026#34;mpg\u0026#34; \u0026#34;cylinders\u0026#34; \u0026#34;displacement\u0026#34; \u0026#34;horsepower\u0026#34; \u0026#34;weight\u0026#34; 2# [6] \u0026#34;acceleration\u0026#34; \u0026#34;year\u0026#34; \u0026#34;origin\u0026#34; \u0026#34;name\u0026#34; \u0026#34;mpg01\u0026#34; 1autoLDA=train(mpg01~cylinders+displacement+horsepower,data=autoTrain,method=\u0026#34;lda\u0026#34;) 2valScoreLDA=predict(autoLDA,autoTest) Now we can check the statistics.\n1confusionMatrix(valScoreLDA,autoTest$mpg01) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction 0 1 5# 0 56 2 6# 1 2 56 7# 8# Accuracy : 0.9655 9# 95% CI : (0.9141, 0.9905) 10# No Information Rate : 0.5 11# P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 12# 13# Kappa : 0.931 14# 15# Mcnemar\u0026#39;s Test P-Value : 1 16# 17# Sensitivity : 0.9655 18# Specificity : 0.9655 19# Pos Pred Value : 0.9655 20# Neg Pred Value : 0.9655 21# Prevalence : 0.5000 22# Detection Rate : 0.4828 23# Detection Prevalence : 0.5000 24# Balanced Accuracy : 0.9655 25# 26# \u0026#39;Positive\u0026#39; Class : 0 27# That is an amazingly accurate model.\n1auto_ldaROC\u0026lt;-roc(predictor=as.numeric(valScoreLDA),response=autoTest$mpg01,levels=levels(autoTest$mpg01)) 1# Setting direction: controls \u0026lt; cases 1ggroc(auto_ldaROC)  e) QDA with Significant Variables Same deal as before.\n1autoQDA=train(mpg01~cylinders+displacement+horsepower,data=autoTrain,method=\u0026#34;qda\u0026#34;) 2valScoreQDA=predict(autoQDA,autoTest) Now we can check the statistics.\n1confusionMatrix(valScoreQDA,autoTest$mpg01) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction 0 1 5# 0 56 2 6# 1 2 56 7# 8# Accuracy : 0.9655 9# 95% CI : (0.9141, 0.9905) 10# No Information Rate : 0.5 11# P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 12# 13# Kappa : 0.931 14# 15# Mcnemar\u0026#39;s Test P-Value : 1 16# 17# Sensitivity : 0.9655 18# Specificity : 0.9655 19# Pos Pred Value : 0.9655 20# Neg Pred Value : 0.9655 21# Prevalence : 0.5000 22# Detection Rate : 0.4828 23# Detection Prevalence : 0.5000 24# Balanced Accuracy : 0.9655 25# 26# \u0026#39;Positive\u0026#39; Class : 0 27# 1auto_qdaROC\u0026lt;-roc(predictor=as.numeric(valScoreQDA),response=autoTest$mpg01,levels=levels(autoTest$mpg01)) 1# Setting direction: controls \u0026lt; cases 1ggroc(auto_qdaROC)  OK, this is weird enough to check if it isn\u0026rsquo;t some sort of artifact.\n1autoQDA2=train(mpg01~horsepower, data=autoTrain,method=\u0026#39;qda\u0026#39;) 2valScoreQDA2=predict(autoQDA2, autoTest) 3confusionMatrix(valScoreQDA2,autoTest$mpg01) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction 0 1 5# 0 42 3 6# 1 16 55 7# 8# Accuracy : 0.8362 9# 95% CI : (0.7561, 0.8984) 10# No Information Rate : 0.5 11# P-Value [Acc \u0026gt; NIR] : 4.315e-14 12# 13# Kappa : 0.6724 14# 15# Mcnemar\u0026#39;s Test P-Value : 0.005905 16# 17# Sensitivity : 0.7241 18# Specificity : 0.9483 19# Pos Pred Value : 0.9333 20# Neg Pred Value : 0.7746 21# Prevalence : 0.5000 22# Detection Rate : 0.3621 23# Detection Prevalence : 0.3879 24# Balanced Accuracy : 0.8362 25# 26# \u0026#39;Positive\u0026#39; Class : 0 27# OK, so the model isn\u0026rsquo;t completely creepily correct all the time. In this case we should probably think about what is going on. I would think it is because of the nature of the train-test split we performed. We have ensured during the sampling of our data that the train and test sets contain the SAME distribution (assumed). So that\u0026rsquo;s why our training result and test results are both incredibly good. They\u0026rsquo;re essentially the same thing.\nIn fact, this is the perfect time to consider a validation set, just to see what the models are really doing. Won\u0026rsquo;t get into it right now though.\nf) Logistic with Significant Variables 1glmAuto.fit=glm(mpg01~cylinders+displacement+horsepower, data=autoTrain, family=binomial) 1glmAuto.probs = predict(glmAuto.fit,autoTest, type = \u0026#34;response\u0026#34;) 2glmAuto.pred = rep(1,length(glmAuto.probs)) 3glmAuto.pred[glmAuto.probs\u0026lt;0.5]=0 4glmAuto.pred=factor(glmAuto.pred) 5confusionMatrix(glmAuto.pred,autoTest$mpg01) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction 0 1 5# 0 56 4 6# 1 2 54 7# 8# Accuracy : 0.9483 9# 95% CI : (0.8908, 0.9808) 10# No Information Rate : 0.5 11# P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 12# 13# Kappa : 0.8966 14# 15# Mcnemar\u0026#39;s Test P-Value : 0.6831 16# 17# Sensitivity : 0.9655 18# Specificity : 0.9310 19# Pos Pred Value : 0.9333 20# Neg Pred Value : 0.9643 21# Prevalence : 0.5000 22# Detection Rate : 0.4828 23# Detection Prevalence : 0.5172 24# Balanced Accuracy : 0.9483 25# 26# \u0026#39;Positive\u0026#39; Class : 0 27# g) KNN Modeling Scale the parameters later.\n1knnAuto=train(mpg01~cylinders+displacement+horsepower,data=autoTrain,method=\u0026#34;knn\u0026#34;) Plot KNN with the best parameters.\n1plot(knnCaret)  Evidently, the accuracy increases with an increase in the number of neighbors considered.\n1plot(knnAuto, print.thres = 0.5, type=\u0026#34;S\u0026#34;)  So we can see that \\(5\\) neighbors is a good compromise.\nQuestion 4.12 - Pages 172-173 This problem involves writing functions.\n(a) Write a function, Power() , that prints out the result of raising 2 to the 3rd power. In other words, your function should compute 2^3 and print out the results.\nHint: Recall that x^a raises x to the power a. Use the print() function to output the result.\n(b) Create a new function, Power2() , that allows you to pass any two numbers, x and a , and prints out the value of x^a . You can do this by beginning your function with the line\n1Power2=function(x,a){} You should be able to call your function by entering, for instance,\n1Power2(3,8) on the command line. This should output the value of \\(3^8\\), namely, \\(6,651\\).\n(c) Using the Power2() function that you just wrote, compute \\(10^3\\), \\(8^{17}\\), and \\(131^3\\).\n(d) Now create a new function, Power3(), that actually returns the result x^a as an R object, rather than simply printing it to the screen. That is, if you store the value x^a in an object called result within your function, then you can simply return() this result, using the following line:\n1return(result) The line above should be the last line in your function, before the } symbol.\n(e) Now using the Power3() function, create a plot of \\(f(x)=x^2\\). The x-axis should display a range of integers from \\(1\\) to \\(10\\), and the y-axis should display \\(x^2\\) . Label the axes appropriately, and use an appropriate title for the figure. Consider displaying either the x-axis, the y-axis, or both on the log-scale. You can do this by using log=‘‘x’’, log=‘‘y’’, or log=‘‘xy’’ as arguments to the plot() function.\n(f) Create a function, PlotPower() , that allows you to create a plot of x against x^a for a fixed a and for a range of values of x. For instance, if you call\n1PlotPower (1:10 ,3) then a plot should be created with an x-axis taking on values \\(1,2,\u0026hellip;,10\\) and a y-axis taking on values \\(1^3,2^3,\u0026hellip;,10^3\\)\nAnswer a) Create a Squaring Function 1Power=function(x){print(2^x)} 2Power(3) 1# [1] 8 b) Generalizing Power to arbitrary numbers 1Power2=function(x,a){print(x^a)} 1Power2(3,8) 1# [1] 6561 c) Random Testing of Power2 1Power2(10,3) 1# [1] 1000 1Power2(8,17) 1# [1] 2.2518e+15 1Power2(131,2) 1# [1] 17161 d) Return a value 1Power3=function(x,a){return(x^a)} e) Plot something with Power3 Actually now would be a good place to introduce LaTeX labeling.\n1#install.packages(\u0026#34;latex2exp\u0026#34;) 2library(latex2exp) No log scale.\n1qplot(x=seq(1,10),y=Power3(seq(1,10),2)) + ggtitle(\u0026#34;Function without a log scale\u0026#34;) + 2geom_point() + xlab(\u0026#34;X\u0026#34;) + ylab(TeX(\u0026#34;$X^2$\u0026#34;))  With a log scale.\n1qplot(x=seq(1,10),y=Power3(seq(1,10),2)) + ggtitle(\u0026#34;Function with a log scale\u0026#34;) + 2geom_point() + xlab(\u0026#34;X\u0026#34;) + ylab(TeX(\u0026#34;$X^2$\u0026#34;)) + scale_y_log10()  f) PlotPower Function 1PlotPower=function(xrange,pow){return(qplot(x=xrange,y=Power3(xrange,pow)))} 1plotter\u0026lt;-PlotPower(1:10,3) 2plotter  The R Cookbook is quite neat for some simple tasks like this.\nQuestion 4.13 - Pages 173 Using the Boston data set, fit classification models in order to predict whether a given suburb has a crime rate above or below the median. Explore logistic regression, LDA, and KNN models using various subsets of the predictors. Describe your findings.\nAnswer OK, to speed this up, I will simply run through all the work done on the Auto set. Recall that details about this data-set are also here.\n1boston\u0026lt;-MASS::Boston  Check unique values   1boston %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) 1# crim zn indus chas nox rm age dis rad tax 2# 504 26 76 2 81 446 356 412 9 66 3# ptratio black lstat medv 4# 46 357 455 229 CHAS is of course something which should be a factor, and with RAD having only \\(9\\) levels, I\u0026rsquo;m inclined to make it a factor as well.\n1boston\u0026lt;-boston %\u0026gt;% mutate(rad=factor(rad),chas=factor(chas))  Make a median variable   1boston$highCrime\u0026lt;- ifelse(boston$crim\u0026lt;boston$crim %\u0026gt;% median(),0,1) %\u0026gt;% factor()  Take a look at the data  Some box-plots:\n1boston %\u0026gt;% pivot_longer(-c(rad,chas,highCrime),names_to=\u0026#34;Param\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=highCrime,y=Value,fill=chas)) + 2geom_boxplot()+ 3facet_wrap(~Param,scales=\u0026#34;free_y\u0026#34;)  It is surprising, but evidently the CHAS variable is strangely relevant. 1 implies the tract bounds the river, otherwise 0.\n Correlations   1boston %\u0026gt;% select(-c(rad,chas,highCrime)) %\u0026gt;% cor 1# crim zn indus nox rm age 2# crim 1.0000000 -0.2004692 0.4065834 0.4209717 -0.2192467 0.3527343 3# zn -0.2004692 1.0000000 -0.5338282 -0.5166037 0.3119906 -0.5695373 4# indus 0.4065834 -0.5338282 1.0000000 0.7636514 -0.3916759 0.6447785 5# nox 0.4209717 -0.5166037 0.7636514 1.0000000 -0.3021882 0.7314701 6# rm -0.2192467 0.3119906 -0.3916759 -0.3021882 1.0000000 -0.2402649 7# age 0.3527343 -0.5695373 0.6447785 0.7314701 -0.2402649 1.0000000 8# dis -0.3796701 0.6644082 -0.7080270 -0.7692301 0.2052462 -0.7478805 9# tax 0.5827643 -0.3145633 0.7207602 0.6680232 -0.2920478 0.5064556 10# ptratio 0.2899456 -0.3916785 0.3832476 0.1889327 -0.3555015 0.2615150 11# black -0.3850639 0.1755203 -0.3569765 -0.3800506 0.1280686 -0.2735340 12# lstat 0.4556215 -0.4129946 0.6037997 0.5908789 -0.6138083 0.6023385 13# medv -0.3883046 0.3604453 -0.4837252 -0.4273208 0.6953599 -0.3769546 14# dis tax ptratio black lstat medv 15# crim -0.3796701 0.5827643 0.2899456 -0.3850639 0.4556215 -0.3883046 16# zn 0.6644082 -0.3145633 -0.3916785 0.1755203 -0.4129946 0.3604453 17# indus -0.7080270 0.7207602 0.3832476 -0.3569765 0.6037997 -0.4837252 18# nox -0.7692301 0.6680232 0.1889327 -0.3800506 0.5908789 -0.4273208 19# rm 0.2052462 -0.2920478 -0.3555015 0.1280686 -0.6138083 0.6953599 20# age -0.7478805 0.5064556 0.2615150 -0.2735340 0.6023385 -0.3769546 21# dis 1.0000000 -0.5344316 -0.2324705 0.2915117 -0.4969958 0.2499287 22# tax -0.5344316 1.0000000 0.4608530 -0.4418080 0.5439934 -0.4685359 23# ptratio -0.2324705 0.4608530 1.0000000 -0.1773833 0.3740443 -0.5077867 24# black 0.2915117 -0.4418080 -0.1773833 1.0000000 -0.3660869 0.3334608 25# lstat -0.4969958 0.5439934 0.3740443 -0.3660869 1.0000000 -0.7376627 26# medv 0.2499287 -0.4685359 -0.5077867 0.3334608 -0.7376627 1.0000000 Now, unsurprisingly, there\u0026rsquo;s nothing which is really strongly correlated here for some reason.\n Train test splits   1set.seed(1984) 2trainIndCri \u0026lt;- createDataPartition(boston$highCrime, # Factor, so class sampling 3p=0.7, # 70-30 train-test 4list=FALSE, # No lists 5times=1) # No bootstrap 6bostonTrain\u0026lt;-boston[trainIndCri,] 7bostonTest\u0026lt;-boston[-trainIndCri,]  Make a bunch of models   1glmBos.fit=glm(highCrime~., data=bostonTrain, family=binomial) 1# Warning: glm.fit: algorithm did not converge 1# Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred 1glmBos.probs = predict(glmBos.fit,bostonTest, type = \u0026#34;response\u0026#34;) 2glmBos.pred = rep(1,length(glmBos.probs)) 3glmBos.pred[glmBos.probs\u0026lt;0.5]=0 4glmBos.pred=factor(glmBos.pred) 5confusionMatrix(glmBos.pred,bostonTest$highCrime) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction 0 1 5# 0 68 6 6# 1 7 69 7# 8# Accuracy : 0.9133 9# 95% CI : (0.8564, 0.953) 10# No Information Rate : 0.5 11# P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 12# 13# Kappa : 0.8267 14# 15# Mcnemar\u0026#39;s Test P-Value : 1 16# 17# Sensitivity : 0.9067 18# Specificity : 0.9200 19# Pos Pred Value : 0.9189 20# Neg Pred Value : 0.9079 21# Prevalence : 0.5000 22# Detection Rate : 0.4533 23# Detection Prevalence : 0.4933 24# Balanced Accuracy : 0.9133 25# 26# \u0026#39;Positive\u0026#39; Class : 0 27# 1bostonLDA=train(highCrime~.,data=bostonTrain,method=\u0026#39;lda\u0026#39;) 2bostonQDA=train(highCrime~tax+crim,data=bostonTrain,method=\u0026#39;qda\u0026#39;) 3bostonKNN=train(highCrime~.,data=bostonTrain,preProcess = c(\u0026#34;center\u0026#34;,\u0026#34;scale\u0026#34;),method=\u0026#39;knn\u0026#39;) 1bLDAp=predict(bostonLDA,bostonTest) 2bQDAp=predict(bostonQDA,bostonTest) 3bKNNp=predict(bostonKNN,bostonTest) 1confusionMatrix(bLDAp,bostonTest$highCrime) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction 0 1 5# 0 72 6 6# 1 3 69 7# 8# Accuracy : 0.94 9# 95% CI : (0.8892, 0.9722) 10# No Information Rate : 0.5 11# P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 12# 13# Kappa : 0.88 14# 15# Mcnemar\u0026#39;s Test P-Value : 0.505 16# 17# Sensitivity : 0.9600 18# Specificity : 0.9200 19# Pos Pred Value : 0.9231 20# Neg Pred Value : 0.9583 21# Prevalence : 0.5000 22# Detection Rate : 0.4800 23# Detection Prevalence : 0.5200 24# Balanced Accuracy : 0.9400 25# 26# \u0026#39;Positive\u0026#39; Class : 0 27# 1confusionMatrix(bQDAp,bostonTest$highCrime) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction 0 1 5# 0 73 5 6# 1 2 70 7# 8# Accuracy : 0.9533 9# 95% CI : (0.9062, 0.981) 10# No Information Rate : 0.5 11# P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 12# 13# Kappa : 0.9067 14# 15# Mcnemar\u0026#39;s Test P-Value : 0.4497 16# 17# Sensitivity : 0.9733 18# Specificity : 0.9333 19# Pos Pred Value : 0.9359 20# Neg Pred Value : 0.9722 21# Prevalence : 0.5000 22# Detection Rate : 0.4867 23# Detection Prevalence : 0.5200 24# Balanced Accuracy : 0.9533 25# 26# \u0026#39;Positive\u0026#39; Class : 0 27# 1confusionMatrix(bKNNp,bostonTest$highCrime) 1# Confusion Matrix and Statistics 2# 3# Reference 4# Prediction 0 1 5# 0 74 6 6# 1 1 69 7# 8# Accuracy : 0.9533 9# 95% CI : (0.9062, 0.981) 10# No Information Rate : 0.5 11# P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 12# 13# Kappa : 0.9067 14# 15# Mcnemar\u0026#39;s Test P-Value : 0.1306 16# 17# Sensitivity : 0.9867 18# Specificity : 0.9200 19# Pos Pred Value : 0.9250 20# Neg Pred Value : 0.9857 21# Prevalence : 0.5000 22# Detection Rate : 0.4933 23# Detection Prevalence : 0.5333 24# Balanced Accuracy : 0.9533 25# 26# \u0026#39;Positive\u0026#39; Class : 0 27# Clearly in this particular case, an LDA model seems to be working out the best for this data when trained on all the parameters, though Logistic Regression is doing quite well too.\n Notes on KNN   1plot(bostonKNN)  1plot(bostonKNN, print.thres = 0.5, type=\u0026#34;S\u0026#34;)   Comparison  Finally, we will quickly plot some indicative measures.\n1knnBosROC\u0026lt;-roc(predictor=as.numeric(bKNNp),response=bostonTest$highCrime) 1# Setting levels: control = 0, case = 1 1# Setting direction: controls \u0026lt; cases 1logiBosROC\u0026lt;-roc(predictor=as.numeric(glmBos.probs),response=bostonTest$highCrime) 1# Setting levels: control = 0, case = 1 2# Setting direction: controls \u0026lt; cases 1ldaBosROC\u0026lt;-roc(predictor=as.numeric(bLDAp),response=bostonTest$highCrime) 1# Setting levels: control = 0, case = 1 2# Setting direction: controls \u0026lt; cases 1qdaBosROC\u0026lt;-roc(predictor=as.numeric(bQDAp),response=bostonTest$highCrime) 1# Setting levels: control = 0, case = 1 2# Setting direction: controls \u0026lt; cases 1ggroc(list(KNN=knnBosROC,Logistic=logiBosROC,LDA=ldaBosROC,QDA=qdaBosROC))  Figure 14: plot of chunk unnamed-chunk-87\n  OK, one of the reasons why these models do so well is because they are all assuming an equal distribution of train and test classes, and they use crim itself as a predictor. This is no doubt a strong reason why these models uniformly perform so well. I\u0026rsquo;d say 5 is the best option.\n  James, G., Witten, D., Hastie, T., \u0026amp; Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Berlin, Germany: Springer Science \u0026amp; Business Media.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n A good introduction to the caret and skimr packages is here\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/islr-ch4/","tags":["solutions","R","ISLR"],"title":"  \"ISLR :: Classification\"\n  "},{"categories":["programming"],"contents":"Chapter II - Statistical Learning All the questions are as per the ISL seventh printing of the First edition 1.\nQuestion 2.8 - Pages 54-55 This exercise relates to the College data set, which can be found in the file College.csv. It contains a number of variables for \\(777\\) different universities and colleges in the US. The variables are\n Private : Public/private indicator Apps : Number of applications received Accept : Number of applicants accepted Enroll : Number of new students enrolled Top10perc : New students from top 10 % of high school class Top25perc : New students from top 25 % of high school class F.Undergrad : Number of full-time undergraduates P.Undergrad : Number of part-time undergraduates Outstate : Out-of-state tuition Room.Board : Room and board costs Books : Estimated book costs Personal : Estimated personal spending PhD : Percent of faculty with Ph.D.\u0026rsquo;s Terminal : Percent of faculty with terminal degree S.F.Ratio : Student/faculty ratio perc.alumni : Percent of alumni who donate Expend : Instructional expenditure per student Grad.Rate : Graduation rate  Before reading the data into R, it can be viewed in Excel or a text editor.\n(a) Use the read.csv() function to read the data into R . Call the loaded data college. Make sure that you have the directory set to the correct location for the data.\n(b) Look at the data using the fix() function. You should notice that the ﬁrst column is just the name of each university. We don\u0026rsquo;t really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands:\n1rownames(college)=college[,1] 2fix(college) You should see that there is now a row.names column with the name of each university recorded. This means that R has given each row a name corresponding to the appropriate university. R will not try to perform calculations on the row names. However, we still need to eliminate the ﬁrst column in the data where the names are stored. Try:\n1college=college[,-1] 2fix(college) (c)\n Use the summary() function to produce a numerical summary of the variables in the data set. Use the pairs() function to produce a scatterplot matrix of the ﬁrst ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix A using A[,1:10] . Use the plot() function to produce side-by-side boxplots of Outstate versus Private . Create a new qualitative variable, called Elite , by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top \\(10%\\) of their high school classes exceeds \\(50%\\).   1Elite = rep(\u0026#34;No\u0026#34;, nrow(college)) 2Elite [college$Top10perc \u0026gt;50]=\u0026#34;Yes\u0026#34; 3Elite = as.factor (Elite) 4college = data.frame (college, Elite) Use the summary() function to see how many elite univer- sities there are. Now use the plot() function to produce side-by-side boxplots of Outstate versus Elite .\n Use the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative vari- ables. You may fnd the command par(mfrow=c(2,2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways. Continue exploring the data, and provide a brief summary of what you discover.  Answer Instead of reading in data, for ISLR in particular we can load the ISLR library which is on CRAN and contains the data-sets required for the book.\n1install.packages(\u0026#34;ISLR\u0026#34;) Thus, we can now read it in as library(\u0026quot;ISLR\u0026quot;)\nThe remaining sections are meant to be executed, and are marked as such, with r in {}.\n(c)\nWe will load the dataset once for the whole document.\n1library(\u0026#34;ISLR\u0026#34;)  Usage of the summary() function   1summary(ISLR::College) 1## Private Apps Accept Enroll Top10perc 2## No :212 Min. : 81 Min. : 72 Min. : 35 Min. : 1.00 3## Yes:565 1st Qu.: 776 1st Qu.: 604 1st Qu.: 242 1st Qu.:15.00 4## Median : 1558 Median : 1110 Median : 434 Median :23.00 5## Mean : 3002 Mean : 2019 Mean : 780 Mean :27.56 6## 3rd Qu.: 3624 3rd Qu.: 2424 3rd Qu.: 902 3rd Qu.:35.00 7## Max. :48094 Max. :26330 Max. :6392 Max. :96.00 8## Top25perc F.Undergrad P.Undergrad Outstate 9## Min. : 9.0 Min. : 139 Min. : 1.0 Min. : 2340 10## 1st Qu.: 41.0 1st Qu.: 992 1st Qu.: 95.0 1st Qu.: 7320 11## Median : 54.0 Median : 1707 Median : 353.0 Median : 9990 12## Mean : 55.8 Mean : 3700 Mean : 855.3 Mean :10441 13## 3rd Qu.: 69.0 3rd Qu.: 4005 3rd Qu.: 967.0 3rd Qu.:12925 14## Max. :100.0 Max. :31643 Max. :21836.0 Max. :21700 15## Room.Board Books Personal PhD 16## Min. :1780 Min. : 96.0 Min. : 250 Min. : 8.00 17## 1st Qu.:3597 1st Qu.: 470.0 1st Qu.: 850 1st Qu.: 62.00 18## Median :4200 Median : 500.0 Median :1200 Median : 75.00 19## Mean :4358 Mean : 549.4 Mean :1341 Mean : 72.66 20## 3rd Qu.:5050 3rd Qu.: 600.0 3rd Qu.:1700 3rd Qu.: 85.00 21## Max. :8124 Max. :2340.0 Max. :6800 Max. :103.00 22## Terminal S.F.Ratio perc.alumni Expend 23## Min. : 24.0 Min. : 2.50 Min. : 0.00 Min. : 3186 24## 1st Qu.: 71.0 1st Qu.:11.50 1st Qu.:13.00 1st Qu.: 6751 25## Median : 82.0 Median :13.60 Median :21.00 Median : 8377 26## Mean : 79.7 Mean :14.09 Mean :22.74 Mean : 9660 27## 3rd Qu.: 92.0 3rd Qu.:16.50 3rd Qu.:31.00 3rd Qu.:10830 28## Max. :100.0 Max. :39.80 Max. :64.00 Max. :56233 29## Grad.Rate 30## Min. : 10.00 31## 1st Qu.: 53.00 32## Median : 65.00 33## Mean : 65.46 34## 3rd Qu.: 78.00 35## Max. :118.00  Usage of pairs()   1tenColl \u0026lt;- ISLR::College[,1:10] # For getting the first ten columns 2pairs(tenColl) # Scatterplot  Figure 1: Pairs\n   Boxplot creation with plot()   1plot(ISLR::College$Private,ISLR::College$Outstate,xlab=\u0026#34;Private\u0026#34;,ylab=\u0026#34;Outstate\u0026#34;)  Figure 2: Boxplots\n   Binning and plotting   1college=ISLR::College 2Elite=rep(\u0026#34;No\u0026#34;,nrow(college)) 3Elite[college$Top10perc\u0026gt;50]=\u0026#34;Yes\u0026#34; 4Elite=as.factor(Elite) 5college\u0026lt;-data.frame(college,Elite) 6summary(college$Elite) 1## No Yes 2## 699 78 1plot(college$Outstate,college$Elite,xlab=\u0026#34;Outstate\u0026#34;,ylab=\u0026#34;Elite\u0026#34;)  Figure 3: Plotting Outstate and Elite\n   Histograms with hist()   1par(mfrow=c(2,2)) 2hist(college$Enroll) 3hist(college$perc.alumni, col=2) 4hist(college$Personal, col=3, breaks=10) 5hist(college$PhD, breaks=10)  Figure 4: Histogram\n  1hist(college$Top10perc, col=\u0026#34;blue\u0026#34;) 2hist(college$Outstate, col=23)  Figure 5: Colored Histogram\n   Explorations (graphical)  \\(0\\) implies the faculty have PhDs. It is clear that people donate more when faculty do not have terminal degrees.\n1plot(college$Terminal-college$PhD, college$perc.alumni)  Figure 6: Terminal degrees and alumni\n  High tuition correlates to high graduation rate.\n1plot(college$Expend, college$Grad.Rate)  Figure 7: Tuiton and graduation\n  Low acceptance implies a low student to faculty ratio.\n1plot(college$Accept / college$Apps, college$S.F.Ratio)  Figure 8: Acceptance and Student/Faculty ratio\n  Question 2.9 - Page 56 This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.\n(a) Which of the predictors are quantitative, and which are qualitative?\n(b) What is the range of each quantitative predictor? You can answer this using the range() function.\n(c) What is the mean and standard deviation of each quantitative predictor?\n(d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?\n(e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.\n(f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.\nAnswer Once again, since the dataset is loaded from the library, we will simply start manipulating it.\n1# Clean data 2autoDat\u0026lt;-na.omit(ISLR::Auto) # renamed for convenience (a) To determine weather the variables a qualitative or quantitative we can either inspect the variables by eye, or query the dataset.\n1summary(autoDat) # Observe the output for variance 1## mpg cylinders displacement horsepower weight 2## Min. : 9.00 Min. :3.000 Min. : 68.0 Min. : 46.0 Min. :1613 3## 1st Qu.:17.00 1st Qu.:4.000 1st Qu.:105.0 1st Qu.: 75.0 1st Qu.:2225 4## Median :22.75 Median :4.000 Median :151.0 Median : 93.5 Median :2804 5## Mean :23.45 Mean :5.472 Mean :194.4 Mean :104.5 Mean :2978 6## 3rd Qu.:29.00 3rd Qu.:8.000 3rd Qu.:275.8 3rd Qu.:126.0 3rd Qu.:3615 7## Max. :46.60 Max. :8.000 Max. :455.0 Max. :230.0 Max. :5140 8## 9## acceleration year origin name 10## Min. : 8.00 Min. :70.00 Min. :1.000 amc matador : 5 11## 1st Qu.:13.78 1st Qu.:73.00 1st Qu.:1.000 ford pinto : 5 12## Median :15.50 Median :76.00 Median :1.000 toyota corolla : 5 13## Mean :15.54 Mean :75.98 Mean :1.577 amc gremlin : 4 14## 3rd Qu.:17.02 3rd Qu.:79.00 3rd Qu.:2.000 amc hornet : 4 15## Max. :24.80 Max. :82.00 Max. :3.000 chevrolet chevette: 4 16## (Other) :365 1str(autoDat) # Directly find find out 1## \u0026#39;data.frame\u0026#39;: 392 obs. of 9 variables: 2## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... 3## $ cylinders : num 8 8 8 8 8 8 8 8 8 8 ... 4## $ displacement: num 307 350 318 304 302 429 454 440 455 390 ... 5## $ horsepower : num 130 165 150 150 140 198 220 215 225 190 ... 6## $ weight : num 3504 3693 3436 3433 3449 ... 7## $ acceleration: num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... 8## $ year : num 70 70 70 70 70 70 70 70 70 70 ... 9## $ origin : num 1 1 1 1 1 1 1 1 1 1 ... 10## $ name : Factor w/ 304 levels \u0026#34;amc ambassador brougham\u0026#34;,..: 49 36 231 14 161 141 54 223 241 2 ... From the above view, we can see that there is only one listed as a qualitative variable or factor, and that is name. However, we can also do this in a cleaner manner or at-least in a different manner with a function.\n1findFactors \u0026lt;- sapply(autoDat,is.factor) 2findFactors 1## mpg cylinders displacement horsepower weight acceleration 2## FALSE FALSE FALSE FALSE FALSE FALSE 3## year origin name 4## FALSE FALSE TRUE Though only name is listed as a qualitative variable, we note that origin seems to be almost qualitative as well.\n1length(unique(autoDat$origin)) 1## [1] 3 1unique(autoDat$origin) 1## [1] 1 3 2 Infact we can check that nothing else has this property by repeated application of sapply, though a pipe would be more satisfying\n1getUniq\u0026lt;-sapply(autoDat, unique) 2getLengths\u0026lt;-sapply(getUniq,length) 3getLengths 1## mpg cylinders displacement horsepower weight acceleration 2## 127 5 81 93 346 95 3## year origin name 4## 13 3 301 This is really nicer with pipes\n1library(dplyr) 1## 2## Attaching package: \u0026#39;dplyr\u0026#39; 1## The following objects are masked from \u0026#39;package:stats\u0026#39;: 2## 3## filter, lag 1## The following objects are masked from \u0026#39;package:base\u0026#39;: 2## 3## intersect, setdiff, setequal, union 1autoDat %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) 1## mpg cylinders displacement horsepower weight acceleration 2## 127 5 81 93 346 95 3## year origin name 4## 13 3 301 At any rate, we know now that origin and name are probably qualitative, and the rest are quantitative.\n(b) Using range()\nA nice feature of the dataset we have is that the suspected qualitative variables are at the end of the dataset. So we can simply select the first \\(7\\) rows and go nuts on them.\n1autoDat[,1:7] %\u0026gt;% sapply(range) # or sapply(autoDat[,1:7],range) 1## mpg cylinders displacement horsepower weight acceleration year 2## [1,] 9.0 3 68 46 1613 8.0 70 3## [2,] 46.6 8 455 230 5140 24.8 82 Once again, more elegant with pipes and subset()\n1autoDat %\u0026gt;% subset(select=-c(name,origin)) %\u0026gt;% sapply(range) 1## mpg cylinders displacement horsepower weight acceleration year 2## [1,] 9.0 3 68 46 1613 8.0 70 3## [2,] 46.6 8 455 230 5140 24.8 82 1# Even simpler with dplyr 2autoDat %\u0026gt;% select(-name,-origin) %\u0026gt;% sapply(range) 1## mpg cylinders displacement horsepower weight acceleration year 2## [1,] 9.0 3 68 46 1613 8.0 70 3## [2,] 46.6 8 455 230 5140 24.8 82 (c) Mean and standard deviation\n1noFactors \u0026lt;- autoDat %\u0026gt;% select(-name,-origin) 2noFactors %\u0026gt;% sapply(mean) 1## mpg cylinders displacement horsepower weight acceleration 2## 23.445918 5.471939 194.411990 104.469388 2977.584184 15.541327 3## year 4## 75.979592 1noFactors %\u0026gt;% sapply(sd) 1## mpg cylinders displacement horsepower weight acceleration 2## 7.805007 1.705783 104.644004 38.491160 849.402560 2.758864 3## year 4## 3.683737 (d) Removing observations 10-85 and testing.\n1noFactors[-(10:85),] %\u0026gt;% sapply(mean) 1## mpg cylinders displacement horsepower weight acceleration 2## 24.404430 5.373418 187.240506 100.721519 2935.971519 15.726899 3## year 4## 77.145570 1noFactors[-(10:85),] %\u0026gt;% sapply(sd) 1## mpg cylinders displacement horsepower weight acceleration 2## 7.867283 1.654179 99.678367 35.708853 811.300208 2.693721 3## year 4## 3.106217 (e) Plots for determining relationships\n1par(mfrow=c(2,2)) 2plot(autoDat$weight, autoDat$horsepower) 3plot(autoDat$weight, autoDat$acceleration) 4plot(autoDat$displacement, autoDat$acceleration) 5plot(autoDat$cylinders, autoDat$acceleration)  Figure 9: Relationship determination\n   Evidently horsepower is directly proportional to weight but acceleration is inversely proportional to weight Acceleration is also inversely proportional to displacement Cylinders are a poor measure, not surprising since there are only \\(5\\) values  (f) Choosing predictors for gas mileage mpg\nLet us recall certain key elements of the quantitative aspects of the dataset.\n1summary(noFactors) # To understand the spread 1## mpg cylinders displacement horsepower weight 2## Min. : 9.00 Min. :3.000 Min. : 68.0 Min. : 46.0 Min. :1613 3## 1st Qu.:17.00 1st Qu.:4.000 1st Qu.:105.0 1st Qu.: 75.0 1st Qu.:2225 4## Median :22.75 Median :4.000 Median :151.0 Median : 93.5 Median :2804 5## Mean :23.45 Mean :5.472 Mean :194.4 Mean :104.5 Mean :2978 6## 3rd Qu.:29.00 3rd Qu.:8.000 3rd Qu.:275.8 3rd Qu.:126.0 3rd Qu.:3615 7## Max. :46.60 Max. :8.000 Max. :455.0 Max. :230.0 Max. :5140 8## acceleration year 9## Min. : 8.00 Min. :70.00 10## 1st Qu.:13.78 1st Qu.:73.00 11## Median :15.50 Median :76.00 12## Mean :15.54 Mean :75.98 13## 3rd Qu.:17.02 3rd Qu.:79.00 14## Max. :24.80 Max. :82.00 1getLengths # To get the number of unique values 1## mpg cylinders displacement horsepower weight acceleration 2## 127 5 81 93 346 95 3## year origin name 4## 13 3 301 From this we can assert easily that the number of cylinders is not of much interest for predictions of the mileage.\n1par(mfrow=c(3,2)) 2plot(noFactors$mpg,noFactors$horsepower) 3plot(noFactors$mpg,noFactors$weight) 4plot(noFactors$mpg,noFactors$displacement) 5plot(noFactors$mpg,noFactors$acceleration) 6plot(noFactors$mpg,noFactors$year)  Figure 10: Predictions\n   So now we know that the mileage increases when horsepower is low, weight is low, displacement is low and acceleration is high  Where low represents an inverse response and high represents a direct response.\n It is also clear that the mileage increases every year  Chapter III - Linear Regression Question 3.9 - Page 122 This question involves the use of multiple linear regression on the Auto data set.\n(a) Produce a scatterplot matrix which includes all of the variables in the data set.\n(b) Compute the matrix of correlations between the variables using the function cor() . You will need to exclude the name variable, cor() which is qualitative.\n(c) Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output. For instance:\n Is there a relationship between the predictors and the response? Which predictors appear to have a statistically significant relationship to the response? What does the coefficient for the year variable suggest?  (d) Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?\n(e) Use the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?\n(f) Try a few different transformations of the variables, such as \\(\\log{X}\\), \\(\\sqrt{X}\\), \\(X^2\\).Comment on your ﬁndings.\nAnswer Once again, we will use the dataset from the library.\n1cleanAuto \u0026lt;- na.omit(autoDat) 2summary(cleanAuto) # Already created above, so no need to do na.omit again 1## mpg cylinders displacement horsepower weight 2## Min. : 9.00 Min. :3.000 Min. : 68.0 Min. : 46.0 Min. :1613 3## 1st Qu.:17.00 1st Qu.:4.000 1st Qu.:105.0 1st Qu.: 75.0 1st Qu.:2225 4## Median :22.75 Median :4.000 Median :151.0 Median : 93.5 Median :2804 5## Mean :23.45 Mean :5.472 Mean :194.4 Mean :104.5 Mean :2978 6## 3rd Qu.:29.00 3rd Qu.:8.000 3rd Qu.:275.8 3rd Qu.:126.0 3rd Qu.:3615 7## Max. :46.60 Max. :8.000 Max. :455.0 Max. :230.0 Max. :5140 8## 9## acceleration year origin name 10## Min. : 8.00 Min. :70.00 Min. :1.000 amc matador : 5 11## 1st Qu.:13.78 1st Qu.:73.00 1st Qu.:1.000 ford pinto : 5 12## Median :15.50 Median :76.00 Median :1.000 toyota corolla : 5 13## Mean :15.54 Mean :75.98 Mean :1.577 amc gremlin : 4 14## 3rd Qu.:17.02 3rd Qu.:79.00 3rd Qu.:2.000 amc hornet : 4 15## Max. :24.80 Max. :82.00 Max. :3.000 chevrolet chevette: 4 16## (Other) :365 (a) Scatterplot\n1pairs(cleanAuto)  Figure 11: Scatterplot\n  (b) Correlation matrix. For this we exclude the qualitative variables either by using select or by using the existing noFactors dataset\n1# A full set 2ISLR::Auto %\u0026gt;% na.omit %\u0026gt;% select(-name,-origin) %\u0026gt;% cor 1## mpg cylinders displacement horsepower weight 2## mpg 1.0000000 -0.7776175 -0.8051269 -0.7784268 -0.8322442 3## cylinders -0.7776175 1.0000000 0.9508233 0.8429834 0.8975273 4## displacement -0.8051269 0.9508233 1.0000000 0.8972570 0.9329944 5## horsepower -0.7784268 0.8429834 0.8972570 1.0000000 0.8645377 6## weight -0.8322442 0.8975273 0.9329944 0.8645377 1.0000000 7## acceleration 0.4233285 -0.5046834 -0.5438005 -0.6891955 -0.4168392 8## year 0.5805410 -0.3456474 -0.3698552 -0.4163615 -0.3091199 9## acceleration year 10## mpg 0.4233285 0.5805410 11## cylinders -0.5046834 -0.3456474 12## displacement -0.5438005 -0.3698552 13## horsepower -0.6891955 -0.4163615 14## weight -0.4168392 -0.3091199 15## acceleration 1.0000000 0.2903161 16## year 0.2903161 1.0000000 (c) Multiple Linear Regression\n1# Fit against every variable 2lm.fit=lm(mpg~.,data=noFactors) 3summary(lm.fit) 1## 2## Call: 3## lm(formula = mpg ~ ., data = noFactors) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -8.6927 -2.3864 -0.0801 2.0291 14.3607 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) -1.454e+01 4.764e+00 -3.051 0.00244 ** 12## cylinders -3.299e-01 3.321e-01 -0.993 0.32122 13## displacement 7.678e-03 7.358e-03 1.044 0.29733 14## horsepower -3.914e-04 1.384e-02 -0.028 0.97745 15## weight -6.795e-03 6.700e-04 -10.141 \u0026lt; 2e-16 *** 16## acceleration 8.527e-02 1.020e-01 0.836 0.40383 17## year 7.534e-01 5.262e-02 14.318 \u0026lt; 2e-16 *** 18## --- 19## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 20## 21## Residual standard error: 3.435 on 385 degrees of freedom 22## Multiple R-squared: 0.8093, Adjusted R-squared: 0.8063 23## F-statistic: 272.2 on 6 and 385 DF, p-value: \u0026lt; 2.2e-16 1# Fit against one variable 2noFactors %\u0026gt;% lm(mpg~horsepower,data=.) %\u0026gt;% summary 1## 2## Call: 3## lm(formula = mpg ~ horsepower, data = .) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -13.5710 -3.2592 -0.3435 2.7630 16.9240 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 39.935861 0.717499 55.66 \u0026lt;2e-16 *** 12## horsepower -0.157845 0.006446 -24.49 \u0026lt;2e-16 *** 13## --- 14## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 15## 16## Residual standard error: 4.906 on 390 degrees of freedom 17## Multiple R-squared: 0.6059, Adjusted R-squared: 0.6049 18## F-statistic: 599.7 on 1 and 390 DF, p-value: \u0026lt; 2.2e-16 1noFactors %\u0026gt;% lm(mpg~year,data=.) %\u0026gt;% summary 1## 2## Call: 3## lm(formula = mpg ~ year, data = .) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -12.0212 -5.4411 -0.4412 4.9739 18.2088 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) -70.01167 6.64516 -10.54 \u0026lt;2e-16 *** 12## year 1.23004 0.08736 14.08 \u0026lt;2e-16 *** 13## --- 14## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 15## 16## Residual standard error: 6.363 on 390 degrees of freedom 17## Multiple R-squared: 0.337, Adjusted R-squared: 0.3353 18## F-statistic: 198.3 on 1 and 390 DF, p-value: \u0026lt; 2.2e-16 1noFactors %\u0026gt;% lm(mpg~acceleration,data=.) %\u0026gt;% summary 1## 2## Call: 3## lm(formula = mpg ~ acceleration, data = .) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -17.989 -5.616 -1.199 4.801 23.239 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 4.8332 2.0485 2.359 0.0188 * 12## acceleration 1.1976 0.1298 9.228 \u0026lt;2e-16 *** 13## --- 14## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 15## 16## Residual standard error: 7.08 on 390 degrees of freedom 17## Multiple R-squared: 0.1792, Adjusted R-squared: 0.1771 18## F-statistic: 85.15 on 1 and 390 DF, p-value: \u0026lt; 2.2e-16 1noFactors %\u0026gt;% lm(mpg~weight,data=.) %\u0026gt;% summary 1## 2## Call: 3## lm(formula = mpg ~ weight, data = .) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -11.9736 -2.7556 -0.3358 2.1379 16.5194 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 46.216524 0.798673 57.87 \u0026lt;2e-16 *** 12## weight -0.007647 0.000258 -29.64 \u0026lt;2e-16 *** 13## --- 14## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 15## 16## Residual standard error: 4.333 on 390 degrees of freedom 17## Multiple R-squared: 0.6926, Adjusted R-squared: 0.6918 18## F-statistic: 878.8 on 1 and 390 DF, p-value: \u0026lt; 2.2e-16 1noFactors %\u0026gt;% lm(mpg~displacement,data=.) %\u0026gt;% summary 1## 2## Call: 3## lm(formula = mpg ~ displacement, data = .) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -12.9170 -3.0243 -0.5021 2.3512 18.6128 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 35.12064 0.49443 71.03 \u0026lt;2e-16 *** 12## displacement -0.06005 0.00224 -26.81 \u0026lt;2e-16 *** 13## --- 14## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 15## 16## Residual standard error: 4.635 on 390 degrees of freedom 17## Multiple R-squared: 0.6482, Adjusted R-squared: 0.6473 18## F-statistic: 718.7 on 1 and 390 DF, p-value: \u0026lt; 2.2e-16   Clearly there is a relationship between the predictors and variables, mostly as described previously, with the following broad trends:\n Inversely proportional to Horsepower, Weight, and Displacement    The predictors which have a relationship to the response are (based on R squared values): \\[ all \u0026gt; weight \u0026gt; displacement \u0026gt; horsepower \u0026gt; year \u0026gt; acceleration \\] However, things lower than horsepower are not statistically significant.\n  The visual analysis of the year variable suggests that the mileage grows every year. However, it is clear from the summary, that there is no statistical significance of year when used to fit a single parameter linear model. We note that when we compare this to the multiple linear regression analysis, we see that the year factor accounts for \\(0.7508\\) of the total, that is, the cars become more efficient every year\n  (d) Lets plot these\n1par(mfrow=c(2,2)) 2noFactors %\u0026gt;% lm(mpg~horsepower,data=.) %\u0026gt;% plot(main=\u0026#34;Mileage v/s Horsepower\u0026#34;)  1noFactors %\u0026gt;% lm(mpg~weight,data=.) %\u0026gt;% plot(main=\u0026#34;Mileage v/s Weight\u0026#34;)  1noFactors %\u0026gt;% lm(mpg~year,data=.) %\u0026gt;% plot(main=\u0026#34;Mileage v/s Year\u0026#34;)  1noFactors %\u0026gt;% lm(mpg~acceleration,data=.) %\u0026gt;% plot(main=\u0026#34;Mileage v/s Acceleration\u0026#34;)  1noFactors %\u0026gt;% lm(mpg~displacement,data=.) %\u0026gt;% plot(main=\u0026#34;Mileage v/s Displacement\u0026#34;)  1noFactors %\u0026gt;% lm(mpg~.,data=.) %\u0026gt;% plot(main=\u0026#34;Mileage Multiple Regression\u0026#34;)  Form this we can see that the fit is not very accurate as there is a clear curve to the residuals. The 14th point has high leverage, though it is of a small magnitude. Thus it is not expected to have affected the plot too much.\nWe know that an observation with a studentized residual greater than \\(3\\) in absolute value are possible outliers. Hence we must plot this.\n1# Predict and get the plot 2fitPlot \u0026lt;- noFactors %\u0026gt;% lm(mpg~.,data=.) 3# See residuals 4plot(xlab=\u0026#34;Prediction\u0026#34;,ylab=\u0026#34;Studentized Residual\u0026#34;,x=predict(fitPlot),y=rstudent(fitPlot))  1# Try a linear fit of studentized residuals 2par(mfrow=c(2,2)) 3plot(lm(predict(fitPlot)~rstudent(fitPlot)))  Clearly the studentized residuals are nonlinear w.r.t the prediction. Also, some points are above the absolute value of \\(3\\) so they might be outliers, in keeping with the leverage plot.\n(e) Interaction effects\nWe recall that x*y corresponds to x+y+x:y\n1# View the correlation matrix 2cleanAuto %\u0026gt;% select(-name,-origin) %\u0026gt;% cor 1## mpg cylinders displacement horsepower weight 2## mpg 1.0000000 -0.7776175 -0.8051269 -0.7784268 -0.8322442 3## cylinders -0.7776175 1.0000000 0.9508233 0.8429834 0.8975273 4## displacement -0.8051269 0.9508233 1.0000000 0.8972570 0.9329944 5## horsepower -0.7784268 0.8429834 0.8972570 1.0000000 0.8645377 6## weight -0.8322442 0.8975273 0.9329944 0.8645377 1.0000000 7## acceleration 0.4233285 -0.5046834 -0.5438005 -0.6891955 -0.4168392 8## year 0.5805410 -0.3456474 -0.3698552 -0.4163615 -0.3091199 9## acceleration year 10## mpg 0.4233285 0.5805410 11## cylinders -0.5046834 -0.3456474 12## displacement -0.5438005 -0.3698552 13## horsepower -0.6891955 -0.4163615 14## weight -0.4168392 -0.3091199 15## acceleration 1.0000000 0.2903161 16## year 0.2903161 1.0000000 1summary(lm(mpg~weight*displacement*year,data=noFactors[(10:85),])) 1## 2## Call: 3## lm(formula = mpg ~ weight * displacement * year, data = noFactors[(10:85), 4## ]) 5## 6## Residuals: 7## Min 1Q Median 3Q Max 8## -5.3020 -0.9055 0.0966 0.8912 3.7049 9## 10## Coefficients: 11## Estimate Std. Error t value Pr(\u0026gt;|t|) 12## (Intercept) 3.961e+02 2.578e+02 1.537 0.129 13## weight -1.030e-01 1.008e-01 -1.021 0.311 14## displacement -1.587e+00 1.308e+00 -1.213 0.229 15## year -4.889e+00 3.623e+00 -1.349 0.182 16## weight:displacement 3.926e-04 3.734e-04 1.051 0.297 17## weight:year 1.317e-03 1.418e-03 0.929 0.356 18## displacement:year 2.150e-02 1.846e-02 1.165 0.248 19## weight:displacement:year -5.287e-06 5.253e-06 -1.007 0.318 20## 21## Residual standard error: 1.8 on 68 degrees of freedom 22## Multiple R-squared: 0.922, Adjusted R-squared: 0.914 23## F-statistic: 114.9 on 7 and 68 DF, p-value: \u0026lt; 2.2e-16 1summary(lm(mpg~weight*displacement*year,data=noFactors)) 1## 2## Call: 3## lm(formula = mpg ~ weight * displacement * year, data = noFactors) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -9.6093 -1.6472 -0.0531 1.2289 14.5604 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) -8.437e+01 3.128e+01 -2.697 0.0073 ** 12## weight 8.489e-03 1.322e-02 0.642 0.5212 13## displacement 3.434e-01 1.969e-01 1.744 0.0820 . 14## year 1.828e+00 4.127e-01 4.430 1.23e-05 *** 15## weight:displacement -6.589e-05 5.055e-05 -1.303 0.1932 16## weight:year -2.433e-04 1.744e-04 -1.395 0.1638 17## displacement:year -5.566e-03 2.674e-03 -2.082 0.0380 * 18## weight:displacement:year 1.144e-06 6.823e-07 1.677 0.0944 . 19## --- 20## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 21## 22## Residual standard error: 2.951 on 384 degrees of freedom 23## Multiple R-squared: 0.8596, Adjusted R-squared: 0.8571 24## F-statistic: 336 on 7 and 384 DF, p-value: \u0026lt; 2.2e-16  Adding the interaction effects of the \\(3\\) most positive R value terms improves the existing prediction to be better than that obtained by considering all effects. We note that the best model is obtained by removing the range identified in chapter 2.  (f) Nonlinear transformations\n1summary(lm(mpg~weight*displacement*year+I(year^2),data=noFactors[(10:85),])) 1## 2## Call: 3## lm(formula = mpg ~ weight * displacement * year + I(year^2), 4## data = noFactors[(10:85), ]) 5## 6## Residuals: 7## Min 1Q Median 3Q Max 8## -5.1815 -0.8235 0.0144 1.0076 3.9420 9## 10## Coefficients: 11## Estimate Std. Error t value Pr(\u0026gt;|t|) 12## (Intercept) -4.205e+03 1.810e+03 -2.324 0.0232 * 13## weight -8.800e-02 9.709e-02 -0.906 0.3680 14## displacement -1.030e+00 1.276e+00 -0.807 0.4225 15## year 1.238e+02 5.026e+01 2.464 0.0163 * 16## I(year^2) -9.000e-01 3.506e-01 -2.567 0.0125 * 17## weight:displacement 2.471e-04 3.634e-04 0.680 0.4988 18## weight:year 1.113e-03 1.365e-03 0.815 0.4177 19## displacement:year 1.368e-02 1.800e-02 0.760 0.4501 20## weight:displacement:year -3.254e-06 5.111e-06 -0.637 0.5264 21## --- 22## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 23## 24## Residual standard error: 1.73 on 67 degrees of freedom 25## Multiple R-squared: 0.929, Adjusted R-squared: 0.9205 26## F-statistic: 109.6 on 8 and 67 DF, p-value: \u0026lt; 2.2e-16 1summary(lm(mpg~.-I(log(acceleration^2)),data=noFactors[(10:85),])) 1## 2## Call: 3## lm(formula = mpg ~ . - I(log(acceleration^2)), data = noFactors[(10:85), 4## ]) 5## 6## Residuals: 7## Min 1Q Median 3Q Max 8## -6.232 -1.470 -0.211 1.075 7.088 9## 10## Coefficients: 11## Estimate Std. Error t value Pr(\u0026gt;|t|) 12## (Intercept) 41.3787633 24.1208720 1.715 0.0907 . 13## cylinders 0.0863161 0.6112822 0.141 0.8881 14## displacement -0.0148491 0.0103249 -1.438 0.1549 15## horsepower -0.0158500 0.0151259 -1.048 0.2984 16## weight -0.0039125 0.0008546 -4.578 2.02e-05 *** 17## acceleration -0.1473786 0.1438220 -1.025 0.3091 18## year -0.0378187 0.3380266 -0.112 0.9112 19## --- 20## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 21## 22## Residual standard error: 2.262 on 69 degrees of freedom 23## Multiple R-squared: 0.8751, Adjusted R-squared: 0.8642 24## F-statistic: 80.55 on 6 and 69 DF, p-value: \u0026lt; 2.2e-16   The best model I found was still the one without the non-linear transformation but with removed outliers and additional interaction effects of displacement,=year= and weight\n  A popular approach is to use a log transform for both the inputs and the outputs\n   1summary(lm(log(mpg)~.,data=noFactors[(10:85),])) 1## 2## Call: 3## lm(formula = log(mpg) ~ ., data = noFactors[(10:85), ]) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -0.285805 -0.052358 -0.001456 0.066521 0.209739 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 3.886e+00 1.028e+00 3.781 0.000328 *** 12## cylinders -1.771e-02 2.604e-02 -0.680 0.498669 13## displacement -1.540e-04 4.399e-04 -0.350 0.727314 14## horsepower -2.343e-03 6.444e-04 -3.636 0.000529 *** 15## weight -1.960e-04 3.641e-05 -5.383 9.51e-07 *** 16## acceleration -1.525e-02 6.128e-03 -2.489 0.015224 * 17## year 4.138e-03 1.440e-02 0.287 0.774703 18## --- 19## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 20## 21## Residual standard error: 0.09636 on 69 degrees of freedom 22## Multiple R-squared: 0.919, Adjusted R-squared: 0.912 23## F-statistic: 130.5 on 6 and 69 DF, p-value: \u0026lt; 2.2e-16 1summary(lm(log(mpg)~log(weight*displacement*year),data=noFactors[(10:85),])) 1## 2## Call: 3## lm(formula = log(mpg) ~ log(weight * displacement * year), data = noFactors[(10:85), 4## ]) 5## 6## Residuals: 7## Min 1Q Median 3Q Max 8## -0.41121 -0.04107 0.01266 0.07791 0.21056 9## 10## Coefficients: 11## Estimate Std. Error t value Pr(\u0026gt;|t|) 12## (Intercept) 8.91995 0.26467 33.70 \u0026lt;2e-16 *** 13## log(weight * displacement * year) -0.34250 0.01508 -22.71 \u0026lt;2e-16 *** 14## --- 15## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 16## 17## Residual standard error: 0.1158 on 74 degrees of freedom 18## Multiple R-squared: 0.8745, Adjusted R-squared: 0.8728 19## F-statistic: 515.6 on 1 and 74 DF, p-value: \u0026lt; 2.2e-16 Question 3.10 - Page 123 This question should be answered using the Carseats data set.\n(a) Fit a multiple regression model to predict Sales using Price, Urban, and US.\n(b) Provide an interpretation of each coefficient in the model. Be careful\u0026mdash;some of the variables in the model are qualitative!\n(c) Write out the model in equation form, being careful to handle the qualitative variables properly.\n(d) For which of the predictors can you reject the null hypothesis \\(H_0:\\beta_j=0\\)?\n(e) On the basis of your response to the previous question, ﬁt a smaller model that only uses the predictors for which there is evidence of association with the outcome.\n(f) How well do the models in (a) and (e) fit the data?\n(g) Using the model from (e), obtain \\(95%\\) confidence intervals for the coefficient(s).\n(h) Is there evidence of outliers or high leverage observations in the model from (e)?\nAnswer Load the dataset (and clean it)\n1cleanCarSeats \u0026lt;- na.omit(ISLR::Carseats) Obtain summary statistics\n1cleanCarSeats %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) 1## Sales CompPrice Income Advertising Population Price 2## 336 73 98 28 275 101 3## ShelveLoc Age Education Urban US 4## 3 56 9 2 2 1str(cleanCarSeats) 1## \u0026#39;data.frame\u0026#39;: 400 obs. of 11 variables: 2## $ Sales : num 9.5 11.22 10.06 7.4 4.15 ... 3## $ CompPrice : num 138 111 113 117 141 124 115 136 132 132 ... 4## $ Income : num 73 48 35 100 64 113 105 81 110 113 ... 5## $ Advertising: num 11 16 10 4 3 13 0 15 0 0 ... 6## $ Population : num 276 260 269 466 340 501 45 425 108 131 ... 7## $ Price : num 120 83 80 97 128 72 108 120 124 124 ... 8## $ ShelveLoc : Factor w/ 3 levels \u0026#34;Bad\u0026#34;,\u0026#34;Good\u0026#34;,\u0026#34;Medium\u0026#34;: 1 2 3 3 1 1 3 2 3 3 ... 9## $ Age : num 42 65 59 55 38 78 71 67 76 76 ... 10## $ Education : num 17 10 12 14 13 16 15 10 10 17 ... 11## $ Urban : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 2 2 2 1 2 2 1 1 ... 12## $ US : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 2 2 1 2 1 2 1 2 ... 1summary(cleanCarSeats) 1## Sales CompPrice Income Advertising 2## Min. : 0.000 Min. : 77 Min. : 21.00 Min. : 0.000 3## 1st Qu.: 5.390 1st Qu.:115 1st Qu.: 42.75 1st Qu.: 0.000 4## Median : 7.490 Median :125 Median : 69.00 Median : 5.000 5## Mean : 7.496 Mean :125 Mean : 68.66 Mean : 6.635 6## 3rd Qu.: 9.320 3rd Qu.:135 3rd Qu.: 91.00 3rd Qu.:12.000 7## Max. :16.270 Max. :175 Max. :120.00 Max. :29.000 8## Population Price ShelveLoc Age Education 9## Min. : 10.0 Min. : 24.0 Bad : 96 Min. :25.00 Min. :10.0 10## 1st Qu.:139.0 1st Qu.:100.0 Good : 85 1st Qu.:39.75 1st Qu.:12.0 11## Median :272.0 Median :117.0 Medium:219 Median :54.50 Median :14.0 12## Mean :264.8 Mean :115.8 Mean :53.32 Mean :13.9 13## 3rd Qu.:398.5 3rd Qu.:131.0 3rd Qu.:66.00 3rd Qu.:16.0 14## Max. :509.0 Max. :191.0 Max. :80.00 Max. :18.0 15## Urban US 16## No :118 No :142 17## Yes:282 Yes:258 18## 19## 20## 21## We can see that:\n Urban, US and ShelveLoc are factors with 2,2 and 3 levels respectively Education has only 9 unique values so we might as well consider it to be a factor too if we need to  (a) Multiple Regression Model\nFit it to things\n1summary(lm(Sales~.,data=cleanCarSeats)) 1## 2## Call: 3## lm(formula = Sales ~ ., data = cleanCarSeats) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -2.8692 -0.6908 0.0211 0.6636 3.4115 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 5.6606231 0.6034487 9.380 \u0026lt; 2e-16 *** 12## CompPrice 0.0928153 0.0041477 22.378 \u0026lt; 2e-16 *** 13## Income 0.0158028 0.0018451 8.565 2.58e-16 *** 14## Advertising 0.1230951 0.0111237 11.066 \u0026lt; 2e-16 *** 15## Population 0.0002079 0.0003705 0.561 0.575 16## Price -0.0953579 0.0026711 -35.700 \u0026lt; 2e-16 *** 17## ShelveLocGood 4.8501827 0.1531100 31.678 \u0026lt; 2e-16 *** 18## ShelveLocMedium 1.9567148 0.1261056 15.516 \u0026lt; 2e-16 *** 19## Age -0.0460452 0.0031817 -14.472 \u0026lt; 2e-16 *** 20## Education -0.0211018 0.0197205 -1.070 0.285 21## UrbanYes 0.1228864 0.1129761 1.088 0.277 22## USYes -0.1840928 0.1498423 -1.229 0.220 23## --- 24## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 25## 26## Residual standard error: 1.019 on 388 degrees of freedom 27## Multiple R-squared: 0.8734, Adjusted R-squared: 0.8698 28## F-statistic: 243.4 on 11 and 388 DF, p-value: \u0026lt; 2.2e-16 1summary(lm(Sales~US*Price*Urban,data=cleanCarSeats)) 1## 2## Call: 3## lm(formula = Sales ~ US * Price * Urban, data = cleanCarSeats) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -6.7952 -1.6659 -0.0984 1.6119 7.2433 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 13.456350 1.727210 7.791 6.03e-14 *** 12## USYes 2.049051 2.322591 0.882 0.378 13## Price -0.061657 0.014875 -4.145 4.17e-05 *** 14## UrbanYes -0.651545 2.071401 -0.315 0.753 15## USYes:Price -0.001567 0.019972 -0.078 0.937 16## USYes:UrbanYes -1.122034 2.759662 -0.407 0.685 17## Price:UrbanYes 0.010793 0.017796 0.606 0.545 18## USYes:Price:UrbanYes 0.001288 0.023619 0.055 0.957 19## --- 20## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 21## 22## Residual standard error: 2.473 on 392 degrees of freedom 23## Multiple R-squared: 0.2467, Adjusted R-squared: 0.2333 24## F-statistic: 18.34 on 7 and 392 DF, p-value: \u0026lt; 2.2e-16 1summary(lm(Sales~US+Price+Urban,data=cleanCarSeats)) 1## 2## Call: 3## lm(formula = Sales ~ US + Price + Urban, data = cleanCarSeats) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -6.9206 -1.6220 -0.0564 1.5786 7.0581 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 13.043469 0.651012 20.036 \u0026lt; 2e-16 *** 12## USYes 1.200573 0.259042 4.635 4.86e-06 *** 13## Price -0.054459 0.005242 -10.389 \u0026lt; 2e-16 *** 14## UrbanYes -0.021916 0.271650 -0.081 0.936 15## --- 16## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 17## 18## Residual standard error: 2.472 on 396 degrees of freedom 19## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2335 20## F-statistic: 41.52 on 3 and 396 DF, p-value: \u0026lt; 2.2e-16 (b) Interpret stuff\nTo interpret the data, we need to determine which of the models fits the data best, we will use anova() to test this:\n1lmCarSAll\u0026lt;-lm(Sales~.,data=cleanCarSeats) 2lmCarStimesPUU\u0026lt;-lm(Sales~US*Price*Urban,data=cleanCarSeats) 3lmCarSplusPUU\u0026lt;-lm(Sales~US+Price+Urban,data=cleanCarSeats) 4anova(lmCarSAll,lmCarStimesPUU,lmCarSplusPUU) 1## Analysis of Variance Table 2## 3## Model 1: Sales ~ CompPrice + Income + Advertising + Population + Price + 4## ShelveLoc + Age + Education + Urban + US 5## Model 2: Sales ~ US * Price * Urban 6## Model 3: Sales ~ US + Price + Urban 7## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) 8## 1 388 402.83 9## 2 392 2397.10 -4 -1994.27 480.2082 \u0026lt; 2.2e-16 *** 10## 3 396 2420.83 -4 -23.73 5.7149 0.0001772 *** 11## --- 12## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 1anova(lmCarStimesPUU,lmCarSplusPUU) 1## Analysis of Variance Table 2## 3## Model 1: Sales ~ US * Price * Urban 4## Model 2: Sales ~ US + Price + Urban 5## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) 6## 1 392 2397.1 7## 2 396 2420.8 -4 -23.734 0.9703 0.4236 Remember that it is not possible to use anova() unless the same variables are present in all the models being tested, so it is meaningless to use anova for lmCarSAll along with the others, because we can\u0026rsquo;t change the interaction model to get only the main effects.\n We note that due to the low value of the F-statistic and the non-zero value of the p-value we cannot disregard the null hypothesis, or in other words, the models are basically the same in terms of their performance.  This means that I would like to continue with the simpler model, since the increase in R squared is too small to account for dealing with the additional factors.\n We see immediately, that there is a positive correlation only with being in the US Increases in price and being in an urban area actually decrease the sales, which is not surprising since being in the an urban area is probably correlated to a higher price, which we can check immediately   1summary(lm(Price~Urban,data=cleanCarSeats)) 1## 2## Call: 3## lm(formula = Price ~ Urban, data = cleanCarSeats) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -92.514 -15.514 1.205 14.595 74.486 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 114.076 2.180 52.330 \u0026lt;2e-16 *** 12## UrbanYes 2.438 2.596 0.939 0.348 13## --- 14## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 15## 16## Residual standard error: 23.68 on 398 degrees of freedom 17## Multiple R-squared: 0.002211, Adjusted R-squared: -0.0002965 18## F-statistic: 0.8817 on 1 and 398 DF, p-value: 0.3483 We see that our assumption is validated. Being in an urban area has a low t-statistic for a positive increase on the slope\n Returning to our previous model, we note that there is a high value of the p-value of the t-statistic for Urban being true, this means there isn\u0026rsquo;t a real relationship between being in an urban area and the sales. This makes intuitive sense as well  note t-test is essentially a linear model with one variable, that is, if we want to find out if there is a relation between having a store in an urban area, we could sum all the urban yes and divide by the number of observations and compare that to the sum of all the urban no divided by the number of observations which is essentially the t-test again.\n Price is significant, and has an inverse relation with the sales, so we should keep that in mind  (c) In Equation Form:\n\\[ Sales=1.200573*USYes - 0.054459*Price - 0.021916*UrbanYes + 13.043469 \\]\n(e) Other models\n We know from our case-study on testing the full multiple linear regression for Sales that there are definitely more important variables being ignored. However, we also know that Urban is not significant, so we can use a smaller model.   1lmCarSplusPU\u0026lt;-lm(Sales~US+Price, data=cleanCarSeats) (f) Comparison of models\n1summary(lmCarSplusPU) 1## 2## Call: 3## lm(formula = Sales ~ US + Price, data = cleanCarSeats) 4## 5## Residuals: 6## Min 1Q Median 3Q Max 7## -6.9269 -1.6286 -0.0574 1.5766 7.0515 8## 9## Coefficients: 10## Estimate Std. Error t value Pr(\u0026gt;|t|) 11## (Intercept) 13.03079 0.63098 20.652 \u0026lt; 2e-16 *** 12## USYes 1.19964 0.25846 4.641 4.71e-06 *** 13## Price -0.05448 0.00523 -10.416 \u0026lt; 2e-16 *** 14## --- 15## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 16## 17## Residual standard error: 2.469 on 397 degrees of freedom 18## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2354 19## F-statistic: 62.43 on 2 and 397 DF, p-value: \u0026lt; 2.2e-16 1anova(lmCarSplusPUU,lmCarSplusPU) 1## Analysis of Variance Table 2## 3## Model 1: Sales ~ US + Price + Urban 4## Model 2: Sales ~ US + Price 5## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) 6## 1 396 2420.8 7## 2 397 2420.9 -1 -0.03979 0.0065 0.9357 As expected, the low value of the F statistic and the high p-value for the anova() test asserts that the null hypothesis cannot be neglected, thus there are no differences between the model with the insignificant parameter, which is also seen in the R squared value, which is the same for both models\n(g) Confidence Intervals\n1confint(lmCarSplusPU) 1## 2.5 % 97.5 % 2## (Intercept) 11.79032020 14.27126531 3## USYes 0.69151957 1.70776632 4## Price -0.06475984 -0.04419543 1confint(lmCarSplusPUU) 1## 2.5 % 97.5 % 2## (Intercept) 11.76359670 14.32334118 3## USYes 0.69130419 1.70984121 4## Price -0.06476419 -0.04415351 5## UrbanYes -0.55597316 0.51214085  ☐ Look into trying to plot this with ggplot  (h) Outliers\n We will first check the leverage plots   1par(mfrow=c(2,2)) 2plot(lmCarSplusPU)  Figure 12: Leverage Plots\n  We can see there is a point with high leverage, but it has a low residual. In any case we should check further.\n Now we will check the studentized residuals to see if they are greater than 3   1# See residuals 2plot(xlab=\u0026#34;Prediction\u0026#34;,ylab=\u0026#34;Studentized Residual\u0026#34;,x=predict(lmCarSplusPU),y=rstudent(lmCarSplusPU))  Figure 13: Studentized residuals\n  Thus I would say there are no outliers in our dataset, as none of our datapoints have an absolute studentized residual above 3.\n  James, G., Witten, D., Hastie, T., \u0026amp; Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Berlin, Germany: Springer Science \u0026amp; Business Media.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://rgoswami.me/posts/islr-ch2-ch3/","tags":["solutions","R","ISLR"],"title":"  \"ISLR :: Multiple Linear Regression\"\n  "}]