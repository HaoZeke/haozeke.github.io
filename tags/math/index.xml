<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>math on Rohit Goswami</title><link>https://rgoswami.me/tags/math/</link><description>Recent content in math on Rohit Goswami</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>&lt;a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0&lt;/a>.</copyright><lastBuildDate>Thu, 26 Mar 2020 00:28:00 +0000</lastBuildDate><atom:link href="https://rgoswami.me/tags/math/index.xml" rel="self" type="application/rss+xml"/><item><title>Trees and Bags</title><link>https://rgoswami.me/posts/trees-and-bags/</link><pubDate>Thu, 26 Mar 2020 00:28:00 +0000</pubDate><guid>https://rgoswami.me/posts/trees-and-bags/</guid><description>Explain why using bagging for prediction trees generally improves predictions over regular prediction trees.
Introduction Bagging (or Bootstrap Aggregation) is one of the most commonly used ensemble method for improving the prediction of trees. We will broadly follow a historical development trend to understand the process. That is, we will begin by considering the Bootstrap method. This in turn requires knowledge of the Jacknife method, which is understandable from a simple bias variance perspective.</description></item></channel></rss>